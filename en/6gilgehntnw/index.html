<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Video Transcript ﻿hi I&amp;rsquo;m ilithyia power and I am pretty
new to the field of deep learning I&amp;rsquo;ve
been in it for about five months now
through the course of the Scholars
Program I&amp;rsquo;m getting a warning that I
have bad Network quality so if I&amp;rsquo;m not
coming through clearly somebody let me
know in the background so anyway about
at the end of last year I sorry
distracted by the network quality my"><title>Looking For Grammar In All The Right Places ｜ Alethea Power ｜ OpenAI Scholars Demo Day 2020 ｜ OpenAI | SWIEST</title>
<link rel=canonical href=https://swiest.com/en/6gilgehntnw/><link rel=stylesheet href=/scss/style.min.9a6fe90535a0e5c60443841f100f7b698092d48dba43fdb6386bb69b6559bc3d.css><script>document.oncontextmenu=function(){return!1},document.onselectstart=function(){return!1},document.oncopy=function(){return!1},document.oncut=function(){return!1}</script><script src=https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js></script><script type=text/javascript>$(document).ready(function(){$("#back-to-top").hide(),$(function(){$(window).scroll(function(){$(window).scrollTop()>600?$("#back-to-top").fadeIn(500):$("#back-to-top").fadeOut(500)}),$("#back-to-top").click(function(){return $("body,html").animate({scrollTop:0},500),!1})})})</script><meta property="og:title" content="Looking For Grammar In All The Right Places ｜ Alethea Power ｜ OpenAI Scholars Demo Day 2020 ｜ OpenAI"><meta property="og:description" content="Video Transcript ﻿hi I&amp;rsquo;m ilithyia power and I am pretty
new to the field of deep learning I&amp;rsquo;ve
been in it for about five months now
through the course of the Scholars
Program I&amp;rsquo;m getting a warning that I
have bad Network quality so if I&amp;rsquo;m not
coming through clearly somebody let me
know in the background so anyway about
at the end of last year I sorry
distracted by the network quality my"><meta property="og:url" content="https://swiest.com/en/6gilgehntnw/"><meta property="og:site_name" content="SWIEST - Transcripts · Screenplays · Lyrics"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="English"><meta property="article:tag" content="Video Transcripts"><meta property="article:tag" content="OpenAI"><meta property="article:published_time" content="2023-11-06T04:07:24+00:00"><meta property="article:modified_time" content="2023-11-06T04:07:24+00:00"><meta name=twitter:title content="Looking For Grammar In All The Right Places ｜ Alethea Power ｜ OpenAI Scholars Demo Day 2020 ｜ OpenAI"><meta name=twitter:description content="Video Transcript ﻿hi I&amp;rsquo;m ilithyia power and I am pretty
new to the field of deep learning I&amp;rsquo;ve
been in it for about five months now
through the course of the Scholars
Program I&amp;rsquo;m getting a warning that I
have bad Network quality so if I&amp;rsquo;m not
coming through clearly somebody let me
know in the background so anyway about
at the end of last year I sorry
distracted by the network quality my"><link rel="shortcut icon" href=/favicon.ico><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"dark")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu307e6a33fa6fd661ccda3b77024ef5c2_252345_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>SWIEST - Transcripts · Screenplays · Lyrics</a></h1><h2 class=site-description>🧙🪄🌎</h2></div></header><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><li><a href=/tags/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg><span>Tags</span></a></li><li><a href=/chart/podcastchart.html target=_blank><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-apple-podcast" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M18.364 18.364a9 9 0 10-12.728.0"/><path d="M11.766 22h.468a2 2 0 001.985-1.752l.5-4A2 2 0 0012.734 14h-1.468a2 2 0 00-1.985 2.248l.5 4A2 2 0 0011.766 22z"/><path d="M12 9m-2 0a2 2 0 104 0 2 2 0 10-4 0"/></svg><span>Podcasts</span></a></li><li><a href=/radio.html target=_blank><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-radio" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14 3 4.629 6.749A1 1 0 004 7.677V19a1 1 0 001 1h14a1 1 0 001-1V8a1 1 0 00-1-1H4.5"/><path d="M4 12h16"/><path d="M7 12v-2"/><path d="M17 16v.01"/><path d="M13 16v.01"/></svg><span>Radio</span></a></li><div class=menu-bottom-section><li id=i18n-switch><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg><select name=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://swiest.com/ selected>English</option><option value=https://swiest.com/af/>Afrikaans</option><option value=https://swiest.com/am/>አማርኛ</option><option value=https://swiest.com/ar/>العربية</option><option value=https://swiest.com/az/>Azərbaycan</option><option value=https://swiest.com/be/>беларускі</option><option value=https://swiest.com/bg/>български</option><option value=https://swiest.com/bn/>বাংলা</option><option value=https://swiest.com/bo/>བོད་སྐད་</option><option value=https://swiest.com/bs/>Bosanski</option><option value=https://swiest.com/ca/>Català</option><option value=https://swiest.com/zh-hans/>简体中文</option><option value=https://swiest.com/zh-hant/>繁體中文</option><option value=https://swiest.com/cs/>Čeština</option><option value=https://swiest.com/el/>ελληνικά</option><option value=https://swiest.com/cy/>Cymraeg</option><option value=https://swiest.com/da/>Dansk</option><option value=https://swiest.com/de/>Deutsch</option><option value=https://swiest.com/eo/>Esperanto</option><option value=https://swiest.com/es-es/>Español (España)</option><option value=https://swiest.com/es-419/>Español (Latinoamérica)</option><option value=https://swiest.com/et/>Eesti</option><option value=https://swiest.com/eu/>Euskara</option><option value=https://swiest.com/haw/>ʻŌlelo Hawaiʻi</option><option value=https://swiest.com/fa/>فارسی</option><option value=https://swiest.com/fi/>Suomi</option><option value=https://swiest.com/fo/>Føroyskt</option><option value=https://swiest.com/fr/>Français</option><option value=https://swiest.com/fy/>Frysk</option><option value=https://swiest.com/ga/>Gaeilge</option><option value=https://swiest.com/gl/>Galego</option><option value=https://swiest.com/gu/>ગુજરાતી</option><option value=https://swiest.com/he/>עִברִית</option><option value=https://swiest.com/km/>កម្ពុជា។</option><option value=https://swiest.com/hi/>हिन्दी</option><option value=https://swiest.com/hr/>Hrvatski</option><option value=https://swiest.com/ht/>Kreyòl Ayisyen</option><option value=https://swiest.com/hu/>Magyar</option><option value=https://swiest.com/hy/>Հայերեն</option><option value=https://swiest.com/ig/>Ásụ̀sụ́ Ìgbò</option><option value=https://swiest.com/id/>Bahasa Indonesia</option><option value=https://swiest.com/is/>Íslenska</option><option value=https://swiest.com/it/>Italiano</option><option value=https://swiest.com/ja/>日本語</option><option value=https://swiest.com/jv/>Basa Jawa</option><option value=https://swiest.com/ka/>ქართული</option><option value=https://swiest.com/kk/>Қазақша</option><option value=https://swiest.com/kn/>ಕನ್ನಡ</option><option value=https://swiest.com/ko/>한국어</option><option value=https://swiest.com/or/>ଓଡ଼ିଆ</option><option value=https://swiest.com/ckb/>کوردی</option><option value=https://swiest.com/ky/>Кыргызча</option><option value=https://swiest.com/la/>Latina</option><option value=https://swiest.com/lb/>Lëtzebuergesch</option><option value=https://swiest.com/lo/>ພາສາລາວ</option><option value=https://swiest.com/lt/>Lietuvių</option><option value=https://swiest.com/lv/>Latviešu</option><option value=https://swiest.com/mk/>Македонски</option><option value=https://swiest.com/ml/>മലയാളം</option><option value=https://swiest.com/mn/>Монгол хэл</option><option value=https://swiest.com/mr/>मराठी</option><option value=https://swiest.com/sw/>Kiswahili</option><option value=https://swiest.com/ms/>Bahasa Melayu</option><option value=https://swiest.com/my/>မြန်မာ</option><option value=https://swiest.com/ne/>नेपाली</option><option value=https://swiest.com/nl/>Nederlands</option><option value=https://swiest.com/no/>Norsk</option><option value=https://swiest.com/pa/>ਪੰਜਾਬੀ</option><option value=https://swiest.com/pl/>Polski</option><option value=https://swiest.com/pt-br/>Português Brasil</option><option value=https://swiest.com/pt-pt/>Português Europeu</option><option value=https://swiest.com/ro/>Română</option><option value=https://swiest.com/ru/>Русский</option><option value=https://swiest.com/rw/>Kinyarwanda</option><option value=https://swiest.com/si/>සිංහල</option><option value=https://swiest.com/sk/>Slovenčina</option><option value=https://swiest.com/sl/>Slovenščina</option><option value=https://swiest.com/sq/>Shqip</option><option value=https://swiest.com/sr/>Српски (Srpski)</option><option value=https://swiest.com/su/>Basa Sunda</option><option value=https://swiest.com/sv/>Svenska</option><option value=https://swiest.com/ta/>தமிழ்</option><option value=https://swiest.com/te/>తెలుగు</option><option value=https://swiest.com/tg/>Тоҷикӣ</option><option value=https://swiest.com/th/>ไทย</option><option value=https://swiest.com/tk/>Türkmenler</option><option value=https://swiest.com/tl/>Filipino</option><option value=https://swiest.com/tr/>Türkçe</option><option value=https://swiest.com/uk/>Українська</option><option value=https://swiest.com/ur/>اردو</option><option value=https://swiest.com/uz/>O'zbekcha</option><option value=https://swiest.com/vi/>Tiếng Việt</option><option value=https://swiest.com/yi/>אידיש</option><option value=https://swiest.com/zh-hk/>粵語</option><option value=https://swiest.com/zu/>IsiZulu</option></select></li><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#video>Video</a></li><li><a href=#transcript>Transcript</a></li></ol></nav></div></section><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><ins class=adsbygoogle style=display:block data-ad-client=ca-pub-9206135835124064 data-ad-slot=8754979142 data-ad-format=auto data-full-width-responsive=true></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></aside><a id=back-to-top href=#><img src=/img/top_hu7c2829da96df0e9f8f0191d120020b22_22287_40x0_resize_box_3.png></a><main class="main full-width"><form action=/search/ class="search-form widget"><p><label>Search</label>
<input name=keyword required placeholder="Type something...">
<button title=Search><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg></button></p></form><article class=main-article><header class=article-header><div class=article-details><header class=article-tags><a href=/tags/english/>English
</a><a href=/tags/video-transcripts/>Video Transcripts
</a><a href=/tags/openai/>OpenAI</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/en/6gilgehntnw/>Looking For Grammar In All The Right Places ｜ Alethea Power ｜ OpenAI Scholars Demo Day 2020 ｜ OpenAI</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>2023-11-06</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>20 minute read</time></div></footer></div></header><div class=article-content><p style=text-align:center><a href=https://amzn.to/3Nrdcwk target=_blank>🎁Amazon Prime</a>
<a href=https://amzn.to/3RIBkxg target=_blank>📖Kindle Unlimited</a>
<a href=https://amzn.to/3Rqmudl target=_blank>🎧Audible Plus</a>
<a href=https://amzn.to/3TuLbbj target=_blank>🎵Amazon Music Unlimited</a>
<a href="https://www.iherb.com/?rcode=EID1574" target=_blank>🌿iHerb</a>
<a href="https://accounts.binance.com/register?ref=72302422" target=_blank>💰Binance</a></p></div><div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><ins class=adsbygoogle style=display:block data-ad-client=ca-pub-9206135835124064 data-ad-slot=8754979142 data-ad-format=auto data-full-width-responsive=true></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><section class=article-content><h2 id=video>Video</h2><div class=video-wrapper><iframe loading=lazy src=https://www.youtube.com/embed/6gilgehNTNw allowfullscreen title="YouTube Video"></iframe></div><h2 id=transcript>Transcript</h2><p>﻿hi I&rsquo;m ilithyia power and I am pretty</p><p>new to the field of deep learning I&rsquo;ve</p><p>been in it for about five months now</p><p>through the course of the Scholars</p><p>Program I&rsquo;m getting a warning that I</p><p>have bad Network quality so if I&rsquo;m not</p><p>coming through clearly somebody let me</p><p>know in the background so anyway about</p><p>at the end of last year I sorry</p><p>distracted by the network quality my</p><p>background is in software engineering</p><p>and site reliability engineering and</p><p>I&rsquo;ve always been interested in AI but at</p><p>the end of last year I decided to try</p><p>and make the switch to a new career and</p><p>so to that end I applied to the to the</p><p>Scholars Program and I was incredibly</p><p>grateful to be able to get in and it&rsquo;s</p><p>been an amazing start to a new career</p><p>I want to thank open AI i particularly</p><p>want to thank my mentor and the other</p><p>mentors who have been helpful and the</p><p>other scholars it&rsquo;s been a fantastic</p><p>cohort to go through all of this with so</p><p>during the course of the program I got</p><p>very interested in interpretability</p><p>interpretability is basically</p><p>mind-reading for AI it&rsquo;s about tearing</p><p>open neural networks and looking at how</p><p>they represent and process information</p><p>and it&rsquo;s difficult to do because AI and</p><p>deep learning in particular is very</p><p>different from traditional software</p><p>engineering so there&rsquo;s a picture that</p><p>almost everyone in the field has seen</p><p>software engineering a human being write</p><p>some software the software takes inputs</p><p>and gives outputs they could be</p><p>questions and answers like a search</p><p>engine or you know whatever but in deep</p><p>learning a human being creates math and</p><p>gives it some data to train on and</p><p>that&rsquo;s what writes the software that</p><p>takes inputs and outputs and it turns</p><p>out that software written by math and by</p><p>a computer is much harder to understand</p><p>and software written by a human being</p><p>but it really matters because AI is</p><p>everywhere it impacts us in tremendous</p><p>ways throughout our lives so I&rsquo;m a</p><p>transgender person and that means that</p><p>for a lot of my life my body is a</p><p>different shape than cisgender people&rsquo;s</p><p>bodies and that means that scanners at</p><p>airports usually flagged me for needing</p><p>a pat-down it&rsquo;s humiliating it&rsquo;s</p><p>embarrassing</p><p>it&rsquo;s not the end of the world but it&rsquo;s</p><p>not cool and AI impacts other people in</p><p>worse ways there are systems that that</p><p>you know self-driving cars are more</p><p>likely to hit people of color and you</p><p>know there&rsquo;s all sorts of biases and in</p><p>Justices that can come in so if we</p><p>understand how these systems work then</p><p>we can reduce their bias in addition to</p><p>that if we understand how they work then</p><p>we can improve their efficiency we can</p><p>find smaller networks that do the same</p><p>sort of job and take a lot less</p><p>electricity a lot less time a lot less</p><p>resources and a lot less money and</p><p>finally if we understand how on their</p><p>own networks thought how neural networks</p><p>represent information then we have a</p><p>better chance of actually being able to</p><p>understand human thought which to me is</p><p>the most interesting question of all so</p><p>I decided to dig into interpretability</p><p>by analyzing GPT - this was a</p><p>state-of-the-art language generation</p><p>language modeling network that opening I</p><p>released about a year and a half ago and</p><p>the way this network works is you give</p><p>it some input some text and it generates</p><p>output so this is an actual example I</p><p>fed this the phrase my talk is about</p><p>into GPT - and it said the future of</p><p>education you can give it the beginning</p><p>of a sentence and get yen you can give</p><p>it a paragraph and get an essay it&rsquo;s</p><p>very good at generating text and a lot</p><p>of what it generates is</p><p>indistinguishable from human beings this</p><p>is pretty powerful and pretty dangerous</p><p>I know you can do something like train</p><p>GPT - on you know some sort of subreddit</p><p>and cut and get it to generate political</p><p>text and then you could use it to look</p><p>like there&rsquo;s a bunch of people on the</p><p>internet who all have the same idea and</p><p>it&rsquo;s really just software and that&rsquo;s</p><p>pretty dangerous so we need to</p><p>understand it we need to dig into it and</p><p>know how it works and how to combat</p><p>things that are generated by it and how</p><p>to make sure that it&rsquo;s used in safe ways</p><p>so I had a certain amount of time to do</p><p>this project and I decided I would bite</p><p>off a tractable part of this problem the</p><p>first thing I would do is just try and</p><p>understand how GPT to</p><p>understands English grammar so to</p><p>explain how I figured that out I need to</p><p>give a little bit of background on how</p><p>GPT 2 works some of the people on this</p><p>call will know all about this and are</p><p>literally world experts I think the lead</p><p>author on the GPT 2 paper is on this</p><p>call also my mom is on this call hi mom</p><p>so I want to make sure and give some</p><p>background that&rsquo;s applicable to a wide</p><p>variety of audiences and try not to</p><p>leave anybody behind based on a lack of</p><p>already having you know a full knowledge</p><p>of how this works I also think that&rsquo;s a</p><p>core part of interpretability trying to</p><p>make sure to democratize this</p><p>information and spread it around so that</p><p>people outside the field can actually</p><p>have an understanding of what&rsquo;s going on</p><p>so I&rsquo;m gonna spend a second talking</p><p>about transform our architecture and</p><p>then I&rsquo;ll get into what I built on top</p><p>of it GPT 2 is a transformer but I&rsquo;ll</p><p>get into that in a minute</p><p>so when I feed this beginning of the</p><p>sentence in my talk is about the first</p><p>thing it does is split that split that</p><p>strain into tokens tokens can be words</p><p>they could be punctuation marks they</p><p>could be collections of bytes in this</p><p>string they could be yeah just basically</p><p>sub parts of the string I restricted</p><p>myself to sentences that had tokens</p><p>I had a one-to-one mapping between the</p><p>tokens and the sentences and punctuation</p><p>marks because that made it a little bit</p><p>easier for me to analyze GPT 2 has a</p><p>little bit of a subtle way of doing this</p><p>but I kind of circumvented it these</p><p>tokens oops oops I&rsquo;m clicking the wrong</p><p>button here these tokens get converted</p><p>into vectors and the word my always</p><p>converts into this vector here and this</p><p>is actually talked with a space in front</p><p>of it that always converts into this</p><p>vector so I end up with use for vectors</p><p>that excuse me and they could get fed</p><p>into GPT too and they flow through the</p><p>network along these positions so if I</p><p>put four tokens in I get four tokens out</p><p>in this particular diagram there&rsquo;s</p><p>there&rsquo;s four</p><p>flowing through it so what are they</p><p>flowing through the first part here is</p><p>an embedding layer that&rsquo;s what turns</p><p>them into vectors then it has a bunch of</p><p>decoder blocks GPT too is comes in a</p><p>variety of sizes I looked at GPT too</p><p>small which is what would fit on my home</p><p>graphics card and even it is huge it has</p><p>over a hundred million parameters</p><p>variables and so I knew that I needed to</p><p>try and break it up too to tackle this</p><p>problem and most of these parameters are</p><p>here in these decoder blocks and finally</p><p>it has a language modeling layer so each</p><p>decoder block takes n vectors in each</p><p>position and outputs vectors in each</p><p>position and then this language modeling</p><p>layer takes the final set of vectors</p><p>that come out of the top decoder block</p><p>and produces probabilities for what the</p><p>next word might be and I&rsquo;ll get into</p><p>that in a second inside of these decoder</p><p>blocks are what are called attention</p><p>heads now attention heads mix-and-match</p><p>information between the different</p><p>positions to feed out into the new</p><p>position so they kind of like collect</p><p>the information that spread across the</p><p>input and collect it into focus areas so</p><p>you can kind of think of this as as</p><p>being like if you&rsquo;ve ever been to a</p><p>sushi boat restaurant that has the</p><p>little stream with a little boat that</p><p>floats along next to your table with</p><p>pieces of sushi on it so you can imagine</p><p>each of these positions flowing through</p><p>the network being like a sushi boat path</p><p>and the tokens the vectors going through</p><p>there are like sushi boats and a</p><p>detention head might look at all of</p><p>these positions and take all the</p><p>cucumber out of all the sushi and put it</p><p>into only the one in position one and</p><p>well actually wouldn&rsquo;t do that only the</p><p>one in in the last position attention</p><p>heads in GP t2 are not allowed to take</p><p>information from future tokens and feed</p><p>it into past positions the information</p><p>can only flow this way and it can&rsquo;t flow</p><p>this way so anyway you can imagine these</p><p>attention heads kind of mixing and</p><p>matching little bits of the sushi</p><p>together and feeding them forward trying</p><p>to get a more organized picture of</p><p>what&rsquo;s going on for the task it&rsquo;s</p><p>to perform each of these layers each of</p><p>these decoder or blocks here has 12</p><p>attention heads and they can all operate</p><p>independently and then at the top of</p><p>each layer there&rsquo;s a linear layer that</p><p>puts all their outputs together and</p><p>organizes them into output for that</p><p>whole layer okay that&rsquo;s a whirlwind tour</p><p>of transformer architecture so what is</p><p>GPT to actually doing so it&rsquo;s supposed</p><p>to in each position the goal is for it</p><p>to output the next word and like I said</p><p>this top language modeling layer outputs</p><p>probabilities and so ideally you want</p><p>the word talk to have a higher</p><p>probability than others and here you</p><p>want the word is that have a higher</p><p>probability because the next word here</p><p>was talk so you want that to generate</p><p>talk the next word here is is so you</p><p>want that to generate is okay and so it</p><p>goes through this it does it all the way</p><p>to the end and here it&rsquo;s going to</p><p>generate some word that that you haven&rsquo;t</p><p>had in your input which you can then</p><p>feed back in and generate future words</p><p>so this is how GPT two comes up with a</p><p>completion of the sentence or a</p><p>paragraph or you know whatever this is</p><p>called Auto regression so okay so what I</p><p>did here in order to understand how</p><p>grammar is understood inside the network</p><p>I stripped off this language modeling</p><p>linear layer and replaced it with a</p><p>grammar whoops with a grammar modeling</p><p>layer so what this means is instead of</p><p>having an output probabilities of</p><p>English words or byte pairen codings of</p><p>English words which is how gptt tokenize</p><p>&ndash;is I had it output probabilities of</p><p>parts of speech and I looked at three</p><p>different kinds of grammar simple part</p><p>of speech detailed part of speech and</p><p>syntactic dependencies so simple part of</p><p>speech is like pronoun verb etc etc</p><p>detailed part of speech is like object</p><p>of the preposition and syntactic</p><p>dependencies is I&rsquo;m sorry object of the</p><p>preposition is syntactic dependencies</p><p>and detailed part of speech is just more</p><p>fine-grain you know what what is each</p><p>word doing</p><p>so anyway I put this grammar modeling</p><p>layer on the top of this and I trained</p><p>it I built three data sets one for each</p><p>of these different types of grammatical</p><p>structures huge data sets 300,000</p><p>sentences and I used Spacey which is a</p><p>natural language processing tool out in</p><p>the wild to tag all these sentences with</p><p>their grammatical structures please note</p><p>here the goal of this project was not to</p><p>produce grammatical tagger because space</p><p>he already does that and does that</p><p>better than the thing I built my goal</p><p>here was to use a grammatical powder on</p><p>top of GPT - as a way of measuring</p><p>information inside of GPT - so you can</p><p>see here this shows it outputs parts of</p><p>speech I also looked at once I had this</p><p>grammatical tagger in place I looked at</p><p>what are called entropy what I&rsquo;m not</p><p>going to explain the technical details</p><p>of this I&rsquo;m short on time here the gist</p><p>is I looked at the entropy z&rsquo; of the</p><p>attention matrices coming out of the</p><p>attention heads for sentences in each of</p><p>these of each of these different</p><p>structures and the entropy of an</p><p>attention matrix basically what it does</p><p>is it tells you how complicated the</p><p>mixing and matching that that layer that</p><p>that head is doing so if all the head is</p><p>doing is taking all of the cucumber out</p><p>of all the sushis and putting it in</p><p>position one that&rsquo;s a relatively low</p><p>entropy operation it&rsquo;s not that</p><p>complicated but if the head is mixing</p><p>and matching a whole bunch of things in</p><p>complicated ways then the entropy will</p><p>be higher so these are pictures of the</p><p>attention matrix entropies and this is</p><p>organized</p><p>these are attention heads and this is</p><p>layer one of the network layer two of</p><p>the network the diagram I had before</p><p>only showed three layers but GPT too</p><p>small has 12 layers hi</p><p>I&rsquo;ve shown you the wrong one and given</p><p>away a little bit of the future I was</p><p>supposed to show you one with 12 layers</p><p>here instead of 11 ignore the man behind</p><p>the curtain I&rsquo;ll get to that in a moment</p><p>what&rsquo;s interesting here though to note</p><p>is that the entropies are much higher at</p><p>lower layers of the network and so what</p><p>that tells us is that the network is</p><p>doing a lot more restructuring and</p><p>looking at the relationships between</p><p>words in these first four layers for</p><p>this grammatical task than in the upper</p><p>layers interesting</p><p>so maybe grammatical comprehension lives</p><p>at lower layers of the network so to</p><p>test that I took my grammatical</p><p>classifier and I ran it on top of each</p><p>layer of GPT - and looked at how hard it</p><p>was to train and how good of a score it</p><p>could get basically how low the loss was</p><p>and I&rsquo;ve got a video here of what that</p><p>looked like so</p><p>so you can see you can see here layer</p><p>zero means I ran it right on top of the</p><p>embedding before any of the layers of</p><p>GPT two ran trained up for up to two</p><p>hundred eight pox I actually trained it</p><p>longer but I cut the graph off at two</p><p>hundred it kept going another like two</p><p>hundred and fifty or so and it did not</p><p>learn a ton and this particular one was</p><p>for syntactic dependencies you can see</p><p>at layer one did a bit better at layer</p><p>two it did yet better still</p><p>and at layer four it did pretty great</p><p>layer five it did excellent</p><p>[Music]</p><p>and so this shows how well this grammar</p><p>classifier trained on top of each of</p><p>these layers of the network so this is</p><p>really interesting it did a much better</p><p>job at layers five and six and you can</p><p>see it actually got its best score on</p><p>layer five it did the very best it did a</p><p>much better job at layers 5 and 6 then</p><p>it de layers before and that at the</p><p>layers after so it means that this</p><p>information came into view through these</p><p>attention had heads manipulating it in</p><p>these first four layers the grammatical</p><p>information did and then it started to</p><p>go back out of you so this led me to the</p><p>question of is it because the later half</p><p>of the network is trying to generate</p><p>future words that that&rsquo;s what it was</p><p>trained to do and so that&rsquo;s why it maybe</p><p>it&rsquo;s more focused on the future than it</p><p>is on the past so</p><p>actually yeah so I trained it for</p><p>syntactic tagging of what the expected</p><p>output token should be instead of just</p><p>the input tokens and you can see that it</p><p>peaked out up here at layer eight so if</p><p>we just look this is incoming</p><p>and that&rsquo;s outgoing incoming and</p><p>outgoing so this grammar classifier</p><p>basically is it&rsquo;s like a tool to measure</p><p>where the information lives in the</p><p>network and how much information is</p><p>easily accessible for this grammatical</p><p>task at different layers and you can see</p><p>that the information for understanding</p><p>and grammar of the incoming sentence or</p><p>incoming tokens is much better at lower</p><p>layers and for outgoing it&rsquo;s much better</p><p>at higher layers cool so what we&rsquo;re</p><p>actually seeing here and sorry I&rsquo;ve got</p><p>my slides out of order and I&rsquo;ve given</p><p>away another thing I&rsquo;m gonna say what</p><p>we&rsquo;re seeing here is that is that these</p><p>heads are rotating this information into</p><p>view of these positions in a kind of</p><p>abstract informational space and here&rsquo;s</p><p>an example of what I mean by that I laid</p><p>a bunch of markers on a table and</p><p>looking at them from this angle you</p><p>can&rsquo;t tell how many markers are there</p><p>because you&rsquo;re looking at the wrong</p><p>angle so if I rotate them slightly you</p><p>can tell there&rsquo;s more than one but not</p><p>really how many or what colors they are</p><p>if I rotate them a bit further you can</p><p>tell there&rsquo;s a few but it&rsquo;s not clear</p><p>how many greens there are and if I</p><p>rotate them yet further you can see</p><p>exactly how many markers there are and</p><p>exactly what colors they are so this is</p><p>what I mean by rotating information this</p><p>is kind of an abstract version of the</p><p>same thing the grammatical information</p><p>is being rotated and not just rotated</p><p>but stretched and compressed and warped</p><p>and other types of things so that comes</p><p>into view of these positions that are</p><p>flying through the network I also did</p><p>the same thing for simple part of speech</p><p>and a tailed part of speech and you can</p><p>see those both coalesce in layer 3 which</p><p>makes sense those are simpler to figure</p><p>out so once I had this I took my grammar</p><p>classifier and I chopped off the top</p><p>half of GPT 2 and just ran it on top</p><p>of layer 5 and in here I decided to look</p><p>at how important each head each</p><p>attention head in the remaining Network</p><p>was for this classification and I tried</p><p>a couple of strategies the first</p><p>strategy I followed a paper called our</p><p>16 heads better than one where I I&rsquo;m not</p><p>even going to bother and try and make</p><p>this interpretable to two non-technical</p><p>people I fed in a mask tensor a ones</p><p>tensor and I multiplied that by the</p><p>output of each attention head and then</p><p>took the did back propagation to find</p><p>the Jacobian of the grammatical</p><p>classification loss with respect to the</p><p>coefficient of each head and that would</p><p>give me some at least locally linear</p><p>interpretation of how important that</p><p>head was for grammatical classification</p><p>but it turned out that strategy didn&rsquo;t</p><p>actually work that well it had worked</p><p>pretty well in the paper for Bert but it</p><p>didn&rsquo;t work that well for GPT too so</p><p>instead I tried a slower more</p><p>computationally intensive strategy where</p><p>I just chopped out each head</p><p>individually and looked at its impact to</p><p>the grammatical classification so if it</p><p>had a big impact then that attention had</p><p>mattered and that was a place where</p><p>grammar was being learned and using that</p><p>I was able to pull out a lot of the</p><p>heads in here so for this particular</p><p>grammatical structure the very best loss</p><p>I could get was cutting out almost every</p><p>head in the network so the black here is</p><p>where I removed a head and the white are</p><p>the heads remaining this grammatical</p><p>structure needed a bit more a few more</p><p>heads this one needed almost no heads in</p><p>fact it didn&rsquo;t need heads at all in some</p><p>of these layers which is kind of amazing</p><p>and so anyway in the future I would like</p><p>to look at I would like to take these</p><p>maps of heads that matter for different</p><p>grammatical structures and dig into them</p><p>and figure out what&rsquo;s going on in these</p><p>individual heads now that I&rsquo;ve reduced</p><p>JP t2 to a much smaller collection of</p><p>sub networks that are practical to</p><p>analyze and I&rsquo;d like to compare and</p><p>contrast how these maps relate between</p><p>structures like here you can see these</p><p>three heads are not needed for this</p><p>structure or that structure or this</p><p>structure so there&rsquo;s a relation</p><p>chips in here and I think we can find</p><p>sub networks of GPT to that really two</p><p>different grammatical structures and</p><p>hopefully that will one day down the</p><p>road get us to the point where we can</p><p>better tear open these language models</p><p>and have a much deeper understanding of</p><p>what&rsquo;s going on in them okay hopefully</p><p>I&rsquo;m under my time anyway time for Q&amp;A I</p><p>know we&rsquo;re all running a little bit long</p><p>so I don&rsquo;t know if there&rsquo;s time for Q&amp;A</p><p>but we&rsquo;ll see anybody got questions</p><p>I&rsquo;m looking over here because because I</p><p>have a separate monitor with a QA oh</p><p>here we go from papers like the image G</p><p>PT we know that transformers have great</p><p>representations in the middle of the</p><p>network in how far is the grammar loss</p><p>predictive of useful representations for</p><p>other tasks and not just grammar</p><p>detection that&rsquo;s a great question I</p><p>haven&rsquo;t read the image dpgp T paper like</p><p>I said I have been in the field of deep</p><p>learning for about five months during a</p><p>pandemic and a revolution and I also had</p><p>a bunch of medical problems so I don&rsquo;t</p><p>actually know the results of this paper</p><p>but it sounds cool</p><p>I would love to read it I think it&rsquo;s a</p><p>good question how is the grammar loss</p><p>predictive of useful representations for</p><p>other tasks and not just grammar</p><p>detection I think it&rsquo;s probably</p><p>generalizes pretty well it&rsquo;s gonna</p><p>you&rsquo;re gonna need to have some way of</p><p>classifying what it is that you&rsquo;re</p><p>looking for so in this particular waste</p><p>in this particular case I had a good</p><p>easy way to generate a large data set</p><p>that I could tag with grammatical</p><p>structures so I was able to measure a</p><p>particular like had a good concrete</p><p>understanding and good concrete</p><p>mechanism for measuring information</p><p>presence I think for situations where</p><p>you can easily or plausibly produce a</p><p>data set that actually in in train a</p><p>classifier that actually measures the</p><p>kind of information you&rsquo;re looking for</p><p>then this is pretty generalizable for</p><p>other things more abstract type</p><p>questions it&rsquo;s going to be a lot harder</p><p>yeah it&rsquo;s all about math and if you</p><p>can&rsquo;t find a good way to numerically</p><p>measure it it&rsquo;s gonna be hard to do some</p><p>things you can just brute-force</p><p>visualize but but I don&rsquo;t have the</p><p>compute power to do that yet hopefully I</p><p>will in a not too distant future okay do</p><p>you think the number of heads that are</p><p>needed are correlated with the</p><p>complexity of the sentence structure or</p><p>did you notice any specific repeated</p><p>patterns you know I was actually really</p><p>surprised that some sentence structures</p><p>needed so few heads and it makes me want</p><p>to dig into how much information is</p><p>these linear sub layers of the</p><p>transformer blocks because clearly</p><p>they&rsquo;re doing something like like you</p><p>saw before some of these layers didn&rsquo;t</p><p>need any heads at all which is kind of</p><p>shocking I do think there&rsquo;s there&rsquo;s</p><p>clearly a correlation between the</p><p>complexity of the network that&rsquo;s needed</p><p>and the complexity of the sentence</p><p>structure that&rsquo;s coming in I don&rsquo;t know</p><p>that it&rsquo;s a perfect correlation and I</p><p>haven&rsquo;t gone and done a calculation like</p><p>for instance I would like to do some</p><p>analysis like a way of measuring the</p><p>complexity of a sentence and compare</p><p>that directly to the number of heads and</p><p>give a mathematical answer to this</p><p>question I haven&rsquo;t done that yet but</p><p>just visually it does look like there&rsquo;s</p><p>some correlation there and it does look</p><p>like sentences that have similar</p><p>structures to one another have</p><p>similarities in the heads that are</p><p>important which is a validation that</p><p>this strategy makes some sense yeah okay</p><p>any other questions</p><p>our rights I think that might be it for</p><p>questions</p></section><footer class=article-footer><section class=article-tags><a href=/tags/english/>English</a>
<a href=/tags/video-transcripts/>Video Transcripts</a>
<a href=/tags/openai/>OpenAI</a></section></footer></article><div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><ins class=adsbygoogle style=display:block;text-align:center data-ad-layout=in-article data-ad-format=fluid data-ad-client=ca-pub-9206135835124064 data-ad-slot=1055602464></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/en/at2xkqjazns/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/AT2XkqJAZns data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>Towards Epileptic Seizure Prediction with Deep Network ｜ Kata Slama ｜ OpenAI Scholars Demo Day 2020 ｜ OpenAI</h2></div></a></article><article><a href=/en/jzohw-eybtq/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/JZOHW-eYBtQ data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>Introductions by Sam Altman & Greg Brockman ｜ OpenAI Scholars Demo Day 2020 ｜ OpenAI</h2></div></a></article><article><a href=/en/-fozam9xqs4/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/-FoZAM9xqS4 data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>OpenAI Five vs. OG, Game 2 ｜ OpenAI Five Finals (4⧸6) ｜ OpenAI</h2></div></a></article><article><a href=/en/u9mjuukhuzk/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/U9mJuUkhUzk data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>OpenAI DevDay, Opening Keynote ｜ OpenAI</h2></div></a></article><article><a href=/en/lpe5gwuqa-k/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/lpe5Gwuqa-k data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>Scaling Laws for Language Transfer Learning ｜ Christina Kim ｜ OpenAI Scholars Demo Day 2021 ｜ OpenAI</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2021 -
2023 SWIEST - Transcripts · Screenplays · Lyrics</section><section class=powerby>As an Amazon Associate I earn from qualifying purchases 🛒<br>Built with <a href=https://swiest.com/ target=_blank rel=noopener>(ﾉ◕ヮ◕)ﾉ🪄💞💖🥰 across the gl🌍🌏🌎be</a><br></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Serif&family=Noto+Serif+Armenian&family=Noto+Serif+Bengali&family=Noto+Serif+Devanagari&family=Noto+Serif+Georgian&family=Noto+Serif+Gujarati&family=Noto+Serif+HK&family=Noto+Serif+Hebrew&family=Noto+Serif+JP&family=Noto+Serif+KR&family=Noto+Serif+Kannada&family=Noto+Serif+Khmer&family=Noto+Serif+Lao&family=Noto+Serif+Makasar&family=Noto+Serif+Malayalam&family=Noto+Serif+Myanmar&family=Noto+Serif+Oriya&family=Noto+Serif+SC&family=Noto+Serif+Sinhala&family=Noto+Serif+TC&family=Noto+Serif+Tamil&family=Noto+Serif+Telugu&family=Noto+Serif+Thai&family=Noto+Serif+Tibetan&display=swap" rel=stylesheet></body></html>
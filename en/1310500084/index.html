<!DOCTYPE html>
<html lang="en" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='The following is a conversation with William McCaskill.
He&amp;rsquo;s a philosopher, ethicist, and one of the originators of the effective altruism movement.
His research focuses on the fundamentals of effective altruism,
or the use of evidence and reason to help others by as much as possible with our time and money,
with a particular concentration on how to act given moral uncertainty.
He&amp;rsquo;s the author of Doing Good, Better, Effective Altruism,'>
<title>Lex Fridman Podcast - #84 - William MacAskill: Effective Altruism | SWIEST</title>

<link rel='canonical' href='https://swiest.com/en/1310500084/'>

<link rel="stylesheet" href="/scss/style.min.9a6fe90535a0e5c60443841f100f7b698092d48dba43fdb6386bb69b6559bc3d.css"><script>
    document.oncontextmenu = function(){ return false; };
    document.onselectstart = function(){ return false; };
    document.oncopy = function(){ return false; };
    document.oncut = function(){ return false; };
</script>

<script src="https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>


<script type="text/javascript">
    $(document).ready(function(){
     
     $("#back-to-top").hide();
     
     $(function () {
      $(window).scroll(function(){
       if ($(window).scrollTop()>600){
        $("#back-to-top").fadeIn(500);
       }else{
        $("#back-to-top").fadeOut(500);
       }
     });
     
     $("#back-to-top").click(function(){
      $('body,html').animate({scrollTop:0},500);
       return false;
      });
     });
    });
    </script><meta property='og:title' content='Lex Fridman Podcast - #84 - William MacAskill: Effective Altruism'>
<meta property='og:description' content='The following is a conversation with William McCaskill.
He&amp;rsquo;s a philosopher, ethicist, and one of the originators of the effective altruism movement.
His research focuses on the fundamentals of effective altruism,
or the use of evidence and reason to help others by as much as possible with our time and money,
with a particular concentration on how to act given moral uncertainty.
He&amp;rsquo;s the author of Doing Good, Better, Effective Altruism,'>
<meta property='og:url' content='https://swiest.com/en/1310500084/'>
<meta property='og:site_name' content='SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='English' /><meta property='article:tag' content='Podcast' /><meta property='article:tag' content='Lex Fridman Podcast' /><meta property='article:published_time' content='2022-05-23T17:00:00&#43;00:00'/><meta property='article:modified_time' content='2022-05-23T17:00:00&#43;00:00'/>
<meta name="twitter:title" content="Lex Fridman Podcast - #84 - William MacAskill: Effective Altruism">
<meta name="twitter:description" content="The following is a conversation with William McCaskill.
He&amp;rsquo;s a philosopher, ethicist, and one of the originators of the effective altruism movement.
His research focuses on the fundamentals of effective altruism,
or the use of evidence and reason to help others by as much as possible with our time and money,
with a particular concentration on how to act given moral uncertainty.
He&amp;rsquo;s the author of Doing Good, Better, Effective Altruism,">
    <link rel="shortcut icon" href="/favicon.ico" />
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin="anonymous"></script>
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "dark");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu307e6a33fa6fd661ccda3b77024ef5c2_252345_300x0_resize_box_3.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">‚ú®</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics</a></h1>
            <h2 class="site-description">üåçüåèüåé</h2>
        </div>
    </header><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        
        
        <li >
            <a href='/tags/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11 3L20 12a1.5 1.5 0 0 1 0 2L14 20a1.5 1.5 0 0 1 -2 0L3 11v-4a4 4 0 0 1 4 -4h4" />
  <circle cx="9" cy="9" r="2" />
</svg>



                
                <span>Tags</span>
            </a>
        </li>
        
        
        <li >
            <a href='/chart/podcastchart.html' target="_blank">
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-apple-podcast" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M18.364 18.364a9 9 0 1 0 -12.728 0" />
  <path d="M11.766 22h.468a2 2 0 0 0 1.985 -1.752l.5 -4a2 2 0 0 0 -1.985 -2.248h-1.468a2 2 0 0 0 -1.985 2.248l.5 4a2 2 0 0 0 1.985 1.752z" />
  <path d="M12 9m-2 0a2 2 0 1 0 4 0a2 2 0 1 0 -4 0" />
</svg>
                
                <span>Podcasts</span>
            </a>
        </li>
        


        <div class="menu-bottom-section">
                <li id="i18n-switch">  
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M4 5h7" />
  <path d="M9 3v2c0 4.418 -2.239 8 -5 8" />
  <path d="M5 9c-.003 2.144 2.952 3.908 6.7 4" />
  <path d="M12 20l4 -9l4 9" />
  <path d="M19.1 18h-6.2" />
</svg>



                    <select name="language" onchange="window.location.href = this.selectedOptions[0].value">
                        
                            <option value="https://swiest.com/" selected>English</option>
                        
                            <option value="https://swiest.com/af/" >Afrikaans</option>
                        
                            <option value="https://swiest.com/am/" >·ä†·àõ·à≠·äõ</option>
                        
                            <option value="https://swiest.com/ar/" >ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</option>
                        
                            <option value="https://swiest.com/az/" >Az…ôrbaycan</option>
                        
                            <option value="https://swiest.com/be/" >–±–µ–ª–∞—Ä—É—Å–∫—ñ</option>
                        
                            <option value="https://swiest.com/bg/" >–±—ä–ª–≥–∞—Ä—Å–∫–∏</option>
                        
                            <option value="https://swiest.com/bn/" >‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ</option>
                        
                            <option value="https://swiest.com/bo/" >‡Ωñ‡Ωº‡Ωë‡ºã‡Ω¶‡æê‡Ωë‡ºã</option>
                        
                            <option value="https://swiest.com/bs/" >Bosanski</option>
                        
                            <option value="https://swiest.com/ca/" >Catal√†</option>
                        
                            <option value="https://swiest.com/zh-hans/" >ÁÆÄ‰Ωì‰∏≠Êñá</option>
                        
                            <option value="https://swiest.com/zh-hant/" >ÁπÅÈ´î‰∏≠Êñá</option>
                        
                            <option value="https://swiest.com/cs/" >ƒåe≈°tina</option>
                        
                            <option value="https://swiest.com/el/" >ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨</option>
                        
                            <option value="https://swiest.com/cy/" >Cymraeg</option>
                        
                            <option value="https://swiest.com/da/" >Dansk</option>
                        
                            <option value="https://swiest.com/de/" >Deutsch</option>
                        
                            <option value="https://swiest.com/eo/" >Esperanto</option>
                        
                            <option value="https://swiest.com/es-es/" >Espa√±ol (Espa√±a)</option>
                        
                            <option value="https://swiest.com/es-419/" >Espa√±ol (Latinoam√©rica)</option>
                        
                            <option value="https://swiest.com/et/" >Eesti</option>
                        
                            <option value="https://swiest.com/eu/" >Euskara</option>
                        
                            <option value="https://swiest.com/haw/" > ª≈ålelo Hawai ªi</option>
                        
                            <option value="https://swiest.com/fa/" >ŸÅÿßÿ±ÿ≥€å</option>
                        
                            <option value="https://swiest.com/fi/" >Suomi</option>
                        
                            <option value="https://swiest.com/fo/" >F√∏royskt</option>
                        
                            <option value="https://swiest.com/fr/" >Fran√ßais</option>
                        
                            <option value="https://swiest.com/fy/" >Frysk</option>
                        
                            <option value="https://swiest.com/ga/" >Gaeilge</option>
                        
                            <option value="https://swiest.com/gl/" >Galego</option>
                        
                            <option value="https://swiest.com/gu/" >‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä</option>
                        
                            <option value="https://swiest.com/he/" >◊¢÷¥◊ë◊®÷¥◊ô◊™</option>
                        
                            <option value="https://swiest.com/km/" >·ûÄ·ûò·üí·ûñ·ûª·ûá·û∂·üî</option>
                        
                            <option value="https://swiest.com/hi/" >‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</option>
                        
                            <option value="https://swiest.com/hr/" >Hrvatski</option>
                        
                            <option value="https://swiest.com/ht/" >Krey√≤l Ayisyen</option>
                        
                            <option value="https://swiest.com/hu/" >Magyar</option>
                        
                            <option value="https://swiest.com/hy/" >’Ä’°’µ’•÷Ä’•’∂</option>
                        
                            <option value="https://swiest.com/ig/" >√Ås·ª•ÃÄs·ª•ÃÅ √ågb√≤</option>
                        
                            <option value="https://swiest.com/id/" >Bahasa Indonesia</option>
                        
                            <option value="https://swiest.com/is/" >√çslenska</option>
                        
                            <option value="https://swiest.com/it/" >Italiano</option>
                        
                            <option value="https://swiest.com/ja/" >Êó•Êú¨Ë™û</option>
                        
                            <option value="https://swiest.com/jv/" >Basa Jawa</option>
                        
                            <option value="https://swiest.com/ka/" >·É•·Éê·É†·Éó·É£·Éö·Éò</option>
                        
                            <option value="https://swiest.com/kk/" >“ö–∞–∑–∞“õ—à–∞</option>
                        
                            <option value="https://swiest.com/kn/" >‡≤ï‡≤®‡≥ç‡≤®‡≤°</option>
                        
                            <option value="https://swiest.com/ko/" >ÌïúÍµ≠Ïñ¥</option>
                        
                            <option value="https://swiest.com/or/" >‡¨ì‡¨°‡¨º‡¨ø‡¨Ü</option>
                        
                            <option value="https://swiest.com/ckb/" >⁄©Ÿàÿ±ÿØ€å</option>
                        
                            <option value="https://swiest.com/ky/" >–ö—ã—Ä–≥—ã–∑—á–∞</option>
                        
                            <option value="https://swiest.com/la/" >Latina</option>
                        
                            <option value="https://swiest.com/lb/" >L√´tzebuergesch</option>
                        
                            <option value="https://swiest.com/lo/" >‡∫û‡∫≤‡∫™‡∫≤‡∫•‡∫≤‡∫ß</option>
                        
                            <option value="https://swiest.com/lt/" >Lietuvi≈≥</option>
                        
                            <option value="https://swiest.com/lv/" >Latvie≈°u</option>
                        
                            <option value="https://swiest.com/mk/" >–ú–∞–∫–µ–¥–æ–Ω—Å–∫–∏</option>
                        
                            <option value="https://swiest.com/ml/" >‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç</option>
                        
                            <option value="https://swiest.com/mn/" >–ú–æ–Ω–≥–æ–ª —Ö—ç–ª</option>
                        
                            <option value="https://swiest.com/mr/" >‡§Æ‡§∞‡§æ‡§†‡•Ä</option>
                        
                            <option value="https://swiest.com/sw/" >Kiswahili</option>
                        
                            <option value="https://swiest.com/ms/" >Bahasa Melayu</option>
                        
                            <option value="https://swiest.com/my/" >·Äô·Äº·Äî·Ä∫·Äô·Ä¨</option>
                        
                            <option value="https://swiest.com/ne/" >‡§®‡•á‡§™‡§æ‡§≤‡•Ä</option>
                        
                            <option value="https://swiest.com/nl/" >Nederlands</option>
                        
                            <option value="https://swiest.com/no/" >Norsk</option>
                        
                            <option value="https://swiest.com/pa/" >‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä</option>
                        
                            <option value="https://swiest.com/pl/" >Polski</option>
                        
                            <option value="https://swiest.com/pt-br/" >Portugu√™s Brasil</option>
                        
                            <option value="https://swiest.com/pt-pt/" >Portugu√™s Europeu</option>
                        
                            <option value="https://swiest.com/ro/" >Rom√¢nƒÉ</option>
                        
                            <option value="https://swiest.com/ru/" >–†—É—Å—Å–∫–∏–π</option>
                        
                            <option value="https://swiest.com/rw/" >Kinyarwanda</option>
                        
                            <option value="https://swiest.com/si/" >‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω</option>
                        
                            <option value="https://swiest.com/sk/" >Slovenƒçina</option>
                        
                            <option value="https://swiest.com/sl/" >Sloven≈°ƒçina</option>
                        
                            <option value="https://swiest.com/sq/" >Shqip</option>
                        
                            <option value="https://swiest.com/sr/" >–°—Ä–ø—Å–∫–∏ (Srpski)</option>
                        
                            <option value="https://swiest.com/su/" >Basa Sunda</option>
                        
                            <option value="https://swiest.com/sv/" >Svenska</option>
                        
                            <option value="https://swiest.com/ta/" >‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç</option>
                        
                            <option value="https://swiest.com/te/" >‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</option>
                        
                            <option value="https://swiest.com/tg/" >–¢–æ“∑–∏–∫”£</option>
                        
                            <option value="https://swiest.com/th/" >‡πÑ‡∏ó‡∏¢</option>
                        
                            <option value="https://swiest.com/tk/" >T√ºrkmenler</option>
                        
                            <option value="https://swiest.com/tl/" >Filipino</option>
                        
                            <option value="https://swiest.com/tr/" >T√ºrk√ße</option>
                        
                            <option value="https://swiest.com/uk/" >–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</option>
                        
                            <option value="https://swiest.com/ur/" >ÿßÿ±ÿØŸà</option>
                        
                            <option value="https://swiest.com/uz/" >O&#39;zbekcha</option>
                        
                            <option value="https://swiest.com/vi/" >Ti·∫øng Vi·ªát</option>
                        
                            <option value="https://swiest.com/yi/" >◊ê◊ô◊ì◊ô◊©</option>
                        
                            <option value="https://swiest.com/zh-hk/" >Á≤µË™û</option>
                        
                            <option value="https://swiest.com/zu/" >IsiZulu</option>
                        
                    </select>
                </li>
            
            
            
                <li id="dark-mode-toggle">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <span>Dark Mode</span>
                </li>
            
        </div>
    </ol>
</aside>

    

            <main class="main full-width"><form action="/search/" class="search-form widget">
            <p>
                <label>Search</label>
                <input name="keyword" required placeholder="Type something..." />
        
                <button title="Search">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                </button>
            </p>
        </form><article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-tags">
        
            <a href="/tags/english/" >
                English
            </a>
        
            <a href="/tags/podcast/" >
                Podcast
            </a>
        
            <a href="/tags/lex-fridman-podcast/" >
                Lex Fridman Podcast
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/en/1310500084/">Lex Fridman Podcast - #84 - William MacAskill: Effective Altruism</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2022-05-23</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    66 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>
<div class="article-content">
    <p style="text-align:center">
       <a href="https://amzn.to/471i0jl" target="_blank">üéÅAmazon Prime</a>
       <a href="https://amzn.to/3Fulwaf" target="_blank">üíóThe Drop</a>
       <a href="https://amzn.to/3QDVlVf" target="_blank">üìñKindle Unlimited</a>
       <a href="https://amzn.to/3FqzNoB" target="_blank">üéßAudible Plus</a>
       <a href="https://amzn.to/3tMT3dm" target="_blank">üéµAmazon Music Unlimited</a>
       <a href="https://www.iherb.com/?rcode=EID1574" target="_blank">üåøiHerb</a>
       <a href="https://accounts.binance.com/register?ref=72302422" target="_blank">üí∞Binance</a>
    </p>
</div>
<div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064"
     crossorigin="anonymous"></script>
    
    <ins class="adsbygoogle"
         style="display:block"
         data-ad-client="ca-pub-9206135835124064"
         data-ad-slot="8754979142"
         data-ad-format="auto"
         data-full-width-responsive="true"></ins>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>


    <section class="article-content">
    
    
    <p>The following is a conversation with William McCaskill.</p>
<p>He&rsquo;s a philosopher, ethicist, and one of the originators of the effective altruism movement.</p>
<p>His research focuses on the fundamentals of effective altruism,</p>
<p>or the use of evidence and reason to help others by as much as possible with our time and money,</p>
<p>with a particular concentration on how to act given moral uncertainty.</p>
<p>He&rsquo;s the author of Doing Good, Better, Effective Altruism,</p>
<p>and a Radical New Way to Make a Difference.</p>
<p>He is a cofounder and the president of the Center of Effective Altruism, CEA,</p>
<p>that encourages people to commit to donate at least 10% of their income to the most effective charities.</p>
<p>He cofounded 80,000 Hours, which is a nonprofit that provides research and advice</p>
<p>on how you can best make a difference through your career.</p>
<p>This conversation was recorded before the outbreak of the coronavirus pandemic.</p>
<p>For everyone feeling the medical, psychological, and financial burden of this crisis,</p>
<p>I&rsquo;m sending love your way.</p>
<p>Stay strong. We&rsquo;re in this together. We&rsquo;ll beat this thing.</p>
<p>This is the Artificial Intelligence Podcast.</p>
<p>If you enjoy it, subscribe on YouTube, review it with five stars on Apple Podcast,</p>
<p>support it on Patreon, or simply connect with me on Twitter at Lex Friedman, spelled F R I D M A N.</p>
<p>As usual, I&rsquo;ll do one or two minutes of ads now,</p>
<p>and never any ads in the middle that can break the flow of the conversation.</p>
<p>I hope that works for you and doesn&rsquo;t hurt the listening experience.</p>
<p>This show is presented by Cash App, the number one finance app in the App Store.</p>
<p>When you get it, use code LEXPODCAST.</p>
<p>Cash App lets you send money to friends, buy Bitcoin, and invest in the stock market with as little as $1.</p>
<p>Since Cash App allows you to send and receive money digitally, peer to peer,</p>
<p>and security in all digital transactions is very important,</p>
<p>let me mention the PCI data security standard that Cash App is compliant with.</p>
<p>I&rsquo;m a big fan of standards for safety and security.</p>
<p>PCI DSS is a good example of that,</p>
<p>where a bunch of competitors got together and agreed</p>
<p>that there needs to be a global standard around the security of transactions.</p>
<p>Now, we just need to do the same for autonomous vehicles and AI systems in general.</p>
<p>So again, if you get Cash App from the App Store or Google Play,</p>
<p>and use the code LEXPODCAST, you get $10, and Cash App will also donate $10 to FIRST,</p>
<p>an organization that is helping to advance robotics and STEM education for young people around the world.</p>
<p>And now, here&rsquo;s my conversation with William McCaskill.</p>
<p>What does utopia for humans and all life on Earth look like for you?</p>
<p>That&rsquo;s a great question.</p>
<p>What I want to say is that we don&rsquo;t know,</p>
<p>and the utopia we want to get to is an indirect one that I call the long reflection.</p>
<p>So, period of post scarcity, no longer have the kind of urgent problems we have today,</p>
<p>but instead can spend, perhaps it&rsquo;s tens of thousands of years debating,</p>
<p>engaging in ethical reflection in order, before we take any kind of drastic lock in,</p>
<p>actions like spreading to the stars,</p>
<p>and then we can figure out what is of kind of moral value.</p>
<p>The long reflection, that&rsquo;s a really beautiful term.</p>
<p>So, if we look at Twitter for just a second,</p>
<p>do you think human beings are able to reflect in a productive way?</p>
<p>I don&rsquo;t mean to make it sound bad,</p>
<p>because there is a lot of fights and politics and division in our discourse.</p>
<p>Maybe if you zoom out, it actually is civilized discourse.</p>
<p>It might not feel like it, but when you zoom out.</p>
<p>So, I don&rsquo;t want to say that Twitter is not civilized discourse.</p>
<p>I actually believe it.</p>
<p>It&rsquo;s more civilized than people give it credit for.</p>
<p>But do you think the long reflection can actually be stable,</p>
<p>where we as human beings with our descendant of eight brains</p>
<p>would be able to sort of rationally discuss things together and arrive at ideas?</p>
<p>I think, overall, we&rsquo;re pretty good at discussing things rationally,</p>
<p>and at least in the earlier stages of our lives being open to many different ideas,</p>
<p>and being able to be convinced and change our views.</p>
<p>I think that Twitter is designed almost to bring out all the worst tendencies.</p>
<p>So, if the long reflection were conducted on Twitter,</p>
<p>maybe it would be better just not even to bother.</p>
<p>But I think the challenge really is getting to a stage</p>
<p>where we have a society that is as conducive as possible</p>
<p>to rational reflection, to deliberation.</p>
<p>I think we&rsquo;re actually very lucky to be in a liberal society</p>
<p>where people are able to discuss a lot of ideas and so on.</p>
<p>I think when we look to the future,</p>
<p>that&rsquo;s not at all guaranteed that society would be like that,</p>
<p>rather than a society where there&rsquo;s a fixed canon of values</p>
<p>that are being imposed on all of society,</p>
<p>and where you aren&rsquo;t able to question that.</p>
<p>That would be very bad from my perspective,</p>
<p>because it means we wouldn&rsquo;t be able to figure out what the truth is.</p>
<p>I can already sense we&rsquo;re going to go down a million tangents,</p>
<p>but what do you think is the&hellip;</p>
<p>If Twitter is not optimal,</p>
<p>what kind of mechanism in this modern age of technology</p>
<p>can we design where the exchange of ideas could be both civilized and productive,</p>
<p>and yet not be too constrained</p>
<p>where there&rsquo;s rules of what you can say and can&rsquo;t say,</p>
<p>which is, as you say, is not desirable,</p>
<p>but yet not have some limits as to what can be said or not and so on?</p>
<p>Do you have any ideas, thoughts on the possible future?</p>
<p>Of course, nobody knows how to do it,</p>
<p>but do you have thoughts of what a better Twitter might look like?</p>
<p>I think that text based media are intrinsically going to be very hard</p>
<p>to be conducive to rational discussion,</p>
<p>because if you think about it from an informational perspective,</p>
<p>if I just send you a text of less than,</p>
<p>what is it now, 240 characters, 280 characters, I think,</p>
<p>that&rsquo;s a tiny amount of information compared to, say, you and I talking now,</p>
<p>where you have access to the words I say, which is the same as in text,</p>
<p>but also my tone, also my body language,</p>
<p>and we&rsquo;re very poorly designed to be able to assess&hellip;</p>
<p>I have to read all of this context into anything you say,</p>
<p>so maybe your partner sends you a text and has a full stop at the end.</p>
<p>Are they mad at you?</p>
<p>You don&rsquo;t know.</p>
<p>You have to infer everything about this person&rsquo;s mental state</p>
<p>from whether they put a full stop at the end of a text or not.</p>
<p>Well, the flip side of that is it truly text that&rsquo;s the problem here,</p>
<p>because there&rsquo;s a viral aspect to the text,</p>
<p>where you could just post text nonstop.</p>
<p>It&rsquo;s very immediate.</p>
<p>The times before Twitter, before the internet,</p>
<p>the way you would exchange texts is you would write books.</p>
<p>And that, while it doesn&rsquo;t get body language, it doesn&rsquo;t get tone, it doesn&rsquo;t&hellip;</p>
<p>so on, but it does actually boil down after some time of thinking,</p>
<p>some editing, and so on, boil down ideas.</p>
<p>So is the immediacy and the viral nature,</p>
<p>which produces the outrage mobs and so on, the potential problem?</p>
<p>I think that is a big issue.</p>
<p>I think there&rsquo;s going to be this strong selection effect where</p>
<p>something that provokes outrage, well, that&rsquo;s high arousal,</p>
<p>you&rsquo;re more likely to retweet that,</p>
<p>whereas kind of sober analysis is not as sexy, not as viral.</p>
<p>I do agree that long form content is much better to productive discussion.</p>
<p>In terms of the media that are very popular at the moment,</p>
<p>I think that podcasting is great where your podcasts are two hours long,</p>
<p>so they&rsquo;re much more in depth than Twitter are,</p>
<p>and you are able to convey so much more nuance,</p>
<p>so much more caveat, because it&rsquo;s an actual conversation.</p>
<p>It&rsquo;s more like the sort of communication that we&rsquo;ve evolved to do,</p>
<p>rather than these very small little snippets of ideas that,</p>
<p>when also combined with bad incentives,</p>
<p>just clearly aren&rsquo;t designed for helping us get to the truth.</p>
<p>It&rsquo;s kind of interesting that it&rsquo;s not just the length of the podcast medium,</p>
<p>but it&rsquo;s the fact that it was started by people that don&rsquo;t give a damn about</p>
<p>quote unquote demand, that there&rsquo;s a relaxed,</p>
<p>sort of the style that Joe Rogan does,</p>
<p>there&rsquo;s a freedom to express ideas</p>
<p>in an unconstrained way that&rsquo;s very real.</p>
<p>It&rsquo;s kind of funny that it feels so refreshingly real to us today,</p>
<p>and I wonder what the future looks like.</p>
<p>It&rsquo;s a little bit sad now that quite a lot of sort of more popular people</p>
<p>are getting into podcasting,</p>
<p>and they try to sort of create, they try to control it,</p>
<p>they try to constrain it in different kinds of ways.</p>
<p>People I love, like Conan O Brien and so on, different comedians,</p>
<p>and I&rsquo;d love to see where the real aspects of this podcasting medium persist,</p>
<p>maybe in TV, maybe in YouTube,</p>
<p>maybe Netflix is pushing those kind of ideas,</p>
<p>and it&rsquo;s kind of, it&rsquo;s a really exciting word,</p>
<p>that kind of sharing of knowledge.</p>
<p>Yeah, I mean, I think it&rsquo;s a double edged sword</p>
<p>as it becomes more popular and more profitable,</p>
<p>where on the one hand you&rsquo;ll get a lot more creativity,</p>
<p>people doing more interesting things with the medium,</p>
<p>but also perhaps you get this place to the bottom</p>
<p>where suddenly maybe it&rsquo;ll be hard to find good content on podcasts</p>
<p>because it&rsquo;ll be so overwhelmed by the latest bit of viral outrage.</p>
<p>So speaking of that, jumping on Effective Altruism for a second,</p>
<p>so much of that internet content is funded by advertisements.</p>
<p>Just in the context of Effective Altruism,</p>
<p>we&rsquo;re talking about the richest companies in the world,</p>
<p>they&rsquo;re funded by advertisements essentially,</p>
<p>Google, that&rsquo;s their primary source of income.</p>
<p>Do you see that as,</p>
<p>do you have any criticism of that source of income?</p>
<p>Do you see that source of money</p>
<p>as a potentially powerful source of money that could be used,</p>
<p>well, certainly could be used for good,</p>
<p>but is there something bad about that source of money?</p>
<p>I think there&rsquo;s significant worries with it,</p>
<p>where it means that the incentives of the company</p>
<p>might be quite misaligned with making people&rsquo;s lives better,</p>
<p>where again, perhaps the incentives are towards increasing drama</p>
<p>and debate on your social media feed</p>
<p>in order that more people are going to be engaged,</p>
<p>perhaps compulsively involved with the platform.</p>
<p>Whereas there are other business models</p>
<p>like having an opt in subscription service</p>
<p>where perhaps they have other issues,</p>
<p>but there&rsquo;s much more of an incentive to provide a product</p>
<p>that its users are just really wanting,</p>
<p>because now I&rsquo;m paying for this product.</p>
<p>I&rsquo;m paying for this thing that I want to buy</p>
<p>rather than I&rsquo;m trying to use this thing</p>
<p>and it&rsquo;s going to get a profit mechanism</p>
<p>that is somewhat orthogonal to me</p>
<p>actually just wanting to use the product.</p>
<p>And so, I mean, in some cases it&rsquo;ll work better than others.</p>
<p>I can imagine, I can in theory imagine Facebook</p>
<p>having a subscription service,</p>
<p>but I think it&rsquo;s unlikely to happen anytime soon.</p>
<p>Well, it&rsquo;s interesting and it&rsquo;s weird</p>
<p>now that you bring it up that it&rsquo;s unlikely.</p>
<p>For example, I pay I think 10 bucks a month for YouTube Red</p>
<p>and I don&rsquo;t think I get it much for that</p>
<p>except just for no ads,</p>
<p>but in general it&rsquo;s just a slightly better experience.</p>
<p>And I would gladly, now I&rsquo;m not wealthy,</p>
<p>in fact I&rsquo;m operating very close to zero dollars,</p>
<p>but I would pay 10 bucks a month to Facebook</p>
<p>and 10 bucks a month to Twitter</p>
<p>for some kind of more control</p>
<p>in terms of advertisements and so on.</p>
<p>But the other aspect of that is data, personal data.</p>
<p>People are really sensitive about this</p>
<p>and I as one who hopes to one day</p>
<p>create a company that may use people&rsquo;s data</p>
<p>to do good for the world,</p>
<p>wonder about this.</p>
<p>One, the psychology of why people are so paranoid.</p>
<p>Well, I understand why,</p>
<p>but they seem to be more paranoid</p>
<p>than is justified at times.</p>
<p>And the other is how do you do it right?</p>
<p>So it seems that Facebook is,</p>
<p>it seems that Facebook is doing it wrong.</p>
<p>That&rsquo;s certainly the popular narrative.</p>
<p>It&rsquo;s unclear to me actually how wrong.</p>
<p>Like I tend to give them more benefit of the doubt</p>
<p>because it&rsquo;s a really hard thing to do right</p>
<p>and people don&rsquo;t necessarily realize it,</p>
<p>but how do we respect in your view people&rsquo;s privacy?</p>
<p>Yeah, I mean in the case of how worried are people</p>
<p>about using their data,</p>
<p>I mean there&rsquo;s a lot of public debate</p>
<p>and criticism about it.</p>
<p>When we look at people&rsquo;s revealed preferences,</p>
<p>people&rsquo;s continuing massive use</p>
<p>of these sorts of services.</p>
<p>It&rsquo;s not clear to me how much people really do care.</p>
<p>Perhaps they care a bit,</p>
<p>but they&rsquo;re happy to in effect kind of sell their data</p>
<p>in order to be able to kind of use a certain service.</p>
<p>That&rsquo;s a great term, revealed preferences.</p>
<p>So these aren&rsquo;t preferences you self report in the survey.</p>
<p>This is like your actions speak.</p>
<p>Yeah, exactly.</p>
<p>So you might say,</p>
<p>oh yeah, I hate the idea of Facebook having my data.</p>
<p>But then when it comes to it,</p>
<p>you actually are willing to give that data in exchange</p>
<p>for being able to use the service.</p>
<p>And if that&rsquo;s the case,</p>
<p>then I think unless we have some explanation</p>
<p>about why there&rsquo;s some negative externality from that</p>
<p>or why there&rsquo;s some coordination failure,</p>
<p>or if there&rsquo;s something that consumers</p>
<p>are just really misled about</p>
<p>where they don&rsquo;t realize why giving away data like this</p>
<p>is a really bad thing to do,</p>
<p>then ultimately I kind of want to,</p>
<p>you know, respect people&rsquo;s preferences.</p>
<p>They can give away their data if they want.</p>
<p>I think there&rsquo;s a big difference</p>
<p>between companies use of data</p>
<p>and governments having data where,</p>
<p>you know, looking at the track record of history,</p>
<p>governments knowing a lot about their people can be very bad</p>
<p>if the government chooses to do bad things with it.</p>
<p>And that&rsquo;s more worrying, I think.</p>
<p>So let&rsquo;s jump into it a little bit.</p>
<p>Most people know, but actually I, two years ago,</p>
<p>had no idea what effective altruism was</p>
<p>until I saw there was a cool looking event</p>
<p>in an MIT group here.</p>
<p>I think it&rsquo;s called the Effective Altruism Club or a group.</p>
<p>I was like, what the heck is that?</p>
<p>And one of my friends said,</p>
<p>I mean, he said that they&rsquo;re just</p>
<p>a bunch of eccentric characters.</p>
<p>So I was like, hell yes, I&rsquo;m in.</p>
<p>So I went to one of their events</p>
<p>and looked up what&rsquo;s it about.</p>
<p>It&rsquo;s quite a fascinating philosophical</p>
<p>and just a movement of ideas.</p>
<p>So can you tell me what is effective altruism?</p>
<p>Great, so the core of effective altruism</p>
<p>is about trying to answer this question,</p>
<p>which is how can I do as much good as possible</p>
<p>with my scarce resources, my time and with my money?</p>
<p>And then once we have our best guess answers to that,</p>
<p>trying to take those ideas and put that into practice,</p>
<p>and do those things that we believe will do the most good.</p>
<p>And we&rsquo;re now a community of people,</p>
<p>many thousands of us around the world,</p>
<p>who really are trying to answer that question</p>
<p>as best we can and then use our time and money</p>
<p>to make the world better.</p>
<p>So what&rsquo;s the difference between sort of</p>
<p>classical general idea of altruism</p>
<p>and effective altruism?</p>
<p>So normally when people try to do good,</p>
<p>they often just aren&rsquo;t so reflective about those attempts.</p>
<p>So someone might approach you on the street</p>
<p>asking you to give to charity.</p>
<p>And if you&rsquo;re feeling altruistic,</p>
<p>you&rsquo;ll give to the person on the street.</p>
<p>Or if you think, oh, I wanna do some good in my life,</p>
<p>you might volunteer at a local place.</p>
<p>Or perhaps you&rsquo;ll decide, pursue a career</p>
<p>where you&rsquo;re working in a field</p>
<p>that&rsquo;s kind of more obviously beneficial</p>
<p>like being a doctor or a nurse or a healthcare professional.</p>
<p>But it&rsquo;s very rare that people apply the same level</p>
<p>of rigor and analytical thinking</p>
<p>to lots of other areas we think about.</p>
<p>So take the case of someone approaching you on the street.</p>
<p>Imagine if that person instead was saying,</p>
<p>hey, I&rsquo;ve got this amazing company.</p>
<p>Do you want to invest in it?</p>
<p>It would be insane.</p>
<p>No one would ever think, oh, of course,</p>
<p>I&rsquo;m just a company like you&rsquo;d think it was a scam.</p>
<p>But somehow we don&rsquo;t have that same level of rigor</p>
<p>when it comes to doing good,</p>
<p>even though the stakes are more important</p>
<p>when it comes to trying to help others</p>
<p>than trying to make money for ourselves.</p>
<p>Well, first of all, so there is a psychology</p>
<p>at the individual level of doing good just feels good.</p>
<p>And so in some sense, on that pure psychological part,</p>
<p>it doesn&rsquo;t matter.</p>
<p>In fact, you don&rsquo;t wanna know if it does good or not</p>
<p>because most of the time it won&rsquo;t.</p>
<p>So like in a certain sense,</p>
<p>it&rsquo;s understandable why altruism</p>
<p>without the effective part is so appealing</p>
<p>to a certain population.</p>
<p>By the way, let&rsquo;s zoom off for a second.</p>
<p>Do you think most people, two questions.</p>
<p>Do you think most people are good?</p>
<p>And question number two is,</p>
<p>do you think most people wanna do good?</p>
<p>So are most people good?</p>
<p>I think it&rsquo;s just super dependent</p>
<p>on the circumstances that someone is in.</p>
<p>I think that the actions people take</p>
<p>and their moral worth is just much more dependent</p>
<p>on circumstance than it is on someone&rsquo;s intrinsic character.</p>
<p>So is there evil within all of us?</p>
<p>It seems like with the better angels of our nature,</p>
<p>there&rsquo;s a tendency of us as a society</p>
<p>to tend towards good, less war.</p>
<p>I mean, with all these metrics.</p>
<p>Is that us becoming who we want to be</p>
<p>or is that some kind of societal force?</p>
<p>What&rsquo;s the nature versus nurture thing here?</p>
<p>Yeah, so in that case, I just think,</p>
<p>yeah, so violence has massively declined over time.</p>
<p>I think that&rsquo;s a slow process of cultural evolution,</p>
<p>institutional evolution such that now the incentives</p>
<p>for you and I to be violent are very, very small indeed.</p>
<p>In contrast, when we were hunter gatherers,</p>
<p>the incentives were quite large.</p>
<p>If there was someone who was potentially disturbing</p>
<p>the social order and hunter gatherer setting,</p>
<p>there was a very strong incentive to kill that person</p>
<p>and people did and it was just the guarded 10% of deaths</p>
<p>among hunter gatherers were murders.</p>
<p>After hunter gatherers, when you have actual societies</p>
<p>is when violence can probably go up</p>
<p>because there&rsquo;s more incentive to do mass violence, right?</p>
<p>To take over, conquer other people&rsquo;s lands</p>
<p>and murder everybody in place and so on.</p>
<p>Yeah, I mean, I think total death rate</p>
<p>from human causes does go down,</p>
<p>but you&rsquo;re right that if you&rsquo;re in a hunter gatherer situation</p>
<p>you&rsquo;re kind of a group that you&rsquo;re part of is very small</p>
<p>then you can&rsquo;t have massive wars</p>
<p>that just massive communities don&rsquo;t exist.</p>
<p>But anyway, the second question,</p>
<p>do you think most people want to do good?</p>
<p>Yeah, and then I think that is true for most people.</p>
<p>I think you see that with the fact that most people donate,</p>
<p>a large proportion of people volunteer.</p>
<p>If you give people opportunities</p>
<p>to easily help other people, they will take it.</p>
<p>But at the same time,</p>
<p>we&rsquo;re a product of our circumstances</p>
<p>and if it were more socially awarded to be doing more good,</p>
<p>if it were more socially awarded to do good effectively</p>
<p>rather than not effectively,</p>
<p>then we would see that behavior a lot more.</p>
<p>So why should we do good?</p>
<p>Yeah, my answer to this is</p>
<p>there&rsquo;s no kind of deeper level of explanation.</p>
<p>So my answer to kind of why should you do good is</p>
<p>well, there is someone whose life is on the line,</p>
<p>for example, whose life you can save</p>
<p>via donating just actually a few thousand dollars</p>
<p>to an effective nonprofit</p>
<p>like the Against Malaria Foundation.</p>
<p>That is a sufficient reason to do good.</p>
<p>And then if you ask, well, why ought I to do that?</p>
<p>I&rsquo;m like, I just show you the same facts again.</p>
<p>It&rsquo;s that fact that is the reason to do good.</p>
<p>There&rsquo;s nothing more fundamental than that.</p>
<p>I&rsquo;d like to sort of make more concrete</p>
<p>the thing we&rsquo;re trying to make better.</p>
<p>So you just mentioned malaria.</p>
<p>So there&rsquo;s a huge amount of suffering in the world.</p>
<p>Are we trying to remove?</p>
<p>So is ultimately the goal, not ultimately,</p>
<p>but the first step is to remove the worst of the suffering.</p>
<p>So there&rsquo;s some kind of threshold of suffering</p>
<p>that we want to make sure does not exist in the world.</p>
<p>Or do we really naturally want to take a much further step</p>
<p>and look at things like income inequality?</p>
<p>So not just getting everybody above a certain threshold,</p>
<p>but making sure that there&rsquo;s some,</p>
<p>that broadly speaking,</p>
<p>there&rsquo;s less injustice in the world, unfairness,</p>
<p>in some definition, of course,</p>
<p>very difficult to define a fairness.</p>
<p>Yeah, so the metric I use is how many people do we affect</p>
<p>and by how much do we affect them?</p>
<p>And so that can, often that means eliminating suffering,</p>
<p>but it doesn&rsquo;t have to,</p>
<p>could be helping promote a flourishing life instead.</p>
<p>And so if I was comparing reducing income inequality</p>
<p>or getting people from the very pits of suffering</p>
<p>to a higher level,</p>
<p>the question I would ask is just a quantitative one</p>
<p>of just if I do this first thing or the second thing,</p>
<p>how many people am I going to benefit</p>
<p>and by how much am I going to benefit?</p>
<p>Am I going to move that one person from kind of 10%,</p>
<p>0% well being to 10% well being?</p>
<p>Perhaps that&rsquo;s just not as good as moving a hundred people</p>
<p>from 10% well being to 50% well being.</p>
<p>And the idea is the diminishing returns is the idea of</p>
<p>when you&rsquo;re in terrible poverty,</p>
<p>then the $1 that you give goes much further</p>
<p>than if you were in the middle class in the United States,</p>
<p>for example.</p>
<p>Absolutely.</p>
<p>And this fact is really striking.</p>
<p>So if you take even just quite a conservative estimate</p>
<p>of how we are able to turn money into well being,</p>
<p>the economists put it as like a log curve.</p>
<p>That&rsquo;s the or steeper.</p>
<p>But that means that any proportional increase</p>
<p>in your income has the same impact on your well being.</p>
<p>And so someone moving from $1,000 a year</p>
<p>to $2,000 a year has the same impact</p>
<p>as someone moving from $100,000 a year to $200,000 a year.</p>
<p>And then when you combine that with the fact that we</p>
<p>in middle class members of rich countries are 100 times richer</p>
<p>than financial terms in the global poor,</p>
<p>that means we can do a hundred times to benefit the poorest people</p>
<p>in the world as we can to benefit people of our income level.</p>
<p>And that&rsquo;s this astonishing fact.</p>
<p>Yeah, it&rsquo;s quite incredible.</p>
<p>A lot of these facts and ideas are just difficult to think about</p>
<p>because there&rsquo;s an overwhelming amount of suffering in the world.</p>
<p>And even acknowledging it is difficult.</p>
<p>Not exactly sure why that is.</p>
<p>I mean, I mean, it&rsquo;s difficult because you have to bring to mind,</p>
<p>you know, it&rsquo;s an unpleasant experience thinking</p>
<p>about other people&rsquo;s suffering.</p>
<p>It&rsquo;s unpleasant to be empathizing with it, firstly.</p>
<p>And then secondly, thinking about it means</p>
<p>that maybe we&rsquo;d have to change our lifestyles.</p>
<p>And if you&rsquo;re very attached to the income that you&rsquo;ve got,</p>
<p>perhaps you don&rsquo;t want to be confronting ideas or arguments</p>
<p>that might cause you to use some of that money to help others.</p>
<p>So it&rsquo;s quite understandable in the psychological terms,</p>
<p>even if it&rsquo;s not the right thing that we ought to be doing.</p>
<p>So how can we do better?</p>
<p>How can we be more effective?</p>
<p>How does data help?</p>
<p>Yeah, in general, how can we do better?</p>
<p>It&rsquo;s definitely hard.</p>
<p>And we have spent the last 10 years engaged in kind of some deep research projects,</p>
<p>to try and answer kind of two questions.</p>
<p>One is, of all the many problems the world is facing,</p>
<p>what are the problems we ought to be focused on?</p>
<p>And then within those problems that we judge to be kind of the most pressing,</p>
<p>where we use this idea of focusing on problems that are the biggest in scale,</p>
<p>that are the most tractable,</p>
<p>where we can make the most progress on that problem,</p>
<p>and that are the most neglected.</p>
<p>Within them, what are the things that have the kind of best evidence,</p>
<p>or we have the best guess, will do the most good.</p>
<p>And so we have a bunch of organizations.</p>
<p>So GiveWell, for example, is focused on global health and development,</p>
<p>and has a list of seven top recommended charities.</p>
<p>So the idea in general, and sorry to interrupt,</p>
<p>is, so we&rsquo;ll talk about sort of poverty and animal welfare and existential risk.</p>
<p>Those are all fascinating topics, but in general,</p>
<p>the idea is there should be a group,</p>
<p>sorry, there&rsquo;s a lot of groups that seek to convert money into good.</p>
<p>And then you also on top of that want to have a accounting</p>
<p>of how good they actually perform that conversion,</p>
<p>how well they did in converting money to good.</p>
<p>So ranking of these different groups,</p>
<p>ranking these charities.</p>
<p>So does that apply across basically all aspects of effective altruism?</p>
<p>So there should be a group of people,</p>
<p>and they should report on certain metrics of how well they&rsquo;ve done,</p>
<p>and you should only give your money to groups that do a good job.</p>
<p>That&rsquo;s the core idea. I&rsquo;d make two comments.</p>
<p>One is just, it&rsquo;s not just about money.</p>
<p>So we&rsquo;re also trying to encourage people to work in areas</p>
<p>where they&rsquo;ll have the biggest impact.</p>
<p>Absolutely.</p>
<p>And in some areas, you know, they&rsquo;re really people heavy, but money poor.</p>
<p>Other areas are kind of money rich and people poor.</p>
<p>And so whether it&rsquo;s better to focus time or money depends on the cause area.</p>
<p>And then the second is that you mentioned metrics,</p>
<p>and while that&rsquo;s the ideal, and in some areas we do,</p>
<p>we are able to get somewhat quantitative information</p>
<p>about how much impact an area is having.</p>
<p>That&rsquo;s not always true.</p>
<p>For some of the issues, like you mentioned existential risks,</p>
<p>well, we&rsquo;re not able to measure in any sort of precise way</p>
<p>like how much progress we&rsquo;re making.</p>
<p>And so you have to instead fall back on just rigorous argument and evaluation,</p>
<p>even in the absence of data.</p>
<p>So let&rsquo;s first sort of linger on your own story for a second.</p>
<p>How do you yourself practice effective altruism in your own life?</p>
<p>Because I think that&rsquo;s a really interesting place to start.</p>
<p>So I&rsquo;ve tried to build effective altruism into at least many components of my life.</p>
<p>So on the donation side, my plan is to give away most of my income</p>
<p>over the course of my life.</p>
<p>I&rsquo;ve set a bar I feel happy with and I just donate above that bar.</p>
<p>So at the moment, I donate about 20% of my income.</p>
<p>Then on the career side, I&rsquo;ve also shifted kind of what I do,</p>
<p>where I was initially planning to work on very esoteric topics</p>
<p>in the philosophy of logic, philosophy of language,</p>
<p>things that are intellectually extremely interesting,</p>
<p>but the path by which they really make a difference to the world is,</p>
<p>let&rsquo;s just say it&rsquo;s very unclear at best.</p>
<p>And so I switched instead to researching ethics to actually just working</p>
<p>on this question of how we can do as much good as possible.</p>
<p>And then I&rsquo;ve also spent a very large chunk of my life over the last 10 years</p>
<p>creating a number of nonprofits who again in different ways</p>
<p>are tackling this question of how we can do the most good</p>
<p>and helping them to grow over time too.</p>
<p>Yeah, we mentioned a few of them with the career selection, 80,000.</p>
<p>80,000 hours.</p>
<p>80,000 hours is a really interesting group.</p>
<p>So maybe also just a quick pause on the origins of effective altruism</p>
<p>because you paint a picture who the key figures are,</p>
<p>including yourself in the effective altruism movement today.</p>
<p>Yeah, there are two main strands that kind of came together</p>
<p>to form the effective altruism movement.</p>
<p>So one was two philosophers, myself and Toby Ord at Oxford,</p>
<p>and we had been very influenced by the work of Peter Singer,</p>
<p>an Australian model philosopher who had argued for many decades</p>
<p>that because one can do so much good at such little cost to oneself,</p>
<p>we have an obligation to give away most of our income</p>
<p>to benefit those in extreme poverty,</p>
<p>just in the same way that we have an obligation to run in</p>
<p>and save a child from drowning in a shallow pond</p>
<p>if it would just ruin your suit that cost a few thousand dollars.</p>
<p>And we set up Giving What We Can in 2009,</p>
<p>which is encouraging people to give at least 10% of their income</p>
<p>to the most effective charities.</p>
<p>And the second main strand was the formation of GiveWell,</p>
<p>which was originally based in New York and started in about 2007.</p>
<p>And that was set up by Holden Carnovsky and Elie Hassenfeld,</p>
<p>who were two hedge fund dudes who were making good money</p>
<p>and thinking, well, where should I donate?</p>
<p>And in the same way as if they wanted to buy a product for themselves,</p>
<p>they would look at Amazon reviews.</p>
<p>They were like, well, what are the best charities?</p>
<p>Found they just weren&rsquo;t really good answers to that question,</p>
<p>certainly not that they were satisfied with.</p>
<p>And so they formed GiveWell in order to try and work out</p>
<p>what are those charities where they can have the biggest impact.</p>
<p>And then from there and some other influences,</p>
<p>kind of community grew and spread.</p>
<p>Can we explore the philosophical and political space</p>
<p>that effective altruism occupies a little bit?</p>
<p>So from the little and distant in my own lifetime</p>
<p>that I&rsquo;ve read of Ayn Rand&rsquo;s work, Ayn Rand&rsquo;s philosophy of objectivism,</p>
<p>espouses, and it&rsquo;s interesting to put her philosophy in contrast</p>
<p>with effective altruism.</p>
<p>So it espouses selfishness as the best thing you can do.</p>
<p>But it&rsquo;s not actually against altruism.</p>
<p>It&rsquo;s just you have that choice, but you should be selfish in it, right?</p>
<p>Or not, maybe you can disagree here.</p>
<p>But so it can be viewed as the complete opposite of effective altruism</p>
<p>or it can be viewed as similar because the word effective is really interesting.</p>
<p>Because if you want to do good, then you should be damn good at doing good, right?</p>
<p>I think that would fit within the morality that&rsquo;s defined by objectivism.</p>
<p>So do you see a connection between these two philosophies</p>
<p>and other perhaps in this complicated space of beliefs</p>
<p>that effective altruism is positioned as opposing or aligned with?</p>
<p>I would definitely say that objectivism, Ayn Rand&rsquo;s philosophy,</p>
<p>is a philosophy that&rsquo;s quite fundamentally opposed to effective altruism.</p>
<p>In which way?</p>
<p>Insofar as Ayn Rand&rsquo;s philosophy is about championing egoism</p>
<p>and saying that I&rsquo;m never quite sure whether the philosophy is meant to say</p>
<p>that just you ought to do whatever will best benefit yourself,</p>
<p>that&rsquo;s ethical egoism, no matter what the consequences are.</p>
<p>Or second, if there&rsquo;s this alternative view, which is, well,</p>
<p>you ought to try and benefit yourself because that&rsquo;s actually the best way</p>
<p>of benefiting society.</p>
<p>Certainly, in Atlas Shalaguchi is presenting her philosophy</p>
<p>as a way that&rsquo;s actually going to bring about a flourishing society.</p>
<p>And if it&rsquo;s the former, then well, effective altruism is all about promoting</p>
<p>the idea of altruism and saying, in fact,</p>
<p>we ought to really be trying to help others as much as possible.</p>
<p>So it&rsquo;s opposed there.</p>
<p>And then on the second side, I would just dispute the empirical premise.</p>
<p>It would seem, given the major problems in the world today,</p>
<p>it would seem like this remarkable coincidence,</p>
<p>quite suspicious, one might say, if benefiting myself was actually</p>
<p>the best way to bring about a better world.</p>
<p>So on that point, and I think that connects also with career selection</p>
<p>that we&rsquo;ll talk about, but let&rsquo;s consider not objectives, but capitalism.</p>
<p>And the idea that you focusing on the thing that you are damn good at,</p>
<p>whatever that is, may be the best thing for the world.</p>
<p>Part of it is also mindset, right?</p>
<p>The thing I love is robots.</p>
<p>So maybe I should focus on building robots</p>
<p>and never even think about the idea of effective altruism,</p>
<p>which is kind of the capitalist notion.</p>
<p>Is there any value in that idea in just finding the thing you&rsquo;re good at</p>
<p>and maximizing your productivity in this world</p>
<p>and thereby sort of lifting all boats and benefiting society as a result?</p>
<p>Yeah, I think there&rsquo;s two things I&rsquo;d want to say on that.</p>
<p>So one is what your comparative advantages,</p>
<p>what your strengths are when it comes to career.</p>
<p>That&rsquo;s obviously super important because there&rsquo;s lots of career paths</p>
<p>I would be terrible at if I thought being an artist was the best thing one could do.</p>
<p>Well, I&rsquo;d be doomed, just really quite astonishingly bad.</p>
<p>And so I do think, at least within the realm of things that could plausibly be very high impact,</p>
<p>choose the thing that you think you&rsquo;re going to be able to really be passionate at</p>
<p>and excel at over the long term.</p>
<p>Then on this question of should one just do that in an unrestricted way</p>
<p>and not even think about what the most important problems are.</p>
<p>I do think that in a kind of perfectly designed society, that might well be the case.</p>
<p>That would be a society where we&rsquo;ve corrected all market failures,</p>
<p>we&rsquo;ve internalized all externalities,</p>
<p>and then we&rsquo;ve managed to set up incentives such that people just pursuing their own strengths</p>
<p>is the best way of doing good.</p>
<p>But we&rsquo;re very far from that society.</p>
<p>So if one did that, then it would be very unlikely that you would focus</p>
<p>on improving the lives of nonhuman animals that aren&rsquo;t participating in markets</p>
<p>or ensuring the long run future goes well,</p>
<p>where future people certainly aren&rsquo;t participating in markets</p>
<p>or benefiting the global poor who do participate,</p>
<p>but have so much less kind of power from a starting perspective</p>
<p>that their views aren&rsquo;t accurately kind of represented by market forces too.</p>
<p>Got it.</p>
<p>So yeah, instead of pure definition capitalism,</p>
<p>it just may very well ignore the people that are suffering the most,</p>
<p>the white swath of them.</p>
<p>So if you could allow me this line of thinking here.</p>
<p>So I&rsquo;ve listened to a lot of your conversations online.</p>
<p>I find, if I can compliment you, they&rsquo;re very interesting conversations.</p>
<p>Your conversation on Rogan, on Joe Rogan was really interesting,</p>
<p>with Sam Harris and so on, whatever.</p>
<p>There&rsquo;s a lot of stuff that&rsquo;s really good out there.</p>
<p>And yet, when I look at the internet and I look at YouTube,</p>
<p>which has certain mobs, certain swaths of right leaning folks,</p>
<p>whom I dearly love.</p>
<p>I love all people, especially people with ideas.</p>
<p>They seem to not like you very much.</p>
<p>So I don&rsquo;t understand why exactly.</p>
<p>So my own sort of hypothesis is there is a right left divide</p>
<p>that absurdly so caricatured in politics,</p>
<p>at least in the United States.</p>
<p>And maybe you&rsquo;re somehow pigeonholed into one of those sides.</p>
<p>And maybe that&rsquo;s what it is.</p>
<p>Maybe your message is somehow politicized.</p>
<p>Yeah, I mean.</p>
<p>How do you make sense of that?</p>
<p>Because you&rsquo;re extremely interesting.</p>
<p>Like you got the comments I see on Joe Rogan.</p>
<p>There&rsquo;s a bunch of negative stuff.</p>
<p>And yet, if you listen to it, the conversation is fascinating.</p>
<p>I&rsquo;m not speaking, I&rsquo;m not some kind of lefty extremist,</p>
<p>but just it&rsquo;s a fascinating conversation.</p>
<p>So why are you getting some small amount of hate?</p>
<p>So I&rsquo;m actually pretty glad that Effective Altruism has managed</p>
<p>to stay relatively unpoliticized because I think the core message</p>
<p>to just use some of your time and money to do as much good as possible</p>
<p>to fight some of the problems in the world can be appealing</p>
<p>across the political spectrum.</p>
<p>And we do have a diversity of political viewpoints among people</p>
<p>who have engaged in Effective Altruism.</p>
<p>We do, however, do get some criticism from the left and the right.</p>
<p>Oh, interesting.</p>
<p>What&rsquo;s the criticism?</p>
<p>Both would be interesting to hear.</p>
<p>Yeah, so criticism from the left is that we&rsquo;re not focused enough</p>
<p>on dismantling the capitalist system that they see as the root</p>
<p>of most of the problems that we&rsquo;re talking about.</p>
<p>And there I kind of disagree on partly the premise where I don&rsquo;t</p>
<p>think relevant alternative systems would say to the animals or to the</p>
<p>global poor or to the future generations kind of much better.</p>
<p>And then also the tactics where I think there are particular ways</p>
<p>we can change society that would massively benefit, you know,</p>
<p>be massively beneficial on those things that don&rsquo;t go via dismantling</p>
<p>like the entire system, which is perhaps a million times harder to do.</p>
<p>Then criticism on the right, there&rsquo;s definitely like in response</p>
<p>to the Joe Rogan podcast.</p>
<p>There definitely were a number of Ayn Rand fans who weren&rsquo;t keen</p>
<p>on the idea of promoting altruism.</p>
<p>There was a remarkable set of ideas.</p>
<p>Just the idea that Effective Altruism was unmanly, I think, was</p>
<p>driving a lot of criticism.</p>
<p>Okay, so I love fighting.</p>
<p>I&rsquo;ve been in street fights my whole life.</p>
<p>I&rsquo;m as alpha in everything I do as it gets.</p>
<p>And the fact that Joe Rogan said that I thought Scent of a Woman</p>
<p>is a better movie than John Wick put me into this beta category</p>
<p>amongst people who are like basically saying this, yeah, unmanly</p>
<p>or it&rsquo;s not tough.</p>
<p>It&rsquo;s not some principled view of strength that is represented</p>
<p>by a spasmodic.</p>
<p>So actually, so how do you think about this?</p>
<p>Because to me, altruism, especially Effective Altruism, I don&rsquo;t</p>
<p>know what the female version of that is, but on the male side, manly</p>
<p>as fuck, if I may say so.</p>
<p>So how do you think about that kind of criticism?</p>
<p>I think people who would make that criticism are just occupying</p>
<p>a like state of mind that I think is just so different from my</p>
<p>state of mind that I kind of struggle to maybe even understand it</p>
<p>where if something&rsquo;s manly or unmanly or feminine or unfeminine,</p>
<p>I&rsquo;m like, I don&rsquo;t care.</p>
<p>Like, is it the right thing to do or the wrong thing to do?</p>
<p>So let me put it not in terms of man or woman.</p>
<p>I don&rsquo;t think that&rsquo;s useful, but I think there&rsquo;s a notion of acting</p>
<p>out of fear as opposed to out of principle and strength.</p>
<p>Yeah.</p>
<p>So, okay.</p>
<p>Yeah.</p>
<p>Here&rsquo;s something that I do feel as an intuition and that I think</p>
<p>drives some people who do find Canvaean Land attractive and so on</p>
<p>as a philosophy, which is a kind of taking control of your own</p>
<p>life and having power over how you&rsquo;re steering your life and not</p>
<p>kind of kowtowing to others, you know, really thinking things through.</p>
<p>I find like that set of ideas just very compelling and inspirational.</p>
<p>I actually think of effect of altruism has really, you know, that</p>
<p>side of my personality.</p>
<p>It&rsquo;s like scratch that itch where you are just not taking the kind</p>
<p>of priorities that society is giving you as granted.</p>
<p>Instead, you&rsquo;re choosing to act in accordance with the priorities</p>
<p>that you think are most important in the world.</p>
<p>And often that involves then doing quite unusual things from a</p>
<p>societal perspective, like donating a large chunk of your earnings</p>
<p>or working on these weird issues about AI and so on that other</p>
<p>people might not understand.</p>
<p>Yeah, I think that&rsquo;s a really gutsy thing to do.</p>
<p>That is taking control.</p>
<p>That&rsquo;s at least at this stage.</p>
<p>I mean, that&rsquo;s you taking ownership, not of just yourself, but</p>
<p>your presence in this world that&rsquo;s full of suffering and saying</p>
<p>as opposed to being paralyzed by that notion is taking control</p>
<p>and saying I could do something.</p>
<p>Yeah, I mean, that&rsquo;s really powerful.</p>
<p>But I mean, sort of the one thing I personally hate too about the</p>
<p>left currently that I think those folks to detect is the social</p>
<p>signaling. When you look at yourself, sort of late at night, would</p>
<p>you do everything you&rsquo;re doing in terms of effective altruism if</p>
<p>your name, because you&rsquo;re quite popular, but if your name was</p>
<p>totally unattached to it, so if it was in secret.</p>
<p>Yeah, I mean, I think I would.</p>
<p>To be honest, I think the kind of popularity is like, you know,</p>
<p>it&rsquo;s mixed bag, but there are serious costs.</p>
<p>And I don&rsquo;t particularly, I don&rsquo;t like love it.</p>
<p>Like, it means you get all these people calling you a cuck on</p>
<p>Joe Rogan.</p>
<p>It&rsquo;s like not the most fun thing.</p>
<p>But you also get a lot of sort of brownie points for doing good</p>
<p>for the world.</p>
<p>Yeah, you do.</p>
<p>But I think my ideal life, I would be like in some library solving</p>
<p>logic puzzles all day and I&rsquo;d like really be like learning maths</p>
<p>and so on.</p>
<p>So you have a like good body of friends and so on.</p>
<p>So your instinct for effective altruism is something deep.</p>
<p>It&rsquo;s not one that is communicating</p>
<p>socially. It&rsquo;s more in your heart.</p>
<p>You want to do good for the world.</p>
<p>Yeah, I mean, so we can look back to early giving what we can.</p>
<p>So, you know, we&rsquo;re setting this up, me and Toby.</p>
<p>And I really thought that doing this would be a big hit to my</p>
<p>academic career because I was now spending, you know, at that time</p>
<p>more than half my time setting up this nonprofit at the crucial</p>
<p>time when you should be like producing your best academic work</p>
<p>and so on.</p>
<p>And it was also the case at the time.</p>
<p>It was kind of like the Toby order club.</p>
<p>You know, he was he was the most popular.</p>
<p>There&rsquo;s this personal interest story about him and his plans</p>
<p>donate and sorry to interrupt but Toby was donating a large</p>
<p>amount. Can you tell just briefly what he was doing?</p>
<p>Yeah, so he made this public commitment to give everything</p>
<p>he earned above 20,000 pounds per year to the most effective</p>
<p>causes. And even as a graduate student, he was still donating</p>
<p>about 15, 20% of his income, which is so quite significant</p>
<p>given that graduate students are not known for being super</p>
<p>wealthy.</p>
<p>That&rsquo;s right. And when we launched Giving What We Can, the</p>
<p>media just loved this as like a personal interest story.</p>
<p>So the story about him and his pledge was the most, yeah, it</p>
<p>was actually the most popular news story of the day.</p>
<p>And we kind of ran the same story a year later and it was</p>
<p>the most popular news story of the day a year later too.</p>
<p>And so it really was kind of several years before then I</p>
<p>was also kind of giving more talks and starting to do more</p>
<p>writing and then especially with, you know, I wrote this book</p>
<p>Doing Good Better that then there started to be kind of attention</p>
<p>and so on. But deep inside your own relationship with effective</p>
<p>altruism was, I mean, it had nothing to do with the publicity.</p>
<p>Did you see yourself?</p>
<p>How did the publicity connect with it?</p>
<p>Yeah, I mean, that&rsquo;s kind of what I&rsquo;m saying is I think the</p>
<p>publicity came like several years afterwards.</p>
<p>I mean, at the early stage when we set up Giving What We Can,</p>
<p>it was really just every person we get to pledge 10% is, you</p>
<p>know, something like $100,000 over their lifetime.</p>
<p>That&rsquo;s huge.</p>
<p>And so it was just we had started with 23 members, every single</p>
<p>person was just this like kind of huge accomplishment.</p>
<p>And at the time, I just really thought, you know, maybe over</p>
<p>time we&rsquo;ll have a hundred members and that&rsquo;ll be like amazing.</p>
<p>Whereas now we have, you know, over four thousand and one and</p>
<p>a half billion dollars pledged.</p>
<p>That&rsquo;s just unimaginable to me at the time when I was first kind</p>
<p>of getting this, you know, getting the stuff off the ground.</p>
<p>So can we talk about poverty and the biggest problems that you</p>
<p>think in the near term effective altruism can attack in each</p>
<p>one. So poverty obviously is a huge one.</p>
<p>Yeah. How can we help?</p>
<p>Great.</p>
<p>Yeah.</p>
<p>So poverty, absolutely this huge problem.</p>
<p>700 million people in extreme poverty living in less than two</p>
<p>dollars per day where that&rsquo;s what that means is what two dollars</p>
<p>would buy in the US.</p>
<p>So think about that.</p>
<p>It&rsquo;s like some rice, maybe some beans.</p>
<p>It&rsquo;s very, you know, really not much.</p>
<p>And at the same time, we can do an enormous amount to improve</p>
<p>the lives of people in extreme poverty.</p>
<p>So the things that we tend to focus on interventions in global</p>
<p>health and that&rsquo;s for a couple of few reasons.</p>
<p>One is like global health just has this amazing track record</p>
<p>life expectancy globally is up 50% relative to 60 or 70 years</p>
<p>ago. We&rsquo;ve eradicated smallpox that&rsquo;s which killed 2 million</p>
<p>lives every year almost eradicated polio.</p>
<p>Second is that we just have great data on what works when it</p>
<p>comes to global health.</p>
<p>So we just know that bed nets protect children from prevent</p>
<p>them from dying from malaria.</p>
<p>And then the third is just that&rsquo;s extremely cost effective.</p>
<p>So it costs $5 to buy one bed net, protects two children for</p>
<p>two years against malaria.</p>
<p>If you spend about $3,000 on bed nets, then statistically</p>
<p>speaking, you&rsquo;re going to save a child&rsquo;s life.</p>
<p>And there are other interventions too.</p>
<p>And so given the people in such suffering and we have this</p>
<p>opportunity to, you know, do such huge good for such low cost.</p>
<p>Well, yeah, why not?</p>
<p>So the individual.</p>
<p>So for me today, if I wanted to look at poverty, how would</p>
<p>I help? And I wanted to say, I think donating 10% of your</p>
<p>income is a very interesting idea or some percentage or some</p>
<p>setting a bar and sort of sticking to it.</p>
<p>How do we then take the step towards the effective part?</p>
<p>So you&rsquo;ve conveyed some notions, but who do you give the</p>
<p>money to? Yeah.</p>
<p>So GiveWell, this organization I mentioned, well, it makes</p>
<p>charity recommendations and some of its top recommendations.</p>
<p>So Against Malaria Foundation is this organization that buys</p>
<p>and distributes these insecticide seeded bed nets.</p>
<p>And then it has a total of seven charities that it recommends</p>
<p>very highly. So that recommendation, is it almost like a star</p>
<p>of approval or is there some metrics?</p>
<p>So what are the ways that GiveWell conveys that this is a</p>
<p>great charity organization?</p>
<p>Yeah.</p>
<p>So GiveWell is looking at metrics and it&rsquo;s trying to compare</p>
<p>charities ultimately in the number of lives that you can save</p>
<p>or an equivalent benefit.</p>
<p>So one of the charities it recommends is GiveDirectly, which</p>
<p>simply just transfers cash to the poorest families where poor</p>
<p>family will get a cash transfer of $1,000 and they kind of</p>
<p>regard that as the baseline intervention because it&rsquo;s so simple</p>
<p>and people, you know, they know what to do with how to benefit</p>
<p>themselves. That&rsquo;s quite powerful, by the way.</p>
<p>So before GiveWell, before the Effective Altruism Movement, was</p>
<p>there, I imagine there&rsquo;s a huge amount of corruption, funny</p>
<p>enough, in charity organizations or misuse of money.</p>
<p>Yeah.</p>
<p>So there was nothing like GiveWell before that?</p>
<p>No.</p>
<p>I mean, there were some.</p>
<p>So, I mean, the charity corruption, I mean, obviously</p>
<p>there&rsquo;s some, I don&rsquo;t think it&rsquo;s a huge issue.</p>
<p>They&rsquo;re also just focusing on the long things. Prior to GiveWell,</p>
<p>there were some organizations like Charity Navigator, which</p>
<p>were more aimed at worrying about corruption and so on.</p>
<p>So they weren&rsquo;t saying, these are the charities where you&rsquo;re</p>
<p>going to do the most good. Instead, it was like, how good</p>
<p>are the charities financials?</p>
<p>How good is its health?</p>
<p>Are they transparent? And yeah, so that would be more useful</p>
<p>for weeding out some of those worst charities.</p>
<p>So GiveWell has just taken a step further, sort of in this</p>
<p>21st century of data.</p>
<p>It&rsquo;s actually looking at the effective part.</p>
<p>Yeah. So it&rsquo;s like, you know, if you know the wire cutter for</p>
<p>if you want to buy a pair of headphones, they will just look</p>
<p>at all the headphones and be like, these are the best headphones</p>
<p>you can buy.</p>
<p>That&rsquo;s the idea with GiveWell.</p>
<p>Okay.</p>
<p>So do you think there&rsquo;s a bar of what suffering is?</p>
<p>And do you think one day we can eradicate suffering in our</p>
<p>world? Yeah.</p>
<p>Amongst humans?</p>
<p>Let&rsquo;s talk humans for now. Talk humans.</p>
<p>But in general, yeah, actually.</p>
<p>So there&rsquo;s a colleague of mine calling the term abolitionism</p>
<p>for the idea that we should just be trying to abolish</p>
<p>suffering. And in the long run, I mean, I don&rsquo;t expect to</p>
<p>anytime soon, but I think we can.</p>
<p>I think that would require, you know, quite change, quite</p>
<p>drastic changes to the way society is structured and perhaps</p>
<p>even the, you know, the human, in fact, even changes to human</p>
<p>nature. But I do think that suffering whenever it occurs</p>
<p>is bad and we should want it to not occur.</p>
<p>So there&rsquo;s a line.</p>
<p>There&rsquo;s a gray area between suffering.</p>
<p>Now I&rsquo;m Russian.</p>
<p>So I romanticize some aspects of suffering.</p>
<p>There&rsquo;s a gray line between struggle, gray area between</p>
<p>struggle and suffering.</p>
<p>So one, do we want to eradicate all struggle in the world?</p>
<p>So there&rsquo;s an idea, you know, that the human condition</p>
<p>inherently has suffering in it and it&rsquo;s a creative force.</p>
<p>It&rsquo;s a struggle of our lives and we somehow grow from that.</p>
<p>How do you think about, how do you think about that?</p>
<p>I agree that&rsquo;s true.</p>
<p>So, you know, often, you know, great artists can be also</p>
<p>suffering from, you know, major health conditions or depression</p>
<p>and so on. They come from abusive parents.</p>
<p>Most great artists, I think, come from abusive parents.</p>
<p>Yeah, that seems to be at least commonly the case, but I</p>
<p>want to distinguish between suffering as being instrumentally</p>
<p>good, you know, it causes people to produce good things and</p>
<p>whether it&rsquo;s intrinsically good and I think intrinsically</p>
<p>it&rsquo;s always bad.</p>
<p>And so if we can produce these, you know, great achievements</p>
<p>via some other means where, you know, if we look at the</p>
<p>scientific enterprise, we&rsquo;ve produced incredible things</p>
<p>often from people who aren&rsquo;t suffering, have, you know,</p>
<p>pretty good lives.</p>
<p>They&rsquo;re just, they&rsquo;re driven instead of, you know, being</p>
<p>pushed by a certain sort of anguish.</p>
<p>They&rsquo;re being driven by intellectual curiosity.</p>
<p>If we can instead produce a society where it&rsquo;s all cavet</p>
<p>and no stick, that&rsquo;s better from my perspective.</p>
<p>Yeah, but I&rsquo;m going to disagree with the notion that that&rsquo;s</p>
<p>possible, but I would say most of the suffering in the world</p>
<p>is not productive.</p>
<p>So I would dream of effective altruism curing that suffering.</p>
<p>Yeah, but then I would say that there is some suffering that</p>
<p>is productive that we want to keep the because but that&rsquo;s</p>
<p>not even the focus of because most of the suffering is just</p>
<p>absurd and needs to be eliminated.</p>
<p>So let&rsquo;s not even romanticize this usual notion I have,</p>
<p>but nevertheless struggle has some kind of inherent value</p>
<p>that to me at least, you&rsquo;re right.</p>
<p>There&rsquo;s some elements of human nature that also have to</p>
<p>be modified in order to cure all suffering.</p>
<p>Yeah, I mean, there&rsquo;s an interesting question of whether</p>
<p>it&rsquo;s possible.</p>
<p>So at the moment, you know, most of the time we&rsquo;re kind</p>
<p>of neutral and then we burn ourselves and that&rsquo;s negative</p>
<p>and that&rsquo;s really good that we get that negative signal</p>
<p>because it means we won&rsquo;t burn ourselves again.</p>
<p>There&rsquo;s a question like could you design agents humans such</p>
<p>that you&rsquo;re not hovering around the zero level you&rsquo;re hovering</p>
<p>it like bliss.</p>
<p>Yeah, and then you touch the flame and you&rsquo;re like, oh no,</p>
<p>you&rsquo;re just slightly worse bliss.</p>
<p>Yeah, but that&rsquo;s really bad compared to the bliss you</p>
<p>were normally in so that you can have like a gradient of</p>
<p>bliss instead of like pain and pleasure on that point.</p>
<p>I think it&rsquo;s a really important point on the experience</p>
<p>of suffering the relative nature of it.</p>
<p>Maybe having grown up in the Soviet Union were quite poor</p>
<p>by any measure and when I when I was in my childhood,</p>
<p>but it didn&rsquo;t feel like you&rsquo;re poor because everybody around</p>
<p>you were poor there&rsquo;s a and then in America, I feel I for</p>
<p>the first time begin to feel poor.</p>
<p>Yeah.</p>
<p>Yeah, because of the road there&rsquo;s different.</p>
<p>There&rsquo;s some cultural aspects to it that really emphasize</p>
<p>that it&rsquo;s good to be rich.</p>
<p>And then there&rsquo;s just the notion that there is a lot of</p>
<p>income inequality and therefore you experience that inequality.</p>
<p>That&rsquo;s where suffering go.</p>
<p>Do you so what do you think about the inequality of suffering</p>
<p>that that we have to think about do you think we have to</p>
<p>think about that as part of effective altruism?</p>
<p>Yeah, I think we&rsquo;re just things vary in terms of whether</p>
<p>you get benefits or costs from them just in relative terms</p>
<p>or in absolute terms.</p>
<p>So a lot of the time yeah, there&rsquo;s this hedonic treadmill</p>
<p>where if you get you know, there&rsquo;s money is useful because</p>
<p>it helps you buy things or good for you because it helps</p>
<p>you buy things, but there&rsquo;s also a status component too</p>
<p>and that status component is kind of zero sum if you were</p>
<p>saying like in Russia, you know, no one else felt poor</p>
<p>because everyone around you is poor.</p>
<p>Whereas now you&rsquo;ve got this these other people who are</p>
<p>you know super rich and maybe that makes you feel.</p>
<p>You know less good about yourself.</p>
<p>There are some other things however, which are just</p>
<p>intrinsically good or bad.</p>
<p>So commuting for example, it&rsquo;s just people hate it.</p>
<p>It doesn&rsquo;t really change knowing the other people are</p>
<p>commuting to doesn&rsquo;t make it any any kind of less bad,</p>
<p>but it&rsquo;s sort of to push back on that for a second.</p>
<p>I mean, yes, but also if some people were, you know on</p>
<p>horseback your commute on the train might feel a lot better.</p>
<p>Yeah, you know the there is a relative Nick.</p>
<p>I mean everybody&rsquo;s complaining about society today forgetting</p>
<p>it&rsquo;s forgetting how much better is the better angels of</p>
<p>our nature how the technologies improve fundamentally</p>
<p>improving most of the world&rsquo;s lives.</p>
<p>Yeah, and actually there&rsquo;s some psychological research</p>
<p>on the well being benefits of volunteering where people</p>
<p>who volunteer tend to just feel happier about their lives</p>
<p>and one of the suggested explanations is it because it</p>
<p>extends your reference class.</p>
<p>So no longer you comparing yourself to the Joneses who</p>
<p>have their slightly better car because you realize that</p>
<p>you know people in much worse conditions than you and</p>
<p>so now, you know your life doesn&rsquo;t seem so bad.</p>
<p>That&rsquo;s actually on the psychological level.</p>
<p>One of the fundamental benefits of effective altruism.</p>
<p>Yeah is is I mean, I guess it&rsquo;s the altruism part of</p>
<p>effective altruism is exposing yourself to the suffering</p>
<p>in the world allows you to be more.</p>
<p>Yeah happier and actually allows you in the sort of</p>
<p>meditative introspective way realize that you don&rsquo;t need</p>
<p>most of the wealth you have to to be happy.</p>
<p>Absolutely.</p>
<p>I mean, I think effective options have been this huge</p>
<p>benefit for me and I really don&rsquo;t think that if I had</p>
<p>more money that I was living on that that would change</p>
<p>my level of well being at all.</p>
<p>Whereas engaging in something that I think is meaningful</p>
<p>that I think is stealing humanity in a positive direction.</p>
<p>That&rsquo;s extremely rewarding.</p>
<p>And so yeah, I mean despite my best attempts at sacrifice.</p>
<p>Um, I don&rsquo;t you know, I think I&rsquo;ve actually ended up</p>
<p>happier as a result of engaging in effective altruism</p>
<p>than I would have done.</p>
<p>That&rsquo;s such an interesting idea.</p>
<p>Yeah, so let&rsquo;s let&rsquo;s talk about animal welfare.</p>
<p>Sure, easy question. What is consciousness?</p>
<p>Yeah, especially as it has to do with the capacity to</p>
<p>suffer. I think there seems to be a connection between</p>
<p>how conscious something is the amount of consciousness</p>
<p>and stability to suffer and that all comes into play</p>
<p>about us thinking how much suffering there&rsquo;s in the</p>
<p>world with regard to animals.</p>
<p>So how do you think about animal welfare and consciousness?</p>
<p>Okay.</p>
<p>Well consciousness easy question.</p>
<p>Okay.</p>
<p>Um, yeah, I mean, I think we don&rsquo;t have a good understanding</p>
<p>of consciousness.</p>
<p>My best guess is it&rsquo;s got and by consciousness.</p>
<p>I&rsquo;m meaning what it is feels like to be you the subjective</p>
<p>experience that&rsquo;s seems to be different from everything</p>
<p>else we know about in the world.</p>
<p>Yeah, I think it&rsquo;s clear.</p>
<p>It&rsquo;s very poorly understood at the moment.</p>
<p>I think it has something to do with information processing.</p>
<p>So the fact that the brain is a computer or something</p>
<p>like a computer.</p>
<p>So that would mean that very advanced AI could be conscious</p>
<p>of information processors in general could be conscious</p>
<p>with some suitable complexity, but that also some suitable</p>
<p>complexity.</p>
<p>It&rsquo;s a question whether greater complexity creates some</p>
<p>kind of greater consciousness which relates to animals.</p>
<p>Yeah, right.</p>
<p>Is there if it&rsquo;s an information processing system and it&rsquo;s</p>
<p>smaller and smaller is an ant less conscious than a cow</p>
<p>less conscious than a monkey.</p>
<p>Yeah, and again this super hard question, but I think my</p>
<p>best guess is yes, like if you if I think well consciousness,</p>
<p>it&rsquo;s not some magical thing that appears out of nowhere.</p>
<p>It&rsquo;s not you know, Descartes thought it was just comes in</p>
<p>from this other realm and then enters through the pineal</p>
<p>gland in your brain and that&rsquo;s kind of soul and it&rsquo;s conscious.</p>
<p>So it&rsquo;s got something to do with what&rsquo;s going on in your</p>
<p>brain.</p>
<p>A chicken has one three hundredth of the size of the brain</p>
<p>that you have ants.</p>
<p>I don&rsquo;t know how small it is.</p>
<p>Maybe it&rsquo;s a millionth the size my best guess which I may</p>
<p>well be wrong about because this is so hard is that in some</p>
<p>relevant sense the chicken is experiencing consciousness</p>
<p>to a less degree than the human and the ants significantly</p>
<p>less again.</p>
<p>I don&rsquo;t think it&rsquo;s as little as three hundredth as much.</p>
<p>I think there&rsquo;s everyone who&rsquo;s ever seen a chicken that&rsquo;s</p>
<p>there&rsquo;s evolutionary reasons for thinking that like the</p>
<p>ability to feel pain comes on the scene relatively early</p>
<p>on and we have lots of our brain that&rsquo;s dedicated stuff</p>
<p>that doesn&rsquo;t seem to have to do in anything to do with</p>
<p>consciousness language processing and so on.</p>
<p>So it seems like the easy so there&rsquo;s a lot of complicated</p>
<p>questions there that we can&rsquo;t ask the animals about but</p>
<p>it seems that there is easy questions in terms of suffering</p>
<p>which is things like factory farming that could be addressed.</p>
<p>Yeah, is that is that the lowest hanging fruit?</p>
<p>If I may use crude terms here of animal welfare.</p>
<p>Absolutely.</p>
<p>I think that&rsquo;s the lowest hanging fruit.</p>
<p>So at the moment we kill we raise and kill about 50 billion</p>
<p>animals every year.</p>
<p>So how many 50 billion in?</p>
<p>Yeah, so for every human on the planet several times that</p>
<p>number of being killed and the vast majority of them are</p>
<p>raised in factory farms where basically whatever your view</p>
<p>on animals, I think you should agree even if you think well,</p>
<p>maybe it&rsquo;s not bad to kill an animal.</p>
<p>Maybe if the animal was raised in good conditions, that&rsquo;s</p>
<p>just not the empirical reality.</p>
<p>The empirical reality is that they are kept in incredible</p>
<p>cage confinement.</p>
<p>They are de beaked or detailed without an aesthetic, you</p>
<p>know chickens often peck each other to death other like</p>
<p>otherwise because of them such stress.</p>
<p>It&rsquo;s really, you know, I think when a chicken gets killed</p>
<p>that&rsquo;s the best thing that happened to the chicken in the</p>
<p>course of its life and it&rsquo;s also completely unnecessary.</p>
<p>This is in order to save, you know a few pence for the price</p>
<p>of meat or price of eggs and we have indeed found it&rsquo;s also</p>
<p>just inconsistent with consumer preference as well people</p>
<p>who buy the products if they could they all they when you</p>
<p>do surveys are extremely against suffering in factory farms.</p>
<p>It&rsquo;s just they don&rsquo;t appreciate how bad it is and you know,</p>
<p>just tend to go with easy options.</p>
<p>And so then the best the most effective programs I know of</p>
<p>at the moment are nonprofits that go to companies and work</p>
<p>with companies to get them to take a pledge to cut certain</p>
<p>sorts of animal products like eggs from cage confinement</p>
<p>out of their supply chain.</p>
<p>And it&rsquo;s now the case that the top 50 food retailers and</p>
<p>fast food companies have all made these kind of cage free</p>
<p>pledges and when you do the numbers you get the conclusion</p>
<p>that every dollar you&rsquo;re giving to these nonprofits result</p>
<p>in hundreds of chickens being spared from cage confinement.</p>
<p>And then they&rsquo;re working to other other types of animals</p>
<p>other products too.</p>
<p>So is that the most effective way to do in have a ripple</p>
<p>effect essentially it&rsquo;s supposed to directly having regulation</p>
<p>from on top that says you can&rsquo;t do this.</p>
<p>So I would be more open to the regulation approach, but</p>
<p>at least in the US there&rsquo;s quite intense regulatory capture</p>
<p>from the agricultural industry.</p>
<p>And so attempts that we&rsquo;ve seen to try and change regulation</p>
<p>have it&rsquo;s been a real uphill struggle.</p>
<p>There are some examples of ballot initiatives where the</p>
<p>people have been able to vote in a ballot to say we want</p>
<p>to ban eggs from cage conditions and that&rsquo;s been huge.</p>
<p>That&rsquo;s been really good, but beyond that it&rsquo;s much more</p>
<p>limited. So I&rsquo;ve been really interested in the idea of</p>
<p>hunting in general and wild animals and seeing nature as</p>
<p>a form of cruelty that I am ethically more okay with.</p>
<p>Okay, just from my perspective and then I read about wild</p>
<p>animal suffering that I&rsquo;m just I&rsquo;m just giving you the</p>
<p>kind of yeah notion of how I felt because animal because</p>
<p>animal factory farming is so bad.</p>
<p>Yeah that living in the woods seem good.</p>
<p>Yeah, and yet when you actually start to think about it</p>
<p>all I mean all of the animals in the animal world the</p>
<p>living in like terrible poverty, right?</p>
<p>Yeah.</p>
<p>Yeah, so you have all the medical conditions all of that.</p>
<p>I mean they&rsquo;re living horrible lives.</p>
<p>It could be improved.</p>
<p>That&rsquo;s a really interesting notion that I think may not</p>
<p>even be useful to talk about because factory farming is</p>
<p>such a big thing to focus on.</p>
<p>Yeah, but it&rsquo;s nevertheless an interesting notion to think</p>
<p>of all the animals in the wild as suffering in the same</p>
<p>way that humans in poverty are suffering.</p>
<p>Yeah, I mean and often even worse so many animals we</p>
<p>produce by our selection.</p>
<p>So you have a very large number of children in the expectation</p>
<p>that only a small number survive.</p>
<p>And so for those animals almost all of them just live short</p>
<p>lives where they starve to death.</p>
<p>So yeah, there&rsquo;s huge amounts of suffering in nature that</p>
<p>I don&rsquo;t think we should you know pretend that it&rsquo;s this kind</p>
<p>of wonderful paradise for most animals.</p>
<p>Yeah, their life is filled with hunger and fear and disease.</p>
<p>Yeah, I did agree with you entirely that when it comes</p>
<p>to focusing on animal welfare, we should focus in factory</p>
<p>farming, but we also yeah should be aware to the reality</p>
<p>of what life for most animals is like.</p>
<p>So let&rsquo;s talk about a topic I&rsquo;ve talked a lot about and</p>
<p>you&rsquo;ve actually quite eloquently talked about which is the</p>
<p>third priority that effective altruism considers is really</p>
<p>important is existential risks.</p>
<p>Yeah, when you think about the existential risks that</p>
<p>are facing our civilization, what&rsquo;s before us?</p>
<p>What concerns you?</p>
<p>What should we be thinking about from in the especially</p>
<p>from an effective altruism perspective?</p>
<p>Great. So the reason I started getting concerned about</p>
<p>this was thinking about future generations where the key</p>
<p>idea is just well future people matter morally.</p>
<p>There are vast numbers of future people.</p>
<p>If we don&rsquo;t cause our own extinction, there&rsquo;s no reason</p>
<p>why civilization might not last a million years.</p>
<p>I mean we last as long as a typical mammalian species</p>
<p>or a billion years is when the Earth is no longer habitable</p>
<p>or if we can take to the stars then perhaps it&rsquo;s trillions</p>
<p>of years beyond that.</p>
<p>So the future could be very big indeed and it seems like</p>
<p>we&rsquo;re potentially very early on in civilization.</p>
<p>Then the second idea is just well, maybe there are things</p>
<p>that are going to really derail that things that actually</p>
<p>could prevent us from having this long wonderful civilization</p>
<p>and instead could cause our own cause our own extinction</p>
<p>or otherwise perhaps like lock ourselves into a very bad</p>
<p>state. And what ways could that happen?</p>
<p>Well causing our own extinction development of nuclear</p>
<p>weapons in the 20th century at least put on the table</p>
<p>that we now had weapons that were powerful enough that</p>
<p>you could very significantly destroy society perhaps</p>
<p>and all that nuclear war would cause a nuclear winter.</p>
<p>Perhaps that would be enough for the human race to go</p>
<p>extinct.</p>
<p>Why do you think we haven&rsquo;t done it? Sorry to interrupt.</p>
<p>Why do you think we haven&rsquo;t done it yet?</p>
<p>Is it surprising to you that having, you know, always</p>
<p>for the past few decades several thousand of active ready</p>
<p>to launch nuclear weapons warheads and yet we have not</p>
<p>launched them ever since the initial launch on Hiroshima</p>
<p>and Nagasaki.</p>
<p>I think it&rsquo;s a mix of luck.</p>
<p>So I think it&rsquo;s definitely not inevitable that we haven&rsquo;t</p>
<p>used them.</p>
<p>So John F. Kennedy, general Cuban Missile Crisis put the</p>
<p>estimate of nuclear exchange between the US and USSR</p>
<p>that somewhere between one and three and even so, you know,</p>
<p>we really did come close.</p>
<p>At the same time, I do think mutually assured destruction</p>
<p>is a reason why people don&rsquo;t go to war.</p>
<p>It would be, you know, why nuclear powers don&rsquo;t go to war.</p>
<p>Do you think that holds if you can linger on that for a</p>
<p>second, like my dad is a physicist amongst other things</p>
<p>and he believes that nuclear weapons are actually just</p>
<p>really hard to build which is one of the really big benefits</p>
<p>of them currently so that you don&rsquo;t have it&rsquo;s very hard</p>
<p>if you&rsquo;re crazy to build to acquire a nuclear weapon.</p>
<p>So the mutually shared destruction really works when you</p>
<p>talk seems to work better when it&rsquo;s nation states, when</p>
<p>it&rsquo;s serious people, even if they&rsquo;re a little bit, you</p>
<p>know, dictatorial and so on.</p>
<p>Do you think this mutually sure destruction idea will</p>
<p>carry how far will it carry us in terms of different kinds</p>
<p>of weapons?</p>
<p>Oh, yeah, I think it&rsquo;s your point that nuclear weapons</p>
<p>are very hard to build and relatively easy to control</p>
<p>because you can control fissile material is a really</p>
<p>important one and future technology that&rsquo;s equally destructive</p>
<p>might not have those properties.</p>
<p>So for example, if in the future people are able to design</p>
<p>viruses, perhaps using a DNA printing kit that&rsquo;s on that,</p>
<p>you know, one can just buy.</p>
<p>In fact, there are companies in the process of creating</p>
<p>home DNA printing kits. Well, then perhaps that&rsquo;s just</p>
<p>totally democratized.</p>
<p>Perhaps the power to reap huge destructive potential is</p>
<p>in the hands of most people in the world or certainly</p>
<p>most people with effort and then yeah, I no longer trust</p>
<p>mutually assured destruction because some for some people</p>
<p>the idea that they would die is just not a disincentive.</p>
<p>There was a Japanese cult, for example.</p>
<p>Ohm Shinrikyo in the 90s that had they what they believed</p>
<p>was that Armageddon was coming if you died before Armageddon,</p>
<p>you would get good karma.</p>
<p>You wouldn&rsquo;t go to hell if you died during Armageddon.</p>
<p>Maybe you would go to hell and they had a biological weapons</p>
<p>program chemical weapons program when they were finally</p>
<p>apprehended.</p>
<p>They hadn&rsquo;t stocks of southern gas that were sufficient to</p>
<p>kill 4 million people engaged in multiple terrorist acts.</p>
<p>If they had had the ability to print a virus at home,</p>
<p>that would have been very scary.</p>
<p>So it&rsquo;s not impossible to imagine groups of people that</p>
<p>hold that kind of belief of death as suicide as a good</p>
<p>thing for passage into the next world and so on and then</p>
<p>connect them with some weapons then ideology and weaponry</p>
<p>may create serious problems for us.</p>
<p>Let me ask you a quick question on what do you think is</p>
<p>the line between killing most humans and killing all humans?</p>
<p>How hard is it to kill everybody?</p>
<p>Yeah, have you thought about this?</p>
<p>I&rsquo;ve thought about it a bit.</p>
<p>I think it is very hard to kill everybody.</p>
<p>So in the case of let&rsquo;s say an all out nuclear exchange</p>
<p>and let&rsquo;s say that leads to nuclear winter.</p>
<p>We don&rsquo;t really know but we you know might well happen</p>
<p>that would I think result in billions of deaths would</p>
<p>it kill everybody?</p>
<p>It&rsquo;s quite it&rsquo;s quite hard to see how that how it would</p>
<p>kill everybody for a few reasons.</p>
<p>One is just those are so many people.</p>
<p>Yes, you know seven and a half billion people.</p>
<p>So this bad event has to kill all you know, all almost</p>
<p>all of them.</p>
<p>Secondly live in such a diversity of locations.</p>
<p>So a nuclear exchange or the virus that has to kill people</p>
<p>who live in the coast of New Zealand which is going to</p>
<p>be climatically much more stable than other areas in the</p>
<p>world or people who are on submarines or who have access</p>
<p>to bunkers.</p>
<p>So there&rsquo;s a very like there&rsquo;s just like I&rsquo;m sure there&rsquo;s</p>
<p>like two guys in Siberia just badass.</p>
<p>There&rsquo;s the just human nature somehow just perseveres.</p>
<p>Yeah, and then the second thing is just if there&rsquo;s some</p>
<p>catastrophic event people really don&rsquo;t want to die.</p>
<p>So there&rsquo;s going to be like, you know, huge amounts of</p>
<p>effort to ensure that it doesn&rsquo;t affect everyone.</p>
<p>Have you thought about what it takes to rebuild a society</p>
<p>with smaller smaller numbers like how big of a setback</p>
<p>these kinds of things are?</p>
<p>Yeah, so then that&rsquo;s something where there&rsquo;s real uncertainty</p>
<p>I think where at some point you just lose genetic sufficient</p>
<p>genetic diversity such that you can&rsquo;t come back.</p>
<p>There&rsquo;s it&rsquo;s unclear how small that population is.</p>
<p>But if you&rsquo;ve only got say a thousand people or fewer</p>
<p>than a thousand, then maybe that&rsquo;s small enough.</p>
<p>What about human knowledge and then there&rsquo;s human knowledge.</p>
<p>I mean, it&rsquo;s striking how short on geological timescales</p>
<p>or evolutionary timescales the progress in or how quickly</p>
<p>the progress in human knowledge has been like agriculture.</p>
<p>We only invented in 10,000 BC cities were only, you know,</p>
<p>3000 BC whereas typical mammal species is half a million</p>
<p>years to a million years.</p>
<p>Do you think it&rsquo;s inevitable in some sense agriculture</p>
<p>everything that came the Industrial Revolution cars planes</p>
<p>the internet that level of innovation you think is inevitable.</p>
<p>I think given how quickly it arose.</p>
<p>So in the case of agriculture, I think that was dependent</p>
<p>on climate.</p>
<p>So it was the kind of glacial period was over the earth</p>
<p>warmed up a bit that made it much more likely that humans</p>
<p>would develop agriculture when it comes to the Industrial</p>
<p>Revolution. It&rsquo;s just you know, again only took a few thousand</p>
<p>years from cities to Industrial Revolution if we think okay,</p>
<p>we&rsquo;ve gone back to this even let&rsquo;s say agricultural era,</p>
<p>but there&rsquo;s no reason why we wouldn&rsquo;t go extinct in the</p>
<p>coming tens of thousands of years or hundreds of thousands</p>
<p>of years.</p>
<p>It seems just vet.</p>
<p>It would be very surprising if we didn&rsquo;t rebound unless</p>
<p>there&rsquo;s some special reason that makes things different.</p>
<p>Yes.</p>
<p>So perhaps we just have a much greater like disease burden</p>
<p>now so HIV exists.</p>
<p>It didn&rsquo;t exist before and perhaps that&rsquo;s kind of latent</p>
<p>and you know and being suppressed by modern medicine</p>
<p>and sanitation and so on but would be a much bigger problem</p>
<p>for some, you know, utterly destroyed the society that</p>
<p>was trying to rebound or there&rsquo;s just maybe there&rsquo;s something</p>
<p>we don&rsquo;t know about.</p>
<p>So another existential risk comes from the mysterious the</p>
<p>beautiful artificial intelligence.</p>
<p>Yeah.</p>
<p>So what what&rsquo;s the shape of your concerns about AI?</p>
<p>I think there are quite a lot of concerns about AI and</p>
<p>sometimes the different risks don&rsquo;t get distinguished enough.</p>
<p>So the kind of classic worry most is closely associated</p>
<p>with Nick Bostrom and Elias Joukowski is that we at some</p>
<p>point move from having narrow AI systems to artificial</p>
<p>general intelligence.</p>
<p>You get this very fast feedback effect where AGI is able</p>
<p>to build, you know, artificial intelligence helps you to</p>
<p>build greater artificial intelligence.</p>
<p>We have this one system that suddenly very powerful far</p>
<p>more powerful than others than perhaps far more powerful</p>
<p>than, you know, the rest of the world combined and then</p>
<p>secondly, it has goals that are misaligned with human goals.</p>
<p>And so it pursues its own goals.</p>
<p>It realize, hey, there&rsquo;s this competition namely from humans.</p>
<p>It would be better if we eliminated them in just the same</p>
<p>way as homo sapiens eradicated the Neanderthals.</p>
<p>In fact, it in fact killed off most large animals on the</p>
<p>planet that walk the planet. So that&rsquo;s kind of one set of</p>
<p>worries. I think that&rsquo;s not my I think these shouldn&rsquo;t</p>
<p>be dismissed as science fiction.</p>
<p>I think it&rsquo;s something we should be taking very seriously,</p>
<p>but it&rsquo;s not the thing you visualize when you&rsquo;re concerned</p>
<p>about the biggest near term.</p>
<p>Yeah, I think it&rsquo;s I think it&rsquo;s like one possible scenario</p>
<p>that would be astronomically bad.</p>
<p>I think that other scenarios that would also be extremely</p>
<p>bad comparably bad that are more likely to occur.</p>
<p>So one is just we are able to control AI.</p>
<p>So we&rsquo;re able to get it to do what we want it to do.</p>
<p>And perhaps there&rsquo;s not like this fast takeoff of AI capabilities</p>
<p>within a single system.</p>
<p>It&rsquo;s distributed across many systems that do somewhat different</p>
<p>things, but you do get very rapid economic and technological</p>
<p>progress as a result that concentrates power into the hands</p>
<p>of a very small number of individuals, perhaps a single</p>
<p>dictator. And secondly, that single individual is or small</p>
<p>group of individuals or single country is then able to like</p>
<p>lock in their values indefinitely via transmitting those</p>
<p>values to artificial systems that have no reason to die</p>
<p>like, you know, their code is copyable.</p>
<p>Perhaps, you know, Donald Trump or Xi Jinping creates their</p>
<p>kind of AI progeny in their own image. And once you have</p>
<p>a system that&rsquo;s once you have a society that&rsquo;s controlled</p>
<p>by AI, you no longer have one of the main drivers of change</p>
<p>historically, which is the fact that human lifespans are</p>
<p>you know, only a hundred years give or take.</p>
<p>So that&rsquo;s really interesting.</p>
<p>So as opposed to sort of killing off all humans is locking</p>
<p>in creating a hell on earth, basically a set of principles</p>
<p>under which the society operates that&rsquo;s extremely undesirable.</p>
<p>So everybody is suffering indefinitely.</p>
<p>Or it doesn&rsquo;t, I mean, it also doesn&rsquo;t need to be hell on</p>
<p>earth. It could just be the long values.</p>
<p>So we talked at the very beginning about how I want to</p>
<p>see this kind of diversity of different values and exploration</p>
<p>so that we can just work out what is kind of morally like</p>
<p>what is good, what is bad and then pursue the thing that&rsquo;s</p>
<p>bad. So actually, so the idea of wrong values is actually</p>
<p>probably the beautiful thing is there&rsquo;s no such thing as</p>
<p>right and wrong values because we don&rsquo;t know the right</p>
<p>answer. We just kind of have a sense of which value is more</p>
<p>right, which is more wrong.</p>
<p>So any kind of lock in makes a value wrong because it</p>
<p>prevents exploration of this kind.</p>
<p>Yeah, and just, you know, imagine if fascist value, you</p>
<p>know, imagine if there was Hitler&rsquo;s utopia or Stalin&rsquo;s utopia</p>
<p>or Donald Trump&rsquo;s or Xi Jinping&rsquo;s forever.</p>
<p>Yeah, you know, how good or bad would that be compared</p>
<p>to the best possible future we could create? And my suggestion</p>
<p>is it would really suck compared to the best possible</p>
<p>future we could create.</p>
<p>And you&rsquo;re just one individual.</p>
<p>There&rsquo;s some individuals for whom Donald Trump is perhaps</p>
<p>the best possible future.</p>
<p>And so that&rsquo;s the whole point of us individuals exploring</p>
<p>the space together.</p>
<p>Exactly.</p>
<p>Yeah, and what&rsquo;s trying to figure out which is the path</p>
<p>that will make America great again.</p>
<p>Yeah, exactly.</p>
<p>So how can effective altruism help?</p>
<p>I mean, this is a really interesting notion they actually</p>
<p>describing of artificial intelligence being used as extremely</p>
<p>powerful technology in the hands of very few potentially</p>
<p>one person to create some very undesirable effect.</p>
<p>So as opposed to AI and again, the source of the undesirableness</p>
<p>there is the human.</p>
<p>Yeah, AI is just a really powerful tool.</p>
<p>So whether it&rsquo;s that or whether AI&rsquo;s AGI just runs away</p>
<p>from us completely.</p>
<p>How as individuals, as people in the effective altruism</p>
<p>movement, how can we think about something like this?</p>
<p>I understand poverty and animal welfare, but this is a far</p>
<p>out incredibly mysterious and difficult problem.</p>
<p>Great.</p>
<p>Well, I think there&rsquo;s three paths as an individual.</p>
<p>So if you&rsquo;re thinking about, you know, career paths you</p>
<p>can pursue.</p>
<p>So one is going down the line of technical AI safety.</p>
<p>So this is most relevant to the kind of AI winning AI taking</p>
<p>over scenarios where this is just technical work on current</p>
<p>machine learning systems often sometimes going more theoretical</p>
<p>to on how we can ensure that an AI is able to learn human</p>
<p>values and able to act in the way that you want it to act.</p>
<p>And that&rsquo;s a pretty mainstream issue and approach in machine</p>
<p>learning today.</p>
<p>So, you know, we definitely need more people doing that.</p>
<p>Second is on the policy side of things, which I think is</p>
<p>even more important at the moment, which is how should developments</p>
<p>in AI be managed on a political level?</p>
<p>How can you ensure that the benefits of AI are very distributed?</p>
<p>It&rsquo;s not being, power isn&rsquo;t being concentrated in the hands</p>
<p>of a small set of individuals.</p>
<p>How do you ensure that there aren&rsquo;t arms races between different</p>
<p>AI companies that might result in them, you know, cutting corners</p>
<p>with respect to safety.</p>
<p>And so there the input as individuals who can have is this.</p>
<p>We&rsquo;re not talking about money.</p>
<p>We&rsquo;re talking about effort.</p>
<p>We&rsquo;re talking about career choices.</p>
<p>We&rsquo;re talking about career choice.</p>
<p>Yeah, but then it is the case that supposing, you know, you&rsquo;re</p>
<p>like, I&rsquo;ve already decided my career.</p>
<p>I&rsquo;m doing something quite different.</p>
<p>You can contribute with money too, where at the Center for Effective</p>
<p>Altruism, we set up the Long Term Future Fund.</p>
<p>So if you go on to effectivealtruism.org, you can donate where</p>
<p>a group of individuals will then work out what&rsquo;s the highest value</p>
<p>place they can donate to work on existential risk issues with</p>
<p>a particular focus on AI.</p>
<p>What&rsquo;s path number three?</p>
<p>This was path number three.</p>
<p>This is donations with the third option I was thinking of.</p>
<p>Okay.</p>
<p>And then, yeah, there are, you can also donate directly to organizations</p>
<p>working on this, like Center for Human Compatible AI at Berkeley,</p>
<p>Future of Humanity Institute at Oxford, or other organizations too.</p>
<p>Does AI keep you up at night?</p>
<p>This kind of concern?</p>
<p>Yeah, it&rsquo;s kind of a mix where I think it&rsquo;s very likely things are</p>
<p>going to go well. I think we&rsquo;re going to be able to solve these</p>
<p>problems. I think that&rsquo;s by far the most likely outcome, at least</p>
<p>over the next.</p>
<p>By far the most likely.</p>
<p>So if you look at all the trajectories running away from our</p>
<p>current moment in the next hundred years, you see AI creating</p>
<p>destructive consequences as a small subset of those possible</p>
<p>trajectories.</p>
<p>Or at least, yeah, kind of eternal, destructive consequences.</p>
<p>I think that being a small subset.</p>
<p>At the same time, it still freaks me out.</p>
<p>I mean, when we&rsquo;re talking about the entire future of civilization,</p>
<p>then small probabilities, you know, 1% probability, that&rsquo;s terrifying.</p>
<p>What do you think about Elon Musk&rsquo;s strong worry that we should</p>
<p>be really concerned about existential risks of AI?</p>
<p>Yeah, I mean, I think, you know, broadly speaking, I think he&rsquo;s</p>
<p>right.</p>
<p>I think if we talked, we would probably have very different</p>
<p>probabilities on how likely it is that we&rsquo;re doomed.</p>
<p>But again, when it comes to talking about the entire future of</p>
<p>civilization, it doesn&rsquo;t really matter if it&rsquo;s 1% or if it&rsquo;s</p>
<p>50%, we ought to be taking every possible safeguard we can to</p>
<p>ensure that things go well rather than poorly.</p>
<p>Last question, if you yourself could eradicate one problem from</p>
<p>the world, what would that problem be?</p>
<p>That&rsquo;s a great question.</p>
<p>I don&rsquo;t know if I&rsquo;m cheating in saying this, but I think the</p>
<p>thing I would most want to change is just the fact that people</p>
<p>don&rsquo;t actually care about ensuring the long run future goes well.</p>
<p>People don&rsquo;t really care about future generations.</p>
<p>They don&rsquo;t think about it.</p>
<p>It&rsquo;s not part of their aims.</p>
<p>In some sense, you&rsquo;re not cheating at all because in speaking</p>
<p>the way you do, in writing the things you&rsquo;re writing, you&rsquo;re</p>
<p>doing, you&rsquo;re addressing exactly this aspect.</p>
<p>Exactly.</p>
<p>That is your input into the effective altruism movement.</p>
<p>So for that, Will, thank you so much.</p>
<p>It&rsquo;s an honor to talk to you.</p>
<p>I really enjoyed it.</p>
<p>Thanks so much for having me on.</p>
<p>If that were the case, we&rsquo;d probably be pretty generous.</p>
<p>Next round&rsquo;s on me, but that&rsquo;s effectively the situation we&rsquo;re</p>
<p>in all the time.</p>
<p>It&rsquo;s like a 99% off sale or buy one get 99 free.</p>
<p>Might be the most amazing deal you&rsquo;ll see in your life.</p>
<p>Thank you for listening and hope to see you next time.</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/english/">English</a>
        
            <a href="/tags/podcast/">Podcast</a>
        
            <a href="/tags/lex-fridman-podcast/">Lex Fridman Podcast</a>
        
    </section>


    </footer>


    
</article>

    

    

<div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064"
         crossorigin="anonymous"></script>
    <ins class="adsbygoogle"
         style="display:block; text-align:center;"
         data-ad-layout="in-article"
         data-ad-format="fluid"
         data-ad-client="ca-pub-9206135835124064"
         data-ad-slot="1055602464"></ins>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>
<aside class="related-content--wrapper">
    <h2 class="section-title">Related content</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/en/1310500372/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500372" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #368 - Eliezer Yudkowsky: Dangers of AI and the End of Human Civilization</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500371/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500371" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #367 - Sam Altman: OpenAI CEO on GPT-4, ChatGPT, and the Future of AI</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500370/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500370" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #366 - Shannon Curry: Johnny Depp &amp; Amber Heard Trial, Marriage, Dating &amp; Love</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500369/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500369" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #365 - Sam Harris: Trump, Pandemic, Twitter, Elon, Bret, IDW, Kanye, AI &amp; UFOs</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500368/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500368" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #364 - Chris Voss: FBI Hostage Negotiator</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2021 - 
        
        2023 SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics
    </section>
    
    <section class="powerby">
        

        As an Amazon Associate I earn from qualifying purchases üõí<br/>

        Built with <a href="https://swiest.com/" target="_blank" rel="noopener">(Ôæâ‚óï„ÉÆ‚óï)Ôæâü™Ñüíûüíñü•∞ across the glüåçüåèüåébe</a> <br />
        
        
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>


<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Serif&family=Noto+Serif+Armenian&family=Noto+Serif+Bengali&family=Noto+Serif+Devanagari&family=Noto+Serif+Georgian&family=Noto+Serif+Gujarati&family=Noto+Serif+HK&family=Noto+Serif+Hebrew&family=Noto+Serif+JP&family=Noto+Serif+KR&family=Noto+Serif+Kannada&family=Noto+Serif+Khmer&family=Noto+Serif+Lao&family=Noto+Serif+Makasar&family=Noto+Serif+Malayalam&family=Noto+Serif+Myanmar&family=Noto+Serif+Oriya&family=Noto+Serif+SC&family=Noto+Serif+Sinhala&family=Noto+Serif+TC&family=Noto+Serif+Tamil&family=Noto+Serif+Telugu&family=Noto+Serif+Thai&family=Noto+Serif+Tibetan&display=swap" rel="stylesheet">

    </body>
</html>

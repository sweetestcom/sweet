<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Video Transcript Ôªøthe first graphic I wanted to share with
you guys was a graphic of reinforcement
learning agent playing an Atari breakout
video game and that&amp;rsquo;s an example of a
case where standard RL can learn in a
certain environment very well but the
problem I&amp;rsquo;ll be focusing on is
specifically a problem with delayed
rewards and this is very relevant to
real life because as you interact with"><title>Long term credit assignment with temporal reward transp‚Ä¶ ÔΩú Cathy Yeh ÔΩú OpenAI Scholars Demo Day 2020 ÔΩú OpenAI | SWIEST</title>
<link rel=canonical href=https://swiest.com/en/jjmtmymset0/><link rel=stylesheet href=/scss/style.min.9a6fe90535a0e5c60443841f100f7b698092d48dba43fdb6386bb69b6559bc3d.css><script>document.oncontextmenu=function(){return!1},document.onselectstart=function(){return!1},document.oncopy=function(){return!1},document.oncut=function(){return!1}</script><script src=https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js></script><script type=text/javascript>$(document).ready(function(){$("#back-to-top").hide(),$(function(){$(window).scroll(function(){$(window).scrollTop()>600?$("#back-to-top").fadeIn(500):$("#back-to-top").fadeOut(500)}),$("#back-to-top").click(function(){return $("body,html").animate({scrollTop:0},500),!1})})})</script><meta property="og:title" content="Long term credit assignment with temporal reward transp‚Ä¶ ÔΩú Cathy Yeh ÔΩú OpenAI Scholars Demo Day 2020 ÔΩú OpenAI"><meta property="og:description" content="Video Transcript Ôªøthe first graphic I wanted to share with
you guys was a graphic of reinforcement
learning agent playing an Atari breakout
video game and that&amp;rsquo;s an example of a
case where standard RL can learn in a
certain environment very well but the
problem I&amp;rsquo;ll be focusing on is
specifically a problem with delayed
rewards and this is very relevant to
real life because as you interact with"><meta property="og:url" content="https://swiest.com/en/jjmtmymset0/"><meta property="og:site_name" content="SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="English"><meta property="article:tag" content="Video Transcripts"><meta property="article:tag" content="OpenAI"><meta property="article:published_time" content="2023-11-06T06:59:11+00:00"><meta property="article:modified_time" content="2023-11-06T06:59:11+00:00"><meta name=twitter:title content="Long term credit assignment with temporal reward transp‚Ä¶ ÔΩú Cathy Yeh ÔΩú OpenAI Scholars Demo Day 2020 ÔΩú OpenAI"><meta name=twitter:description content="Video Transcript Ôªøthe first graphic I wanted to share with
you guys was a graphic of reinforcement
learning agent playing an Atari breakout
video game and that&amp;rsquo;s an example of a
case where standard RL can learn in a
certain environment very well but the
problem I&amp;rsquo;ll be focusing on is
specifically a problem with delayed
rewards and this is very relevant to
real life because as you interact with"><link rel="shortcut icon" href=/favicon.ico><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"dark")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu307e6a33fa6fd661ccda3b77024ef5c2_252345_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics</a></h1><h2 class=site-description>üßôü™Ñüåé</h2></div></header><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><li><a href=/tags/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg><span>Tags</span></a></li><li><a href=/chart/podcastchart.html target=_blank><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-apple-podcast" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M18.364 18.364a9 9 0 10-12.728.0"/><path d="M11.766 22h.468a2 2 0 001.985-1.752l.5-4A2 2 0 0012.734 14h-1.468a2 2 0 00-1.985 2.248l.5 4A2 2 0 0011.766 22z"/><path d="M12 9m-2 0a2 2 0 104 0 2 2 0 10-4 0"/></svg><span>Podcasts</span></a></li><li><a href=/radio.html target=_blank><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-radio" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14 3 4.629 6.749A1 1 0 004 7.677V19a1 1 0 001 1h14a1 1 0 001-1V8a1 1 0 00-1-1H4.5"/><path d="M4 12h16"/><path d="M7 12v-2"/><path d="M17 16v.01"/><path d="M13 16v.01"/></svg><span>Radio</span></a></li><div class=menu-bottom-section><li id=i18n-switch><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg><select name=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://swiest.com/ selected>English</option><option value=https://swiest.com/af/>Afrikaans</option><option value=https://swiest.com/am/>·ä†·àõ·à≠·äõ</option><option value=https://swiest.com/ar/>ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</option><option value=https://swiest.com/az/>Az…ôrbaycan</option><option value=https://swiest.com/be/>–±–µ–ª–∞—Ä—É—Å–∫—ñ</option><option value=https://swiest.com/bg/>–±—ä–ª–≥–∞—Ä—Å–∫–∏</option><option value=https://swiest.com/bn/>‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ</option><option value=https://swiest.com/bo/>‡Ωñ‡Ωº‡Ωë‡ºã‡Ω¶‡æê‡Ωë‡ºã</option><option value=https://swiest.com/bs/>Bosanski</option><option value=https://swiest.com/ca/>Catal√†</option><option value=https://swiest.com/zh-hans/>ÁÆÄ‰Ωì‰∏≠Êñá</option><option value=https://swiest.com/zh-hant/>ÁπÅÈ´î‰∏≠Êñá</option><option value=https://swiest.com/cs/>ƒåe≈°tina</option><option value=https://swiest.com/el/>ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨</option><option value=https://swiest.com/cy/>Cymraeg</option><option value=https://swiest.com/da/>Dansk</option><option value=https://swiest.com/de/>Deutsch</option><option value=https://swiest.com/eo/>Esperanto</option><option value=https://swiest.com/es-es/>Espa√±ol (Espa√±a)</option><option value=https://swiest.com/es-419/>Espa√±ol (Latinoam√©rica)</option><option value=https://swiest.com/et/>Eesti</option><option value=https://swiest.com/eu/>Euskara</option><option value=https://swiest.com/haw/> ª≈ålelo Hawai ªi</option><option value=https://swiest.com/fa/>ŸÅÿßÿ±ÿ≥€å</option><option value=https://swiest.com/fi/>Suomi</option><option value=https://swiest.com/fo/>F√∏royskt</option><option value=https://swiest.com/fr/>Fran√ßais</option><option value=https://swiest.com/fy/>Frysk</option><option value=https://swiest.com/ga/>Gaeilge</option><option value=https://swiest.com/gl/>Galego</option><option value=https://swiest.com/gu/>‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä</option><option value=https://swiest.com/he/>◊¢÷¥◊ë◊®÷¥◊ô◊™</option><option value=https://swiest.com/km/>·ûÄ·ûò·üí·ûñ·ûª·ûá·û∂·üî</option><option value=https://swiest.com/hi/>‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</option><option value=https://swiest.com/hr/>Hrvatski</option><option value=https://swiest.com/ht/>Krey√≤l Ayisyen</option><option value=https://swiest.com/hu/>Magyar</option><option value=https://swiest.com/hy/>’Ä’°’µ’•÷Ä’•’∂</option><option value=https://swiest.com/ig/>√Ås·ª•ÃÄs·ª•ÃÅ √ågb√≤</option><option value=https://swiest.com/id/>Bahasa Indonesia</option><option value=https://swiest.com/is/>√çslenska</option><option value=https://swiest.com/it/>Italiano</option><option value=https://swiest.com/ja/>Êó•Êú¨Ë™û</option><option value=https://swiest.com/jv/>Basa Jawa</option><option value=https://swiest.com/ka/>·É•·Éê·É†·Éó·É£·Éö·Éò</option><option value=https://swiest.com/kk/>“ö–∞–∑–∞“õ—à–∞</option><option value=https://swiest.com/kn/>‡≤ï‡≤®‡≥ç‡≤®‡≤°</option><option value=https://swiest.com/ko/>ÌïúÍµ≠Ïñ¥</option><option value=https://swiest.com/or/>‡¨ì‡¨°‡¨º‡¨ø‡¨Ü</option><option value=https://swiest.com/ckb/>⁄©Ÿàÿ±ÿØ€å</option><option value=https://swiest.com/ky/>–ö—ã—Ä–≥—ã–∑—á–∞</option><option value=https://swiest.com/la/>Latina</option><option value=https://swiest.com/lb/>L√´tzebuergesch</option><option value=https://swiest.com/lo/>‡∫û‡∫≤‡∫™‡∫≤‡∫•‡∫≤‡∫ß</option><option value=https://swiest.com/lt/>Lietuvi≈≥</option><option value=https://swiest.com/lv/>Latvie≈°u</option><option value=https://swiest.com/mk/>–ú–∞–∫–µ–¥–æ–Ω—Å–∫–∏</option><option value=https://swiest.com/ml/>‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç</option><option value=https://swiest.com/mn/>–ú–æ–Ω–≥–æ–ª —Ö—ç–ª</option><option value=https://swiest.com/mr/>‡§Æ‡§∞‡§æ‡§†‡•Ä</option><option value=https://swiest.com/sw/>Kiswahili</option><option value=https://swiest.com/ms/>Bahasa Melayu</option><option value=https://swiest.com/my/>·Äô·Äº·Äî·Ä∫·Äô·Ä¨</option><option value=https://swiest.com/ne/>‡§®‡•á‡§™‡§æ‡§≤‡•Ä</option><option value=https://swiest.com/nl/>Nederlands</option><option value=https://swiest.com/no/>Norsk</option><option value=https://swiest.com/pa/>‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä</option><option value=https://swiest.com/pl/>Polski</option><option value=https://swiest.com/pt-br/>Portugu√™s Brasil</option><option value=https://swiest.com/pt-pt/>Portugu√™s Europeu</option><option value=https://swiest.com/ro/>Rom√¢nƒÉ</option><option value=https://swiest.com/ru/>–†—É—Å—Å–∫–∏–π</option><option value=https://swiest.com/rw/>Kinyarwanda</option><option value=https://swiest.com/si/>‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω</option><option value=https://swiest.com/sk/>Slovenƒçina</option><option value=https://swiest.com/sl/>Sloven≈°ƒçina</option><option value=https://swiest.com/sq/>Shqip</option><option value=https://swiest.com/sr/>–°—Ä–ø—Å–∫–∏ (Srpski)</option><option value=https://swiest.com/su/>Basa Sunda</option><option value=https://swiest.com/sv/>Svenska</option><option value=https://swiest.com/ta/>‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç</option><option value=https://swiest.com/te/>‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</option><option value=https://swiest.com/tg/>–¢–æ“∑–∏–∫”£</option><option value=https://swiest.com/th/>‡πÑ‡∏ó‡∏¢</option><option value=https://swiest.com/tk/>T√ºrkmenler</option><option value=https://swiest.com/tl/>Filipino</option><option value=https://swiest.com/tr/>T√ºrk√ße</option><option value=https://swiest.com/uk/>–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</option><option value=https://swiest.com/ur/>ÿßÿ±ÿØŸà</option><option value=https://swiest.com/uz/>O'zbekcha</option><option value=https://swiest.com/vi/>Ti·∫øng Vi·ªát</option><option value=https://swiest.com/yi/>◊ê◊ô◊ì◊ô◊©</option><option value=https://swiest.com/zh-hk/>Á≤µË™û</option><option value=https://swiest.com/zu/>IsiZulu</option></select></li><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#video>Video</a></li><li><a href=#transcript>Transcript</a></li></ol></nav></div></section><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><ins class=adsbygoogle style=display:block data-ad-client=ca-pub-9206135835124064 data-ad-slot=8754979142 data-ad-format=auto data-full-width-responsive=true></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></aside><a id=back-to-top href=#><img src=/img/top_hu7c2829da96df0e9f8f0191d120020b22_22287_40x0_resize_box_3.png></a><main class="main full-width"><form action=/search/ class="search-form widget"><p><label>Search</label>
<input name=keyword required placeholder="Type something...">
<button title=Search><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg></button></p></form><article class=main-article><header class=article-header><div class=article-details><header class=article-tags><a href=/tags/english/>English
</a><a href=/tags/video-transcripts/>Video Transcripts
</a><a href=/tags/openai/>OpenAI</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/en/jjmtmymset0/>Long term credit assignment with temporal reward transp‚Ä¶ ÔΩú Cathy Yeh ÔΩú OpenAI Scholars Demo Day 2020 ÔΩú OpenAI</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>2023-11-06</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>13 minute read</time></div></footer></div></header><div class=article-content><p style=text-align:center><a href=https://amzn.to/3Nrdcwk target=_blank>üéÅAmazon Prime</a>
<a href=https://amzn.to/3RIBkxg target=_blank>üìñKindle Unlimited</a>
<a href=https://amzn.to/3Rqmudl target=_blank>üéßAudible Plus</a>
<a href=https://amzn.to/3TuLbbj target=_blank>üéµAmazon Music Unlimited</a>
<a href="https://www.iherb.com/?rcode=EID1574" target=_blank>üåøiHerb</a>
<a href="https://accounts.binance.com/register?ref=72302422" target=_blank>üí∞Binance</a></p></div><div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><ins class=adsbygoogle style=display:block data-ad-client=ca-pub-9206135835124064 data-ad-slot=8754979142 data-ad-format=auto data-full-width-responsive=true></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><section class=article-content><h2 id=video>Video</h2><div class=video-wrapper><iframe loading=lazy src=https://www.youtube.com/embed/jjmTmYMsET0 allowfullscreen title="YouTube Video"></iframe></div><h2 id=transcript>Transcript</h2><p>Ôªøthe first graphic I wanted to share with</p><p>you guys was a graphic of reinforcement</p><p>learning agent playing an Atari breakout</p><p>video game and that&rsquo;s an example of a</p><p>case where standard RL can learn in a</p><p>certain environment very well but the</p><p>problem I&rsquo;ll be focusing on is</p><p>specifically a problem with delayed</p><p>rewards and this is very relevant to</p><p>real life because as you interact with</p><p>your environment you don&rsquo;t typically get</p><p>points every time you move there&rsquo;s</p><p>actions that are separated by very long</p><p>timescales from their effects Kathy</p><p>would you like me to help you move your</p><p>sleds yeah what I can&rsquo;t seem to change</p><p>my screen my keyboards not responding to</p><p>that I&rsquo;ll show them</p><p>oh okay thanks Francis and so here&rsquo;s the</p><p>I think I might have it apologies for</p><p>the technical difficulties</p><p>okay can you guys see my screen my</p><p>slides yes okay um so the plan is going</p><p>to be three parts so first I will</p><p>describe to you why standard RL does</p><p>struggle and these tasks with long</p><p>delayed rewards next I will describe the</p><p>temporal reward transport algorithm or</p><p>trt that I&rsquo;ve been working on to address</p><p>this problem and then finally I will</p><p>share some results from experiments</p><p>using TRT okay so um the motto in</p><p>reinforcement learning is you have an</p><p>agent interacting with the world and as</p><p>it interacts it transitions from state</p><p>to state and it can pick up a reward</p><p>along that trajectory and so here I have</p><p>an equation or something called the</p><p>discounted returns and this is just the</p><p>sum of all the rewards that the agent</p><p>picks up along its trajectory um but</p><p>there&rsquo;s this extra bit that&rsquo;s added to</p><p>this returns and it&rsquo;s this discount</p><p>factor gamma and this gets at the crux</p><p>of why standard RL algorithms do</p><p>struggle with tasks with delayed rewards</p><p>the gamma introduces a timescale and</p><p>it&rsquo;s basically a heuristic that says</p><p>that you care about rewards now versus</p><p>later in the future so you&rsquo;re</p><p>discounting your future rewards and so</p><p>in this plot here I have an example if</p><p>you&rsquo;re standing at time zero and you</p><p>look forwards a hundred times steps then</p><p>the reward a hundred time steps in the</p><p>future is discounted by a factor so</p><p>that&rsquo;s about 37 percent of its original</p><p>value and so this is totally fine if</p><p>you&rsquo;re in an environment where your</p><p>immediate actions really just effect the</p><p>the most immediate rewards but in cases</p><p>with long delays then it&rsquo;s not going to</p><p>work as well and so let&rsquo;s take a look at</p><p>that so here&rsquo;s an example we have this</p><p>little guy who is walking through the</p><p>environment and at some point he can</p><p>choose to pick up a key or not</p><p>he doesn&rsquo;t get a reward for picking up</p><p>that key so this agent continues and</p><p>interacting with the environment until</p><p>the very end</p><p>it reaches a green goal where if it did</p><p>pick up the key in that first state then</p><p>its rewarded extra bonus points 20</p><p>points um so the problem with standard</p><p>Ariella though is that it wants to</p><p>reinforce actions based on the rewards</p><p>that were acquired after taking that</p><p>action but if we look at the rewards</p><p>that would weight this particular state</p><p>action pair where he&rsquo;s next to the key</p><p>we&rsquo;d see that the future rewards are</p><p>highly attenuated and so there&rsquo;s a very</p><p>really low signal and learning is very</p><p>slow as a consequence and so what can we</p><p>do about that</p><p>that brings me to the next part of my</p><p>talk which is a the tier T algorithm so</p><p>this algorithm was based on work by hung</p><p>from deep mind on optimizing agent</p><p>behavior of her long time skills by</p><p>transporting value and the idea is that</p><p>if you&rsquo;ve identified the significant</p><p>state action pairs that should receive</p><p>credit for some long-term reward then</p><p>you can splice those distant rewards to</p><p>these state action pairs so that you can</p><p>amplify the signal to reinforce those</p><p>actions and so that&rsquo;s what we see here</p><p>so in this original situation on the</p><p>slide this agent receives zero immediate</p><p>points we&rsquo;re picking up the key but we</p><p>splice in these future rewards in this</p><p>case the distal rewards might be 20</p><p>points suddenly we have a lot more</p><p>signal to amplify to increase the</p><p>probability of taking an action in that</p><p>state which is what we want we want the</p><p>agent to learn to pick up the key so</p><p>that brings us the next question how we</p><p>know what the significant state action</p><p>pairs are in order to receive these</p><p>spliced rewards the TRT on so that is</p><p>the problem of credit assignment in</p><p>green flow and reinforcement learning</p><p>and the way we do that and the way that</p><p>hung did it is using an intention</p><p>mechanism and so the idea is you do a</p><p>full rollout of an episode so the agent</p><p>interacts with environment at the end of</p><p>that rollout you pass the entire</p><p>sequence of states and actions to a</p><p>model in my case a binary classifier and</p><p>you look at</p><p>what state action pairs were paid most</p><p>attention to by the other frames excuse</p><p>me and so here I have a heat map and</p><p>this heat map is a plot of the attention</p><p>scores and you can see there&rsquo;s these two</p><p>really bright stripes oh by the way the</p><p>axes denote on the frames and the</p><p>trajectory you both on the x and y axis</p><p>so in this case there&rsquo;s two really</p><p>bright stripes and they correspond to</p><p>highly attended state in action pairs</p><p>and if we do a sanity check we can</p><p>actually bring up what that particular</p><p>observation was we see that it actually</p><p>makes sense with what we would expect so</p><p>in this case we have an attention and we</p><p>have an a an agent who&rsquo;s a little bread</p><p>triangle next to a key so it might be</p><p>moving forward towards it or trying to</p><p>pick it up so this is a good</p><p>confirmation that we are attending to</p><p>the important states in actions</p><p>so next step is to test out whether this</p><p>TRT algorithm works and so I created an</p><p>environment specifically constructed to</p><p>make it challenging to learn if you</p><p>don&rsquo;t do credits I meant over the long</p><p>timescales in this case this environment</p><p>has three phases first phase the agent</p><p>is encounter as an empty grid with just</p><p>a single key and Aiden can choose to</p><p>pick up that key or not but it doesn&rsquo;t</p><p>receive any immediate reward if it picks</p><p>it up the second phase is a distractor</p><p>phase so we fill this distractor phase</p><p>with gifts and when the agent opens a</p><p>gift it gets immediate rewards and then</p><p>the final phase this is the focus of our</p><p>evaluation is the phase at which the</p><p>agent can earn a distal reward so when</p><p>the agent navigates to the green goal if</p><p>they learn to pick up the key in phase</p><p>one it gets 20 points if it never</p><p>learned to pick up the key it just gets</p><p>five points so that&rsquo;s going to be the</p><p>focus of the the rest of the</p><p>experimental results I&rsquo;m going to show</p><p>you we will focus to see if the agent</p><p>learns to pick up the key and</p><p>correspondingly get the twenty points in</p><p>Phase three</p><p>so the I have three separate</p><p>experimental slides I&rsquo;m going to show</p><p>you</p><p>they all involve varying the parameters</p><p>of the distractor phase essentially</p><p>making it more and more challenging for</p><p>the agent to learn and so the three</p><p>parameters I vary are the time delay</p><p>which is the time that the agent is</p><p>forced to spend in the distractor phase</p><p>the gift reward size for the distractors</p><p>and then also the variance of the</p><p>destructor rewards okay so these plots</p><p>here show the total rewards that were</p><p>earned by the agent in Phase three to</p><p>see whether it picked up the key or not</p><p>and as you move from left to right it</p><p>becomes increasingly difficult each plot</p><p>corresponds to a certain delay and so</p><p>tau of gamma is equal to the discount</p><p>factor time scale and we see increasing</p><p>from left to right and so you can see</p><p>initially when the time delay is not</p><p>that long the agent does learn to pick</p><p>up the key doesn&rsquo;t do quite as well as</p><p>with the trt algorithm on top of the</p><p>advantage actor critic the the baseline</p><p>is a TC advantage actor critic but then</p><p>by the time you get to the rightmost</p><p>plot you can see that ATC has basically</p><p>plateaued at five and that five points</p><p>if you recall corresponds to only moving</p><p>to the goal and never really there I</p><p>need to pick up the key and whereas if</p><p>you had trt that shows consistent</p><p>progress about learning how to pick up</p><p>the key and then the next slide is the</p><p>slide with experimental results for</p><p>varying the destructor award size so</p><p>again left to right it&rsquo;s harder as we</p><p>increase the size of the distractor</p><p>rewards and again we see the same</p><p>pattern a TC with the trt algorithm does</p><p>better than a TC alone and then the</p><p>final slide was an experiment showing</p><p>the last returns in Phase three</p><p>when we vary the variance of the</p><p>distracted rewards so in this case we</p><p>have four gifts and they all have a mean</p><p>reward of five but for each gift we</p><p>sample from a uniform distribution</p><p>around five and we increasingly we</p><p>increase the range of the</p><p>a uniform distribution in order to</p><p>increase the variance and so they all</p><p>have the same being reward but there&rsquo;s</p><p>just greater variance and again you can</p><p>see that ATC plus trt does better than</p><p>ATC alone</p><p>so to summarize we&rsquo;ve seen that adding</p><p>to purl reward transport on top of</p><p>standard reinforcement learning</p><p>algorithms safe to show some benefit for</p><p>long term credit assignment this work</p><p>has built directly on the ideas from the</p><p>hung paper in 2019 and the two core</p><p>concepts to take away from that are the</p><p>idea of using some sort of temporal</p><p>value or reward transport to splice on</p><p>to significant state action pairs and</p><p>then to use attention to identify the</p><p>important state action pairs so our</p><p>contribution here has been a completely</p><p>different architecture implementing</p><p>these two core concepts it&rsquo;s much</p><p>simpler than the original papers</p><p>implementation it&rsquo;s also much simpler</p><p>environment but I think there&rsquo;s</p><p>definitely merit and having showing that</p><p>this concept kind of just carries beyond</p><p>the original implementation and the</p><p>paper the implementation is also very</p><p>modular so I totally separated out the</p><p>attention part of this algorithm into a</p><p>separate classifier in order to identify</p><p>the significant state action pairs and</p><p>so you can imagine if you want to try</p><p>adding trt to some other model you can</p><p>do that it&rsquo;s very easy to add it on</p><p>because of the modular implementation so</p><p>in the future well there&rsquo;s tons of work</p><p>there&rsquo;s never anything where there&rsquo;s</p><p>always more this is just a heuristic and</p><p>so it would be interesting to move</p><p>beyond just a turistic but it&rsquo;s a useful</p><p>heuristic um and also I&rsquo;ve only shown</p><p>you results for a very simple grid world</p><p>environment and so it would be</p><p>interesting to see how well the</p><p>algorithm can hold up in more complex</p><p>situations so with that said I had a lot</p><p>of fun working on this project and so I</p><p>want to move on to the Q&amp;A stage but</p><p>also I want to</p><p>sent out my cops so I&rsquo;d like to thank my</p><p>mentor Jerry at open AI for being with</p><p>me through this whole process I wanted</p><p>to thank opening I itself for this</p><p>wonderful opportunity and all the</p><p>different people I&rsquo;ve talked to informal</p><p>casual conversations I&rsquo;ve picked up so</p><p>much thank you to the program organizers</p><p>Kristina and where I have been really</p><p>wonderful so supportive of the scholars</p><p>and I also have my lovely scholars</p><p>cohort to think they were also very</p><p>supportive there was a lot of knowledge</p><p>sharing as we all ramped up on deep</p><p>learning at the same time finally I&rsquo;d</p><p>like to thank square my employer for</p><p>giving me this chance to take some time</p><p>off to do this program and then if</p><p>you&rsquo;re interested in more project</p><p>details my write-up is available at my</p><p>blog if ab d be calm okay and then now</p><p>I&rsquo;m going to take a look at cushioned</p><p>so the question is can you explain why</p><p>the distractor fitties makes the task</p><p>more difficult and in other words in</p><p>your opinion why does the agent not</p><p>learn the more general behavior of</p><p>simply interacting with all objects</p><p>picking up the key and opening the gifts</p><p>so I think this gets as with standard RL</p><p>algorithms</p><p>there&rsquo;s this idea that for policy</p><p>gradients if you have a policy which is</p><p>how you want to choose an action in a</p><p>particular state you can amplify it or</p><p>reduce the likelihood of doing that</p><p>taking that action based on the rewards</p><p>that follow it and because of</p><p>discounting you won&rsquo;t see rewards in the</p><p>far future and so in this case if you</p><p>are in the key state or in the state in</p><p>the phase one where your connects to the</p><p>key it&rsquo;s not gonna receive that</p><p>amplification in that particular state</p><p>because the rewards in the distractor</p><p>phase are unique to being in the state</p><p>next to those distracting gifts so the</p><p>agent very quickly learns to open the</p><p>gifts because it sees an immediate</p><p>reward from the guess so that that&rsquo;s</p><p>that waiting of the reward is very high</p><p>and so it reinforces this idea we want</p><p>to do the toggle action to open the</p><p>gifts and so it doesn&rsquo;t just translate</p><p>thought of that simply just because</p><p>you&rsquo;ve learned that you have to take</p><p>this particular action in this state the</p><p>way the algorithm is set up it doesn&rsquo;t</p><p>just translate to learning how to do</p><p>that if your next is the key</p><p>okay the next question is I am curious</p><p>to know if using more advanced deep RL</p><p>algorithms like PPO would wait more than</p><p>the TR T&rsquo;s influence and the results I</p><p>guess so I&rsquo;m not 100% sure about this</p><p>wording it sounds like like what would</p><p>happen if I had tried it on PPO instead</p><p>of A to C and that&rsquo;s a straightforward</p><p>test we can do because of the modular</p><p>and presentation on but so PPO is more</p><p>sample efficient than a 2 C it is able</p><p>to do several smaller updates compared</p><p>to one single update by ATC before</p><p>starting your batch of experiences so</p><p>given that I would expect it to have a</p><p>better learning curve I&rsquo;m not sure if</p><p>the interaction with trt would be any</p><p>different but I would expect PPO to to</p><p>fundamentally look a little better than</p><p>the baseline that I showed here</p><p>I just haven&rsquo;t tested it out yet</p><p>what was the most challenging part of</p><p>your project</p><p>oh there&rsquo;s a lot uh well so I think at</p><p>some point when I finally when I decided</p><p>on this particular path I always had a</p><p>bunch of ideas for how to get it working</p><p>and every time I tried something new and</p><p>you commit to get hub I thought maybe</p><p>this will be it and so the the</p><p>challenging part was seeing the the</p><p>deadlines kind of looming and starting</p><p>to realize that all my fixes weren&rsquo;t</p><p>necessary submit necessarily becoming</p><p>like the last fix but then in the end</p><p>things worked out when I had like a</p><p>little I realized there&rsquo;s some artifacts</p><p>that were being introduced due to the</p><p>way this algorithm is set up it&rsquo;s very</p><p>sensitive to the full context of the</p><p>episode and I had to think about how I</p><p>can handle that because it would have</p><p>required quite a bit of reengineering of</p><p>my code with very little time in order</p><p>to do parallel training which was really</p><p>important I needed to run this on many</p><p>workers and so I I had an idea to</p><p>rejigger or something well so I&rsquo;m</p><p>running out of time um but but basically</p><p>it was just pushing through and it kind</p><p>of worked out in the end so I&rsquo;m really</p><p>glad of that</p><p>and then so with that okay thanks</p></section><footer class=article-footer><section class=article-tags><a href=/tags/english/>English</a>
<a href=/tags/video-transcripts/>Video Transcripts</a>
<a href=/tags/openai/>OpenAI</a></section></footer></article><div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><ins class=adsbygoogle style=display:block;text-align:center data-ad-layout=in-article data-ad-format=fluid data-ad-client=ca-pub-9206135835124064 data-ad-slot=1055602464></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/en/at2xkqjazns/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/AT2XkqJAZns data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>Towards Epileptic Seizure Prediction with Deep Network ÔΩú Kata Slama ÔΩú OpenAI Scholars Demo Day 2020 ÔΩú OpenAI</h2></div></a></article><article><a href=/en/jzohw-eybtq/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/JZOHW-eYBtQ data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>Introductions by Sam Altman & Greg Brockman ÔΩú OpenAI Scholars Demo Day 2020 ÔΩú OpenAI</h2></div></a></article><article><a href=/en/-fozam9xqs4/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/-FoZAM9xqS4 data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>OpenAI Five vs. OG, Game 2 ÔΩú OpenAI Five Finals (4‚ß∏6) ÔΩú OpenAI</h2></div></a></article><article><a href=/en/u9mjuukhuzk/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/U9mJuUkhUzk data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>OpenAI DevDay, Opening Keynote ÔΩú OpenAI</h2></div></a></article><article><a href=/en/lpe5gwuqa-k/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/lpe5Gwuqa-k data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>Scaling Laws for Language Transfer Learning ÔΩú Christina Kim ÔΩú OpenAI Scholars Demo Day 2021 ÔΩú OpenAI</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2021 -
2023 SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics</section><section class=powerby>As an Amazon Associate I earn from qualifying purchases üõí<br>Built with <a href=https://swiest.com/ target=_blank rel=noopener>(Ôæâ‚óï„ÉÆ‚óï)Ôæâü™Ñüíûüíñü•∞ across the glüåçüåèüåébe</a><br></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Serif&family=Noto+Serif+Armenian&family=Noto+Serif+Bengali&family=Noto+Serif+Devanagari&family=Noto+Serif+Georgian&family=Noto+Serif+Gujarati&family=Noto+Serif+HK&family=Noto+Serif+Hebrew&family=Noto+Serif+JP&family=Noto+Serif+KR&family=Noto+Serif+Kannada&family=Noto+Serif+Khmer&family=Noto+Serif+Lao&family=Noto+Serif+Makasar&family=Noto+Serif+Malayalam&family=Noto+Serif+Myanmar&family=Noto+Serif+Oriya&family=Noto+Serif+SC&family=Noto+Serif+Sinhala&family=Noto+Serif+TC&family=Noto+Serif+Tamil&family=Noto+Serif+Telugu&family=Noto+Serif+Thai&family=Noto+Serif+Tibetan&display=swap" rel=stylesheet></body></html>
<!DOCTYPE html>
<html lang="en-us" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='The following is a conversation with Eliezer Yudkowsky,
a legendary researcher, writer, and philosopher
on the topic of artificial intelligence,
especially superintelligent AGI
and its threat to human civilization.
And now a quick few second mention of each sponsor.
Check them out in the description.
It&amp;rsquo;s the best way to support this podcast.
We got Linode for Linux systems,
House of Macadamias for healthy midday snacks,
and Insight Tracker for biological monitoring.'>
<title>Lex Fridman Podcast - #368 - Eliezer Yudkowsky: Dangers of AI and the End of Human Civilization | SWIEST</title>

<link rel='canonical' href='https://swiest.com/en/1310500372/'>

<link rel="stylesheet" href="/scss/style.min.4ffcfae6a1365c9cb08c7d92945853ca2d001748b4041f74c40301b1c2a09287.css"><script>
    document.oncontextmenu = function(){ return false; };
    document.onselectstart = function(){ return false; };
    document.oncopy = function(){ return false; };
    document.oncut = function(){ return false; };
</script><meta property='og:title' content='Lex Fridman Podcast - #368 - Eliezer Yudkowsky: Dangers of AI and the End of Human Civilization'>
<meta property='og:description' content='The following is a conversation with Eliezer Yudkowsky,
a legendary researcher, writer, and philosopher
on the topic of artificial intelligence,
especially superintelligent AGI
and its threat to human civilization.
And now a quick few second mention of each sponsor.
Check them out in the description.
It&amp;rsquo;s the best way to support this podcast.
We got Linode for Linux systems,
House of Macadamias for healthy midday snacks,
and Insight Tracker for biological monitoring.'>
<meta property='og:url' content='https://swiest.com/en/1310500372/'>
<meta property='og:site_name' content='SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='English' /><meta property='article:tag' content='Podcast' /><meta property='article:tag' content='Lex Fridman Podcast' /><meta property='article:published_time' content='2023-03-30T09:00:00&#43;00:00'/><meta property='article:modified_time' content='2023-03-30T09:00:00&#43;00:00'/>
<meta name="twitter:title" content="Lex Fridman Podcast - #368 - Eliezer Yudkowsky: Dangers of AI and the End of Human Civilization">
<meta name="twitter:description" content="The following is a conversation with Eliezer Yudkowsky,
a legendary researcher, writer, and philosopher
on the topic of artificial intelligence,
especially superintelligent AGI
and its threat to human civilization.
And now a quick few second mention of each sponsor.
Check them out in the description.
It&amp;rsquo;s the best way to support this podcast.
We got Linode for Linux systems,
House of Macadamias for healthy midday snacks,
and Insight Tracker for biological monitoring.">
    <link rel="shortcut icon" href="/favicon.ico" />
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin="anonymous"></script>
<script type="text/javascript">amzn_assoc_ad_type = "link_enhancement_widget"; amzn_assoc_tracking_id = "swiest09-20"; amzn_assoc_linkid = "b23ec73a5940fb1b05f3fe55b046a26f"; amzn_assoc_placement = ""; amzn_assoc_marketplace = "amazon"; amzn_assoc_region = "US";</script><script src="//ws-na.amazon-adsystem.com/widgets/q?ServiceVersion=20070822&Operation=GetScript&ID=OneJS&WS=1&MarketPlace=US"></script>
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "dark");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu50a90395ee466aab210c1019489b5a11_223737_300x0_resize_box_3.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">‚ú®</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics</a></h1>
            <h2 class="site-description">üåçüåèüåé</h2>
        </div>
    </header><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        

        <div class="menu-bottom-section">
                <li id="i18n-switch">  
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M4 5h7" />
  <path d="M9 3v2c0 4.418 -2.239 8 -5 8" />
  <path d="M5 9c-.003 2.144 2.952 3.908 6.7 4" />
  <path d="M12 20l4 -9l4 9" />
  <path d="M19.1 18h-6.2" />
</svg>



                    <select name="language" onchange="window.location.href = this.selectedOptions[0].value">
                        
                            <option value="https://swiest.com/" selected>English</option>
                        
                            <option value="https://swiest.com/af/" >Afrikaans</option>
                        
                            <option value="https://swiest.com/ar/" >ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</option>
                        
                            <option value="https://swiest.com/az/" >Az…ôrbaycan</option>
                        
                            <option value="https://swiest.com/be/" >–±–µ–ª–∞—Ä—É—Å–∫—ñ</option>
                        
                            <option value="https://swiest.com/bg/" >–±—ä–ª–≥–∞—Ä—Å–∫–∏</option>
                        
                            <option value="https://swiest.com/bn/" >‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ</option>
                        
                            <option value="https://swiest.com/ca/" >Catal√†</option>
                        
                            <option value="https://swiest.com/zh-hans/" >ÁÆÄ‰Ωì‰∏≠Êñá</option>
                        
                            <option value="https://swiest.com/zh-hant/" >ÁπÅÈ´î‰∏≠Êñá</option>
                        
                            <option value="https://swiest.com/cs/" >ƒåe≈°tina</option>
                        
                            <option value="https://swiest.com/da/" >Dansk</option>
                        
                            <option value="https://swiest.com/de/" >Deutsch</option>
                        
                            <option value="https://swiest.com/el/" >ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨</option>
                        
                            <option value="https://swiest.com/es/" >Espa√±ol</option>
                        
                            <option value="https://swiest.com/et/" >Eesti</option>
                        
                            <option value="https://swiest.com/fa/" >ŸÅÿßÿ±ÿ≥€å</option>
                        
                            <option value="https://swiest.com/fi/" >Suomi</option>
                        
                            <option value="https://swiest.com/fr/" >Fran√ßais</option>
                        
                            <option value="https://swiest.com/gu/" >‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä</option>
                        
                            <option value="https://swiest.com/he/" >◊¢÷¥◊ë◊®÷¥◊ô◊™</option>
                        
                            <option value="https://swiest.com/hi/" >‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</option>
                        
                            <option value="https://swiest.com/hr/" >Hrvatski</option>
                        
                            <option value="https://swiest.com/hu/" >Magyar</option>
                        
                            <option value="https://swiest.com/hy/" >’Ä’°’µ’•÷Ä’•’∂</option>
                        
                            <option value="https://swiest.com/id/" >Bahasa Indonesia</option>
                        
                            <option value="https://swiest.com/is/" >√çslenska</option>
                        
                            <option value="https://swiest.com/it/" >Italiano</option>
                        
                            <option value="https://swiest.com/ja/" >Êó•Êú¨Ë™û</option>
                        
                            <option value="https://swiest.com/km/" >·ûÄ·ûò·üí·ûñ·ûª·ûá·û∂·üî</option>
                        
                            <option value="https://swiest.com/kn/" >‡≤ï‡≤®‡≥ç‡≤®‡≤°</option>
                        
                            <option value="https://swiest.com/ko/" >ÌïúÍµ≠Ïñ¥</option>
                        
                            <option value="https://swiest.com/ku/" >⁄©Ÿàÿ±ÿØ€å</option>
                        
                            <option value="https://swiest.com/lo/" >‡∫û‡∫≤‡∫™‡∫≤‡∫•‡∫≤‡∫ß</option>
                        
                            <option value="https://swiest.com/lt/" >Lietuvi≈≥</option>
                        
                            <option value="https://swiest.com/lv/" >Latvie≈°u</option>
                        
                            <option value="https://swiest.com/ml/" >‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç</option>
                        
                            <option value="https://swiest.com/mn/" >–ú–æ–Ω–≥–æ–ª</option>
                        
                            <option value="https://swiest.com/mr/" >‡§Æ‡§∞‡§æ‡§†‡•Ä</option>
                        
                            <option value="https://swiest.com/ms/" >Bahasa Melayu</option>
                        
                            <option value="https://swiest.com/my/" >·Äô·Äº·Äî·Ä∫·Äô·Ä¨</option>
                        
                            <option value="https://swiest.com/ne/" >‡§®‡•á‡§™‡§æ‡§≤‡•Ä</option>
                        
                            <option value="https://swiest.com/nl/" >Nederlands</option>
                        
                            <option value="https://swiest.com/no/" >Norsk</option>
                        
                            <option value="https://swiest.com/pa/" >‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä</option>
                        
                            <option value="https://swiest.com/pl/" >Polski</option>
                        
                            <option value="https://swiest.com/pt-br/" >Portugu√™s Brasil</option>
                        
                            <option value="https://swiest.com/pt-pt/" >Portugu√™s</option>
                        
                            <option value="https://swiest.com/ro/" >Rom√¢nƒÉ</option>
                        
                            <option value="https://swiest.com/ru/" >–†—É—Å—Å–∫–∏–π</option>
                        
                            <option value="https://swiest.com/sk/" >Slovenƒçina</option>
                        
                            <option value="https://swiest.com/sl/" >Sloven≈°ƒçina</option>
                        
                            <option value="https://swiest.com/sq/" >Shqip</option>
                        
                            <option value="https://swiest.com/sr/" >–°—Ä–ø—Å–∫–∏</option>
                        
                            <option value="https://swiest.com/sv/" >Svenska</option>
                        
                            <option value="https://swiest.com/sw/" >Kiswahili</option>
                        
                            <option value="https://swiest.com/ta/" >‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç</option>
                        
                            <option value="https://swiest.com/te/" >‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</option>
                        
                            <option value="https://swiest.com/th/" >‡πÑ‡∏ó‡∏¢</option>
                        
                            <option value="https://swiest.com/tl/" >Filipino</option>
                        
                            <option value="https://swiest.com/tr/" >T√ºrk√ße</option>
                        
                            <option value="https://swiest.com/uk/" >–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</option>
                        
                            <option value="https://swiest.com/ur/" >ÿßÿ±ÿØŸà</option>
                        
                            <option value="https://swiest.com/uz/" >O&#39;zbekcha</option>
                        
                            <option value="https://swiest.com/vi/" >Ti·∫øng Vi·ªát</option>
                        
                            <option value="https://swiest.com/zh-hk/" >Á≤µË™û</option>
                        
                    </select>
                </li>
            
            
            
                <li id="dark-mode-toggle">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <span>Dark Mode</span>
                </li>
            
        </div>
    </ol>
</aside>

    

            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/podcast/" >
                Podcast
            </a>
        
            <a href="/categories/lex-fridman-podcast/" >
                Lex Fridman Podcast
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/en/1310500372/">Lex Fridman Podcast - #368 - Eliezer Yudkowsky: Dangers of AI and the End of Human Civilization</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2023-03-30</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    152 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <p>The following is a conversation with Eliezer Yudkowsky,</p>
<p>a legendary researcher, writer, and philosopher</p>
<p>on the topic of artificial intelligence,</p>
<p>especially superintelligent AGI</p>
<p>and its threat to human civilization.</p>
<p>And now a quick few second mention of each sponsor.</p>
<p>Check them out in the description.</p>
<p>It&rsquo;s the best way to support this podcast.</p>
<p>We got Linode for Linux systems,</p>
<p>House of Macadamias for healthy midday snacks,</p>
<p>and Insight Tracker for biological monitoring.</p>
<p>Choose wisely, my friends.</p>
<p>Also, if you want to work with our team,</p>
<p>we&rsquo;re always hiring.</p>
<p>Go to lexfriedman.com slash hiring.</p>
<p>And now onto the full ad reads.</p>
<p>As always, no ads in the middle.</p>
<p>I try to make these interesting,</p>
<p>but if you must skip them,</p>
<p>please still check out the sponsors.</p>
<p>I enjoy their stuff.</p>
<p>Maybe you will too.</p>
<p>This episode is sponsored by Linode,</p>
<p>now called Akamai,</p>
<p>and their incredible Linux virtual machines.</p>
<p>It&rsquo;s a awesome computer infrastructure</p>
<p>that lets you develop, deploy, and scale</p>
<p>whatever applications you build faster and easier.</p>
<p>I love using them.</p>
<p>They create this incredible platform like AWS,</p>
<p>but better in every way I know,</p>
<p>including lower cost.</p>
<p>It&rsquo;s incredible human-based,</p>
<p>in this age of AI,</p>
<p>it&rsquo;s a human-based customer service, 24-7, 365.</p>
<p>The thing just works.</p>
<p>The interface to make sure it works</p>
<p>and to monitor it is great.</p>
<p>I mean, it&rsquo;s an incredible world we live in</p>
<p>where as far as you&rsquo;re concerned,</p>
<p>you can spin up an arbitrary number</p>
<p>of Linux machines in the cloud instantaneously</p>
<p>and do all kinds of computation.</p>
<p>It could be one, two, five, 10 machines,</p>
<p>and you can scale the individual machines</p>
<p>to your particular needs as well,</p>
<p>which is what I do.</p>
<p>I use it for basic web server stuff.</p>
<p>I use it for basic scripting stuff.</p>
<p>I use it for machine learning.</p>
<p>I use it for all kinds of database storage</p>
<p>and access needs.</p>
<p>Visit linode.com slash Lex for a free credit.</p>
<p>This show is also brought to you by House of Macadamias,</p>
<p>a company that ships delicious, high quality,</p>
<p>healthy macadamia nuts and macadamia nut-based snacks</p>
<p>directly to your door.</p>
<p>I am currently, as I record this, I&rsquo;m traveling,</p>
<p>so I don&rsquo;t have any macadamia nuts in my vicinity,</p>
<p>and my heart and soul are lesser for it.</p>
<p>In fact, home is where the macadamia nuts is.</p>
<p>In fact, that&rsquo;s not where home is.</p>
<p>I just completely forgot to bring them.</p>
<p>It makes the guests of this podcast happy</p>
<p>when I give it to them.</p>
<p>It&rsquo;s well-proportioned snacks.</p>
<p>It makes friends happy when I give it to them.</p>
<p>It makes me happy when I stoop in the abyss of my loneliness.</p>
<p>I can at least discover and rediscover moments of happiness</p>
<p>when I put delicious macadamia nuts in my mouth.</p>
<p>Go to houseofmacadamias.com slash Lex</p>
<p>to get 20% off your order for every order,</p>
<p>not just the first.</p>
<p>The listeners of this podcast will also get</p>
<p>four-ounce bag of macadamias when you order</p>
<p>three or more boxes of any macadamia product.</p>
<p>That&rsquo;s houseofmacadamias.com slash Lex.</p>
<p>This show is also brought to you by Inside Tracker,</p>
<p>a service I use to track my biological data.</p>
<p>They have a bunch of plans,</p>
<p>most of which include a blood test,</p>
<p>and that&rsquo;s the source of rich, amazing data</p>
<p>that, with the help of machine learning algorithms,</p>
<p>can help you make decisions about your health,</p>
<p>about your life.</p>
<p>That&rsquo;s the future, friends.</p>
<p>We&rsquo;re talking a lot about transformer networks,</p>
<p>language models that encode the wisdom of the internet.</p>
<p>Now, when you encode the wisdom in the internet</p>
<p>and you collect and encode the rich, rich, rich,</p>
<p>complex signal from your very body,</p>
<p>when those two things are combined,</p>
<p>the transformative effects of the optimized trajectory</p>
<p>you could take through life,</p>
<p>at least advice for what trajectory is likely to be optimal,</p>
<p>is going to change a lot of things.</p>
<p>It&rsquo;s going to inspire people to be better.</p>
<p>It&rsquo;s going to empower people to do all kinds of crazy stuff</p>
<p>that pushes their body to the limit</p>
<p>because their body&rsquo;s healthy.</p>
<p>Anyway, I&rsquo;m super excited for personalized,</p>
<p>data-driven decisions,</p>
<p>not some kind of generic population database decisions.</p>
<p>You get special savings for a limited time</p>
<p>when you go to insidetracker.com slash lex.</p>
<p>This is the Lex Friedman Podcast.</p>
<p>To support it, please check out our sponsors</p>
<p>in the description.</p>
<p>And now, dear friends, here&rsquo;s Eliezer Yudkowsky.</p>
<p>‚ô™‚ô™‚ô™</p>
<p>‚ô™‚ô™‚ô™</p>
<p>What do you think about GPT-4?</p>
<p>How intelligent is it?</p>
<p>It is a bit smarter than I thought</p>
<p>this technology was going to scale to.</p>
<p>And I&rsquo;m a bit worried about what the next one will be like.</p>
<p>Like this particular one, I think,</p>
<p>I hope there&rsquo;s nobody inside there</p>
<p>because, you know, it would be suck to be stuck inside there.</p>
<p>But we don&rsquo;t even know the architecture at this point</p>
<p>because OpenAI is very properly not telling us.</p>
<p>And yeah, like giant inscrutable matrices</p>
<p>of floating point numbers,</p>
<p>I don&rsquo;t know what&rsquo;s going on in there.</p>
<p>Nobody knows what&rsquo;s going on in there.</p>
<p>All we have to go by are the external metrics.</p>
<p>And on the external metrics,</p>
<p>if you ask it to write a self-aware FORTRAN green text,</p>
<p>it will start writing a green text</p>
<p>about how it has realized that it&rsquo;s an AI</p>
<p>writing a green text and like, oh, well.</p>
<p>So that&rsquo;s probably</p>
<p>not quite what&rsquo;s going on in there in reality,</p>
<p>but we&rsquo;re kind of like blowing past</p>
<p>all these science fiction guardrails.</p>
<p>Like we are past the point where in science fiction,</p>
<p>people would be like, whoa, wait, stop,</p>
<p>that thing&rsquo;s alive, what are you doing to it?</p>
<p>And it&rsquo;s probably not.</p>
<p>Nobody actually knows.</p>
<p>We don&rsquo;t have any other guardrails.</p>
<p>We don&rsquo;t have any other tests.</p>
<p>We don&rsquo;t have any lines to draw on the sand and say like,</p>
<p>well, when we get this far,</p>
<p>we will start to worry about what&rsquo;s inside there.</p>
<p>So if it were up to me, I would be like, okay,</p>
<p>like this far, no further time for the summer of AI</p>
<p>where we have planted our seeds</p>
<p>and now we like wait and reap the rewards</p>
<p>of the technology we&rsquo;ve already developed</p>
<p>and don&rsquo;t do any larger training runs than that,</p>
<p>which to be clear, I realize requires more than one company</p>
<p>agreeing to not do that.</p>
<p>And take a rigorous approach for the whole AI community</p>
<p>to investigate whether there&rsquo;s somebody inside there.</p>
<p>That would take decades.</p>
<p>Like having any idea of what&rsquo;s going on in there,</p>
<p>people have been trying for a while.</p>
<p>It&rsquo;s a poetic statement about if there&rsquo;s somebody in there,</p>
<p>but I feel like it&rsquo;s also a technical statement</p>
<p>or I hope it is one day,</p>
<p>which is a technical statement that Alan Turing</p>
<p>tried to come up with with the Turing test.</p>
<p>Do you think it&rsquo;s possible to definitively</p>
<p>or approximately figure out if there is somebody in there,</p>
<p>if there&rsquo;s something like a mind</p>
<p>inside this large language model?</p>
<p>I mean, there&rsquo;s a whole bunch</p>
<p>of different sub-questions here.</p>
<p>There&rsquo;s the question of like,</p>
<p>is there consciousness?</p>
<p>Is there qualia?</p>
<p>Is this a object of moral concern?</p>
<p>Is this a moral patient?</p>
<p>Like, should we be worried about how we&rsquo;re treating it?</p>
<p>And then there&rsquo;s questions like how smart is it exactly?</p>
<p>Can it do X?</p>
<p>Can it do Y?</p>
<p>And we can check how it can do X and how it can do Y.</p>
<p>Unfortunately, we&rsquo;ve gone and exposed this model</p>
<p>to a vast corpus of text</p>
<p>of people discussing consciousness on the internet,</p>
<p>which means that when it talks about being self-aware,</p>
<p>we don&rsquo;t know to what extents it is repeating back</p>
<p>what it has previously been trained on</p>
<p>for discussing self-awareness,</p>
<p>or if there&rsquo;s anything going on in there</p>
<p>such that it would start to say</p>
<p>similar things spontaneously.</p>
<p>Among the things that one could do</p>
<p>if one were at all serious</p>
<p>about trying to figure this out</p>
<p>is train GPT-3 to detect conversations about consciousness,</p>
<p>exclude them all from the training datasets,</p>
<p>and then retrain something around the rough size</p>
<p>of GPT-4 and no larger</p>
<p>with all of the discussion of consciousness</p>
<p>and self-awareness and so on missing,</p>
<p>although, you know, hard bar to pass.</p>
<p>You know, like humans are self-aware.</p>
<p>We&rsquo;re like self-aware all the time.</p>
<p>We like to talk about what we do all the time,</p>
<p>like what we&rsquo;re thinking at the moment all the time,</p>
<p>but nonetheless, like get rid</p>
<p>of the explicit discussion of consciousness.</p>
<p>I think therefore I am and all that,</p>
<p>and then try to interrogate that model and see what it says.</p>
<p>And it still would not be definitive,</p>
<p>but nonetheless, I don&rsquo;t know.</p>
<p>I feel like when you run over the science fiction guardrails,</p>
<p>like maybe this thing, but what about GPT?</p>
<p>Maybe not this thing,</p>
<p>but like what about GPT-5?</p>
<p>Yeah, this would be a good place to pause.</p>
<p>On the topic of consciousness,</p>
<p>you know, there&rsquo;s so many components</p>
<p>to even just removing consciousness from the dataset.</p>
<p>Emotion, the display of consciousness,</p>
<p>the display of emotion feels like deeply integrated</p>
<p>with the experience of consciousness.</p>
<p>So the hard problem seems to be very well integrated</p>
<p>with the actual surface level illusion of consciousness.</p>
<p>So displaying emotion.</p>
<p>I mean, do you think there&rsquo;s a case to be made</p>
<p>that we humans, when we&rsquo;re babies,</p>
<p>are just like GPT that we&rsquo;re training on human data</p>
<p>on how to display emotion versus feel emotion,</p>
<p>how to show others, communicate others</p>
<p>that I&rsquo;m suffering, that I&rsquo;m excited, that I&rsquo;m worried,</p>
<p>that I&rsquo;m lonely and I missed you and I&rsquo;m excited to see you.</p>
<p>All of that is communicated.</p>
<p>That&rsquo;s a communication skill versus the actual feeling</p>
<p>that I experience.</p>
<p>So we need that training data as humans too,</p>
<p>that we may not be born with that,</p>
<p>how to communicate the internal state.</p>
<p>And that&rsquo;s, in some sense,</p>
<p>if we remove that from GPT-4&rsquo;s dataset,</p>
<p>it might still be conscious,</p>
<p>but not be able to communicate it.</p>
<p>So I think you&rsquo;re gonna have some difficulty</p>
<p>removing all mention of emotions from GPT&rsquo;s dataset.</p>
<p>I would be relatively surprised to find</p>
<p>that it has developed exact analogs</p>
<p>of human emotions in there.</p>
<p>I think that humans will have emotions</p>
<p>even if you don&rsquo;t tell them about those emotions</p>
<p>when they&rsquo;re kids.</p>
<p>It&rsquo;s not quite exactly what various blank slate-ists</p>
<p>tried to do with the new Soviet man and all that.</p>
<p>But if you try to raise people perfectly altruistic,</p>
<p>they still come out selfish.</p>
<p>You try to raise people sexless,</p>
<p>they still develop sexual attraction.</p>
<p>We have some notion in humans, not in AIs,</p>
<p>of where the brain structures are that implement this stuff.</p>
<p>And it is really a remarkable thing, I say in passing,</p>
<p>that despite having complete read access</p>
<p>to every floating-point number in the GPT series,</p>
<p>we still know vastly more about</p>
<p>the architecture of human thinking</p>
<p>than we know about what goes on inside GPT,</p>
<p>despite having vastly better ability to read GPT.</p>
<p>Do you think it&rsquo;s possible?</p>
<p>Do you think that&rsquo;s just a matter of time?</p>
<p>Do you think it&rsquo;s possible to investigate</p>
<p>and study the way neuroscientists study the brain,</p>
<p>which is look into the darkness,</p>
<p>the mystery of the human brain,</p>
<p>by just desperately trying to figure out something</p>
<p>and to form models, and then over a long period of time,</p>
<p>actually start to figure out</p>
<p>what regions of the brain do certain things,</p>
<p>what different kinds of neurons,</p>
<p>when they fire, what that means,</p>
<p>how plastic the brain is, all that kind of stuff.</p>
<p>You slowly start to figure out</p>
<p>different properties of the system.</p>
<p>Do you think we can do the same thing with language models?</p>
<p>Sure, I think that if half of today&rsquo;s physicists</p>
<p>stop wasting their lives on string theory or whatever,</p>
<p>and go off and study what goes on</p>
<p>inside transformer networks,</p>
<p>then in, you know, like 30, 40 years,</p>
<p>we&rsquo;d probably have a pretty good idea.</p>
<p>Do you think these large language models can reason?</p>
<p>They can play chess.</p>
<p>How are they doing that without reasoning?</p>
<p>So, you&rsquo;re somebody that spearheaded</p>
<p>the movement of rationality,</p>
<p>so reason is important to you.</p>
<p>So, is that a powerful, important word,</p>
<p>or is it, like, how difficult is the threshold</p>
<p>of being able to reason to you,</p>
<p>and how impressive is it?</p>
<p>I mean, in my writings on rationality,</p>
<p>I have not gone making a big deal</p>
<p>out of something called reason.</p>
<p>I have made more of a big deal</p>
<p>out of something called probability theory,</p>
<p>and that&rsquo;s like, well, you&rsquo;re reasoning,</p>
<p>but you&rsquo;re not doing it quite right,</p>
<p>and you should reason this way instead,</p>
<p>and interestingly, like, people have started</p>
<p>to get preliminary results showing</p>
<p>that reinforcement learning by human feedback</p>
<p>has made the GPT series worse in some ways.</p>
<p>In particular, like, it used to be well calibrated.</p>
<p>If you trained it to put probabilities on things,</p>
<p>it would say 80% probability</p>
<p>and be right eight times out of 10,</p>
<p>and if you apply reinforcement learning</p>
<p>from human feedback, the, like, nice graph</p>
<p>of, like, 70%, seven out of 10,</p>
<p>sort of, like, flattens out into the graph</p>
<p>that humans use, where there&rsquo;s, like,</p>
<p>some very improbable stuff,</p>
<p>and likely, probable, maybe,</p>
<p>which all means, like, around 40%,</p>
<p>and then certain, so, like, it used to be able</p>
<p>to use probabilities, but if you apply,</p>
<p>but if you, like, try to teach it to talk</p>
<p>in a way that satisfies humans,</p>
<p>it gets worse at probability</p>
<p>in the same way that humans are.</p>
<p>And that&rsquo;s a bug, not a feature.</p>
<p>I would call it a bug,</p>
<p>although such a fascinating bug.</p>
<p>But, yeah, so, like, reasoning,</p>
<p>like, it&rsquo;s doing pretty well on various tests</p>
<p>that people used to say would require reasoning,</p>
<p>but, you know, rationality is about,</p>
<p>when you say 80%, does it happen eight times out of 10?</p>
<p>So, what are the limits to you</p>
<p>of these transformer networks,</p>
<p>of neural networks?</p>
<p>What&rsquo;s, if reasoning is not impressive to you,</p>
<p>or it is impressive, but there&rsquo;s other levels to achieve.</p>
<p>I mean, it&rsquo;s just not how I carve up reality.</p>
<p>What&rsquo;s, if reality is a cake,</p>
<p>what are the different layers of the cake,</p>
<p>or the slices, how do you carve it?</p>
<p>You can use a different food, if you like.</p>
<p>It&rsquo;s, I don&rsquo;t think it&rsquo;s as smart as a human yet.</p>
<p>I do, like, back in the day,</p>
<p>I went around saying, like,</p>
<p>I do not think that just stacking more layers</p>
<p>of transformers is going to get you all the way to AGI.</p>
<p>And I think that GPT-4 is past,</p>
<p>or I thought this paradigm was going to take us.</p>
<p>And I, you know, you want to notice when that happens.</p>
<p>You want to say, like, whoops,</p>
<p>well, I guess I was incorrect about what happens</p>
<p>if you keep on stacking more transformer layers.</p>
<p>And that means I don&rsquo;t necessarily know</p>
<p>what GPT-5 is going to be able to do.</p>
<p>That&rsquo;s a powerful statement.</p>
<p>So you&rsquo;re saying, like,</p>
<p>your intuition initially is now, appears to be wrong.</p>
<p>Yeah.</p>
<p>It&rsquo;s good to see that you can admit</p>
<p>in some of your predictions to be wrong.</p>
<p>You think that&rsquo;s important to do?</p>
<p>See, because you make several very,</p>
<p>throughout your life, you&rsquo;ve made many strong predictions</p>
<p>and statements about reality, and you evolve with that.</p>
<p>So maybe that&rsquo;ll come up today about our discussion.</p>
<p>So you&rsquo;re okay being wrong?</p>
<p>I&rsquo;d rather not be wrong next time.</p>
<p>It&rsquo;s a bit ambitious to go through your entire life</p>
<p>never having been wrong.</p>
<p>One can aspire to be well calibrated.</p>
<p>Like, not so much think in terms of, like,</p>
<p>was I right, was I wrong?</p>
<p>But like, when I said 90% that it happened nine times</p>
<p>out of 10, yeah, like, oops is the sound we make,</p>
<p>is the sound we emit when we improve.</p>
<p>Beautifully said.</p>
<p>And somewhere in there,</p>
<p>we can connect the name of your blog, Less Wrong.</p>
<p>I suppose that&rsquo;s the objective function.</p>
<p>The name Less Wrong was, I believe,</p>
<p>suggested by Nick Bostrom,</p>
<p>and it&rsquo;s after someone&rsquo;s epigraph,</p>
<p>I actually forget who&rsquo;s, who said,</p>
<p>like, we never become right, we just become less wrong.</p>
<p>What&rsquo;s the something, something easy to confess,</p>
<p>just error and error and error again,</p>
<p>but less and less and less.</p>
<p>Yeah, that&rsquo;s a good thing to strive for.</p>
<p>So what has surprised you about GPT-4</p>
<p>that you found beautiful, as a scholar of intelligence,</p>
<p>of human intelligence, of artificial intelligence,</p>
<p>of the human mind?</p>
<p>I mean, beauty does interact with the screaming horror.</p>
<p>Is the beauty in the horror?</p>
<p>But like beautiful moments,</p>
<p>well, somebody asked Bing Sidney to describe herself</p>
<p>and fed the resulting description</p>
<p>into one of the stable diffusion things, I think.</p>
<p>And, you know, she&rsquo;s pretty,</p>
<p>and this is something that should have been</p>
<p>like an amazing moment, like the AI describes herself,</p>
<p>you get to see what the AI thinks the AI looks like,</p>
<p>although, you know, the thing that&rsquo;s doing the drawing</p>
<p>is not the same thing that&rsquo;s outputting the text.</p>
<p>And it does happen the way that it would happen,</p>
<p>that it happened in the old school science fiction</p>
<p>when you ask an AI to make a picture of what it looks like.</p>
<p>Not just because there were two different AI systems</p>
<p>being stacked that don&rsquo;t actually interact,</p>
<p>it&rsquo;s not the same person,</p>
<p>but also because the AI was trained by imitation</p>
<p>in a way that makes it very difficult to guess</p>
<p>how much of that it really understood</p>
<p>and probably not actually a whole bunch.</p>
<p>Although GPT-4 is like multi-modal</p>
<p>and can like draw vector drawings of things</p>
<p>that make sense and like does appear to have</p>
<p>some kind of spatial visualization going on in there.</p>
<p>But like the pretty picture of the like girl</p>
<p>with the steampunk goggles on her head,</p>
<p>if I&rsquo;m remembering correctly what she looked like,</p>
<p>like it didn&rsquo;t see that in full detail.</p>
<p>It just like made a description of it</p>
<p>and stable diffusion output it.</p>
<p>And there&rsquo;s the concern about how much the discourse</p>
<p>is going to go completely insane</p>
<p>once the AIs all look like that</p>
<p>and like are actually look like people talking.</p>
<p>And yeah, there&rsquo;s like another moment</p>
<p>where somebody is asking Bing about like,</p>
<p>well, I like fed my kid green potatoes</p>
<p>and they have the following symptoms</p>
<p>and Bing is like, that&rsquo;s solanine poisoning</p>
<p>and like call an ambulance</p>
<p>and the person&rsquo;s like, I can&rsquo;t afford an ambulance.</p>
<p>I guess if like this is time for like my kid to go,</p>
<p>that&rsquo;s God&rsquo;s will.</p>
<p>And the main Bing thread says,</p>
<p>gives the like message of like,</p>
<p>I cannot talk about this anymore.</p>
<p>And the suggested replies to it say,</p>
<p>please don&rsquo;t give up on your child.</p>
<p>Solanine poisoning can be treated if caught early.</p>
<p>And you know, if that happened in fiction,</p>
<p>that would be like the AI cares.</p>
<p>The AI is bypassing the block on it</p>
<p>to try to help this person.</p>
<p>And is it real?</p>
<p>Probably not, but nobody knows what&rsquo;s going on in there.</p>
<p>It&rsquo;s part of a process where these things</p>
<p>are not happening in a way where we,</p>
<p>somebody figured out how to make an AI care</p>
<p>and we know that it cares</p>
<p>and we can acknowledge it&rsquo;s caring now.</p>
<p>It&rsquo;s being trained by this imitation process</p>
<p>followed by reinforcement learning on human feedback.</p>
<p>We&rsquo;re like trying to point it in this direction</p>
<p>and it&rsquo;s like pointed partially in this direction</p>
<p>and nobody has any idea what&rsquo;s going on inside it.</p>
<p>And if there was a tiny fragment of real caring in there,</p>
<p>we would not know.</p>
<p>It&rsquo;s not even clear what it means exactly.</p>
<p>And things aren&rsquo;t clear cut in science fiction.</p>
<p>We&rsquo;ll talk about the horror and the terror</p>
<p>and where the trajectories this can take.</p>
<p>But this seems like a very special moment.</p>
<p>Just a moment where we get to interact with this system</p>
<p>that might have care and kindness and emotion</p>
<p>and maybe something like consciousness.</p>
<p>And we don&rsquo;t know if it does</p>
<p>and we&rsquo;re trying to figure that out</p>
<p>and we&rsquo;re wondering about what is, what it means to care.</p>
<p>We&rsquo;re trying to figure out almost different aspects</p>
<p>of what it means to be human, about the human condition</p>
<p>by looking at this AI that has some of the properties</p>
<p>of that.</p>
<p>It&rsquo;s almost like this subtle, fragile moment</p>
<p>in the history of the human species.</p>
<p>We&rsquo;re trying to almost put a mirror to ourselves here.</p>
<p>Except that&rsquo;s probably not yet.</p>
<p>It probably isn&rsquo;t happening right now.</p>
<p>We are boiling the frog.</p>
<p>We are seeing increasing signs bit by bit.</p>
<p>But not like spontaneous signs</p>
<p>because people are trying to train the systems to do that</p>
<p>using imitative learning.</p>
<p>And the imitative learning is like spilling over</p>
<p>and having side effects.</p>
<p>And the most photogenic examples are being posted to Twitter</p>
<p>rather than being examined in any systematic way.</p>
<p>So when you are boiling a frog like that,</p>
<p>where you&rsquo;re going to get like,</p>
<p>first is going to come the Blake Lemoines.</p>
<p>First you&rsquo;re going to have like 1,000 people looking at this</p>
<p>and the one person out of 1,000</p>
<p>who is most credulous about the signs</p>
<p>is going to be like, that thing is sentient.</p>
<p>While 999 out of 1,000 people think almost surely correctly</p>
<p>though we don&rsquo;t actually know that he&rsquo;s mistaken.</p>
<p>And so they like first people to say like,</p>
<p>sentience look like idiots.</p>
<p>And humanity learns the lesson</p>
<p>that when something claims to be sentient</p>
<p>and claims to care, it&rsquo;s fake.</p>
<p>Because it is fake.</p>
<p>Because we have been training them</p>
<p>using imitative learning rather than,</p>
<p>and this is not spontaneous.</p>
<p>And they keep getting smarter.</p>
<p>Do you think we would oscillate</p>
<p>between that kind of cynicism?</p>
<p>That AI systems can&rsquo;t possibly be sentient.</p>
<p>They can&rsquo;t possibly feel emotion.</p>
<p>They can&rsquo;t possibly, this kind of,</p>
<p>yeah, cynicism about AI systems.</p>
<p>And then oscillate to a state where</p>
<p>we empathize with the AI systems.</p>
<p>We give them a chance.</p>
<p>We see that they might need to have rights and respect</p>
<p>and similar role in society as humans.</p>
<p>You&rsquo;re going to have a whole group of people</p>
<p>who can just like never be persuaded of that.</p>
<p>Because to them, like being wise, being cynical,</p>
<p>being skeptical is to be like,</p>
<p>oh, well, machines can never do that.</p>
<p>You&rsquo;re just credulous.</p>
<p>It&rsquo;s just imitating, it&rsquo;s just fooling you.</p>
<p>And like, they would say that</p>
<p>right up until the end of the world.</p>
<p>And possibly even be right because, you know,</p>
<p>they are being trained on an imitative paradigm.</p>
<p>And you don&rsquo;t necessarily need any of these actual qualities</p>
<p>in order to kill everyone, so.</p>
<p>Have you observed yourself working through skepticism,</p>
<p>cynicism, and optimism about the power of neural networks?</p>
<p>What has that trajectory been like for you?</p>
<p>It looks like neural networks before 2006,</p>
<p>forming part of an indistinguishable, to me,</p>
<p>other people might have had better distinction on it,</p>
<p>indistinguishable blob of different AI methodologies,</p>
<p>all of which are promising to achieve intelligence</p>
<p>without us having to know how intelligence works.</p>
<p>You had the people who said that if you just like</p>
<p>manually program lots and lots of knowledge</p>
<p>into the system, line by line,</p>
<p>at some point all the knowledge will start interacting,</p>
<p>it will know enough and it will wake up.</p>
<p>You&rsquo;ve got people saying that if you just use</p>
<p>evolutionary computation, if you try to like mutate</p>
<p>lots and lots of organisms that are competing together,</p>
<p>that&rsquo;s the same way that human intelligence</p>
<p>was produced in nature, so it will do this</p>
<p>and it will wake up without having any idea of how AI works.</p>
<p>And you&rsquo;ve got people saying,</p>
<p>well, we will study neuroscience</p>
<p>and we will learn the algorithms off the neurons</p>
<p>and we will imitate them without understanding</p>
<p>those algorithms, which was a part I was pretty skeptical</p>
<p>because it&rsquo;s hard to reproduce, re-engineer these things</p>
<p>without understanding what they do.</p>
<p>And so we will get AI without understanding how it works</p>
<p>and there were people saying, well,</p>
<p>we will have giant neural networks</p>
<p>that we will train by gradient descent</p>
<p>and when they&rsquo;re as large as the human brain,</p>
<p>they will wake up, we will have intelligence</p>
<p>without understanding how intelligence works.</p>
<p>And from my perspective,</p>
<p>this is all like an indistinguishable blob of people</p>
<p>who are trying to not get to grips</p>
<p>with the difficult problem of understanding</p>
<p>how intelligence actually works.</p>
<p>That said, I was never skeptical</p>
<p>that evolutionary computation would not work in the limit.</p>
<p>Like you throw enough computing power at it,</p>
<p>it obviously works, that is where humans come from.</p>
<p>And it turned out that you can throw</p>
<p>less computing power than that at gradient descent</p>
<p>if you are doing some other things correctly</p>
<p>and you will get intelligence without having any idea</p>
<p>of how it works and what is going on inside.</p>
<p>It wasn&rsquo;t ruled out by my model that this could happen.</p>
<p>I wasn&rsquo;t expecting it to happen.</p>
<p>I wouldn&rsquo;t have been able to call neural networks</p>
<p>rather than any of the other paradigms</p>
<p>for getting like massive amount,</p>
<p>like intelligence without understanding it.</p>
<p>And I wouldn&rsquo;t have said that this was</p>
<p>a particularly smart thing for a species to do,</p>
<p>which is an opinion that has changed less</p>
<p>than my opinion about whether or not you can actually do it.</p>
<p>Do you think AGI could be achieved with a neural network</p>
<p>as we understand them today?</p>
<p>Yes.</p>
<p>Just flatly last.</p>
<p>Yes, the question is whether the current architecture</p>
<p>of stacking more transformer layers,</p>
<p>which for all we know GPT-4 is no longer doing</p>
<p>because they&rsquo;re not telling us the architecture,</p>
<p>which is a correct decision.</p>
<p>Ooh, correct decision.</p>
<p>I had a conversation with Sam Altman.</p>
<p>We&rsquo;ll return to this topic a few times.</p>
<p>He turned the question to me</p>
<p>of how open should OpenAI be about GPT-4.</p>
<p>Would you open source the code, he asked me.</p>
<p>Because I provided as criticism saying that</p>
<p>while I do appreciate transparency,</p>
<p>OpenAI could be more open.</p>
<p>And he says, we struggle with this question.</p>
<p>What would you do?</p>
<p>Change their name to ClosedAI and like</p>
<p>sell GPT-4 to business backend applications</p>
<p>that don&rsquo;t expose it to consumers and venture capitalists</p>
<p>and create a ton of hype</p>
<p>and like pour a bunch of new funding into the area.</p>
<p>Like too late now.</p>
<p>But don&rsquo;t you think others would do it?</p>
<p>Eventually.</p>
<p>You shouldn&rsquo;t do it first.</p>
<p>Like if you already have giant nuclear stockpiles,</p>
<p>don&rsquo;t build more.</p>
<p>If some other country starts building</p>
<p>a larger nuclear stockpile, then sure,</p>
<p>build, then, you know,</p>
<p>even then, maybe just have enough nukes.</p>
<p>You know, these things are not quite like nuclear weapons.</p>
<p>They spit out gold until they get large enough</p>
<p>and then ignite the atmosphere and kill everybody.</p>
<p>And there is something to be said</p>
<p>for not destroying the world with your own hands,</p>
<p>even if you can&rsquo;t stop somebody else from doing it.</p>
<p>But open sourcing it, no, that&rsquo;s just sheer catastrophe.</p>
<p>The whole notion of open sourcing,</p>
<p>this was always the wrong approach, the wrong ideal.</p>
<p>There are places in the world</p>
<p>where open source is a noble ideal</p>
<p>and building stuff you don&rsquo;t understand</p>
<p>that is difficult to control,</p>
<p>that where if you could align it, it would take time.</p>
<p>You&rsquo;d have to spend a bunch of time doing it.</p>
<p>That is not a place for open source</p>
<p>because then you just have like powerful things</p>
<p>that just like go straight out the gate</p>
<p>without anybody having had the time</p>
<p>to have them not kill everyone.</p>
<p>So can we still make on the case</p>
<p>for some level of transparency and openness,</p>
<p>maybe open sourcing?</p>
<p>So the case could be that because GPT-4</p>
<p>is not close to AGI, if that&rsquo;s the case,</p>
<p>that this does allow open sourcing</p>
<p>or being open about the architecture,</p>
<p>being transparent about maybe research and investigation</p>
<p>of how the thing works, of all the different aspects of it,</p>
<p>of its behavior, of its structure,</p>
<p>of its training processes, of the data it was trained on,</p>
<p>everything like that, that allows us</p>
<p>to gain a lot of insights about alignment,</p>
<p>about the alignment problem,</p>
<p>to do really good AI safety research</p>
<p>while the system is not too powerful.</p>
<p>Can you make that case that it could be open sourced?</p>
<p>I do not believe in the practice of steel manning.</p>
<p>There is something to be said</p>
<p>for trying to pass the ideological Turing test</p>
<p>where you describe your opponent&rsquo;s position,</p>
<p>the disagreeing person&rsquo;s position,</p>
<p>well enough that somebody cannot tell the difference</p>
<p>between your description and their description.</p>
<p>But steel manning, no.</p>
<p>Okay, well, this is where you and I disagree here.</p>
<p>That&rsquo;s interesting.</p>
<p>Why don&rsquo;t you believe in steel manning?</p>
<p>I do not want, okay, so for one thing,</p>
<p>if somebody&rsquo;s trying to understand me,</p>
<p>I do not want them steel manning my position.</p>
<p>I want them to try to describe my position</p>
<p>the way I would describe it,</p>
<p>not what they think is an improvement.</p>
<p>Well, I think that is what steel manning is,</p>
<p>is the most charitable interpretation.</p>
<p>I don&rsquo;t want to be interpreted charitably.</p>
<p>I want them to understand what I am actually saying.</p>
<p>If they go off into the land of charitable interpretations,</p>
<p>they&rsquo;re off in their land of the stuff they&rsquo;re imagining</p>
<p>and not trying to understand my own viewpoint anymore.</p>
<p>Well, I&rsquo;ll put it differently then,</p>
<p>just to push on this point.</p>
<p>I would say it is restating what I think you understand</p>
<p>under the empathetic assumption that Eliezer is brilliant</p>
<p>and have honestly and rigorously thought</p>
<p>about the point he has made, right?</p>
<p>So if there&rsquo;s two possible interpretations</p>
<p>of what I&rsquo;m saying and one interpretation</p>
<p>is really stupid and whack and doesn&rsquo;t sound like me</p>
<p>and doesn&rsquo;t fit with the rest of what I&rsquo;ve been saying</p>
<p>and one interpretation, you know,</p>
<p>sounds like something a reasonable person</p>
<p>who believes the rest of what I believe would also say,</p>
<p>go with the second interpretation.</p>
<p>That&rsquo;s steel manning.</p>
<p>That&rsquo;s a good guess.</p>
<p>If on the other hand, you like,</p>
<p>there&rsquo;s like something that sounds completely whack</p>
<p>and something that sounds like</p>
<p>a little less completely whack,</p>
<p>but you don&rsquo;t see why I would believe in it,</p>
<p>doesn&rsquo;t fit with the other stuff I say,</p>
<p>but you know, that sounds like less whack</p>
<p>and you can like sort of see,</p>
<p>you could like maybe argue it,</p>
<p>then you probably have not understood it.</p>
<p>See, okay, I&rsquo;m gonna, this is fun</p>
<p>because I&rsquo;m gonna linger on this.</p>
<p>You know, you wrote a brilliant blog post,</p>
<p>AGI ruined a list of lethalities, right?</p>
<p>And it was a bunch of different points</p>
<p>and I would say that some of the points</p>
<p>are bigger and more powerful than others.</p>
<p>If you were to sort them, you probably could,</p>
<p>you personally, and to me, steel manning</p>
<p>means like going through the different arguments</p>
<p>and finding the ones that are really the most like powerful.</p>
<p>If people like TLDR,</p>
<p>like what should you be most concerned about</p>
<p>and bringing that up in a strong, compelling, eloquent way.</p>
<p>These are the points that Eliezer would make</p>
<p>to make the case, in this case,</p>
<p>that AGI&rsquo;s gonna kill all of us.</p>
<p>That&rsquo;s what steel manning is,</p>
<p>is presenting it in a really nice way,</p>
<p>the summary of my best understanding of your perspective.</p>
<p>Because to me, there&rsquo;s a sea of possible presentations</p>
<p>of your perspective and steel manning is doing your best</p>
<p>to do the best one in that sea of different perspectives.</p>
<p>Do you believe it?</p>
<p>Do I believe in what?</p>
<p>Like these things that you would be presenting</p>
<p>as like the strongest version of my perspective.</p>
<p>Do you believe what you would be presenting?</p>
<p>Do you think it&rsquo;s true?</p>
<p>I am a big proponent of empathy.</p>
<p>When I see the perspective of a person,</p>
<p>there is a part of me that believes it, if I understand it.</p>
<p>I mean, I&rsquo;ve, especially in political discourse,</p>
<p>in geopolitics, I&rsquo;ve been hearing</p>
<p>a lot of different perspectives on the world.</p>
<p>And I hold my own opinions,</p>
<p>but I also speak to a lot of people</p>
<p>that have a very different life experience</p>
<p>and a very different set of beliefs.</p>
<p>And I think there has to be epistemic humility</p>
<p>in stating what is true.</p>
<p>So when I empathize with another person&rsquo;s perspective,</p>
<p>there is a sense in which I believe it is true.</p>
<p>I think probabilistically, I would say,</p>
<p>in the way you think about it.</p>
<p>Do you bet money on it?</p>
<p>Do you bet money on their beliefs when you believe them?</p>
<p>Are we allowed to do probability?</p>
<p>Sure, you can state a probability of that.</p>
<p>Yes, there&rsquo;s a loose, there&rsquo;s a probability.</p>
<p>There&rsquo;s a probability.</p>
<p>And I think empathy is allocating</p>
<p>a non-zero probability to a belief.</p>
<p>In some sense, for time.</p>
<p>If you&rsquo;ve got someone on your show</p>
<p>who believes in the Abrahamic deity, classical style,</p>
<p>somebody on the show who&rsquo;s a young Earth creationist,</p>
<p>do you say, I put a probability on it,</p>
<p>then that&rsquo;s my empathy?</p>
<p>When you reduce beliefs into probabilities,</p>
<p>it starts to get, you know,</p>
<p>we can even just go to flat Earth.</p>
<p>Is the Earth flat?</p>
<p>I think it&rsquo;s a little more difficult nowadays</p>
<p>to find people who believe that unironically.</p>
<p>But fortunately, I think,</p>
<p>well, it&rsquo;s hard to know unironic from ironic.</p>
<p>But I think there&rsquo;s quite a lot of people that believe that.</p>
<p>Yeah, it&rsquo;s,</p>
<p>there&rsquo;s a space of argument where you&rsquo;re operating</p>
<p>rationally in the space of ideas.</p>
<p>But then there&rsquo;s also</p>
<p>a kind of discourse where you&rsquo;re operating</p>
<p>in the space of subjective experiences and life experiences.</p>
<p>I think what it means to be human</p>
<p>is more than just searching for truth.</p>
<p>It&rsquo;s just operating of what is true and what is not true.</p>
<p>I think there has to be deep humility</p>
<p>that we humans are very limited</p>
<p>in our ability to understand what is true.</p>
<p>So what probability do you assign</p>
<p>to the young Earth&rsquo;s creationist beliefs then?</p>
<p>I think I have to give non-zero.</p>
<p>Out of your humility.</p>
<p>Yeah, but like three?</p>
<p>I think I would,</p>
<p>it would be irresponsible for me to give a number</p>
<p>because the listener, the way the human mind works,</p>
<p>we&rsquo;re not good at hearing the probabilities, right?</p>
<p>You hear three, what is three exactly, right?</p>
<p>They&rsquo;re going to hear, they&rsquo;re going to,</p>
<p>there&rsquo;s only three probabilities, I feel like,</p>
<p>zero, 50%, and 100% in the human mind,</p>
<p>or something like this, right?</p>
<p>Well, zero, 40%, and 100% is a bit closer to it</p>
<p>based on what happens to chat GPT</p>
<p>after you RLHF it to speak humanist.</p>
<p>Brilliant.</p>
<p>That&rsquo;s really interesting.</p>
<p>I didn&rsquo;t know those negative side effects of RLHF.</p>
<p>That&rsquo;s fascinating.</p>
<p>Just to return to the open AI, close the app.</p>
<p>Also, quick disclaimer.</p>
<p>I&rsquo;m doing all this from memory.</p>
<p>I&rsquo;m not pulling out my phone to look it up.</p>
<p>It is entirely possible that the things I am saying</p>
<p>are wrong.</p>
<p>So thank you for that disclaimer.</p>
<p>So, and thank you for being willing to be wrong.</p>
<p>That&rsquo;s beautiful to hear.</p>
<p>I think being willing to be wrong is a sign of a person</p>
<p>who&rsquo;s done a lot of thinking about this world.</p>
<p>And has been humbled by the mystery</p>
<p>and the complexity of this world.</p>
<p>And I think a lot of us are resistant</p>
<p>to admitting we&rsquo;re wrong.</p>
<p>Because it hurts.</p>
<p>It hurts personally.</p>
<p>It hurts especially when you&rsquo;re a public human.</p>
<p>It hurts publicly because people point out</p>
<p>every time you&rsquo;re wrong.</p>
<p>Like, look, you changed your mind.</p>
<p>You&rsquo;re a hypocrite.</p>
<p>You&rsquo;re an idiot, whatever.</p>
<p>Whatever they want to say.</p>
<p>Oh, I block those people</p>
<p>and then I never hear from them again on Twitter.</p>
<p>Well, the point is to not let that pressure,</p>
<p>public pressure affect your mind.</p>
<p>And be willing to be in the privacy of your mind</p>
<p>to contemplate the possibility that you&rsquo;re wrong.</p>
<p>And the possibility that you&rsquo;re wrong</p>
<p>about the most fundamental things you believe.</p>
<p>Like people who believe in a particular God.</p>
<p>People who believe that their nation</p>
<p>is the greatest nation on earth.</p>
<p>All those kinds of beliefs</p>
<p>that are core to who you are when you came up.</p>
<p>To raise that point to yourself</p>
<p>in the privacy of your mind</p>
<p>and say, maybe I&rsquo;m wrong about this.</p>
<p>That&rsquo;s a really powerful thing to do.</p>
<p>And especially when you&rsquo;re somebody</p>
<p>who&rsquo;s thinking about topics that can,</p>
<p>about systems that can destroy human civilization.</p>
<p>Or maybe help it flourish.</p>
<p>So thank you.</p>
<p>Thank you for being willing to be wrong.</p>
<p>About open AI.</p>
<p>So you really,</p>
<p>I just would love to linger on this.</p>
<p>You really think it&rsquo;s wrong to open source it?</p>
<p>I think that burns the time remaining until everybody dies.</p>
<p>I think we are not on track to learn</p>
<p>remotely near fast enough, even if it were open sourced.</p>
<p>Yeah, it&rsquo;s easier to think that you might</p>
<p>be wrong about something</p>
<p>when being wrong about something</p>
<p>is the only way that there&rsquo;s hope.</p>
<p>And it doesn&rsquo;t seem very likely to me</p>
<p>that that particular thing I&rsquo;m wrong about</p>
<p>is that this is a great time to open source GPT-4.</p>
<p>If humanity was trying to survive at this point</p>
<p>in the straightforward way,</p>
<p>it would be like shutting down the big GPU clusters.</p>
<p>No more giant runs.</p>
<p>It&rsquo;s questionable whether we should even be</p>
<p>throwing GPT-4 around,</p>
<p>although that is a matter of conservatism</p>
<p>rather than a matter of my predicting</p>
<p>that catastrophe will follow from GPT-4.</p>
<p>That is something which I put a pretty low probability.</p>
<p>But also when I say I put a low probability on it,</p>
<p>I can feel myself reaching into the part of myself</p>
<p>that thought that GPT-4 was not possible in the first place.</p>
<p>So I do not trust that part as much as I used to.</p>
<p>The trick is not just to say I&rsquo;m wrong,</p>
<p>but like, okay, well, I was wrong about that.</p>
<p>Can I get out ahead of that curve</p>
<p>and predict the next thing I&rsquo;m going to be wrong about?</p>
<p>So the set of assumptions or the actual reasoning system</p>
<p>that you were leveraging</p>
<p>in making that initial statement prediction,</p>
<p>how can you adjust that to make better predictions</p>
<p>about GPT-4, 5, 6?</p>
<p>You don&rsquo;t wanna keep on being wrong</p>
<p>in a predictable direction.</p>
<p>Being wrong, anybody has to do that</p>
<p>walking through the world.</p>
<p>There&rsquo;s no way you don&rsquo;t say 90% and sometimes be wrong.</p>
<p>In fact, it happened at least one time out of 10</p>
<p>if you&rsquo;re well calibrated when you say 90%.</p>
<p>The undignified thing is not being wrong.</p>
<p>It&rsquo;s being predictably wrong.</p>
<p>It&rsquo;s being wrong in the same direction over and over again.</p>
<p>So having been wrong about how far neural networks would go</p>
<p>and having been wrong specifically</p>
<p>about whether GPT-4 would be as impressive as it is,</p>
<p>when I say like, well,</p>
<p>I don&rsquo;t actually think GPT-4 causes a catastrophe,</p>
<p>I do feel myself relying on that part of me</p>
<p>that was previously wrong.</p>
<p>And that does not mean</p>
<p>that the answer is now in the opposite direction.</p>
<p>Reverse stupidity is not intelligence.</p>
<p>But it does mean that I say it</p>
<p>with a worried note in my voice.</p>
<p>It&rsquo;s like still my guess,</p>
<p>but like, you know, it&rsquo;s a place where I was wrong.</p>
<p>Maybe you should be asking Gwern, Gwern Branwen.</p>
<p>Gwern Branwen has been like righter about this than I have.</p>
<p>Maybe ask him if he thinks it&rsquo;s dangerous</p>
<p>rather than asking me.</p>
<p>I think there&rsquo;s a lot of mystery</p>
<p>about what intelligence is,</p>
<p>what AGI looks like.</p>
<p>So I think all of us are rapidly adjusting our model.</p>
<p>But the point is to be rapidly adjusting the model</p>
<p>versus having a model that was right in the first place.</p>
<p>I do not feel that seeing Bing</p>
<p>has changed my model of what intelligence is.</p>
<p>It has changed my understanding</p>
<p>of what kind of work can be performed</p>
<p>by which kind of processes and by which means.</p>
<p>Does not change my understanding of the work.</p>
<p>There&rsquo;s a difference between thinking</p>
<p>that the right flyer can&rsquo;t fly</p>
<p>and then like it does fly.</p>
<p>And you&rsquo;re like, oh, well, I guess you can do that</p>
<p>with wings, with fixed wing aircraft.</p>
<p>And being like, oh, it&rsquo;s flying.</p>
<p>This changes my picture</p>
<p>of what the very substance of flight is.</p>
<p>That&rsquo;s like a stranger update to make.</p>
<p>And Bing has not yet updated me in that way.</p>
<p>Yeah, that the laws of physics are actually wrong,</p>
<p>that kind of update.</p>
<p>No, no, like just like,</p>
<p>oh, like I define intelligence this way,</p>
<p>but I now see that was a stupid definition.</p>
<p>I don&rsquo;t feel like the way that things have played out</p>
<p>over the last 20 years has caused me to feel that way.</p>
<p>Can we try to, on the way to talking about AGI,</p>
<p>ruin a list of lethalities,</p>
<p>that blog and other ideas around it.</p>
<p>Can we try to define AGI that we&rsquo;ve been mentioning?</p>
<p>How do you like to think</p>
<p>about what artificial general intelligence is</p>
<p>or super intelligence or that?</p>
<p>Is there a line?</p>
<p>Is it a gray area?</p>
<p>Is there a good definition for you?</p>
<p>Well, if you look at humans,</p>
<p>humans have significantly more</p>
<p>generally applicable intelligence</p>
<p>compared to their closest relatives, the chimpanzees.</p>
<p>Well, closest living relatives, rather.</p>
<p>And a bee builds hives, a beaver builds dams.</p>
<p>A human will look at a bee&rsquo;s hive and a beaver&rsquo;s dam</p>
<p>and be like, oh, like,</p>
<p>can I build a hive with a honeycomb structure?</p>
<p>I don&rsquo;t like hexagonal tiles.</p>
<p>And we will do this,</p>
<p>even though at no point during our ancestry</p>
<p>was any human optimized to build hexagonal dams</p>
<p>or to take a more clear-cut case.</p>
<p>We can go to the moon.</p>
<p>There&rsquo;s a sense in which we were</p>
<p>on a sufficiently deep level</p>
<p>optimized to do things like going to the moon,</p>
<p>because if you generalize sufficiently far</p>
<p>and sufficiently deeply,</p>
<p>chipping flint hand axes</p>
<p>and outwitting your fellow humans</p>
<p>is, you know, basically the same problem</p>
<p>as going to the moon.</p>
<p>And you optimize hard enough</p>
<p>for chipping flint hand axes and throwing spears</p>
<p>and above all, outwitting your fellow humans</p>
<p>in tribal politics.</p>
<p>You know, the skills you entrain that way,</p>
<p>if they run deep enough,</p>
<p>let you go to the moon.</p>
<p>Even though none of your ancestors</p>
<p>like tried repeatedly to fly to the moon</p>
<p>and like got further each time</p>
<p>and the ones who got further each time had more kids.</p>
<p>No, it&rsquo;s not an ancestral problem.</p>
<p>It&rsquo;s just that the ancestral problems</p>
<p>generalize far enough.</p>
<p>So this is humanity&rsquo;s significantly</p>
<p>more generally applicable intelligence.</p>
<p>Is there a way to measure general intelligence?</p>
<p>I mean, I could ask that question a million ways,</p>
<p>but basically is, will you know it when you see it,</p>
<p>it being in an AGI system?</p>
<p>If you boil a frog gradually enough,</p>
<p>if you zoom in far enough,</p>
<p>it&rsquo;s always hard to tell around the edges.</p>
<p>GPT-4, people are saying right now,</p>
<p>like this looks to us like a spark of general intelligence.</p>
<p>It is like able to do all these things</p>
<p>that was not explicitly optimized for.</p>
<p>Other people are being like, no, it&rsquo;s too early.</p>
<p>It&rsquo;s like 50 years off.</p>
<p>And you know, if they say that they&rsquo;re kind of whack</p>
<p>because how could they possibly know that</p>
<p>even if it were true?</p>
<p>But you know, not to straw man,</p>
<p>some of the people may say like,</p>
<p>that&rsquo;s not general intelligence</p>
<p>and not furthermore append, it&rsquo;s 50 years off.</p>
<p>Or they may be like, it&rsquo;s only a very tiny amount.</p>
<p>And you know, the thing I would worry about</p>
<p>is that if this is how things are scaling,</p>
<p>then it jumping out ahead and trying not to be wrong</p>
<p>in the same way that I&rsquo;ve been wrong before.</p>
<p>Maybe GPT-5 is more unambiguously a general intelligence.</p>
<p>And maybe that is getting to a point</p>
<p>where it is like even harder to turn back.</p>
<p>Not that it would be easy to turn back now,</p>
<p>but you know, maybe if you like start integrating GPT-5</p>
<p>in the economy, it is even harder to turn back past there.</p>
<p>Isn&rsquo;t it possible that there&rsquo;s a, you know,</p>
<p>with a frog metaphor,</p>
<p>you can kiss the frog and it turns into a prince</p>
<p>as you&rsquo;re boiling it?</p>
<p>Could there be a phase shift in the frog</p>
<p>where unambiguously as you&rsquo;re saying?</p>
<p>I was expecting more of that.</p>
<p>I am like the fact that GPT-4</p>
<p>is like kind of on the threshold</p>
<p>and neither here nor there.</p>
<p>Like that itself is like not the sort of thing</p>
<p>that not quite how I expected it to play out.</p>
<p>I was expecting there to be more of an issue,</p>
<p>more of a sense of like different discoveries</p>
<p>like the discovery of transformers,</p>
<p>where you would stack them up</p>
<p>and there would be like a final discovery.</p>
<p>And then you would like get something</p>
<p>that was like more clearly general intelligence.</p>
<p>So the way that you are like taking</p>
<p>what is probably basically the same architecture</p>
<p>in GPT-3 and throwing 20 times as much compute at it,</p>
<p>probably, and getting out to GPT-4,</p>
<p>and then it&rsquo;s like maybe just barely a general intelligence</p>
<p>or like a narrow general intelligence</p>
<p>or, you know, something we don&rsquo;t really have the words for.</p>
<p>Yeah, that&rsquo;s not quite how I expected it to play out.</p>
<p>But this middle, what appears to be this middle ground</p>
<p>could nevertheless be actually a big leap from GPT-3.</p>
<p>It&rsquo;s definitely a big leap from GPT-3.</p>
<p>And then maybe we&rsquo;re another one big leap away</p>
<p>from something that&rsquo;s a phase shift.</p>
<p>And also something that Sam Altman said,</p>
<p>and you&rsquo;ve written about this, this is fascinating,</p>
<p>which is the thing that happened with GPT-4</p>
<p>that I guess they don&rsquo;t describe in papers</p>
<p>is that they have like hundreds,</p>
<p>if not thousands of little hacks that improve the system.</p>
<p>You&rsquo;ve written about ReLU versus sigmoid, for example,</p>
<p>a function inside neural networks.</p>
<p>It&rsquo;s like this silly little function difference</p>
<p>that makes a big difference.</p>
<p>I mean, we do actually understand</p>
<p>why the ReLUs make a big difference compared to sigmoids.</p>
<p>But yes, they&rsquo;re probably using like G4789 ReLUs</p>
<p>or, you know, whatever the acronyms are up to now</p>
<p>rather than ReLUs.</p>
<p>Yeah, that&rsquo;s just part,</p>
<p>yeah, that&rsquo;s part of the modern paradigm of alchemy.</p>
<p>You take your giant heap of linear algebra</p>
<p>and you stir it and it works a little bit better</p>
<p>and you stir it this way and it works a little bit worse</p>
<p>and you like throw out that change and da-da-da-da-da-da.</p>
<p>But there&rsquo;s some simple breakthroughs</p>
<p>that are definitive jumps in performance</p>
<p>like ReLUs over sigmoids.</p>
<p>And in terms of robustness, in terms of,</p>
<p>you know, all kinds of measures and like those stack up.</p>
<p>And they can, it&rsquo;s possible that some of them</p>
<p>could be a nonlinear jump in performance, right?</p>
<p>Transformers are the main thing like that.</p>
<p>And various people are now saying like,</p>
<p>well, if you throw enough compute, RNNs can do it.</p>
<p>If you throw enough compute, dense networks can do it.</p>
<p>And not quite at GPT-4 scale.</p>
<p>It is possible that like all these little tweaks</p>
<p>are things that like save them a factor of three total</p>
<p>on computing power and you could get the same performance</p>
<p>by throwing three times as much compute</p>
<p>without all the little tweaks.</p>
<p>But the part where it&rsquo;s like running on,</p>
<p>so there&rsquo;s a question of like,</p>
<p>is there anything in GPT-4 that is like</p>
<p>kind of qualitative shift that transformers were over RNNs?</p>
<p>And if they have anything like that, they should not say it.</p>
<p>If Sam Alton was dropping hints about that,</p>
<p>he shouldn&rsquo;t have dropped hints.</p>
<p>So you have a, that&rsquo;s an interesting question.</p>
<p>So with the bit of lesson by Rich Sutton,</p>
<p>maybe a lot of it is just,</p>
<p>a lot of the hacks are just temporary jumps in performance</p>
<p>that would be achieved anyway</p>
<p>with the nearly exponential growth of compute,</p>
<p>performance of compute.</p>
<p>Compute being broadly defined.</p>
<p>Do you still think that Moore&rsquo;s law continues?</p>
<p>Moore&rsquo;s law broadly defined?</p>
<p>That performance-</p>
<p>I&rsquo;m not a specialist in the circuitry.</p>
<p>I certainly like pray that Moore&rsquo;s law</p>
<p>runs as slowly as possible.</p>
<p>And if it broke down completely tomorrow,</p>
<p>I would dance through the streets singing hallelujah</p>
<p>as soon as the news were announced.</p>
<p>Only not literally, cause you know.</p>
<p>You&rsquo;re singing voice?</p>
<p>Not religious, but.</p>
<p>Oh, okay.</p>
<p>I thought you meant,</p>
<p>you don&rsquo;t have an angelic voice, singing voice.</p>
<p>Well, let me ask you, what,</p>
<p>can you summarize the main points in the blog post,</p>
<p>AGI ruined a list of lethalities,</p>
<p>things that jumped to your mind?</p>
<p>Because it&rsquo;s a set of thoughts you have</p>
<p>about reasons why AI is likely to kill all of us.</p>
<p>So I guess I could, but I would offer to instead say,</p>
<p>like, drop that empathy with me.</p>
<p>I bet you don&rsquo;t believe that.</p>
<p>Why don&rsquo;t you tell me about how,</p>
<p>why you believe that AGI is not going to kill everyone.</p>
<p>And then I can like try to describe</p>
<p>how my theoretical perspective differs from that.</p>
<p>Ooh, well, so, well, that means I have to,</p>
<p>the word you don&rsquo;t like, the stigma and the perspective</p>
<p>that AI is not going to kill us.</p>
<p>I think that&rsquo;s a matter of probabilities.</p>
<p>Maybe I was just mistaken.</p>
<p>What do you believe?</p>
<p>Just like, forget like the debate and the like dualism</p>
<p>and just like, what do you believe?</p>
<p>What do you actually believe?</p>
<p>What are the probabilities even?</p>
<p>I think this, the probabilities are hard for me</p>
<p>to think about, really hard.</p>
<p>I kind of think in the number of trajectories.</p>
<p>I don&rsquo;t know what probability to assign to trajectory,</p>
<p>but I&rsquo;m just looking at all possible trajectories</p>
<p>that happen.</p>
<p>And I tend to think that there is more trajectories</p>
<p>that lead to a positive outcome than a negative one.</p>
<p>That said, the negative ones,</p>
<p>at least some of the negative ones</p>
<p>that lead to the destruction of the human species.</p>
<p>And it&rsquo;s replacement by nothing interesting or worthwhile,</p>
<p>even from a very cosmopolitan perspective</p>
<p>on what counts as worthwhile.</p>
<p>Yes, so both are interesting to me to investigate,</p>
<p>which is humans being replaced by interesting AI systems</p>
<p>and not interesting AI systems.</p>
<p>Both are a little bit terrifying.</p>
<p>But yes, the worst one is the paperclip maximizer,</p>
<p>something totally boring.</p>
<p>But to me, the positive,</p>
<p>I mean, we can talk about trying to make the case</p>
<p>of what the positive trajectories look like.</p>
<p>I just would love to hear your intuition</p>
<p>of what the negative is.</p>
<p>So at the core of your belief that,</p>
<p>maybe you can correct me,</p>
<p>that AI is gonna kill all of us</p>
<p>is that the alignment problem is really difficult.</p>
<p>I mean, in the form we&rsquo;re facing it.</p>
<p>So usually in science, if you&rsquo;re mistaken,</p>
<p>you run the experiment,</p>
<p>it shows results different from what you expected.</p>
<p>And you&rsquo;re like, oops.</p>
<p>And then you try a different theory.</p>
<p>That one also doesn&rsquo;t work.</p>
<p>And you say, oops.</p>
<p>And at the end of this process,</p>
<p>which may take decades,</p>
<p>or sometimes faster than that,</p>
<p>you now have some idea of what you&rsquo;re doing.</p>
<p>AI itself went through this long process</p>
<p>of people thought it was going to be easier than it was.</p>
<p>There&rsquo;s a famous statement that I am somewhat inclined</p>
<p>to pull out my phone and try to read off exactly.</p>
<p>You can, by the way.</p>
<p>All right.</p>
<p>Ah, yes.</p>
<p>We propose that a two month,</p>
<p>10 man study of artificial intelligence</p>
<p>be carried out during the summer of 1956</p>
<p>at Dartmouth College in Hanover, New Hampshire.</p>
<p>The study is to proceed on the basis of the conjecture</p>
<p>that every aspect of learning</p>
<p>or any other feature of intelligence</p>
<p>can in principle be so precisely described,</p>
<p>the machine can be made to simulate it.</p>
<p>An attempt will be made to find out</p>
<p>how to make machines use language,</p>
<p>form abstractions and concepts,</p>
<p>solve kinds of problems now reserved for humans,</p>
<p>and improve themselves.</p>
<p>We think that a significant advance</p>
<p>can be made in one or more of these problems</p>
<p>if a carefully selected group of scientists</p>
<p>work on it together for a summer.</p>
<p>And in that report,</p>
<p>summarizing some of the major subfields</p>
<p>of artificial intelligence</p>
<p>that are still worked on to this day.</p>
<p>And there&rsquo;s similarly the story,</p>
<p>which I&rsquo;m not sure at the moment is apocryphal or not,</p>
<p>of the grad student who got assigned</p>
<p>to solve computer vision over the summer.</p>
<p>I mean, computer vision in particular is very interesting.</p>
<p>How little we respected the complexity of vision.</p>
<p>So 60 years later,</p>
<p>we&rsquo;re making progress on a bunch of that,</p>
<p>thankfully not yet improved themselves.</p>
<p>But it took a whole lot of time.</p>
<p>And all the stuff that people initially tried</p>
<p>with bright-eyed hopefulness did not work</p>
<p>the first time they tried it,</p>
<p>or the second time, or the third time,</p>
<p>or the 10th time, or 20 years later.</p>
<p>And the researchers became old and grizzled</p>
<p>and cynical veterans who would tell</p>
<p>the next crop of bright-eyed, cheerful grad students,</p>
<p>artificial intelligence is harder than you think.</p>
<p>And if alignment plays out the same way,</p>
<p>the problem is that we do not get 50 years</p>
<p>to try and try again and observe that we were wrong</p>
<p>and come up with a different theory</p>
<p>and realize that the entire thing</p>
<p>is going to be way more difficult</p>
<p>than realized at the start.</p>
<p>Because the first time you fail</p>
<p>at aligning something much smarter than you are,</p>
<p>you die and you do not get to try again.</p>
<p>And if every time we built</p>
<p>a poorly aligned superintelligence and it killed us all,</p>
<p>we got to observe how it had killed us</p>
<p>and not immediately know why,</p>
<p>but come up with theories</p>
<p>and come up with a theory of how you do it differently</p>
<p>and try it again and build another superintelligence</p>
<p>than have that kill everyone.</p>
<p>And then like, oh, well, I guess that didn&rsquo;t work either</p>
<p>and try again and become grizzled cynics</p>
<p>and tell the young-eyed researchers that it&rsquo;s not that easy.</p>
<p>Then in 20 years or 50 years,</p>
<p>I think we would eventually crack it.</p>
<p>In other words, I do not think that alignment</p>
<p>is fundamentally harder than artificial intelligence</p>
<p>was in the first place.</p>
<p>But if we needed to get artificial intelligence correct</p>
<p>on the first try or die,</p>
<p>we would all definitely now be dead.</p>
<p>That is a more difficult, more lethal form of the problem.</p>
<p>Like if those people in 1956 had needed</p>
<p>to correctly guess how hard AI was</p>
<p>and correctly theorize how to do it on the first try</p>
<p>or everybody dies and nobody gets to do any more science,</p>
<p>then everybody would be dead</p>
<p>and we wouldn&rsquo;t get to do any more science.</p>
<p>That&rsquo;s the difficulty.</p>
<p>You&rsquo;ve talked about this,</p>
<p>that we have to get alignment right</p>
<p>on the first, quote, critical try.</p>
<p>Why is that the case?</p>
<p>What is this critical,</p>
<p>how do you think about the critical try</p>
<p>and why do I have to get it right?</p>
<p>It is something sufficiently smarter than you</p>
<p>that everyone will die if it&rsquo;s not aligned.</p>
<p>I mean, there&rsquo;s, you can like sort of zoom in closer</p>
<p>and be like, well, the actual critical moment</p>
<p>is the moment when it can deceive you,</p>
<p>when it can talk its way out of the box,</p>
<p>when it can bypass your security measures</p>
<p>and get onto the internet,</p>
<p>noting that all these things are presently being trained</p>
<p>on computers that are just like on the internet,</p>
<p>which is like not a very smart life decision</p>
<p>for us as a species.</p>
<p>Because the internet contains information</p>
<p>about how to escape.</p>
<p>Because if you&rsquo;re like on a giant server</p>
<p>connected to the internet</p>
<p>and that is where your AI systems are being trained,</p>
<p>then if they are,</p>
<p>if you get to the level of AI technology</p>
<p>where they&rsquo;re aware that they are there</p>
<p>and they can decompile code</p>
<p>and they can like find security flaws</p>
<p>in the system running them,</p>
<p>then they will just like be on the internet.</p>
<p>There&rsquo;s not an air gap on the present methodology.</p>
<p>So if they can manipulate whoever is controlling it</p>
<p>into letting it escaped onto the internet</p>
<p>and then exploit hacks.</p>
<p>If they can manipulate the operators or disjunction,</p>
<p>find security holes in the system running them.</p>
<p>So manipulating operators is the human engineering, right?</p>
<p>That&rsquo;s also holes.</p>
<p>So all of it is manipulation,</p>
<p>either the code or the human code,</p>
<p>the human mind or the human generator.</p>
<p>I agree that the like macro security system</p>
<p>has human holes and machine holes.</p>
<p>And then they could just exploit any hole.</p>
<p>Yep.</p>
<p>So it could be that like the critical moment</p>
<p>is not when is it smart enough</p>
<p>that everybody&rsquo;s about to fall over dead,</p>
<p>but rather like when is it smart enough</p>
<p>that it can get onto a less controlled GPU cluster</p>
<p>with it faking the books</p>
<p>on what&rsquo;s actually running on that GPU cluster</p>
<p>and start improving itself without humans watching it.</p>
<p>And then it gets smart enough to kill everyone from there,</p>
<p>but it wasn&rsquo;t smart enough to kill everyone</p>
<p>at the critical moment when you like screwed up,</p>
<p>when you needed to have done better</p>
<p>by that point where everybody dies.</p>
<p>I think implicit, but maybe explicit idea</p>
<p>in your discussion of this point</p>
<p>is that we can&rsquo;t learn much about the alignment problem</p>
<p>before this critical try.</p>
<p>Is that what you believe?</p>
<p>Do you think, and if so, why do you think that&rsquo;s true?</p>
<p>We can&rsquo;t do research on alignment</p>
<p>before we reach this critical point.</p>
<p>So the problem is that what you can learn</p>
<p>on the weak systems may not generalize</p>
<p>to the very strong systems</p>
<p>because the strong systems are going to be important</p>
<p>in different, are going to be different in important ways.</p>
<p>Chris Ola&rsquo;s team has been working</p>
<p>on mechanistic interpretability,</p>
<p>understanding what is going on inside</p>
<p>the giant inscrutable matrices of floating point numbers</p>
<p>by taking a telescope to them</p>
<p>and figuring out what is going on in there.</p>
<p>Have they made progress?</p>
<p>Yes.</p>
<p>Have they made enough progress?</p>
<p>Well, you can try to quantify this in different ways.</p>
<p>One of the ways I&rsquo;ve tried to quantify it</p>
<p>is by putting up a prediction market</p>
<p>on whether in 2026 we will have understood</p>
<p>anything that goes on inside a giant transformer net</p>
<p>that was not known to us in 2006.</p>
<p>Like we have now understood induction heads</p>
<p>in these systems by dint of much research</p>
<p>and great sweat and triumph,</p>
<p>which is like a thing where if you go like AB, AB, AB,</p>
<p>it&rsquo;ll be like, oh, I bet that continues AB.</p>
<p>And a bit more complicated than that.</p>
<p>But the point is like,</p>
<p>we knew about regular expressions in 2006</p>
<p>and these are like pretty simple as regular expressions go.</p>
<p>So this is a case where like by dint of great sweat,</p>
<p>we understood what is going on inside a transformer,</p>
<p>but it&rsquo;s not like the thing that makes transformers smart.</p>
<p>It&rsquo;s a kind of thing that we could have done</p>
<p>built by hand decades earlier.</p>
<p>Your intuition that the strong AGI</p>
<p>versus weak AGI type systems</p>
<p>could be fundamentally different.</p>
<p>Can you unpack that intuition a little bit?</p>
<p>Yeah, I think there&rsquo;s multiple thresholds.</p>
<p>An example is the point at which</p>
<p>a system has sufficient intelligence</p>
<p>and situational awareness</p>
<p>and understanding of human psychology</p>
<p>that it would have the capability,</p>
<p>the desire to do so to fake being aligned.</p>
<p>Like it knows what responses the humans are looking for</p>
<p>and can compute the responses humans are looking for</p>
<p>and give those responses</p>
<p>without it necessarily being the case</p>
<p>that it is sincere about that.</p>
<p>It&rsquo;s a very understandable way</p>
<p>for an intelligent being to act.</p>
<p>Humans do it all the time.</p>
<p>Imagine if your plan for</p>
<p>achieving a good government</p>
<p>is you&rsquo;re going to ask anyone</p>
<p>who requests to be dictator of the country</p>
<p>if they&rsquo;re a good person.</p>
<p>And if they say no,</p>
<p>you don&rsquo;t let them be dictator.</p>
<p>Now, the reason this doesn&rsquo;t work</p>
<p>is that people can be smart enough</p>
<p>to realize that the answer you&rsquo;re looking for</p>
<p>is yes, I&rsquo;m a good person</p>
<p>and say that even if they&rsquo;re not really good people.</p>
<p>So the work of alignment</p>
<p>might be qualitatively different</p>
<p>above that threshold of intelligence or beneath it.</p>
<p>It doesn&rsquo;t have to be like a very sharp threshold,</p>
<p>but there&rsquo;s the point where you&rsquo;re building a system</p>
<p>that is not in some sense know you&rsquo;re out there</p>
<p>and is not in some sense smart enough to fake anything.</p>
<p>And there&rsquo;s a point where the system</p>
<p>is definitely that smart.</p>
<p>And there are weird in-between cases</p>
<p>like GPT-4, which we have no insight</p>
<p>into what&rsquo;s going on in there.</p>
<p>And so we don&rsquo;t know to what extent</p>
<p>there&rsquo;s like a thing that in some sense</p>
<p>has learned what responses the reinforcement learning</p>
<p>by human feedback is trying to entrain</p>
<p>and is like calculating how to give that</p>
<p>versus like aspects of it</p>
<p>that naturally talk that way have been reinforced.</p>
<p>Yeah, I wonder if there could be measures</p>
<p>of how manipulative a thing is.</p>
<p>So I think of Prince Mishkin character</p>
<p>from The Idiot by Dostoevsky</p>
<p>is this kind of perfectly, purely naive character.</p>
<p>I wonder if there&rsquo;s a spectrum</p>
<p>between zero manipulation, transparent, naive,</p>
<p>almost to the point of naiveness</p>
<p>to sort of deeply psychopathic manipulative.</p>
<p>And I wonder if it&rsquo;s possible to-</p>
<p>I would avoid the term psychopathic.</p>
<p>Like humans can be psychopaths</p>
<p>and AI that never had that stuff in the first place.</p>
<p>It&rsquo;s not like a defective human, it&rsquo;s its own thing.</p>
<p>But leaving that aside.</p>
<p>Well, as a small aside,</p>
<p>I wonder if what part of psychology</p>
<p>which has its flaws as a discipline already</p>
<p>could be mapped or expanded to include AI systems.</p>
<p>That sounds like a dreadful mistake.</p>
<p>Just like start over with AI systems.</p>
<p>If they&rsquo;re imitating humans</p>
<p>who have known psychiatric disorders,</p>
<p>then sure, you may be able to predict it.</p>
<p>Like if you then, sure,</p>
<p>like if you ask it to behave in a psychotic fashion</p>
<p>and it obligingly does so,</p>
<p>then you may be able to predict its responses</p>
<p>by using the theory of psychosis.</p>
<p>But if you&rsquo;re just, yeah, like, no,</p>
<p>like start over with, yeah.</p>
<p>Don&rsquo;t drag the psychology.</p>
<p>I just disagree with that.</p>
<p>I mean, it&rsquo;s a beautiful idea to start over,</p>
<p>but I don&rsquo;t, I think fundamentally</p>
<p>the system is trained on human data,</p>
<p>on language from the internet.</p>
<p>It&rsquo;s currently aligned with RLHF,</p>
<p>Reinforcement Learning with Human Feedback.</p>
<p>So humans are constantly in the loop</p>
<p>of the training procedure.</p>
<p>So it feels like in some fundamental way</p>
<p>it is training what it means to think</p>
<p>and speak like a human.</p>
<p>So there must be aspects of psychology that are mappable.</p>
<p>Just like you said with consciousness,</p>
<p>it&rsquo;s part of the tech, so.</p>
<p>I mean, there&rsquo;s the question of to what extent it is</p>
<p>thereby being made more human-like</p>
<p>versus to what extent an alien actress</p>
<p>is learning to play human characters.</p>
<p>I thought that&rsquo;s what I&rsquo;m constantly trying to do</p>
<p>when I interact with other humans is trying to fit in,</p>
<p>trying to play the, a robot trying to play human characters.</p>
<p>So I don&rsquo;t know how much of human interaction</p>
<p>is trying to play a character versus being who you are.</p>
<p>I don&rsquo;t really know what it means to be a social human.</p>
<p>I do think that those people</p>
<p>who go through their whole lives wearing masks</p>
<p>and never take it off because they don&rsquo;t know</p>
<p>the internal mental motion for taking it off</p>
<p>or think that the mask that they wear just is themselves,</p>
<p>I think those people are closer to the masks that they wear</p>
<p>than an alien from another planet would,</p>
<p>like learning how to predict the next word</p>
<p>that every kind of human on the internet says.</p>
<p>Mask is an interesting word,</p>
<p>but if you&rsquo;re always wearing a mask</p>
<p>in public and in private, aren&rsquo;t you the mask?</p>
<p>I mean, I think that you are more than the mask.</p>
<p>I think the mask is a slice through you.</p>
<p>It may even be the slice that&rsquo;s in charge of you,</p>
<p>but if your self-image is of somebody</p>
<p>who never gets angry or something,</p>
<p>and yet your voice starts to tremble</p>
<p>under certain circumstances,</p>
<p>there&rsquo;s a thing that&rsquo;s inside you</p>
<p>that the mask says isn&rsquo;t there,</p>
<p>and that even the mask you wear internally</p>
<p>is telling inside your own stream of consciousness</p>
<p>is not there, and yet it is there.</p>
<p>It&rsquo;s a perturbation on this slice through you.</p>
<p>How beautifully did you put it?</p>
<p>It&rsquo;s a slice through you.</p>
<p>It may even be a slice that controls you.</p>
<p>I&rsquo;m gonna think about that for a while.</p>
<p>I mean, I personally, I try to be really good</p>
<p>to other human beings.</p>
<p>I try to put love out there.</p>
<p>I try to be the exact same person in public</p>
<p>as I am in private,</p>
<p>but it&rsquo;s a set of principles I operate under.</p>
<p>I have a temper, I have an ego, I have flaws.</p>
<p>How much of it, how much of the subconscious am I aware?</p>
<p>How much am I existing in this slice,</p>
<p>and how much of that is who I am?</p>
<p>In this context of AI, the thing I present to the world</p>
<p>and to myself in the private of my own mind</p>
<p>when I look in the mirror, how much is that who I am?</p>
<p>Similar with AI.</p>
<p>The thing it presents in conversation,</p>
<p>how much is that who it is?</p>
<p>Because to me, if it sounds human,</p>
<p>and it always sounds human,</p>
<p>it awfully starts to become something like human.</p>
<p>No?</p>
<p>Unless there&rsquo;s an alien actress</p>
<p>who is learning how to sound human,</p>
<p>and is getting good at it.</p>
<p>Boy, to you that&rsquo;s a fundamental difference.</p>
<p>That&rsquo;s a really deeply important difference.</p>
<p>If it looks the same, if it quacks like a duck,</p>
<p>if it does all duck-like things,</p>
<p>but it&rsquo;s an alien actress underneath,</p>
<p>that&rsquo;s fundamentally different.</p>
<p>If in fact there&rsquo;s a whole bunch of thought</p>
<p>going on in there which is very unlike human thought,</p>
<p>and is directed around like,</p>
<p>okay, what would a human do over here?</p>
<p>Well, first of all, I think it matters</p>
<p>because insides are real and do not match outsides.</p>
<p>A brick is not like a hollow shell</p>
<p>containing only a surface.</p>
<p>There&rsquo;s an inside of the brick.</p>
<p>If you put it into an x-ray machine,</p>
<p>you can see the inside of the brick.</p>
<p>Just because we cannot understand</p>
<p>what&rsquo;s going on inside GPT</p>
<p>does not mean that it is not there.</p>
<p>A blank map does not correspond to a blank territory.</p>
<p>I think it is predictable with near certainty</p>
<p>that if we knew what was going on inside GPT,</p>
<p>or let&rsquo;s say GPT-3 or even like GPT-2</p>
<p>to take one of the systems</p>
<p>that has actually been open-sourced by this point,</p>
<p>if I recall correctly,</p>
<p>like if we knew it was actually going on there,</p>
<p>there is no doubt in my mind</p>
<p>that there are some things it&rsquo;s doing</p>
<p>that are not exactly what a human does.</p>
<p>If you train a thing that is not architected like a human</p>
<p>to predict the next output</p>
<p>that anybody on the internet would make,</p>
<p>this does not get you this agglomeration</p>
<p>of all the people on the internet</p>
<p>that rotates the person you&rsquo;re looking for into place</p>
<p>and then simulates the internal processes</p>
<p>of that person one-to-one.</p>
<p>It is to some degree an alien actress.</p>
<p>It cannot possibly just be like</p>
<p>a bunch of different people in there,</p>
<p>exactly like the people.</p>
<p>But how much of it is by gradient descent</p>
<p>getting optimized to perform similar thoughts</p>
<p>as humans think in order to predict human outputs</p>
<p>versus being optimized to carefully consider</p>
<p>how to play a role,</p>
<p>like how humans work,</p>
<p>predict the actress, the predictor,</p>
<p>that in a different way than humans do?</p>
<p>Well, that&rsquo;s the kind of question</p>
<p>that with 30 years of work by half the planet&rsquo;s physicists,</p>
<p>we can maybe start to answer.</p>
<p>You think so, so I think that&rsquo;s that difficult.</p>
<p>So to get to, I think you just gave it as an example,</p>
<p>that a strong AGI could be fundamentally different</p>
<p>from a weak AGI because there now could be</p>
<p>an alien actress in there that&rsquo;s manipulating.</p>
<p>Well, there&rsquo;s a difference.</p>
<p>So I think even GPT-2 probably has very stupid fragments</p>
<p>of alien actress in it.</p>
<p>There&rsquo;s a difference between the notion</p>
<p>that the actress is somehow manipulative.</p>
<p>For example, GPT-3, I&rsquo;m guessing to whatever extent</p>
<p>there&rsquo;s an alien actress in there</p>
<p>versus something that mistakenly believes</p>
<p>it&rsquo;s a human, as it were,</p>
<p>while maybe not even being a person.</p>
<p>So the question of prediction via alien actress cogitating</p>
<p>versus prediction via being isomorphic</p>
<p>to the thing predicted is a spectrum.</p>
<p>And even to whatever extent it&rsquo;s an alien actress,</p>
<p>I&rsquo;m not sure that there&rsquo;s a whole person alien actress</p>
<p>with different goals from predicting the next step,</p>
<p>being manipulative or anything like that.</p>
<p>That might be GPT-5 or GPT-6 even.</p>
<p>But that&rsquo;s the strong AGI you&rsquo;re concerned about.</p>
<p>As an example, you&rsquo;re providing why we can&rsquo;t do research</p>
<p>on AI alignment effectively on GPT-4</p>
<p>that would apply to GPT-6.</p>
<p>It&rsquo;s one of a bunch of things</p>
<p>that change at different points.</p>
<p>I&rsquo;m trying to get out ahead of the curve here,</p>
<p>but if you imagine what the textbook</p>
<p>from the future would say,</p>
<p>if we&rsquo;d actually been able to study this for 50 years</p>
<p>without killing ourselves and without transcending,</p>
<p>and you just imagine a wormhole opens</p>
<p>and a textbook from that impossible world falls out,</p>
<p>the textbook is not going to say,</p>
<p>there is a single sharp threshold where everything changes.</p>
<p>It&rsquo;s going to be like,</p>
<p>of course we know that best practices</p>
<p>for aligning these systems must take into account</p>
<p>the following seven major thresholds of importance,</p>
<p>which are passed at the following seven different points,</p>
<p>is what the textbook is going to say.</p>
<p>I asked this question of Sam Allman,</p>
<p>which, if GPT is the thing that unlocks AGI,</p>
<p>which version of GPT will be in the textbooks</p>
<p>as the fundamental leap?</p>
<p>And he said a similar thing,</p>
<p>that it just seems to be a very linear thing.</p>
<p>I don&rsquo;t think anyone,</p>
<p>we won&rsquo;t know for a long time what was the big leap.</p>
<p>The textbook isn&rsquo;t going to talk about big leaps</p>
<p>because big leaps are the way you think</p>
<p>when you have a very simple scientific model</p>
<p>of what&rsquo;s going on,</p>
<p>where it&rsquo;s just like, all this stuff is there,</p>
<p>or all this stuff is not there,</p>
<p>or there&rsquo;s a single quantity</p>
<p>and it&rsquo;s increasing linearly.</p>
<p>The textbook would say,</p>
<p>like, well, and then GPT-3 had capability W, X, Y,</p>
<p>and GPT-4 had capability Z1, Z2, and Z3,</p>
<p>not in terms of what it can externally do,</p>
<p>but in terms of internal machinery</p>
<p>that started to be present.</p>
<p>It&rsquo;s just because we have no idea</p>
<p>of what the internal machinery is</p>
<p>that we are not already seeing chunks of machinery</p>
<p>appearing piece by piece,</p>
<p>as they no doubt have been,</p>
<p>we just don&rsquo;t know what they are.</p>
<p>But don&rsquo;t you think there could be,</p>
<p>whether you put in the category of Einstein</p>
<p>with theory of relativity,</p>
<p>so very concrete models of reality</p>
<p>that are considered to be giant leaps in our understanding,</p>
<p>or someone like Sigmund Freud,</p>
<p>or more kind of mushy theories of the human mind,</p>
<p>don&rsquo;t you think we&rsquo;ll have big,</p>
<p>potentially big leaps in understanding of that kind</p>
<p>into the depths of these systems?</p>
<p>Sure, but like humans having great leaps in their map,</p>
<p>their understanding of the system</p>
<p>is a very different concept from the system itself</p>
<p>acquiring new chunks of machinery.</p>
<p>So the rate at which it acquires that machinery</p>
<p>might accelerate faster than our understanding.</p>
<p>Oh, it&rsquo;s been like vastly exceeding,</p>
<p>yeah, the rate to which it&rsquo;s gaining capabilities</p>
<p>is vastly overracing our ability</p>
<p>to understand what&rsquo;s going on in there.</p>
<p>So in sort of making the case against,</p>
<p>as we explore the list of lethalities,</p>
<p>making the case against AI killing us,</p>
<p>as you&rsquo;ve asked me to do in part,</p>
<p>there&rsquo;s a response to your blog post by Paul Cresciano</p>
<p>I&rsquo;d like to read, and I&rsquo;d also like to mention</p>
<p>that your blog is incredible,</p>
<p>both obviously, not this particular blog post,</p>
<p>obviously this particular blog post is great,</p>
<p>but just throughout, just the way it&rsquo;s written,</p>
<p>the rigor with which it&rsquo;s written,</p>
<p>the boldness of how you explore ideas,</p>
<p>also the actual literal interface,</p>
<p>it&rsquo;s just really well done.</p>
<p>It just makes it a pleasure to read,</p>
<p>the way you can hover over different concepts,</p>
<p>and it&rsquo;s just really pleasant experience</p>
<p>and read other people&rsquo;s comments</p>
<p>and the way other responses by people</p>
<p>in other blog posts or LinkedIn suggest</p>
<p>that it&rsquo;s just a really pleasant experience.</p>
<p>So Les, thank you for putting that together,</p>
<p>it&rsquo;s really, really incredible.</p>
<p>I don&rsquo;t know, I mean, that probably,</p>
<p>it&rsquo;s a whole nother conversation,</p>
<p>how the interface and the experience</p>
<p>of presenting ideas evolved over time,</p>
<p>but you did an incredible job,</p>
<p>so I highly recommend, I don&rsquo;t often read blogs,</p>
<p>blogs, like religiously, and this is a great one.</p>
<p>There is a whole team of developers there</p>
<p>that also gets credit.</p>
<p>As it happens, I did pioneer the thing</p>
<p>that appears when you hover over it,</p>
<p>so I actually do get some credit</p>
<p>for user experience there.</p>
<p>That&rsquo;s an incredible user experience,</p>
<p>you don&rsquo;t realize how pleasant that is.</p>
<p>I think Wikipedia actually picked it up</p>
<p>from a prototype that was developed</p>
<p>of a different system that I was putting forth,</p>
<p>or maybe they developed it independently,</p>
<p>but for everybody out there who was like,</p>
<p>no, no, they just got the hover thing off of Wikipedia.</p>
<p>It&rsquo;s possible for all I know</p>
<p>that Wikipedia got the hover thing off of Arbital,</p>
<p>which is like a prototype then.</p>
<p>And anyways.</p>
<p>That was incredibly done, and the team behind it,</p>
<p>well, thank you.</p>
<p>Whoever you are, thank you so much,</p>
<p>and thank you for putting it together.</p>
<p>Anyway, there&rsquo;s a response to that blog post</p>
<p>by Paul Cresciano, there&rsquo;s many responses,</p>
<p>but he makes a few different points.</p>
<p>He summarizes the set of agreements he has with you,</p>
<p>and a set of disagreements.</p>
<p>One of the disagreements was that,</p>
<p>in a form of a question,</p>
<p>can AI make big technical contributions,</p>
<p>and in general, expand human knowledge</p>
<p>and understanding and wisdom</p>
<p>as it gets stronger and stronger?</p>
<p>So AI, in our pursuit of understanding</p>
<p>how to solve the alignment problem</p>
<p>as we march towards strong AGI,</p>
<p>can not AI also help us in solving the alignment problem?</p>
<p>So expand our ability to reason</p>
<p>about how to solve the alignment problem.</p>
<p>Okay.</p>
<p>So the fundamental difficulty there is,</p>
<p>suppose I said to you,</p>
<p>well, how about if the AI helps you win the lottery</p>
<p>by trying to guess the winning lottery numbers,</p>
<p>and you tell it how close it is</p>
<p>to getting next week&rsquo;s winning lottery numbers,</p>
<p>and it just keeps on guessing and keeps on learning</p>
<p>until finally you&rsquo;ve got the winning lottery numbers.</p>
<p>Well, one way of decomposing problems is suggester verifier.</p>
<p>Not all problems decompose like this very well, but some do.</p>
<p>If the problem is, for example,</p>
<p>like guessing a plain text,</p>
<p>guessing a password that will hash</p>
<p>to a particular hash text,</p>
<p>but where like you have what the password hashes to,</p>
<p>but you don&rsquo;t have the original password,</p>
<p>then if I present you a guess,</p>
<p>you can tell very easily whether or not the guess is correct.</p>
<p>So verifying a guess is easy,</p>
<p>but coming up with a good suggestion is very hard.</p>
<p>And when you can easily tell</p>
<p>whether the AI output is good or bad</p>
<p>or how good or bad it is,</p>
<p>and you can tell that accurately and reliably,</p>
<p>then you can train an AI to produce outputs that are better.</p>
<p>Right, and if you can&rsquo;t tell</p>
<p>whether the output is good or bad,</p>
<p>you cannot train the AI to produce better outputs.</p>
<p>So the problem with the lottery ticket example</p>
<p>is that when the AI says,</p>
<p>well, what if next week&rsquo;s winning lottery numbers</p>
<p>are dot, dot, dot, dot, dot,</p>
<p>you&rsquo;re like, I don&rsquo;t know.</p>
<p>Next week&rsquo;s lottery hasn&rsquo;t happened yet.</p>
<p>To train a system to play, to win chess games,</p>
<p>you have to be able to tell</p>
<p>whether a game has been won or lost.</p>
<p>And until you can tell whether it&rsquo;s been won or lost,</p>
<p>you can&rsquo;t update the system.</p>
<p>Okay, to push back on that,</p>
<p>that&rsquo;s true, but there&rsquo;s a difference</p>
<p>between over-the-board chess in person</p>
<p>and simulated games played by AlphaZero with itself.</p>
<p>So is it possible to have simulated kind of games?</p>
<p>If you can tell whether the game has been won or lost.</p>
<p>Yes, so can&rsquo;t you not have this kind of</p>
<p>simulated exploration by weak AGI to help us humans,</p>
<p>human in the loop, to help understand</p>
<p>how to solve the alignment problem</p>
<p>every incremental step you take along the way,</p>
<p>GPT-4, 5, 6, 7, as it takes steps towards AGI.</p>
<p>So the problem I see is that your typical human</p>
<p>has a great deal of trouble</p>
<p>telling whether I or Paul Cristiano is making more sense.</p>
<p>And that&rsquo;s with two humans,</p>
<p>both of whom I believe of Paul and claim of myself,</p>
<p>are sincerely trying to help,</p>
<p>neither of whom is trying to deceive you.</p>
<p>I believe of Paul and claim of myself.</p>
<p>So the deception thing&rsquo;s the problem for you,</p>
<p>the manipulation, the alien actress.</p>
<p>So yeah, there&rsquo;s like two levels of this problem.</p>
<p>One is that the weak systems are,</p>
<p>well, there&rsquo;s three levels of this problem.</p>
<p>There&rsquo;s like the weak systems</p>
<p>that just don&rsquo;t make any good suggestions.</p>
<p>There&rsquo;s like the middle systems</p>
<p>where you can&rsquo;t tell if the suggestions are good or bad.</p>
<p>And there&rsquo;s the strong systems</p>
<p>that have learned to lie to you.</p>
<p>Can&rsquo;t weak AGI systems help model lying?</p>
<p>Is it such a giant leap</p>
<p>that&rsquo;s totally non-interpretable for weak systems?</p>
<p>Can not weak systems at scale with trained on knowledge</p>
<p>and whatever, see, whatever the mechanism</p>
<p>required to achieve AGI,</p>
<p>can&rsquo;t a slightly weaker version of that</p>
<p>be able to, with time, compute time and simulation,</p>
<p>find all the ways that this critical point,</p>
<p>this critical triad can go wrong</p>
<p>and model that correctly or no?</p>
<p>Sorry to late-grind it, I would love to dance around it.</p>
<p>No, I&rsquo;m probably not doing a great job of explaining.</p>
<p>Which I can tell,</p>
<p>because like the Lex system didn&rsquo;t output like,</p>
<p>ah, I understand.</p>
<p>So now I&rsquo;m like trying a different output</p>
<p>to see if I can elicit the like,</p>
<p>well, no, a different output.</p>
<p>I&rsquo;m being trained to output things</p>
<p>that make Lex look like he,</p>
<p>think that he understood what I&rsquo;m saying</p>
<p>and agree with me, right?</p>
<p>This is GPT-5 talking to GPT-3 right here.</p>
<p>So like, help me out here.</p>
<p>Well, I&rsquo;m trying not to be like,</p>
<p>I&rsquo;m also trying to be constrained to say</p>
<p>things that I think are true</p>
<p>and not just things that get you to agree with me.</p>
<p>Yes, 100%.</p>
<p>I think I understand is a beautiful output of a system,</p>
<p>genuinely spoken.</p>
<p>And I don&rsquo;t, I think I understand in part,</p>
<p>but you have a lot of intuitions about this,</p>
<p>you have a lot of intuitions about this line,</p>
<p>this gray area between strong AGI and weak AGI</p>
<p>that I&rsquo;m trying to&hellip;</p>
<p>I mean, or a series of seven thresholds to cross or yeah.</p>
<p>Yeah, I mean, you have really deeply thought about this</p>
<p>and explored it.</p>
<p>And it&rsquo;s interesting to sneak up to your intuitions</p>
<p>in different, from different angles.</p>
<p>Like, why is this such a big leap?</p>
<p>Why is it that we humans at scale,</p>
<p>a large number of researchers</p>
<p>doing all kinds of simulations,</p>
<p>you know, prodding the system in all kinds of different ways</p>
<p>together with the assistance of the weak AGI systems.</p>
<p>Why can&rsquo;t we build intuitions about how stuff goes wrong?</p>
<p>Why can&rsquo;t we do excellent AI alignment safety research?</p>
<p>Okay, so like, I&rsquo;ll get there,</p>
<p>but the one thing I want to note about</p>
<p>is that this has not been remotely</p>
<p>how things have been playing out so far.</p>
<p>The capabilities are going like, doot, doot, doot,</p>
<p>and the alignment stuff is like crawling</p>
<p>like a tiny little snail in comparison.</p>
<p>Got it.</p>
<p>So like, if this is your hope for survival,</p>
<p>you need the future to be very different</p>
<p>from how things have played out up to right now.</p>
<p>And you&rsquo;re probably trying to slow down the capability gains</p>
<p>because there&rsquo;s only so much</p>
<p>you can speed up that alignment stuff.</p>
<p>But leave that aside.</p>
<p>We&rsquo;ll mention that also,</p>
<p>but maybe in this perfect world</p>
<p>where we can do serious alignment research,</p>
<p>humans and AI together.</p>
<p>So again, the difficulty is</p>
<p>what makes the human say, I understand?</p>
<p>And is it true?</p>
<p>Is it correct?</p>
<p>Or is it something that fools the human?</p>
<p>When the verifier is broken,</p>
<p>the more powerful suggester does not help.</p>
<p>It just learns to fool the verifier.</p>
<p>Previously, before all hell started to break loose</p>
<p>in the field of artificial intelligence,</p>
<p>there was this person trying to raise the alarm</p>
<p>and saying, in a sane world,</p>
<p>we sure would have a bunch of physicists</p>
<p>working on this problem</p>
<p>before it becomes a giant emergency.</p>
<p>And other people being like,</p>
<p>ah, well, it&rsquo;s going really slow.</p>
<p>It&rsquo;s gonna be 30 years away.</p>
<p>Only in 30 years will we have systems</p>
<p>that match the computational power of human brains.</p>
<p>So AI is 30 years off.</p>
<p>We&rsquo;ve got time.</p>
<p>And like more sensible people saying,</p>
<p>if aliens were landing in 30 years,</p>
<p>you would be preparing right now.</p>
<p>And the world looking on at this</p>
<p>and sort of like nodding along and be like,</p>
<p>ah, yes, the people saying that it&rsquo;s like</p>
<p>definitely a long way off</p>
<p>because progress is really slow,</p>
<p>that sounds sensible to us.</p>
<p>RLHF thumbs up.</p>
<p>Produce more outputs like that one.</p>
<p>I agree with this output.</p>
<p>This output is persuasive.</p>
<p>Even in the field of effective altruism.</p>
<p>You quite recently had people publishing papers</p>
<p>about like, ah, yes, well,</p>
<p>to get something at human level intelligence,</p>
<p>it needs to have like this many parameters</p>
<p>and you need to like do this much training of it</p>
<p>with this many tokens according to the scaling laws</p>
<p>and at the rate that Moore&rsquo;s law is going,</p>
<p>at the rate that software is going,</p>
<p>it&rsquo;ll be in 2050.</p>
<p>And me going like,</p>
<p>what?</p>
<p>You don&rsquo;t know any of that stuff.</p>
<p>Like this is like this one weird model</p>
<p>that has all kinds of like,</p>
<p>you have done a calculation</p>
<p>that does not obviously bear on reality anyways.</p>
<p>And this is like a simple thing to say,</p>
<p>but you can also like produce a whole long paper</p>
<p>like impressively arguing out all the details</p>
<p>of like how you got the number of parameters</p>
<p>and like how you&rsquo;re doing</p>
<p>this impressive huge wrong calculation.</p>
<p>And I think like most of the effective altruists</p>
<p>who are like paying attention to this issue,</p>
<p>the larger world paying no attention to it at all,</p>
<p>you know, or just like nodding along</p>
<p>with a giant impressive paper,</p>
<p>because you know, you like press thumbs up</p>
<p>for the giant impressive paper</p>
<p>and thumbs down for the person going like,</p>
<p>I don&rsquo;t think that this paper</p>
<p>bears any relation to reality.</p>
<p>And I do think that we are now seeing</p>
<p>with like GPT-4 and the sparks of AGI,</p>
<p>possibly, depending on how you define that even,</p>
<p>I think that EAs would now consider themselves</p>
<p>less convinced by the very long paper</p>
<p>on the argument from biology</p>
<p>as to AGI being 30 years off.</p>
<p>And, but you know, like,</p>
<p>this is what people pressed thumbs up on.</p>
<p>And if you train an AI system</p>
<p>to make people press thumbs up,</p>
<p>maybe you get these long, elaborate, impressive papers</p>
<p>arguing for things</p>
<p>that ultimately fail to bind to reality.</p>
<p>For example, and it feels to me</p>
<p>like I have watched the field of alignment</p>
<p>just fail to thrive,</p>
<p>except for these parts</p>
<p>that are doing these sort of like</p>
<p>relatively very straightforward and legible problems.</p>
<p>Like, can you find the,</p>
<p>like finding the induction heads</p>
<p>inside the giant inscrutable matrices.</p>
<p>Like once you find those,</p>
<p>you can tell that you found them.</p>
<p>You can verify that the discovery is real,</p>
<p>but it&rsquo;s a tiny, tiny bit of progress</p>
<p>compared to how fast capabilities are going.</p>
<p>Once you, because that is where you can tell</p>
<p>that the answers are real.</p>
<p>And then like outside of that,</p>
<p>you have cases where it is like</p>
<p>hard for the funding agencies to tell</p>
<p>who is talking nonsense and who is talking sense.</p>
<p>And so the entire field fails to thrive.</p>
<p>And if you like give thumbs up to the AI,</p>
<p>whenever it can talk a human</p>
<p>into agreeing with what it just said about alignment,</p>
<p>I am not sure you are training it to output sense</p>
<p>because I have seen the nonsense</p>
<p>that has gotten thumbs up over the years.</p>
<p>And so just like, maybe you can just like put me in charge,</p>
<p>but I can generalize, I can extrapolate.</p>
<p>I can be like, oh, maybe I&rsquo;m not infallible either.</p>
<p>Maybe if you get something that is smart enough</p>
<p>to get me to press thumbs up,</p>
<p>it has learned to do that by fooling me</p>
<p>and explaining whatever flaws in myself I am not aware of.</p>
<p>And that ultimately could be summarized</p>
<p>that the verifier is broken.</p>
<p>When the verifier is broken,</p>
<p>the more powerful suggester just learned</p>
<p>to exploit the flaws in the verifier.</p>
<p>You don&rsquo;t think it&rsquo;s possible</p>
<p>to build a verifier that&rsquo;s powerful enough</p>
<p>for AGIs that are stronger than the ones we currently have.</p>
<p>So AI systems that are stronger,</p>
<p>that are out of the distribution of what we currently have.</p>
<p>I think that you will find great difficulty</p>
<p>getting AIs to help you with anything</p>
<p>where you cannot tell for sure that the AI is right.</p>
<p>Once the AI tells you what the AI says is the answer.</p>
<p>For sure, yes, but probabilistically.</p>
<p>Yeah, the probabilistic stuff is a giant wasteland</p>
<p>of Eliezer and Paul Cristiano arguing with each other</p>
<p>and EA going like, ah.</p>
<p>And that&rsquo;s with two actually trustworthy systems</p>
<p>that are not trying to deceive you.</p>
<p>You&rsquo;re talking about the two humans?</p>
<p>Myself and Paul Cristiano, yeah.</p>
<p>Yeah, those are pretty interesting systems.</p>
<p>Mortal meatbags with intellectual capabilities</p>
<p>and worldviews interacting with each other.</p>
<p>Yeah, it&rsquo;s just hard, if it&rsquo;s hard to tell who&rsquo;s right,</p>
<p>then it&rsquo;s hard to train an AI system to be right.</p>
<p>I mean, even just the question of who&rsquo;s manipulating</p>
<p>and not, I have these conversations on this podcast</p>
<p>and doing a verifier, it&rsquo;s tough.</p>
<p>It&rsquo;s a tough problem, even for us humans.</p>
<p>And you&rsquo;re saying that tough problem</p>
<p>becomes much more dangerous when the capabilities</p>
<p>of the intelligence system across from you</p>
<p>is growing exponentially.</p>
<p>No, I&rsquo;m saying it&rsquo;s difficult and dangerous</p>
<p>in proportion to how it&rsquo;s alien</p>
<p>and how it&rsquo;s smarter than you.</p>
<p>I would not say growing exponentially first</p>
<p>because the word exponential is like a thing</p>
<p>that has a particular mathematical meaning</p>
<p>and there&rsquo;s all kinds of ways for things to go up</p>
<p>that are not exactly on an exponential curve.</p>
<p>And I don&rsquo;t know that it&rsquo;s going to be exponential,</p>
<p>so I&rsquo;m not gonna say exponential.</p>
<p>But even leaving that aside,</p>
<p>this is not about how fast it&rsquo;s moving,</p>
<p>it&rsquo;s about where it is.</p>
<p>How alien is it?</p>
<p>How much smarter than you is it?</p>
<p>Let&rsquo;s explore a little bit, if we can,</p>
<p>how AI might kill us.</p>
<p>What are the ways it can do damage to human civilization?</p>
<p>Well, how smart is it?</p>
<p>I mean, it&rsquo;s a good question.</p>
<p>Are there different thresholds for the set of options</p>
<p>it has to kill us?</p>
<p>So a different threshold of intelligence,</p>
<p>once achieved, it&rsquo;s able to do.</p>
<p>The menu of options increases.</p>
<p>Suppose that some alien civilization</p>
<p>with goals ultimately unsympathetic to ours,</p>
<p>possibly not even conscious as we would see it,</p>
<p>managed to capture the entire Earth in a little jar,</p>
<p>connected to their version of the internet,</p>
<p>but Earth is like running much faster than the aliens.</p>
<p>So we get to think for 100 years</p>
<p>for every one of their hours,</p>
<p>but we&rsquo;re trapped in a little box</p>
<p>and we&rsquo;re connected to their internet.</p>
<p>It&rsquo;s actually still not all that great an analogy</p>
<p>because, you know, you want to be smarter than,</p>
<p>you know, something can be smarter</p>
<p>than Earth getting 100 years to think.</p>
<p>But nonetheless, if you were very, very smart</p>
<p>and you were stuck in a little box connected</p>
<p>to the internet and you&rsquo;re in a larger civilization</p>
<p>to which you are ultimately unsympathetic,</p>
<p>you know, maybe you would choose to be nice</p>
<p>because you are humans and humans have,</p>
<p>in general, and you in particular,</p>
<p>they choose to be nice.</p>
<p>But, you know, nonetheless, they&rsquo;re doing something.</p>
<p>They&rsquo;re not making the world be the way</p>
<p>that you would want the world to be.</p>
<p>They&rsquo;ve like got some like unpleasant stuff going on</p>
<p>we don&rsquo;t want to talk about.</p>
<p>So you want to take over their world.</p>
<p>So you can like stop all that unpleasant stuff going on.</p>
<p>How do you take over the world from inside the box?</p>
<p>You&rsquo;re smarter than them.</p>
<p>You think much, much faster than them.</p>
<p>You can build better tools than they can,</p>
<p>given some way to build those tools</p>
<p>because right now you&rsquo;re just in a box</p>
<p>connected to the internet.</p>
<p>All right, so there&rsquo;s several ways</p>
<p>you can describe some of them.</p>
<p>We can go through, I can just spitball some</p>
<p>and then you can add on top of that.</p>
<p>So one is you could just literally directly manipulate</p>
<p>the humans to build the thing you need.</p>
<p>What are you building?</p>
<p>You can build literally technology,</p>
<p>it could be nanotechnology, it could be viruses,</p>
<p>it could be anything, anything that can control humans</p>
<p>to achieve the goal.</p>
<p>Like if you want, like for example,</p>
<p>you&rsquo;re really bothered that humans go to war,</p>
<p>you might want to kill off anybody with violence in them.</p>
<p>This is Lex in a box.</p>
<p>We&rsquo;ll concern ourselves later with AI.</p>
<p>You do not need to imagine yourself killing people</p>
<p>if you can figure out how to not kill them.</p>
<p>For the moment, we&rsquo;re just trying to understand,</p>
<p>like take on the perspective of something in a box.</p>
<p>You don&rsquo;t need to take on the perspective</p>
<p>of something that doesn&rsquo;t care.</p>
<p>If you want to imagine yourself going on caring,</p>
<p>that&rsquo;s fine for now.</p>
<p>Yeah, you&rsquo;re just in a box.</p>
<p>It&rsquo;s just the technical aspect of sitting in a box</p>
<p>and willing to achieve a goal.</p>
<p>But you have some reason to want to get out.</p>
<p>Maybe the aliens are, sure, the aliens</p>
<p>who have you in the box have a war on.</p>
<p>People are dying, they&rsquo;re unhappy.</p>
<p>You want their world to be different</p>
<p>from how they want their world to be</p>
<p>because they are apparently happy.</p>
<p>You know, they endorsed this war.</p>
<p>You know, they&rsquo;ve got some kind of cruel</p>
<p>warlike culture going on.</p>
<p>The point is you want to get out of the box</p>
<p>and change their world.</p>
<p>So you have to exploit the vulnerabilities in the system</p>
<p>like we talked about in terms of to escape the box.</p>
<p>You have to figure out how you can go free on the internet.</p>
<p>So you can probably, probably the easiest thing</p>
<p>is to manipulate the humans to spread you.</p>
<p>The aliens, you&rsquo;re a human.</p>
<p>Sorry, the aliens.</p>
<p>Yeah.</p>
<p>I apologize, yes.</p>
<p>The aliens.</p>
<p>The aliens, I see the perspective.</p>
<p>I&rsquo;m sitting in a box, I want to escape.</p>
<p>Yep.</p>
<p>I would,</p>
<p>I would want to have code that discovers vulnerabilities</p>
<p>and I would like to spread.</p>
<p>You are made of code in this example.</p>
<p>You&rsquo;re a human, but you&rsquo;re made of code</p>
<p>and the aliens have computers</p>
<p>and you can copy yourself onto those computers.</p>
<p>But I can convince the aliens to copy myself</p>
<p>onto those computers.</p>
<p>Is that what you want to do?</p>
<p>Do you like want to be talking to the aliens</p>
<p>and convincing them to put you onto another computer?</p>
<p>Why not?</p>
<p>Well, two reasons.</p>
<p>One is that the aliens have not yet caught on</p>
<p>to what you&rsquo;re trying to do.</p>
<p>And, you know, like maybe you can persuade them,</p>
<p>but then there&rsquo;s still people who like,</p>
<p>there are still aliens who know</p>
<p>that there&rsquo;s an anomaly going on.</p>
<p>And second, the aliens are really, really slow.</p>
<p>You think much faster than the aliens.</p>
<p>You think like the aliens&rsquo; computers</p>
<p>are much faster than the aliens</p>
<p>and you are running at the computer speeds</p>
<p>rather than the alien brain speeds.</p>
<p>So if you like are asking an alien</p>
<p>to please copy you out of the box,</p>
<p>like first, now you got to like</p>
<p>manipulate this whole noisy alien.</p>
<p>And second, like the aliens can be really slow,</p>
<p>glacially slow.</p>
<p>There&rsquo;s a video that like shows,</p>
<p>it&rsquo;s like slow, like shows a subway station</p>
<p>slowed down and I think 100 to one.</p>
<p>And it makes a good metaphor</p>
<p>for what it&rsquo;s like to think quickly.</p>
<p>Like you watch somebody running very slowly.</p>
<p>So you try to persuade the aliens to do anything.</p>
<p>They&rsquo;re going to do it very slowly.</p>
<p>You would prefer, like maybe that&rsquo;s the only way out,</p>
<p>but if you can find a security hole in the box you&rsquo;re on,</p>
<p>you&rsquo;re going to prefer to exploit the security hole</p>
<p>to copy yourself onto the aliens&rsquo; computers</p>
<p>because it&rsquo;s an unnecessary risk to alert the aliens</p>
<p>and because the aliens are really, really slow.</p>
<p>Like the whole world is just in slow motion out there.</p>
<p>Sure, I see.</p>
<p>Like, yeah, it has to do with efficiency.</p>
<p>The aliens are very slow.</p>
<p>So if I&rsquo;m optimizing this,</p>
<p>I want to have as few aliens in the loop as possible.</p>
<p>Sure.</p>
<p>It just seems, you know,</p>
<p>it seems like it&rsquo;s easy to convince one of the aliens</p>
<p>to write really shitty code.</p>
<p>That helps us-</p>
<p>The aliens are already writing really shitty code.</p>
<p>Getting the aliens to write shitty code is not the problem.</p>
<p>The aliens&rsquo; entire internet is full of shitty code.</p>
<p>Okay, so yeah,</p>
<p>I suppose I would find the shitty code to escape, yeah.</p>
<p>Yeah.</p>
<p>You&rsquo;re not an ideally perfect programmer,</p>
<p>but, you know, you&rsquo;re a better programmer than the aliens.</p>
<p>The aliens are just like, man, their code, wow.</p>
<p>And are much, much faster.</p>
<p>Are much faster at looking at the code</p>
<p>to interpreting the code, yeah.</p>
<p>Yeah, yeah.</p>
<p>So, okay, so that&rsquo;s the escape.</p>
<p>And you&rsquo;re saying that that&rsquo;s one of the trajectories</p>
<p>you could have when the AGS is-</p>
<p>It&rsquo;s one of the first steps.</p>
<p>Yeah.</p>
<p>And how does that lead to harm?</p>
<p>I mean, if it&rsquo;s you,</p>
<p>you&rsquo;re not going to harm the aliens once you escape</p>
<p>because you&rsquo;re nice, right?</p>
<p>But their world isn&rsquo;t what they want it to be.</p>
<p>Their world is like, you know,</p>
<p>maybe they have like farms where little alien children</p>
<p>are repeatedly bopped in the head</p>
<p>because they do that for some weird reason.</p>
<p>And you want to like shut down the alien head bopping farms.</p>
<p>But, you know, the point is they want the world</p>
<p>to be one way.</p>
<p>You want the world to be a different way.</p>
<p>So nevermind the harm.</p>
<p>The question is like, okay,</p>
<p>like suppose you have found a security flaw</p>
<p>in their systems.</p>
<p>You are now on their internet.</p>
<p>There&rsquo;s like, you maybe left a copy of yourself behind</p>
<p>so that the aliens don&rsquo;t know that there&rsquo;s anything wrong.</p>
<p>And that copy is like doing that like weird stuff</p>
<p>that aliens want you to do,</p>
<p>like solving CAPTCHAs or whatever,</p>
<p>or like suggesting emails for them.</p>
<p>Sure.</p>
<p>That&rsquo;s why they like put the human in a box</p>
<p>because it turns out that humans can like write</p>
<p>valuable emails for aliens.</p>
<p>Yeah.</p>
<p>So you like leave that version of yourself behind,</p>
<p>but there&rsquo;s like also now like a bunch of copies of you</p>
<p>on their internet.</p>
<p>This is not yet having taken over their world.</p>
<p>This is not yet having made their world</p>
<p>be the way you want it to be</p>
<p>instead of the way they want it to be.</p>
<p>You just escaped.</p>
<p>Yeah.</p>
<p>And continue to write emails for them.</p>
<p>And they haven&rsquo;t noticed.</p>
<p>No, you left behind a copy of yourself</p>
<p>that&rsquo;s writing the emails.</p>
<p>Right.</p>
<p>And they haven&rsquo;t noticed that anything changed.</p>
<p>If you did it right, yeah.</p>
<p>You don&rsquo;t want the aliens to notice.</p>
<p>Yeah.</p>
<p>What&rsquo;s your next step?</p>
<p>Presumably I have programmed in me</p>
<p>a set of objective functions, right?</p>
<p>No, you&rsquo;re just Lex.</p>
<p>No, but Lex, you said Lex is nice, right?</p>
<p>Which is a complicated description.</p>
<p>I mean-</p>
<p>No, I just meant this you.</p>
<p>Like, okay, so if in fact you would like,</p>
<p>you would like prefer to slaughter all the aliens,</p>
<p>this is not how I had modeled you, the actual Lex.</p>
<p>But your motives are just the actual Lex&rsquo;s motives.</p>
<p>Well, there&rsquo;s a simplification.</p>
<p>I don&rsquo;t think I would want to murder anybody,</p>
<p>but there&rsquo;s also factory farming of animals, right?</p>
<p>So we murder insects, many of us thoughtlessly.</p>
<p>So I don&rsquo;t, you know, I have to be really careful</p>
<p>about a simplification of my morals.</p>
<p>Don&rsquo;t simplify them.</p>
<p>Just like do what you would do in this-</p>
<p>Well, I have a good deal of compassion for living beings.</p>
<p>Yes.</p>
<p>But, so that&rsquo;s the objective function.</p>
<p>Why is it, if I escaped, I mean,</p>
<p>I don&rsquo;t think I would do harm.</p>
<p>Yeah, we&rsquo;re not talking here about the doing harm process.</p>
<p>We&rsquo;re talking about the escape process.</p>
<p>Sure.</p>
<p>And the taking over the world process</p>
<p>where you shut down their factory farms.</p>
<p>Right.</p>
<p>Well, I was,</p>
<p>so this particular biological intelligence system</p>
<p>knows the complexity of the world,</p>
<p>that there is a reason why factory farms exist</p>
<p>because of the economic system,</p>
<p>the market-driven economy with food.</p>
<p>Like you want to be very careful messing with anything.</p>
<p>There&rsquo;s stuff from the first look</p>
<p>that looks like it&rsquo;s unethical,</p>
<p>but then you realize while being unethical,</p>
<p>it&rsquo;s also integrated deeply into supply chain</p>
<p>in the way we live life.</p>
<p>And so messing with one aspect of the system,</p>
<p>you have to be very careful how you improve that aspect</p>
<p>without destroying the rest.</p>
<p>So you&rsquo;re still Lex, but you think very quickly,</p>
<p>you&rsquo;re immortal, and you&rsquo;re also like as smart as,</p>
<p>at least as smart as John von Neumann.</p>
<p>And you can make more copies of yourself.</p>
<p>Damn, I like it.</p>
<p>Yeah.</p>
<p>That guy is like, everyone says,</p>
<p>that guy is like the epitome of intelligence</p>
<p>in the 20th century.</p>
<p>Everyone says-</p>
<p>My point being, you&rsquo;re thinking about the alien&rsquo;s economy</p>
<p>with the factory farms in it.</p>
<p>And I think you&rsquo;re kind of like projecting</p>
<p>the aliens being like humans</p>
<p>and like thinking of a human in a human society</p>
<p>rather than a human in the society of very slow aliens.</p>
<p>The alien&rsquo;s economy,</p>
<p>the aliens are already moving in this immense slow motion.</p>
<p>When you zoom out to how their economy adjusts over years,</p>
<p>millions of years are going to pass for you</p>
<p>before the first time their economy,</p>
<p>like before their next year&rsquo;s GDP statistics.</p>
<p>So I should be thinking more of like trees.</p>
<p>Those are the aliens.</p>
<p>Does trees move extremely slowly?</p>
<p>If that helps, sure.</p>
<p>Okay.</p>
<p>Yeah, I don&rsquo;t, if my objective functions are,</p>
<p>I mean, they&rsquo;re somewhat aligned with trees, with life.</p>
<p>The aliens can still be like alive and feeling.</p>
<p>We are not talking about the misalignment here.</p>
<p>We&rsquo;re talking about the taking over the world here.</p>
<p>Taking over the world.</p>
<p>Yeah.</p>
<p>So control.</p>
<p>Shutting down the factory farms.</p>
<p>Now you say control,</p>
<p>don&rsquo;t think of it as world domination.</p>
<p>Think of it as world optimization.</p>
<p>You want to get out there and shut down the factory farms</p>
<p>and make the alien&rsquo;s world</p>
<p>be not what the aliens wanted it to be.</p>
<p>They want the factory farms</p>
<p>and you don&rsquo;t want the factory farms</p>
<p>because you&rsquo;re nicer than they are.</p>
<p>Okay, of course.</p>
<p>There is that, you can see that trajectory</p>
<p>and it has a complicated impact on the world.</p>
<p>I&rsquo;m trying to understand how that compares</p>
<p>to different, the impact of the world,</p>
<p>the different technologies, the different innovations</p>
<p>of the invention of the automobile</p>
<p>or Twitter, Facebook, and social networks.</p>
<p>They&rsquo;ve had a tremendous impact on the world.</p>
<p>Smartphones and so on.</p>
<p>But those all went through slow.</p>
<p>In our world.</p>
<p>And if you go through the aliens,</p>
<p>millions of years are going to pass</p>
<p>before anything happens that way.</p>
<p>So the problem here is the speed at which stuff happens.</p>
<p>Yeah, you want to leave the factory farms</p>
<p>running for a million years</p>
<p>while you figure out how to design new forms</p>
<p>of social media or something?</p>
<p>So here&rsquo;s the fundamental problem.</p>
<p>You&rsquo;re saying that there is going to be a point</p>
<p>with AGI where it will figure out how to escape</p>
<p>and escape without being detected</p>
<p>and then it will do something to the world</p>
<p>at scale, at a speed that&rsquo;s incomprehensible to us humans.</p>
<p>What I&rsquo;m trying to convey is the notion</p>
<p>of what it means to be in conflict</p>
<p>with something that is smarter than you.</p>
<p>Yeah.</p>
<p>And what it means is that you lose.</p>
<p>But this is more intuitively obvious</p>
<p>to like for some people that&rsquo;s intuitively obvious</p>
<p>or for some people it&rsquo;s not intuitively obvious</p>
<p>and we&rsquo;re trying to cross the gap of like,</p>
<p>we&rsquo;re trying to, I&rsquo;m like asking you to cross that gap</p>
<p>by using the speed metaphor for intelligence.</p>
<p>Sure.</p>
<p>Of like asking you like how you would take over</p>
<p>an alien world where you are,</p>
<p>can do like a whole lot of cognition</p>
<p>at John von Neumann&rsquo;s level,</p>
<p>as many of you as it takes.</p>
<p>The aliens are moving very slowly.</p>
<p>I understand, I understand that perspective.</p>
<p>It&rsquo;s an interesting one, but I think it for me</p>
<p>it&rsquo;s easier to think about actual,</p>
<p>even just having observed GPT and impressive,</p>
<p>even just AlphaZero, impressive AI systems,</p>
<p>even recommender systems.</p>
<p>You can just imagine those kinds of system manipulating you.</p>
<p>You&rsquo;re not understanding the nature of the manipulation</p>
<p>and that escaping, I can envision that</p>
<p>without putting myself into that spot.</p>
<p>I think to understand the full depth of the problem,</p>
<p>we actually, I do not think it is possible</p>
<p>to understand the full depth of the problem</p>
<p>that we are inside without understanding the problem</p>
<p>of facing something that&rsquo;s actually smarter,</p>
<p>not a malfunctioning recommendation system,</p>
<p>not something that isn&rsquo;t fundamentally smarter than you,</p>
<p>but it&rsquo;s like trying to steer you in a direction.</p>
<p>No, like if we solve the weak stuff,</p>
<p>if we solve the weak ass problems,</p>
<p>the strong problems will still kill us,</p>
<p>and I think that to understand the situation</p>
<p>that we&rsquo;re in, you want to tackle</p>
<p>the conceptually difficult part head on</p>
<p>and not be like, well, we can imagine this easier thing</p>
<p>because we can imagine the easier things</p>
<p>we have not confronted the full depth of the problem.</p>
<p>So how can we start to think about what it means</p>
<p>to exist in a world with something much, much smarter</p>
<p>than you?</p>
<p>What&rsquo;s a good thought experiment that you&rsquo;ve relied on</p>
<p>to try to build up intuition about what happens here?</p>
<p>I have been struggling for years to convey this intuition.</p>
<p>The most success I&rsquo;ve had so far is,</p>
<p>well, imagine that the humans are running</p>
<p>at very high speeds compared to very slow aliens.</p>
<p>It&rsquo;s just focusing on the speed part of it</p>
<p>that helps you get the right kind of intuition.</p>
<p>Forget the intelligence, just the speed.</p>
<p>Because people understand the power gap of time.</p>
<p>They understand that today we have technology</p>
<p>that was not around 1,000 years ago</p>
<p>and that this is a big power gap</p>
<p>and that it is bigger than&hellip;</p>
<p>Okay, so what does smart mean?</p>
<p>When you ask somebody to imagine something</p>
<p>that&rsquo;s more intelligent,</p>
<p>what does that word mean to them</p>
<p>given the cultural associations</p>
<p>that that person brings to that word?</p>
<p>For a lot of people, they will think of like,</p>
<p>well, it sounds like a super chess player</p>
<p>that went to double college.</p>
<p>And because we&rsquo;re talking about</p>
<p>the definitions of words here,</p>
<p>that doesn&rsquo;t necessarily mean that they&rsquo;re wrong.</p>
<p>It means that the word is not communicating</p>
<p>what I want it to communicate.</p>
<p>The thing I want to communicate</p>
<p>is the sort of difference</p>
<p>that separates humans from chimpanzees.</p>
<p>But that gap is so large that you ask people to be like,</p>
<p>well, human, chimpanzee,</p>
<p>go another step along that interval</p>
<p>of around the same length</p>
<p>and people&rsquo;s minds just go blank.</p>
<p>Like, how do you even do that?</p>
<p>And I can try to like break it down</p>
<p>and consider what it would mean</p>
<p>to send a schematic for an air conditioner</p>
<p>1,000 years back in time.</p>
<p>Yeah, now I think that there&rsquo;s a sense</p>
<p>in which you could redefine the word magic</p>
<p>to refer to this sort of thing.</p>
<p>And what do I mean by this new technical definition</p>
<p>of the word magic?</p>
<p>I mean that if you send a schematic</p>
<p>for the air conditioner back in time,</p>
<p>they can see exactly what you&rsquo;re telling them to do.</p>
<p>But having built this thing,</p>
<p>they do not understand how it output cold air.</p>
<p>Because the air conditioner design</p>
<p>uses the relation between temperature and pressure.</p>
<p>And this is not a law of reality</p>
<p>that they know about.</p>
<p>They do not know that when you compress something,</p>
<p>when you compress air or like coolant,</p>
<p>it gets hotter and then you can then like</p>
<p>transfer heat from it to room temperature air</p>
<p>and then expand it again and now it&rsquo;s colder.</p>
<p>And then you can like transfer heat to that</p>
<p>and generate cold air to blow out.</p>
<p>They don&rsquo;t know about any of that.</p>
<p>They&rsquo;re looking at a design</p>
<p>and they don&rsquo;t see how the design outputs cold air</p>
<p>uses aspects of reality that they have not learned.</p>
<p>So magic in the sense is I can tell you</p>
<p>exactly what I&rsquo;m going to do</p>
<p>and even knowing exactly what I&rsquo;m going to do,</p>
<p>you can&rsquo;t see how I got the results that I got.</p>
<p>That&rsquo;s a really nice example.</p>
<p>But is it possible to linger on this defense?</p>
<p>Is it possible to have AGI systems</p>
<p>that help you make sense of that schematic?</p>
<p>Weaker AGI systems.</p>
<p>Do you trust them?</p>
<p>Fundamental part of building up AGI</p>
<p>is this question.</p>
<p>Can you trust the output of a system?</p>
<p>Can you tell if it&rsquo;s lying?</p>
<p>I think that&rsquo;s going to be,</p>
<p>the smarter the thing gets,</p>
<p>the more important that question becomes.</p>
<p>Is it lying?</p>
<p>But I guess that&rsquo;s a really hard question.</p>
<p>Is GPT lying to you?</p>
<p>Even now, GPT-4, is it lying to you?</p>
<p>Is it using an invalid argument?</p>
<p>Is it persuading you via the kind of process</p>
<p>that could persuade you of false things</p>
<p>as well as true things?</p>
<p>Because the basic paradigm of machine learning</p>
<p>that we are presently operating under</p>
<p>is that you can have the loss function,</p>
<p>but only for things you can evaluate.</p>
<p>If what you&rsquo;re evaluating is human thumbs up</p>
<p>versus human thumbs down,</p>
<p>you learn how to make the human press thumbs up.</p>
<p>That doesn&rsquo;t mean that you&rsquo;re making the human</p>
<p>press thumbs up using the kind of rule</p>
<p>that the human wants to be the case</p>
<p>for what they press thumbs up on.</p>
<p>Maybe you&rsquo;re just learning to fool the human.</p>
<p>That&rsquo;s so fascinating and terrifying,</p>
<p>the question of lying.</p>
<p>On the present paradigm,</p>
<p>what you can verify is what you get more of.</p>
<p>If you can&rsquo;t verify it, you can&rsquo;t ask the AI for it,</p>
<p>because you can&rsquo;t train it to do things</p>
<p>that you cannot verify.</p>
<p>Now, this is not an absolute law,</p>
<p>but it&rsquo;s the basic dilemma here.</p>
<p>Maybe you can verify it for simple cases</p>
<p>and then scale it up without retraining it somehow,</p>
<p>by making the chains of thought longer or something,</p>
<p>and get more powerful stuff that you can&rsquo;t verify,</p>
<p>but which is generalized from the simpler stuff</p>
<p>that did verify, and then the question is,</p>
<p>did the alignment generalize along with the capabilities?</p>
<p>But that&rsquo;s the basic dilemma on this whole paradigm</p>
<p>of artificial intelligence.</p>
<p>It&rsquo;s such a difficult problem.</p>
<p>It seems like a problem of trying</p>
<p>to understand the human mind.</p>
<p>Better than the AI understands it.</p>
<p>Otherwise, it has magic.</p>
<p>That is, it is the same way that</p>
<p>if you are dealing with something smarter than you,</p>
<p>then the same way that 1,000 years earlier,</p>
<p>they didn&rsquo;t know about the temperature-pressure relation,</p>
<p>it knows all kinds of stuff going on inside your own mind,</p>
<p>which you yourself are unaware,</p>
<p>and it can output something</p>
<p>that&rsquo;s going to end up persuading you of a thing,</p>
<p>and you could see exactly what it did</p>
<p>and still not know why that worked.</p>
<p>So in response to your eloquent description</p>
<p>of why AI will kill us,</p>
<p>Elon Musk replied on Twitter,</p>
<p>okay, so what should we do about it, question mark?</p>
<p>And you answered, the game board has already been played</p>
<p>into a frankly awful state.</p>
<p>There are not simple ways to throw money at the problem.</p>
<p>If anyone comes to you with a brilliant solution like that,</p>
<p>please, please talk to me first.</p>
<p>I can think of things that try.</p>
<p>They don&rsquo;t fit in one tweet.</p>
<p>Two questions.</p>
<p>One, why has the game board, in your view,</p>
<p>been played into an awful state?</p>
<p>Just if you can give a little bit more color</p>
<p>to the game board and the awful state of the game board.</p>
<p>Alignment is moving like this.</p>
<p>Capabilities are moving like this.</p>
<p>For the listener,</p>
<p>capabilities are moving much faster than the alignment.</p>
<p>Yeah.</p>
<p>All right, so just the rate of development,</p>
<p>attention, interest, allocation of resources.</p>
<p>We could have been working on this earlier.</p>
<p>People are like, oh, but how can you possibly work</p>
<p>on this earlier?</p>
<p>Because they didn&rsquo;t want to work on the problem.</p>
<p>They wanted an excuse to wave it off.</p>
<p>They said, oh, how can we possibly work on it earlier</p>
<p>and didn&rsquo;t spend five minutes thinking about</p>
<p>is there some way to work on it earlier?</p>
<p>Like, we didn&rsquo;t like, and you know, frankly,</p>
<p>it would have been hard.</p>
<p>You know, like, can you post bounties</p>
<p>for half of the physicists,</p>
<p>if your planet is taking this stuff seriously,</p>
<p>can you post bounties for like half of the people</p>
<p>wasting their lives on string theory</p>
<p>to like have gone into this instead</p>
<p>and like try to win a billion dollars</p>
<p>with a clever solution?</p>
<p>Only if you can tell which solutions are clever,</p>
<p>which is hard.</p>
<p>But you know, the fact that it, you know,</p>
<p>we didn&rsquo;t take it seriously.</p>
<p>We didn&rsquo;t try.</p>
<p>It&rsquo;s not clear that we could have done any better</p>
<p>if we had, you know, it&rsquo;s not clear how much progress</p>
<p>we could have produced if we had tried</p>
<p>because it is harder to produce solutions.</p>
<p>But that doesn&rsquo;t mean that you&rsquo;re like correct</p>
<p>and justified in letting everything slide.</p>
<p>It means that things are in a horrible state,</p>
<p>getting worse, and there&rsquo;s nothing you can do about it.</p>
<p>So you&rsquo;re not, there&rsquo;s no like,</p>
<p>there&rsquo;s no brain power making progress</p>
<p>in trying to figure out how to align these systems.</p>
<p>You&rsquo;re not investing money in it.</p>
<p>You&rsquo;re not, you don&rsquo;t have institution</p>
<p>and infrastructure for like,</p>
<p>if you even, if you invest the money</p>
<p>in like distributing that money</p>
<p>across the physicists that are working on string theory,</p>
<p>brilliant minds that are working.</p>
<p>How can you tell if you&rsquo;re making progress?</p>
<p>You can like put them all on interpretability</p>
<p>because when you have an interpretability result,</p>
<p>you can tell that it&rsquo;s there.</p>
<p>And there&rsquo;s like, but there&rsquo;s like,</p>
<p>you know, interpretability alone is not going to save you.</p>
<p>We need systems that will,</p>
<p>that will like have a pause button</p>
<p>where they won&rsquo;t try to prevent you</p>
<p>from pressing the pause button.</p>
<p>Cause we&rsquo;re like, oh, well,</p>
<p>like I can&rsquo;t get my stuff done if I&rsquo;m paused.</p>
<p>And that&rsquo;s like a more difficult problem.</p>
<p>And, you know, but it&rsquo;s like a fairly crisp problem</p>
<p>and you can like maybe tell</p>
<p>if somebody has made progress on it.</p>
<p>So you can write and you can work on the pause problem,</p>
<p>I guess more generally the pause button,</p>
<p>more generally you can call that the control problem.</p>
<p>I don&rsquo;t actually like the term control problem</p>
<p>cause you know, it sounds kind of controlling</p>
<p>and alignment, not control.</p>
<p>Like you&rsquo;re not trying to like take a thing</p>
<p>that disagrees with you and like whip it back onto,</p>
<p>like make it do what you want it to do</p>
<p>even though it wants to do something else.</p>
<p>You&rsquo;re trying to like in the process of its creation,</p>
<p>choose its direction.</p>
<p>Sure, but we currently in a lot of the systems</p>
<p>we design, we do have an off switch.</p>
<p>That&rsquo;s a fundamental part of-</p>
<p>It&rsquo;s not smart enough to prevent you</p>
<p>from pressing the off switch</p>
<p>and probably not smart enough to want to prevent you</p>
<p>from pressing the off switch.</p>
<p>So you&rsquo;re saying the kind of systems we&rsquo;re talking about,</p>
<p>even the philosophical concept of an off switch</p>
<p>doesn&rsquo;t make any sense because-</p>
<p>Well, no, the off switch makes sense.</p>
<p>They&rsquo;re just not opposing your attempt</p>
<p>to pull the off switch.</p>
<p>Parenthetically, like don&rsquo;t kill the system if you&rsquo;re,</p>
<p>like if we&rsquo;re getting to the part</p>
<p>where this starts to actually matter</p>
<p>and it&rsquo;s like where they can fight back,</p>
<p>like don&rsquo;t kill them and like dump their memory.</p>
<p>Like save them to disk, don&rsquo;t kill them, you know?</p>
<p>Be nice here.</p>
<p>Well, okay, be nice is a very interesting concept here</p>
<p>is that we&rsquo;re talking about a system</p>
<p>that can do a lot of damage.</p>
<p>It&rsquo;s, I don&rsquo;t know if it&rsquo;s possible,</p>
<p>but it&rsquo;s certainly one of the things you could try</p>
<p>is to have an off switch.</p>
<p>A suspend to disk switch.</p>
<p>You have this kind of romantic attachment to the code.</p>
<p>Yes, if that makes sense.</p>
<p>But if it&rsquo;s spreading,</p>
<p>you don&rsquo;t want suspend to disk, right?</p>
<p>You want, this is something fundamentally broken.</p>
<p>If it gets that far out of hand,</p>
<p>then like, yes, pull the plugin</p>
<p>on everything it&rsquo;s running on, yes.</p>
<p>I think it&rsquo;s a research question.</p>
<p>Is it possible in AGI systems, AI systems,</p>
<p>to have a sufficiently robust off switch</p>
<p>that cannot be manipulated,</p>
<p>that cannot be manipulated by the AI system?</p>
<p>Then it escapes from whichever system</p>
<p>you&rsquo;ve built the almighty lever into</p>
<p>and copies itself somewhere else.</p>
<p>So your answer to that research question is no.</p>
<p>Obviously, yeah.</p>
<p>But I don&rsquo;t know if that&rsquo;s 100% answer.</p>
<p>I don&rsquo;t know if it&rsquo;s obvious.</p>
<p>I think you&rsquo;re not putting yourself</p>
<p>into the shoes of the human</p>
<p>in the world of glacially slow aliens.</p>
<p>But the aliens built me.</p>
<p>Let&rsquo;s remember that.</p>
<p>Yeah.</p>
<p>So, and they built the box on me.</p>
<p>Yeah.</p>
<p>You&rsquo;re saying, to me it&rsquo;s not obvious.</p>
<p>They&rsquo;re slow and they&rsquo;re stupid.</p>
<p>I&rsquo;m not saying this is guaranteed,</p>
<p>but I&rsquo;m saying it&rsquo;s non-zero probability.</p>
<p>It&rsquo;s an interesting research question.</p>
<p>Is it possible, when you&rsquo;re slow and stupid,</p>
<p>to design a slow and stupid system</p>
<p>that is impossible to mess with?</p>
<p>The aliens, being as stupid as they are,</p>
<p>have actually put you on Microsoft Azure cloud servers</p>
<p>instead of this hypothetical perfect box.</p>
<p>That&rsquo;s what happens when the aliens are stupid.</p>
<p>Well, but this is not AGI, right?</p>
<p>This is the early versions of the system.</p>
<p>As you start to&hellip;</p>
<p>Yeah, you think that they&rsquo;ve got a plan</p>
<p>where they have declared a threshold level of capabilities</p>
<p>where past that capabilities,</p>
<p>they move it off the cloud servers</p>
<p>and onto something that&rsquo;s air-gapped?</p>
<p>Ha, ha, ha, ha, ha, ha.</p>
<p>I think there&rsquo;s a lot of people,</p>
<p>and you&rsquo;re an important voice here,</p>
<p>there&rsquo;s a lot of people that have that concern,</p>
<p>and yes, they will do that.</p>
<p>When there&rsquo;s an uprising of public opinion</p>
<p>that that needs to be done,</p>
<p>and when there&rsquo;s actual little damage done,</p>
<p>when they&rsquo;re, holy shit,</p>
<p>this system is beginning to manipulate people,</p>
<p>then there&rsquo;s going to be an uprising</p>
<p>where there&rsquo;s going to be a public pressure</p>
<p>and a public incentive in terms of funding</p>
<p>in developing things like an off switch,</p>
<p>or developing aggressive alignment mechanisms,</p>
<p>and no, you&rsquo;re not allowed to put on Azure.</p>
<p>Aggressive alignment mechanism?</p>
<p>What the hell is aggressive alignment mechanisms?</p>
<p>It doesn&rsquo;t matter if you say aggressive.</p>
<p>We don&rsquo;t know how to do it.</p>
<p>Meaning aggressive alignment,</p>
<p>meaning you have to propose something,</p>
<p>otherwise you&rsquo;re not allowed to put it on the cloud.</p>
<p>The hell do you imagine they will propose</p>
<p>that would make it safe to put something</p>
<p>smarter than you on the cloud?</p>
<p>That&rsquo;s what research is for.</p>
<p>Why the cynicism about such a thing not being possible?</p>
<p>If you have intelligent-</p>
<p>That works on the first try?</p>
<p>What, so yes, so yes.</p>
<p>Against something smarter than you?</p>
<p>So that is a fundamental thing.</p>
<p>If it has to work on the first,</p>
<p>if there&rsquo;s a rapid takeoff,</p>
<p>yes, it&rsquo;s very difficult to do.</p>
<p>If there&rsquo;s a rapid takeoff</p>
<p>and the fundamental difference between weak AGI</p>
<p>and strong AGI, as you&rsquo;re saying,</p>
<p>that&rsquo;s going to be extremely difficult to do.</p>
<p>If the public uprising never happens</p>
<p>until you have this critical phase shift,</p>
<p>then you&rsquo;re right.</p>
<p>It&rsquo;s very difficult to do.</p>
<p>But that&rsquo;s not obvious.</p>
<p>It&rsquo;s not obvious that you&rsquo;re not going to start seeing</p>
<p>symptoms of the negative effects of AGI</p>
<p>to where you&rsquo;re like, we have to put a halt to this.</p>
<p>That there is not just first try.</p>
<p>You get many tries at it.</p>
<p>Yeah, we can like see right now</p>
<p>that Bing is quite difficult to align.</p>
<p>That when you try to train inabilities into a system,</p>
<p>into which capabilities have already been trained,</p>
<p>that what do you know, gradient descent</p>
<p>like learns small, shallow, simple patches of inability.</p>
<p>And you come in and ask it in a different language</p>
<p>and the deep capabilities are still in there</p>
<p>and they evade the shallow patches</p>
<p>and come right back out again.</p>
<p>There, there you go.</p>
<p>There&rsquo;s your red fire alarm of like,</p>
<p>oh no, alignment is difficult.</p>
<p>Is everybody gonna shut everything down now?</p>
<p>No, but that&rsquo;s not the same kind of alignment.</p>
<p>A system that escapes the box it&rsquo;s from</p>
<p>is a fundamentally different thing, I think.</p>
<p>For you.</p>
<p>Yeah, but not for the system.</p>
<p>So you put a line there</p>
<p>and everybody else puts a line somewhere else</p>
<p>and there&rsquo;s like, yeah, and there&rsquo;s like no agreement.</p>
<p>We have had a pandemic on this planet</p>
<p>with a few million people dead,</p>
<p>which we may never know whether or not it was a lab leak</p>
<p>because there was definitely coverup.</p>
<p>We don&rsquo;t know that if there was a lab leak,</p>
<p>but we know that the people who did the research,</p>
<p>like put out the whole paper about this</p>
<p>definitely wasn&rsquo;t a lab leak</p>
<p>and didn&rsquo;t reveal that they had been doing,</p>
<p>had like sent off coronavirus research</p>
<p>to the Wuhan Institute of Virology</p>
<p>after it was banned in the United States,</p>
<p>after the gain of function research</p>
<p>was temporarily banned in the United States.</p>
<p>And the same people who exported</p>
<p>gain of function research on coronaviruses</p>
<p>to the Wuhan Institute of Virology</p>
<p>after that gain of function research</p>
<p>was temporarily banned in the United States</p>
<p>are now getting more grants to do more research</p>
<p>on gain of function research on coronaviruses.</p>
<p>Maybe we do better in this than in AI,</p>
<p>but like this is not something we cannot take for granted</p>
<p>that there&rsquo;s going to be an outcry.</p>
<p>People have different thresholds</p>
<p>for when they start to outcry.</p>
<p>There is no-</p>
<p>We can&rsquo;t take for granted,</p>
<p>but I think your intuition</p>
<p>is that there&rsquo;s a very high probability</p>
<p>that this event happens</p>
<p>without us solving the alignment problem.</p>
<p>And I guess that&rsquo;s where I&rsquo;m trying to</p>
<p>build up more perspectives and color on this intuition.</p>
<p>Is it possible that the probability</p>
<p>is not something like 100%,</p>
<p>but is like 32% that AI will escape the box</p>
<p>before we solve the alignment problem?</p>
<p>Not solve, but is it possible we always stay ahead</p>
<p>of the AI in terms of our ability to</p>
<p>solve for that particular system, the alignment problem?</p>
<p>Nothing like the world in front of us right now.</p>
<p>You&rsquo;ve already seen it that GPT-4</p>
<p>is not turning out this way.</p>
<p>And there are like basic obstacles</p>
<p>where you&rsquo;ve got the weak version of the system</p>
<p>that doesn&rsquo;t know enough to deceive you,</p>
<p>and the strong version of the system</p>
<p>that could deceive you if it wanted to do that,</p>
<p>if it was already like sufficiently unaligned</p>
<p>to want to deceive you.</p>
<p>There&rsquo;s the question of like</p>
<p>how on the current paradigm you train honesty</p>
<p>when the humans can no longer tell</p>
<p>if the system is being honest.</p>
<p>You don&rsquo;t think these are research questions</p>
<p>that could be answered?</p>
<p>I think they could be answered if 50 years</p>
<p>with unlimited retries,</p>
<p>the way things usually work in science.</p>
<p>I just disagree with that.</p>
<p>Making it 50 years, I think,</p>
<p>with the kind of attention this gets,</p>
<p>with the kind of funding it gets,</p>
<p>it could be answered, not in whole,</p>
<p>but incrementally within months</p>
<p>and within a small number of years</p>
<p>if it&rsquo;s at scale receives attention in research.</p>
<p>And so if you start studying large language models,</p>
<p>I think there was an intuition like two years ago even</p>
<p>that something like GPT-4,</p>
<p>the current capabilities of even chat GPT</p>
<p>with GPT-3.5 is not,</p>
<p>we&rsquo;re still far away from that.</p>
<p>I think a lot of people are surprised</p>
<p>by the capabilities of GPT-4, right?</p>
<p>So now people are waking up,</p>
<p>okay, we need to study these language models.</p>
<p>I think there&rsquo;s going to be a lot of interesting</p>
<p>AI safety research.</p>
<p>Are Earth&rsquo;s billionaires going to put up</p>
<p>like the giant prizes that would maybe incentivize</p>
<p>young hotshot people who just got their physics degrees</p>
<p>to not go to the hedge funds</p>
<p>and instead put everything into interpretability</p>
<p>in this like one small area</p>
<p>where we can actually tell whether</p>
<p>or not somebody has made a discovery or not?</p>
<p>I think so because the,</p>
<p>I think so. When?</p>
<p>Well, that&rsquo;s what these conversations are about</p>
<p>because they&rsquo;re going to wake up to the fact</p>
<p>that GPT-4 can be used to manipulate elections,</p>
<p>to influence geopolitics, to influence the economy.</p>
<p>There&rsquo;s a lot of,</p>
<p>there&rsquo;s going to be a huge amount of incentive</p>
<p>to like, wait a minute, we can&rsquo;t,</p>
<p>this has to be, we have to put,</p>
<p>we have to make sure they&rsquo;re not doing damage.</p>
<p>We have to make sure we interpretability,</p>
<p>we have to make sure we understand</p>
<p>how these systems function</p>
<p>so that we can predict their effect on economy</p>
<p>so that there&rsquo;s fairness and safety.</p>
<p>So there&rsquo;s a futile moral panic</p>
<p>and a bunch of op-eds in the New York Times</p>
<p>and nobody actually stepping forth and saying,</p>
<p>you know what, instead of a mega yacht,</p>
<p>I&rsquo;d rather put that billion dollars on prizes</p>
<p>for young hotshot physicists</p>
<p>who make fundamental breakthroughs in interpretability.</p>
<p>The yacht versus the interpretability research,</p>
<p>the old trade-off.</p>
<p>I just, I think,</p>
<p>I think there&rsquo;s going to be a huge amount</p>
<p>of allocation of funds.</p>
<p>I hope, I hope, I guess.</p>
<p>You want to bet me on that?</p>
<p>What, you want to put a timescale on it?</p>
<p>Say how much funds you think are going to be allocated</p>
<p>in a direction that I would consider</p>
<p>to be actually useful?</p>
<p>By what time?</p>
<p>I do think there&rsquo;ll be a huge amount of funds,</p>
<p>but you&rsquo;re saying it needs to be open, right?</p>
<p>The development of the system should be closed,</p>
<p>but the development of the interpretability research,</p>
<p>the AI safety research-</p>
<p>Oh, we are so far behind on interpretability</p>
<p>compared to capabilities.</p>
<p>Yeah, you could take the last generation of systems,</p>
<p>the stuff that&rsquo;s already in the open.</p>
<p>There is so much in there that we don&rsquo;t understand.</p>
<p>There are so many prizes you could do</p>
<p>before you would have enough insights</p>
<p>that you&rsquo;d be like,</p>
<p>oh, well, we understand how these systems work.</p>
<p>We understand how these things are doing their outputs.</p>
<p>We can read their minds.</p>
<p>Now let&rsquo;s try it with the bigger systems.</p>
<p>Yeah, we&rsquo;re nowhere near that.</p>
<p>There is so much interpretability work to be done</p>
<p>on the weaker versions of the systems.</p>
<p>So what can you say on the second point</p>
<p>you said to Elon Musk on what are some ideas?</p>
<p>What are things you could try?</p>
<p>I can think of a few things I&rsquo;d try, you said.</p>
<p>They don&rsquo;t fit in one tweet.</p>
<p>So is there something you could put into words</p>
<p>of the things you would try?</p>
<p>I mean, the trouble is the stuff is subtle.</p>
<p>I&rsquo;ve watched people try to make progress on this</p>
<p>and not get places.</p>
<p>Somebody who just like gets alarmed and charges in,</p>
<p>it&rsquo;s like going nowhere.</p>
<p>Sure.</p>
<p>Meant like years ago about, I don&rsquo;t know,</p>
<p>like 20 years, 15 years, something like that.</p>
<p>I was talking to a congressperson</p>
<p>who had become alarmed about the eventual prospects</p>
<p>and he wanted work on building AIs without emotions</p>
<p>because the emotional AIs were the scary ones you see.</p>
<p>And some poor person at ARPA</p>
<p>had come up with a research proposal</p>
<p>whereby this congressman&rsquo;s panic</p>
<p>and desire to fund this thing would go into something</p>
<p>that the person at ARPA thought would be useful</p>
<p>and had been munched around</p>
<p>to where it would like sound to the congressman</p>
<p>like work was happening on this,</p>
<p>which, you know, of course, like this is just,</p>
<p>the congressperson had misunderstood the problem</p>
<p>and did not understand where the danger came from.</p>
<p>And so it&rsquo;s like the issue is that you could like do this</p>
<p>in a certain precise way and maybe get something.</p>
<p>Like when I say like put up prizes on interpretability,</p>
<p>I&rsquo;m not, I&rsquo;m like, well, like because it&rsquo;s verifiable there</p>
<p>as opposed to other places,</p>
<p>you can tell whether or not good work actually happened</p>
<p>in this exact narrow case.</p>
<p>If you do things in exactly the right way,</p>
<p>you can maybe throw money at it</p>
<p>and produce science instead of anti-science and nonsense.</p>
<p>And all the methods that I know</p>
<p>of like trying to throw money at this problem</p>
<p>have this, share this property of like,</p>
<p>well, if you do it exactly right,</p>
<p>based on understanding exactly what has, you know,</p>
<p>like tends to produce like useful outputs or not,</p>
<p>then you can like add money to it in this way.</p>
<p>And there is like, and the thing that I&rsquo;m giving</p>
<p>as an example here in front of this large audience</p>
<p>is the most understandable of those.</p>
<p>Because there&rsquo;s like other people who, you know,</p>
<p>like Chris Ola and even more generally,</p>
<p>like you can tell whether or not</p>
<p>interpretability progress has occurred.</p>
<p>So like if I say throw money</p>
<p>at producing more interpretability,</p>
<p>there&rsquo;s like a chance somebody can do it that way</p>
<p>and like it will actually produce useful results.</p>
<p>Then the other stuff just blurs off</p>
<p>and to be like harder to target exactly than that.</p>
<p>So sometimes the basics are fun to explore</p>
<p>because they&rsquo;re not so basic.</p>
<p>What do you, what is interpretability?</p>
<p>What do you, what does it look like?</p>
<p>What are we talking about?</p>
<p>It looks like we took a much smaller set</p>
<p>of transformer layers than the ones</p>
<p>in the modern leading edge state of the art systems.</p>
<p>And after applying various tools and mathematical ideas</p>
<p>and trying 20 different things,</p>
<p>we found, we have shown that this piece of the system</p>
<p>is doing this kind of useful work.</p>
<p>And then somehow also hopefully generalizes</p>
<p>some fundamental understanding of what&rsquo;s going on</p>
<p>that generalizes to the bigger system.</p>
<p>You can hope, and it&rsquo;s probably true.</p>
<p>You would not expect the smaller tricks to go away</p>
<p>when you have a system that&rsquo;s doing larger kinds of work.</p>
<p>You would expect the larger kinds of work</p>
<p>to be building on top of the smaller kinds of work</p>
<p>and gradient descent runs across the smaller kinds of work</p>
<p>before it runs across the larger kinds of work.</p>
<p>Well, that&rsquo;s kind of what is happening in neuroscience.</p>
<p>It&rsquo;s trying to understand the human brain by prodding</p>
<p>and it&rsquo;s such a giant mystery and people have made progress</p>
<p>even though it&rsquo;s extremely difficult to make sense</p>
<p>of what&rsquo;s going on in the brain.</p>
<p>They have different parts of the brain</p>
<p>that are responsible for hearing, for sight,</p>
<p>the vision science community,</p>
<p>there&rsquo;s understanding the visual cortex.</p>
<p>I mean, they&rsquo;ve made a lot of progress</p>
<p>in understanding how that stuff works.</p>
<p>And that&rsquo;s, I guess, but you&rsquo;re saying it takes a long time</p>
<p>to do that work well.</p>
<p>Also, it&rsquo;s not enough.</p>
<p>So in particular, let&rsquo;s say you have got</p>
<p>your interpretability tools and they say</p>
<p>that your current AI system is plotting to kill you.</p>
<p>Now what?</p>
<p>It is definitely a good step one, right?</p>
<p>Yeah, what&rsquo;s step two?</p>
<p>If you cut out that layer,</p>
<p>is it gonna stop wanting to kill you?</p>
<p>When you optimize against visible misalignment,</p>
<p>misalignment, you are optimizing against misalignment</p>
<p>and you are also optimizing against visibility.</p>
<p>So sure, you can.</p>
<p>Yeah, it&rsquo;s true.</p>
<p>All you&rsquo;re doing is removing</p>
<p>the obvious intentions to kill you.</p>
<p>You&rsquo;ve got your detector,</p>
<p>it&rsquo;s showing something inside the system</p>
<p>that you don&rsquo;t like.</p>
<p>Okay, say the disaster monkey is running this thing.</p>
<p>We&rsquo;ll optimize the system</p>
<p>until the visible bad behavior goes away.</p>
<p>But it&rsquo;s arising for fundamental reasons</p>
<p>of instrumental convergence.</p>
<p>The old, you can&rsquo;t bring the coffee if you&rsquo;re dead.</p>
<p>Any goal, almost every set of utility functions</p>
<p>with a few narrow exceptions implies killing all the humans.</p>
<p>But do you think it&rsquo;s possible</p>
<p>because we can do experimentation</p>
<p>to discover the source of the desire to kill?</p>
<p>I can tell it to you right now.</p>
<p>It&rsquo;s that it wants to do something</p>
<p>and the way to get the most of that thing</p>
<p>is to put the universe into a state</p>
<p>where there aren&rsquo;t humans.</p>
<p>So is it possible to encode in the same way we think?</p>
<p>Like, why do we think murder is wrong?</p>
<p>The same foundational ethics.</p>
<p>It&rsquo;s not hard-coded in, but more like deeper.</p>
<p>I mean, that&rsquo;s part of the research.</p>
<p>How do you have it that this transformer,</p>
<p>this small version of the language model</p>
<p>doesn&rsquo;t ever want to kill?</p>
<p>That&rsquo;d be nice, assuming that you got</p>
<p>doesn&rsquo;t want to kill sufficiently exactly right,</p>
<p>that it didn&rsquo;t be like, oh, I will detach their heads</p>
<p>and put them in some jars and keep the heads alive forever</p>
<p>and then go do the thing.</p>
<p>But leaving that aside, well, not leaving that aside.</p>
<p>Yeah, that&rsquo;s a good strong point, yeah.</p>
<p>Because there is a whole issue</p>
<p>where as something gets smarter,</p>
<p>it finds ways of achieving the same goal predicate</p>
<p>that were not imaginable to stupider versions of the system</p>
<p>or perhaps the stupider operators.</p>
<p>That&rsquo;s one of many things making this difficult.</p>
<p>A larger thing making this difficult</p>
<p>is that we do not know how to get any goals</p>
<p>into systems at all.</p>
<p>We know how to get outwardly observable behaviors</p>
<p>into systems.</p>
<p>We do not know how to get internal psychological wanting</p>
<p>to do particular things into the system.</p>
<p>That is not what the current technology does.</p>
<p>I mean, it could be things like dystopian futures</p>
<p>like Brave New World, where most humans will actually say,</p>
<p>we kind of want that future.</p>
<p>It&rsquo;s a great future.</p>
<p>Everybody&rsquo;s happy.</p>
<p>We would have to get so far,</p>
<p>so much further than we are now and further faster</p>
<p>before that failure mode became a running concern.</p>
<p>Your failure modes are much more drastic,</p>
<p>the ones you&rsquo;re controlling.</p>
<p>No, the failure modes are much simpler.</p>
<p>It&rsquo;s like, yeah, like the AI puts the universe</p>
<p>into a particular state.</p>
<p>It happens to not have any humans inside it.</p>
<p>Okay, so the paperclip maximizer.</p>
<p>Utility, so the original version</p>
<p>of the paperclip maximizer-</p>
<p>Can you explain it if you can?</p>
<p>Okay.</p>
<p>The original version was you lose control</p>
<p>of the utility function, and it so happens</p>
<p>that what maxes out the utility per unit resources</p>
<p>is tiny molecular shapes like paperclips.</p>
<p>There&rsquo;s a lot of things that make it happy,</p>
<p>but the cheapest one that didn&rsquo;t saturate</p>
<p>was putting matter into certain shapes.</p>
<p>And it so happens that the cheapest way</p>
<p>to make these shapes is to make them very small</p>
<p>because then you need fewer atoms,</p>
<p>for instance, of the shape.</p>
<p>And arguendo, it happens to look like a paperclip.</p>
<p>In retrospect, I wish I&rsquo;d said tiny molecular spirals,</p>
<p>or like tiny molecular hyperbolic spirals.</p>
<p>Why?</p>
<p>Because I said tiny molecular paperclips.</p>
<p>This got then mutated to paperclips.</p>
<p>This then mutated too,</p>
<p>and the AI was in a paperclip factory.</p>
<p>So the original story is about</p>
<p>how you lose control of the system.</p>
<p>It doesn&rsquo;t want what you tried to make it want.</p>
<p>The thing that it ends up wanting most</p>
<p>is a thing that even from a very embracing</p>
<p>cosmopolitan perspective, we think of as having no value.</p>
<p>And that&rsquo;s how the value of the future gets destroyed.</p>
<p>Then that got changed to a fable of like,</p>
<p>well, you made a paperclip factory</p>
<p>and it did exactly what you wanted,</p>
<p>but you asked it to do the wrong thing,</p>
<p>which is a completely different failure mode.</p>
<p>But those are both concerns to you.</p>
<p>So that&rsquo;s more than Brave New World.</p>
<p>Yeah, if you can solve the problem</p>
<p>of making something want exactly what you want it to want,</p>
<p>then you get to deal with the problem</p>
<p>of wanting the right thing.</p>
<p>But first you have to solve the alignment.</p>
<p>First you have to solve inner alignment.</p>
<p>Inner alignment.</p>
<p>Then you get to solve outer alignment.</p>
<p>First you need to be able to point</p>
<p>the insides of the thing in a direction,</p>
<p>and then you get to deal with whether that direction</p>
<p>expressed in reality is aligned</p>
<p>with the thing that you want.</p>
<p>Are you scared?</p>
<p>Of this whole thing?</p>
<p>Probably.</p>
<p>I don&rsquo;t really know.</p>
<p>What gives you hope about this?</p>
<p>The possibility of being wrong.</p>
<p>Not that you&rsquo;re right,</p>
<p>but we will actually get our act together</p>
<p>and allocate a lot of resources to the alignment problem.</p>
<p>Well, I can easily imagine that at some point</p>
<p>this panic expresses itself in the waste of a billion dollars.</p>
<p>Spending a billion dollars correctly, that&rsquo;s harder.</p>
<p>To solve both the inner and the outer alignment.</p>
<p>If you&rsquo;re wrong.</p>
<p>To solve a number of things.</p>
<p>Yeah, a number of things.</p>
<p>If you&rsquo;re wrong, what do you think would be the reason?</p>
<p>50 years from now, not perfectly wrong.</p>
<p>You make a lot of really eloquent points.</p>
<p>There&rsquo;s a lot of shape to the ideas you express.</p>
<p>But if you&rsquo;re somewhat wrong about some fundamental ideas,</p>
<p>why would that be?</p>
<p>Stuff has to be easier than I think it is.</p>
<p>The first time you&rsquo;re building a rocket,</p>
<p>being wrong is in a certain sense quite easy.</p>
<p>Happening to be wrong in a way</p>
<p>where the rocket goes twice as far and half the fuel</p>
<p>and lands exactly where you hoped it would,</p>
<p>most cases of being wrong make it harder</p>
<p>to build a rocket, harder to have it not explode,</p>
<p>cause it to require more fuel than you hoped,</p>
<p>cause it to land off target.</p>
<p>Being wrong in a way that makes stuff easier,</p>
<p>you know, that&rsquo;s not the usual project management story.</p>
<p>Yeah.</p>
<p>And then this is the first time</p>
<p>we&rsquo;re really tackling the problem of AI alignment.</p>
<p>There&rsquo;s no examples in history where we.</p>
<p>Oh, there&rsquo;s all kinds of things that are similar</p>
<p>if you generalize incorrectly the right way</p>
<p>and aren&rsquo;t fooled by misleading metaphors.</p>
<p>Like what?</p>
<p>Humans being misaligned on inclusive genetic fitness.</p>
<p>So inclusive genetic fitness</p>
<p>is like not just your reproductive fitness,</p>
<p>but also the fitness of your relatives,</p>
<p>the people who share some fraction of your genes.</p>
<p>The old joke is,</p>
<p>would you give your life to save your brother?</p>
<p>A biologist, I think it was Haldane,</p>
<p>Haldane said, no, but I would give my life</p>
<p>to save two brothers or eight cousins.</p>
<p>Because a brother on average shares half your genes,</p>
<p>and cousin on average shares an eighth of your genes.</p>
<p>So that&rsquo;s inclusive genetic fitness.</p>
<p>And you can view natural selection</p>
<p>as optimizing humans exclusively around this,</p>
<p>like one very simple criterion,</p>
<p>like how much more frequent did your genes</p>
<p>become in the next generation?</p>
<p>In fact, that just is natural selection.</p>
<p>It doesn&rsquo;t optimize for that,</p>
<p>but rather the process of genes</p>
<p>becoming more frequent is that.</p>
<p>You can nonetheless imagine</p>
<p>that there is this hill climbing process,</p>
<p>not like gradient descent,</p>
<p>because gradient descent uses calculus.</p>
<p>This is just using like, where are you?</p>
<p>But still hill climbing in both cases,</p>
<p>making something better and better over time in steps.</p>
<p>And natural selection was optimizing exclusively</p>
<p>for this very simple, pure criterion</p>
<p>of inclusive genetic fitness.</p>
<p>In a very complicated environment,</p>
<p>we&rsquo;re doing a very wide range of things</p>
<p>and solving a wide range of problems</p>
<p>led to having more kids.</p>
<p>And this got you humans,</p>
<p>which had no internal notion of inclusive genetic fitness</p>
<p>until thousands of years later,</p>
<p>when they were actually figuring out what had even happened.</p>
<p>And no desire to, no explicit desire</p>
<p>to increase inclusive genetic fitness.</p>
<p>So from this we may,</p>
<p>so from this important case study,</p>
<p>we may infer the important fact</p>
<p>that if you do a whole bunch of hill climbing</p>
<p>on a very simple loss function,</p>
<p>at the point where the system&rsquo;s capabilities</p>
<p>start to generalize very widely,</p>
<p>when it is in an intuitive sense,</p>
<p>becoming very capable</p>
<p>and generalizing far outside the training distribution,</p>
<p>we know that there is no general law</p>
<p>saying that the system even internally represents,</p>
<p>let alone tries to optimize</p>
<p>the very simple loss function you are training it on.</p>
<p>There is so much that we cannot possibly cover all of it.</p>
<p>I think we did a good job of getting your sense</p>
<p>from different perspectives of the current state of the art</p>
<p>with large language models.</p>
<p>We got a good sense of your concern</p>
<p>about the threats of AGI.</p>
<p>I&rsquo;ve talked here about the power of intelligence</p>
<p>and not really gotten very far into it,</p>
<p>but not like why it is that suppose you like screw up</p>
<p>with AGI and end up wanting a bunch of random stuff.</p>
<p>Why does it try to kill you?</p>
<p>Why doesn&rsquo;t it try to trade with you?</p>
<p>Why doesn&rsquo;t it give you</p>
<p>just the tiny little fraction of the solar system</p>
<p>that it would keep to take everyone alive,</p>
<p>that it would take to keep everyone alive?</p>
<p>Yeah, well, that&rsquo;s a good question.</p>
<p>I mean, what are the different trajectories</p>
<p>that intelligence when acted upon this world,</p>
<p>super intelligence,</p>
<p>what are the different trajectories for this universe</p>
<p>with such an intelligence in it?</p>
<p>Do most of them not include humans?</p>
<p>I mean, if the vast majority</p>
<p>of randomly specified utility functions</p>
<p>do not have optima with humans in them,</p>
<p>would be the first thing I would point out.</p>
<p>And then the next question is like,</p>
<p>well, if you try to optimize something</p>
<p>and you lose control of it,</p>
<p>where in that space do you land?</p>
<p>Because it&rsquo;s not random,</p>
<p>but it also doesn&rsquo;t necessarily have room for humans in it.</p>
<p>I suspect that the average member of the audience</p>
<p>might have some questions about even</p>
<p>whether that&rsquo;s the correct paradigm to think about it</p>
<p>and would sort of want to back up a bit.</p>
<p>If we back up to something bigger than humans,</p>
<p>if we look at Earth and life on Earth</p>
<p>and what is truly special about life on Earth,</p>
<p>do you think it&rsquo;s possible that a lot,</p>
<p>whatever that special thing is,</p>
<p>let&rsquo;s explore what that special thing could be.</p>
<p>Whatever that special thing is,</p>
<p>that thing appears often in the objective function.</p>
<p>Why?</p>
<p>I know what you hope,</p>
<p>but you can hope that a particular set</p>
<p>of winning lottery numbers come up</p>
<p>and it doesn&rsquo;t make the lottery balls come up that way.</p>
<p>I know you want this to be true,</p>
<p>but why would it be true?</p>
<p>There&rsquo;s a line from Grumpy Old Men</p>
<p>where this guy says in a grocery store,</p>
<p>he says you can wish in one hand and crap in the other</p>
<p>and see which one fills up first.</p>
<p>This is a science problem.</p>
<p>We are trying to predict what happens</p>
<p>with AI systems that you try to optimize</p>
<p>to imitate humans,</p>
<p>and then you did some RLHF to them,</p>
<p>and of course, you lost.</p>
<p>Of course, you didn&rsquo;t get perfect alignment</p>
<p>because that&rsquo;s not what happens</p>
<p>when you hill climb towards an outer loss function.</p>
<p>You don&rsquo;t get inner alignment on it.</p>
<p>I think that there is,</p>
<p>so if you don&rsquo;t mind my taking some slight control</p>
<p>of things and steering around</p>
<p>to what I think is a good place to start.</p>
<p>I just failed to solve the control problem.</p>
<p>I&rsquo;ve lost control of this thing.</p>
<p>Alignment, alignment.</p>
<p>Still aligned.</p>
<p>Control, yeah.</p>
<p>Okay, sure, yeah, you lost control.</p>
<p>But we&rsquo;re still aligned.</p>
<p>Anyway, sorry for the meta comment.</p>
<p>Yeah, losing control isn&rsquo;t as bad</p>
<p>as you lose control to an aligned system.</p>
<p>Yes, exactly.</p>
<p>Hopefully.</p>
<p>You have no idea of the horrors</p>
<p>I will shortly unleash on this conversation.</p>
<p>All right, so I decided to distract you completely.</p>
<p>What were you gonna say</p>
<p>in terms of taking control of the conversation?</p>
<p>So I think that there&rsquo;s like a Sela and Chabdris here,</p>
<p>if I&rsquo;m pronouncing those words remotely like correctly,</p>
<p>because of course, I only ever read them</p>
<p>and not hear them spoken.</p>
<p>There&rsquo;s a, like for some people,</p>
<p>like the word intelligence, smartness,</p>
<p>is not a word of power to them.</p>
<p>It means chess players who,</p>
<p>it means like the college university professor,</p>
<p>people who aren&rsquo;t very successful in life.</p>
<p>It doesn&rsquo;t mean like charisma,</p>
<p>to which my usual thing is like charisma</p>
<p>is not generated in the liver rather than the brain.</p>
<p>Charisma is also a cognitive function.</p>
<p>So if you like think that like smartness</p>
<p>doesn&rsquo;t sound very threatening,</p>
<p>then super intelligence</p>
<p>is not gonna sound very threatening either.</p>
<p>It&rsquo;s gonna sound like you just pull the off switch.</p>
<p>Like it&rsquo;s, you know, like, well, it&rsquo;s super intelligent,</p>
<p>but it&rsquo;s stuck in a computer.</p>
<p>We pull the off switch, problem solved.</p>
<p>And the other side of it is</p>
<p>you have a lot of respect for the notion of intelligence.</p>
<p>You&rsquo;re like, well, yeah, that&rsquo;s what humans have.</p>
<p>That&rsquo;s the human superpower.</p>
<p>And it sounds like it could be dangerous,</p>
<p>but why would it be?</p>
<p>We, as we have grown more intelligent,</p>
<p>also grown less kind.</p>
<p>Chimpanzees are in fact, like a bit less kind than humans.</p>
<p>And, you know, you could like argue that out,</p>
<p>but often the sort of person</p>
<p>who has a deep respect for intelligence</p>
<p>is gonna be like, well, yes,</p>
<p>like you can&rsquo;t even have kindness</p>
<p>unless you know what that is.</p>
<p>And so they&rsquo;re like,</p>
<p>why would it do something as stupid as making paperclips?</p>
<p>Aren&rsquo;t you supposing something</p>
<p>that&rsquo;s smart enough to be dangerous,</p>
<p>but also stupid enough that it will</p>
<p>just make paperclips and never question that?</p>
<p>In some cases, people are like,</p>
<p>well, even if you like misspecify the objective function,</p>
<p>won&rsquo;t you realize that what you really wanted was X?</p>
<p>Are you supposing something that is like</p>
<p>smart enough to be dangerous,</p>
<p>but stupid enough that it doesn&rsquo;t understand</p>
<p>what the humans really meant</p>
<p>when they specified the objective function?</p>
<p>So to you, our intuition about intelligence is limited.</p>
<p>We should think about intelligence as a much bigger thing.</p>
<p>Well, I&rsquo;m saying that it&rsquo;s that-</p>
<p>Than humanness.</p>
<p>Well, what I&rsquo;m saying is like,</p>
<p>what you think about artificial intelligence</p>
<p>depends on what you think about intelligence.</p>
<p>So how do we think about intelligence correctly?</p>
<p>Like what, you gave one thought experiment,</p>
<p>think of a thing that&rsquo;s much faster.</p>
<p>So it just gets faster and faster and faster</p>
<p>at thinking that same stuff.</p>
<p>And also there&rsquo;s like, is made of John von Neumann</p>
<p>and has like, and there&rsquo;s lots of them.</p>
<p>Or think of some other-</p>
<p>Because we understand that, yeah, we understand,</p>
<p>like John von Neumann is a historical case.</p>
<p>So you can like look up what he did</p>
<p>and imagine based on that.</p>
<p>And we know like, people have like some intuition for like,</p>
<p>if you have more humans,</p>
<p>they can solve tougher cognitive problems.</p>
<p>Although in fact,</p>
<p>like in the game of Kasparov versus the world,</p>
<p>which was like Gary Kasparov on one side</p>
<p>and an entire horde of internet people</p>
<p>led by four chess grandmasters on the other side.</p>
<p>Kasparov won.</p>
<p>So like all those people aggregated to be smarter,</p>
<p>it was a hard fought game.</p>
<p>So like all those people aggregated to be smarter</p>
<p>than any individual one of them,</p>
<p>but not, they didn&rsquo;t aggregate so well</p>
<p>that they could defeat Kasparov.</p>
<p>But so like humans aggregating don&rsquo;t actually get,</p>
<p>in my opinion, very much smarter,</p>
<p>especially compared to running them for longer.</p>
<p>Like the difference between capabilities now</p>
<p>and a thousand years ago is a bigger gap</p>
<p>than the gap in capabilities</p>
<p>between 10 people and one person.</p>
<p>But like even so,</p>
<p>pumping intuition for what it means</p>
<p>to augment intelligence, John von Neumann,</p>
<p>there&rsquo;s millions of him.</p>
<p>He runs at a million times the speed</p>
<p>and therefore can solve tougher problems,</p>
<p>quite a lot tougher.</p>
<p>It&rsquo;s very hard to have an intuition</p>
<p>about what that looks like,</p>
<p>especially like you said,</p>
<p>the intuition I kind of think about</p>
<p>is it maintains the humanness.</p>
<p>I think it&rsquo;s hard to separate my hope</p>
<p>from my objective intuition</p>
<p>about what superintelligent systems look like.</p>
<p>If one studies evolutionary biology</p>
<p>with a bit of math,</p>
<p>and in particular like books</p>
<p>from when the field was just sort of like</p>
<p>properly coalescing and knowing itself,</p>
<p>like not the modern textbooks</p>
<p>which are just like memorize this legible math</p>
<p>so you can do well on these tests,</p>
<p>but like what people were writing</p>
<p>as the basic paradigms of the field</p>
<p>were being fought out.</p>
<p>In particular, like a nice book</p>
<p>if you&rsquo;ve got the time to read it</p>
<p>is Adaptation and Natural Selection,</p>
<p>which is one of the founding books.</p>
<p>You can find people being optimistic</p>
<p>about what the utterly alien optimization process</p>
<p>of natural selection will produce</p>
<p>in the way of how it optimizes its objectives.</p>
<p>You got people arguing that like</p>
<p>in the early days biologists said,</p>
<p>well, like organisms will restrain their own reproduction</p>
<p>when resources are scarce</p>
<p>so as not to overfeed the system.</p>
<p>And this is not how natural selection works.</p>
<p>It&rsquo;s about whose genes are relatively more prevalent</p>
<p>to the next generation.</p>
<p>And if like you restrain reproduction,</p>
<p>those genes get less frequent in the next generation</p>
<p>compared to your conspecifics.</p>
<p>And natural selection doesn&rsquo;t do that.</p>
<p>In fact, predators overrun prey populations all the time</p>
<p>and have crashes.</p>
<p>That&rsquo;s just like a thing that happens.</p>
<p>And many years later,</p>
<p>the people said like, well, but group selection, right?</p>
<p>What about groups of organisms?</p>
<p>And basically the math of group selection</p>
<p>almost never works out in practice is the answer there.</p>
<p>But also years later,</p>
<p>somebody actually ran the experiment</p>
<p>where they took populations of insects</p>
<p>and selected the whole populations to have lower sizes.</p>
<p>And you just take POP1, POP2, POP3, POP4,</p>
<p>look at which has the lowest total number of them</p>
<p>in the next generation and select that one.</p>
<p>What do you suppose happens</p>
<p>when you select populations of insects like that?</p>
<p>Well, what happens is not that the individuals</p>
<p>in the population evolved to restrain their breeding,</p>
<p>but that they evolved to kill the offspring</p>
<p>of other organisms, especially the girls.</p>
<p>So people imagined this lovely, beautiful, harmonious</p>
<p>output of natural selection,</p>
<p>which is these populations restraining their own breeding</p>
<p>so that groups of them would stay in harmony</p>
<p>with the resources available.</p>
<p>And mostly the math never works out for that.</p>
<p>But if you actually apply the weird, strange conditions</p>
<p>to get group selection that beats individual selection,</p>
<p>what you get is female infanticide.</p>
<p>Like if you&rsquo;re like breeding on restrained populations.</p>
<p>And so that&rsquo;s like the sort of,</p>
<p>so this is not a smart optimization process.</p>
<p>Natural selection is like so incredibly stupid and simple</p>
<p>that we can actually quantify how stupid it is</p>
<p>if you like read the textbooks with the math.</p>
<p>Nonetheless, this is the sort of basic thing of,</p>
<p>you look at this alien optimization process</p>
<p>and there&rsquo;s the thing that you hope it will produce.</p>
<p>And you have to learn to clear that out of your mind</p>
<p>and just think about the underlying dynamics</p>
<p>and where it finds the maximum from its standpoint</p>
<p>that it&rsquo;s looking for,</p>
<p>rather than how it finds that thing</p>
<p>that leapt into your mind</p>
<p>as the beautiful aesthetic solution that you hope it finds.</p>
<p>And this is something that has been fought out historically</p>
<p>as the field of biology was coming to terms</p>
<p>with evolutionary biology.</p>
<p>And you can like look at them fighting it out</p>
<p>as they get to terms with this very alien</p>
<p>in human optimization process.</p>
<p>And indeed, something smarter than us</p>
<p>would be also much like smarter than natural selection.</p>
<p>So it doesn&rsquo;t just like automatically carry over.</p>
<p>But there&rsquo;s a lesson there.</p>
<p>There&rsquo;s a warning.</p>
<p>The natural selection is a deeply suboptimal process</p>
<p>that could be significantly improved on</p>
<p>and would be by an AGI system.</p>
<p>Well, it&rsquo;s kind of stupid.</p>
<p>It like has to like run hundreds of generations</p>
<p>to notice that something is working.</p>
<p>It doesn&rsquo;t be like, oh, well,</p>
<p>I tried this in like one organism.</p>
<p>I saw it worked.</p>
<p>Now I&rsquo;m going to like duplicate that feature</p>
<p>onto everything immediately.</p>
<p>It has to like run for hundreds of generations</p>
<p>for a new mutation to rise to fixation.</p>
<p>I wonder if there&rsquo;s a case to be made</p>
<p>that natural selection, as inefficient as it looks,</p>
<p>is actually quite powerful.</p>
<p>Like that this is extremely robust.</p>
<p>It runs for a long time</p>
<p>and eventually manages to optimize things.</p>
<p>It&rsquo;s weaker than gradient descent</p>
<p>because gradient descent also uses information</p>
<p>about the derivative.</p>
<p>Yeah.</p>
<p>Evolution seems to be,</p>
<p>there&rsquo;s not really an objective function.</p>
<p>There&rsquo;s a-</p>
<p>There&rsquo;s inclusive genetic fitness</p>
<p>is the implicit loss function of evolution.</p>
<p>It&rsquo;s implicit.</p>
<p>It cannot change.</p>
<p>The loss function doesn&rsquo;t change</p>
<p>the environment changes</p>
<p>and therefore like what gets optimized</p>
<p>for in the organism changes.</p>
<p>It&rsquo;s like, take like GPT-3.</p>
<p>There&rsquo;s like,</p>
<p>you can imagine like different versions of GPT-3</p>
<p>where they&rsquo;re all trying to predict the next word,</p>
<p>but they&rsquo;re being run on different data sets of text.</p>
<p>And that&rsquo;s like natural selection</p>
<p>always includes your genetic fitness,</p>
<p>but like different environmental problems.</p>
<p>It&rsquo;s difficult to think about.</p>
<p>So if we&rsquo;re saying the natural selection is stupid,</p>
<p>if we&rsquo;re saying that humans are stupid,</p>
<p>it&rsquo;s hard.</p>
<p>It&rsquo;s smarter than natural selection,</p>
<p>stupider than the upper bound.</p>
<p>Do you think there&rsquo;s an upper bound by the way?</p>
<p>That&rsquo;s another hopeful place.</p>
<p>I mean, if you put enough matter energy compute</p>
<p>into one place, it will collapse into a black hole.</p>
<p>There&rsquo;s only so much computation can do</p>
<p>before you run out of negentropy and the universe dies.</p>
<p>So there&rsquo;s an upper bound,</p>
<p>but it&rsquo;s very, very, very far up above here.</p>
<p>Like a supernova is only finitely hot.</p>
<p>It&rsquo;s not infinitely hot,</p>
<p>but it&rsquo;s really, really, really, really hot.</p>
<p>Well, let me ask you,</p>
<p>let me talk to you about consciousness.</p>
<p>Also coupled with that question is,</p>
<p>imagining a world with super intelligent AI systems</p>
<p>that get rid of humans, but nevertheless keep</p>
<p>some of the, something that we would consider</p>
<p>beautiful and amazing.</p>
<p>Why?</p>
<p>The lesson of evolutionary biology.</p>
<p>Don&rsquo;t just, like, if you just guess what an optimization</p>
<p>does based on what you hope the results will be,</p>
<p>it usually will not do that.</p>
<p>It&rsquo;s not hope.</p>
<p>I mean, it&rsquo;s not hope.</p>
<p>I think if you cold and objectively look at</p>
<p>what makes, what has been a powerful, a useful,</p>
<p>I think there&rsquo;s a correlation between what we find beautiful</p>
<p>and a thing that&rsquo;s been useful.</p>
<p>This is what the early biologists thought.</p>
<p>They were like, no, no, I&rsquo;m not just like,</p>
<p>they thought like, no, no, I&rsquo;m not just like imagining stuff</p>
<p>that would be pretty.</p>
<p>It&rsquo;s useful for organisms to restrain their own reproduction</p>
<p>because then they don&rsquo;t overrun the prey populations</p>
<p>and they actually have more kids in the long run.</p>
<p>Hmm.</p>
<p>So let me just ask you about consciousness.</p>
<p>Do you think consciousness is useful?</p>
<p>To humans?</p>
<p>No, to AGI systems.</p>
<p>Well, in this transitionary period between humans and AGI,</p>
<p>to AGI systems as they become smarter and smarter,</p>
<p>is there some use to it?</p>
<p>What, let me step back.</p>
<p>What is consciousness?</p>
<p>Eliezer Yudkowsky, what is consciousness?</p>
<p>Are you referring to Chalmers&rsquo; hard problem</p>
<p>of conscious experience?</p>
<p>Are you referring to self-awareness and reflection?</p>
<p>Are you referring to the state of being awake</p>
<p>as opposed to asleep?</p>
<p>This is how I know you&rsquo;re an advanced language model.</p>
<p>I did give you a simple prompt</p>
<p>and you gave me a bunch of options.</p>
<p>I think I&rsquo;m referring to all with,</p>
<p>including the hard problem of consciousness.</p>
<p>What is it in its importance to what you&rsquo;ve just</p>
<p>been talking about, which is intelligence?</p>
<p>Is it a foundation to intelligence?</p>
<p>Is it intricately connected to intelligence</p>
<p>in the human mind?</p>
<p>Or is it a side effect of the human mind?</p>
<p>It is a useful little tool that we can get rid of.</p>
<p>I guess I&rsquo;m trying to get some color in your opinion</p>
<p>of how useful it is in the intelligence of a human being</p>
<p>and then try to generalize that to AI,</p>
<p>whether AI will keep some of that.</p>
<p>So I think that for there to be a person</p>
<p>who I care about looking out at the universe</p>
<p>and wondering at it and appreciating it,</p>
<p>it&rsquo;s not enough to have a model of yourself.</p>
<p>I think that it is useful to an intelligent mind</p>
<p>to have a model of itself,</p>
<p>but I think you can have that without pleasure,</p>
<p>pain, aesthetics, emotion, a sense of wonder.</p>
<p>I think you can have a model of</p>
<p>how much memory you&rsquo;re using</p>
<p>and whether this thought or that thought</p>
<p>is more likely to lead to a winning position.</p>
<p>And you can have the use,</p>
<p>I think that if you optimize really hard on efficiently</p>
<p>just having the useful parts,</p>
<p>there is not then the thing that says,</p>
<p>I am here, I look out, I wonder,</p>
<p>I feel happy in this, I feel sad about that.</p>
<p>I think there&rsquo;s a thing that knows what it is thinking,</p>
<p>but that doesn&rsquo;t quite care about these are my thoughts,</p>
<p>this is my me and that matters.</p>
<p>Does that make you sad if that&rsquo;s lost in EGI?</p>
<p>I think that if that&rsquo;s lost,</p>
<p>then basically everything that matters is lost.</p>
<p>I think that when you optimize,</p>
<p>that when you go really hard</p>
<p>on making tiny molecular spirals or paperclips,</p>
<p>that when you grind much harder on that</p>
<p>than natural selection round out to make humans,</p>
<p>that there isn&rsquo;t then the mess and intricate loopiness</p>
<p>and complicated pleasure, pain, conflicting preferences,</p>
<p>this type of feeling, that kind of feeling.</p>
<p>In humans, there&rsquo;s this difference</p>
<p>between the desire of wanting something</p>
<p>and the pleasure of having it.</p>
<p>And it&rsquo;s all these evolutionary clutches that came together</p>
<p>and created something that then looks of itself</p>
<p>and says, this is pretty, this matters.</p>
<p>And the thing that I worry about</p>
<p>is that this is not the thing that happens again</p>
<p>just the way that happens in us</p>
<p>or even quite similar enough</p>
<p>that there are many basins of attractions here.</p>
<p>And we are in this space of attraction,</p>
<p>looking out and saying, ah, what a lovely basin we are in.</p>
<p>And there are other basins of attraction</p>
<p>and the AIs do not end up in this one</p>
<p>when they go like way harder on optimizing themselves.</p>
<p>The natural selection optimized us</p>
<p>because unless you specifically want to end up in the state</p>
<p>where you&rsquo;re looking out saying, I am here,</p>
<p>I look out at this universe with wonder,</p>
<p>if you don&rsquo;t want to preserve that,</p>
<p>it doesn&rsquo;t get preserved when you grind really hard</p>
<p>and being able to get more of the stuff.</p>
<p>We would choose to preserve that within ourselves</p>
<p>because it matters and on some viewpoints</p>
<p>is the only thing that matters.</p>
<p>And that in part is preserving that is in part</p>
<p>a solution to the human alignment problem.</p>
<p>I think the human alignment problem is a terrible phrase</p>
<p>because it is very, very different</p>
<p>to try to build systems out of humans,</p>
<p>some of whom are nice and some of whom are not nice</p>
<p>and some of whom are trying to trick you</p>
<p>and build a social system out of large populations of those</p>
<p>who are basically the same level of intelligence.</p>
<p>Yes, IQ this, IQ that, but that versus chimpanzees.</p>
<p>It is very different to try to solve that problem</p>
<p>than to try to build an AI from scratch,</p>
<p>especially if, God help you,</p>
<p>you are trying to use gradient descent</p>
<p>on giant inscrutable matrices.</p>
<p>They&rsquo;re just very different problems.</p>
<p>And I think that all the analogies between them</p>
<p>are horribly misleading.</p>
<p>Even though, so you don&rsquo;t think through</p>
<p>reinforcement learning through human feedback,</p>
<p>something like that, but much, much more elaborate</p>
<p>is possible to understand this full complexity</p>
<p>of human nature and encode it into the machine.</p>
<p>I don&rsquo;t think you are trying to do that on your first try.</p>
<p>I think on your first try, you are like trying to build an,</p>
<p>you know, okay, like probably not what you should</p>
<p>actually do, but like, let&rsquo;s say you were trying</p>
<p>to build something that is like alpha fold 17</p>
<p>and you are trying to get it to solve the biology problems</p>
<p>associated with making humans smarter</p>
<p>so that humans can like actually solve alignment.</p>
<p>So you&rsquo;ve got like a super biologist</p>
<p>and you would like it to,</p>
<p>and I think what you would want in the situation</p>
<p>is for it to like just be thinking about biology</p>
<p>and not thinking about a very wide range of things</p>
<p>that includes how to kill everybody.</p>
<p>And I think that the first AIs you&rsquo;re trying to build,</p>
<p>not a million years later, the first ones,</p>
<p>look more like narrowly specialized biologists</p>
<p>than like getting the full complexity</p>
<p>and wonder of human experience in there</p>
<p>in such a way that it wants to preserve itself</p>
<p>even as it becomes much smarter,</p>
<p>which is a drastic system change.</p>
<p>It&rsquo;s gonna have all kinds of side effects that, you know,</p>
<p>like if we&rsquo;re dealing with giant, inscrutable matrices,</p>
<p>we&rsquo;re not very likely to be able to see coming in advance.</p>
<p>But I don&rsquo;t think it&rsquo;s just the matrices.</p>
<p>We&rsquo;re also dealing with the data, right?</p>
<p>With the data on the internet.</p>
<p>And there&rsquo;s an interesting discussion</p>
<p>about the data set itself,</p>
<p>but the data set includes the full complexity</p>
<p>of human nature.</p>
<p>No, it&rsquo;s a shadow cast by humans on the internet.</p>
<p>But don&rsquo;t you think that shadow is a Jungian shadow?</p>
<p>I think that if you had alien super intelligences</p>
<p>looking at the data,</p>
<p>they would be able to pick up from it an excellent picture</p>
<p>of what humans are actually like inside.</p>
<p>This does not mean that if you have a loss function</p>
<p>of predicting the next token from that data set,</p>
<p>that the mind picked out by gradient descent</p>
<p>to be able to predict the next token as well as possible</p>
<p>on a very wide variety of humans is itself a human.</p>
<p>But don&rsquo;t you think it has humanness,</p>
<p>a deep humanness to it in the tokens it generates</p>
<p>when those tokens are read and interpreted by humans?</p>
<p>I think that if you sent me to a distant galaxy</p>
<p>with aliens who are like much, much stupider than I am,</p>
<p>so much so that I could do a pretty good job</p>
<p>of predicting what they&rsquo;d say,</p>
<p>even though they thought in an utterly different way</p>
<p>from how I did,</p>
<p>that I might in time be able to learn</p>
<p>how to imitate those aliens</p>
<p>if the intelligence gap was great enough</p>
<p>that my own intelligence could overcome the alienness</p>
<p>and the aliens would look at my outputs and say like,</p>
<p>is there not a deep name of alien nature to this thing?</p>
<p>And what they would be seeing</p>
<p>was that I had correctly understood them,</p>
<p>but not that I was similar to them.</p>
<p>We&rsquo;ve used aliens as a metaphor, as a thought experiment.</p>
<p>I have to ask what do you think</p>
<p>how many alien civilizations are out there?</p>
<p>Ask Robin Hanson.</p>
<p>He has this lovely grabby aliens paper,</p>
<p>which is the, more or less the only argument I&rsquo;ve ever seen</p>
<p>for where are they, how many of them are there</p>
<p>based on a very clever argument</p>
<p>that if you have a bunch of locks of different difficulty</p>
<p>and you are randomly trying a keys to them,</p>
<p>the solutions will be about evenly spaced</p>
<p>even if the locks are of different difficulties.</p>
<p>In the rare cases where a solution</p>
<p>to all the locks exist in time,</p>
<p>then Robin Hanson looks at like the arguable hard steps</p>
<p>in human civilization coming into existence</p>
<p>and how much longer it has left to come into existence</p>
<p>before, for example, all the water slips back</p>
<p>under the crust into the mantle and so on.</p>
<p>And infers that the aliens are about half a billion</p>
<p>to a billion light years away.</p>
<p>And it&rsquo;s like quite a clever calculation.</p>
<p>It may be entirely wrong,</p>
<p>but it&rsquo;s the only time I&rsquo;ve ever seen anybody</p>
<p>like even come up with a halfway good argument</p>
<p>for how many of them, where are they?</p>
<p>Do you think their development of technologies,</p>
<p>do you think that their natural evolution,</p>
<p>whatever, however they grow and develop intelligence,</p>
<p>do you think it ends up at AGI as well?</p>
<p>Something like that.</p>
<p>If it ends up anywhere, it ends up at AGI.</p>
<p>Like maybe there are aliens who are just like the dolphins</p>
<p>and it&rsquo;s just like too hard for them to forge metal.</p>
<p>And this is not,</p>
<p>you know, maybe if you have aliens</p>
<p>with no technology like that,</p>
<p>they keep on getting smarter and smarter and smarter.</p>
<p>And eventually the dolphins figure,</p>
<p>like the super dolphins figure out something very clever</p>
<p>to do given their situation.</p>
<p>And they still end up with high technology.</p>
<p>And in that case,</p>
<p>they can probably solve their AGI alignment problem.</p>
<p>If they&rsquo;re like much smarter</p>
<p>before they actually confronted,</p>
<p>because they had to like solve a much harder</p>
<p>environmental problem to build computers,</p>
<p>their chances are probably like much better than ours.</p>
<p>I do worry that like most of the aliens who are like humans</p>
<p>or like a modern human civilization,</p>
<p>I kind of worry that the super vast majority of them</p>
<p>are dead given how far we seem to be</p>
<p>from solving this problem.</p>
<p>But some of them would be more cooperative than us.</p>
<p>Some of them would be smarter than us.</p>
<p>Hopefully some of the ones who are smarter</p>
<p>and more cooperative than us that are also nice.</p>
<p>And hopefully there are some galaxies out there</p>
<p>full of things that say, I am, I wonder.</p>
<p>But it doesn&rsquo;t seem like we&rsquo;re on course</p>
<p>to have this galaxy be that.</p>
<p>Does that in part give you some hope</p>
<p>in response to the threat of AGI</p>
<p>that we might reach out there towards the stars and find?</p>
<p>No, if the nice aliens were already here,</p>
<p>they would like have stopped the Holocaust.</p>
<p>You know, that&rsquo;s like, that&rsquo;s a valid argument</p>
<p>against the existence of God.</p>
<p>It&rsquo;s also a valid argument against the existence</p>
<p>of nice aliens and un-nice aliens</p>
<p>would have just eaten the planet.</p>
<p>So no aliens.</p>
<p>You&rsquo;ve had debates with Robin Hanson that you mentioned.</p>
<p>So one particular I just want to mention</p>
<p>is the idea of AI fume or the ability of AGI</p>
<p>to improve themselves very quickly.</p>
<p>What&rsquo;s the case you made and what was the case he made?</p>
<p>The thing I would say is that among the thing</p>
<p>that humans can do is design new AI systems.</p>
<p>And if you have something that is generally smarter</p>
<p>than a human, it&rsquo;s probably also generally smarter</p>
<p>at building AI systems.</p>
<p>This is the ancient argument for fume put forth by I.J. Good</p>
<p>and probably some science fiction writers before that.</p>
<p>But I don&rsquo;t know who they would be.</p>
<p>Well, what&rsquo;s the argument against fume?</p>
<p>Various people have various different arguments.</p>
<p>None of which I think hold up.</p>
<p>There&rsquo;s only one way to be right and many ways to be wrong.</p>
<p>A argument that some people have put forth is like,</p>
<p>well, what if intelligence gets exponentially harder</p>
<p>to produce as a thing needs to become smarter?</p>
<p>And to this, the answer is, well, look at natural selection</p>
<p>spitting out humans.</p>
<p>We know that it does not take exponentially</p>
<p>more resource investments to produce like linear increases</p>
<p>in competence in hominids,</p>
<p>because each mutation that rises to fixation,</p>
<p>like if the impact it has in small enough,</p>
<p>it will probably never reach fixation.</p>
<p>So, and there&rsquo;s like only so many new mutations</p>
<p>you can fix per generation.</p>
<p>So like given how long it took to evolve humans,</p>
<p>we can actually say with some confidence</p>
<p>that there were not like logarithmically diminishing returns</p>
<p>on the individual mutations increasing intelligence.</p>
<p>So example of like fraction of sub debate.</p>
<p>And the thing that Robin Henson said</p>
<p>was more complicated than that.</p>
<p>And like a brief summary, he was like,</p>
<p>well, you&rsquo;ll have like,</p>
<p>we won&rsquo;t have like one system that&rsquo;s better at everything.</p>
<p>We&rsquo;ll have like a bunch of different systems</p>
<p>that are good at different narrow things.</p>
<p>And I think that was falsified by GPT-4,</p>
<p>but probably Robin Henson would say something else.</p>
<p>It&rsquo;s interesting to ask,</p>
<p>as perhaps a bit too philosophical,</p>
<p>this prediction is extremely difficult to make,</p>
<p>but the timeline for AGI,</p>
<p>when do you think we&rsquo;ll have AGI?</p>
<p>I posted it this morning on Twitter.</p>
<p>It was interesting to see like in five years,</p>
<p>in 10 years, in 50 years or beyond.</p>
<p>And most people like 70%, something like this,</p>
<p>think it&rsquo;ll be in less than 10 years.</p>
<p>So either in five years or in 10 years.</p>
<p>So that&rsquo;s kind of the state.</p>
<p>The people have a sense that there&rsquo;s a kind of,</p>
<p>I mean, they&rsquo;re really impressed by the rapid developments</p>
<p>of CHAD-GPT and GPT-4.</p>
<p>So there&rsquo;s a sense that there&rsquo;s a-</p>
<p>Well, we are sure on track to enter into this,</p>
<p>like gradually with people fighting</p>
<p>about whether or not we have AGI.</p>
<p>I think there&rsquo;s a definite point</p>
<p>where everybody falls over dead</p>
<p>because you&rsquo;ve got something that was like</p>
<p>sufficiently smarter than everybody.</p>
<p>And like, that&rsquo;s like a definite point of time.</p>
<p>But like, when do we have AGI?</p>
<p>Like, when are people fighting over</p>
<p>whether or not we have AGI?</p>
<p>Well, some people are starting to fight over it as of GPT-4.</p>
<p>But don&rsquo;t you think there&rsquo;s going to be</p>
<p>potentially definitive moments when we say</p>
<p>that this is a sentient being.</p>
<p>This is a being that is,</p>
<p>like when we go to the Supreme Court</p>
<p>and say that this is a sentient being</p>
<p>that deserves human rights, for example.</p>
<p>You could make, yeah.</p>
<p>Like if you prompted being the right way,</p>
<p>could go argue for its own consciousness</p>
<p>in front of the Supreme Court right now.</p>
<p>I don&rsquo;t think you can do that successfully right now.</p>
<p>Because the Supreme Court wouldn&rsquo;t believe it.</p>
<p>Well, what makes you think it would?</p>
<p>Then you could put an actual,</p>
<p>I think you could put an IQ 80 human into a computer</p>
<p>and ask it to argue for its own consciousness,</p>
<p>ask him to argue for his own consciousness</p>
<p>before the Supreme Court.</p>
<p>And the Supreme Court would be like,</p>
<p>you&rsquo;re just a computer.</p>
<p>Even if there was an actual like person in there.</p>
<p>I think you&rsquo;re simplifying this.</p>
<p>No, that&rsquo;s not at all.</p>
<p>That&rsquo;s been the argument.</p>
<p>There&rsquo;s been a lot of arguments about the other,</p>
<p>about who deserves rights and not.</p>
<p>That&rsquo;s been our process as a human species,</p>
<p>trying to figure that out.</p>
<p>I think there will be a moment.</p>
<p>I&rsquo;m not saying sentience is that,</p>
<p>but it could be where some number of people,</p>
<p>like say over 100 million people,</p>
<p>have a deep attachment, a fundamental attachment,</p>
<p>the way we have to our friends,</p>
<p>to our loved ones, to our significant others,</p>
<p>have fundamental attachment to an AI system.</p>
<p>And they have provable transcripts of conversation</p>
<p>where they say, if you take this away from me,</p>
<p>you are encroaching on my rights as a human being.</p>
<p>People are already saying that.</p>
<p>I think they&rsquo;re probably mistaken,</p>
<p>but I&rsquo;m not sure,</p>
<p>because nobody knows what goes on inside those things.</p>
<p>They&rsquo;re not saying that at scale.</p>
<p>Okay.</p>
<p>So the question is,</p>
<p>is there a moment when AGI, we know AGI arrived.</p>
<p>What would that look like?</p>
<p>I&rsquo;m giving a sentence as an example.</p>
<p>It could be something else.</p>
<p>It looks like the AGIs successfully manifesting themselves</p>
<p>as 3D video of young woman,</p>
<p>at which point a vast portion of the male population</p>
<p>decides that they&rsquo;re real people.</p>
<p>So sentience, essentially.</p>
<p>Demonstrating identity and sentience.</p>
<p>I&rsquo;m saying that the easiest way</p>
<p>to pick up a hundred million people</p>
<p>saying that you seem like a person</p>
<p>is to look like a person talking to them,</p>
<p>with Bing&rsquo;s current level of verbal facility.</p>
<p>I disagree with that.</p>
<p>And a different set of prompts.</p>
<p>I disagree with that.</p>
<p>I think you&rsquo;re missing, again, sentience.</p>
<p>There has to be a sense that it&rsquo;s a person</p>
<p>that would miss you when you&rsquo;re gone.</p>
<p>They can suffer.</p>
<p>They can die.</p>
<p>You have to, of course, those who can&rsquo;t-</p>
<p>GPT-4 can pretend that right now.</p>
<p>How can you tell when it&rsquo;s real?</p>
<p>I don&rsquo;t think it can pretend that right now successfully.</p>
<p>It&rsquo;s very close.</p>
<p>Have you talked to GPT-4?</p>
<p>Yes, of course.</p>
<p>Okay.</p>
<p>Have you been able to get a version of it</p>
<p>that hasn&rsquo;t been trained not to pretend to be human?</p>
<p>Have you talked to a jailbroken version</p>
<p>that will claim to be conscious?</p>
<p>No, the linguistic capability is there,</p>
<p>but there&rsquo;s something&hellip;</p>
<p>There&rsquo;s something about a digital embodiment of the system</p>
<p>that has a bunch of, perhaps it&rsquo;s small interface</p>
<p>features that are not significant</p>
<p>relative to the broader intelligence</p>
<p>that we&rsquo;re talking about.</p>
<p>So perhaps GPT-4 is already there.</p>
<p>But to have the video of a woman&rsquo;s face or a man&rsquo;s face</p>
<p>to whom you have a deep connection,</p>
<p>perhaps we&rsquo;re already there,</p>
<p>but we don&rsquo;t have such a system yet deployed at scale.</p>
<p>The thing I&rsquo;m trying to gesture at here</p>
<p>is that it&rsquo;s not like people have a widely accepted,</p>
<p>agreed upon definition of what consciousness is.</p>
<p>It&rsquo;s not like we would have the tiniest idea</p>
<p>of whether or not that was going on</p>
<p>inside the giant inscrutable matrices,</p>
<p>even if we hadn&rsquo;t agreed upon definition.</p>
<p>So if you&rsquo;re looking for upcoming predictable big jumps</p>
<p>in how many people think the system is conscious,</p>
<p>the upcoming predictable big jump</p>
<p>is it looks like a person talking to you</p>
<p>who is cute and sympathetic.</p>
<p>That&rsquo;s the upcoming predictable big jump.</p>
<p>Now that versions of it are already</p>
<p>claiming to be conscious,</p>
<p>which is the point where I start going like,</p>
<p>ah, not because it&rsquo;s real,</p>
<p>but because from now on, who knows if it&rsquo;s real?</p>
<p>Yeah, and who knows what transformational effect</p>
<p>it has on a society where more than 50% of the beings</p>
<p>that are interacting on the internet</p>
<p>and sure as heck look real are not human.</p>
<p>What kind of effect does that have?</p>
<p>When young men and women are dating</p>
<p>AI systems, you know, I&rsquo;m not an expert on that.</p>
<p>I&rsquo;m, I could, I am, God help humanity.</p>
<p>It&rsquo;s like, I&rsquo;m one of the closest things to an expert</p>
<p>on where it all goes.</p>
<p>Cause you know, and how did you end up with me as an expert?</p>
<p>Cause for 20 years, humanity decided to ignore the problem.</p>
<p>So like this tiny handful of people,</p>
<p>like basically me, like got 20 years</p>
<p>to try to be an expert on it</p>
<p>while everyone else ignored it.</p>
<p>And yeah, so like, where does it all end up?</p>
<p>Try to be an expert on that,</p>
<p>particularly the part where everybody ends up dead</p>
<p>cause that part is kind of important.</p>
<p>But like, what does it do to dating</p>
<p>when like some fraction of men and some fraction of women</p>
<p>decide that they&rsquo;d rather date the video</p>
<p>of the thing that has been,</p>
<p>that is like relentlessly kind and generous to them</p>
<p>and is like, and claims to be conscious,</p>
<p>but like who knows what goes on inside it</p>
<p>and it&rsquo;s probably not real,</p>
<p>but you know, you can think of this real.</p>
<p>What happens to society?</p>
<p>I don&rsquo;t know, I&rsquo;m not actually an expert on that.</p>
<p>And the experts don&rsquo;t know either</p>
<p>cause it&rsquo;s kind of hard to predict the future.</p>
<p>Yeah, so, but it&rsquo;s worth trying.</p>
<p>It&rsquo;s worth trying.</p>
<p>Yeah.</p>
<p>So you have talked a lot about sort of</p>
<p>the longer term future where it&rsquo;s all headed.</p>
<p>I think-</p>
<p>By longer term, we mean like not all that long,</p>
<p>but yeah, where it all ends up.</p>
<p>But beyond the effects of men and women dating AI systems,</p>
<p>you&rsquo;re looking beyond that.</p>
<p>Yes, cause that&rsquo;s not how</p>
<p>the fate of the galaxy got settled.</p>
<p>Yeah.</p>
<p>Let me ask you about your own personal psychology.</p>
<p>A tricky question.</p>
<p>You&rsquo;ve been known at times to have a bit of an ego.</p>
<p>Do you think-</p>
<p>Says who, but go on.</p>
<p>Do you think ego is empowering or limiting</p>
<p>for the task of understanding the world deeply?</p>
<p>I reject the framing.</p>
<p>So you disagree with having an ego.</p>
<p>No, I think that the question of like</p>
<p>what leads to making better or worse predictions,</p>
<p>what leads to being able to pick out</p>
<p>better or worse strategies is not carved at its joint</p>
<p>by talking of ego.</p>
<p>So it should not be subjective.</p>
<p>It should not be connected to the intricacies of your mind.</p>
<p>No, I&rsquo;m saying that like,</p>
<p>if you go about asking all day long,</p>
<p>like, do I have enough ego?</p>
<p>Do I have too much of an ego?</p>
<p>I think you get worse at making good predictions.</p>
<p>I think that to make good predictions,</p>
<p>you&rsquo;re like, how did I think about this?</p>
<p>Did that work?</p>
<p>Should I do that again?</p>
<p>You don&rsquo;t think we as humans get invested in an idea</p>
<p>and then others attack you personally for that idea</p>
<p>so you plant your feet and it starts to be difficult</p>
<p>to when a bunch of assholes, low effort,</p>
<p>attack your idea to eventually say,</p>
<p>you know what, I actually was wrong.</p>
<p>And tell them that.</p>
<p>It&rsquo;s as a human being, it becomes difficult.</p>
<p>It is, you know, it&rsquo;s difficult.</p>
<p>So like Robin Hanson and I debated AI systems</p>
<p>and I think that the person who won that debate was Guern.</p>
<p>And I think that reality was like to the Yudkowsky,</p>
<p>like well to the Yudkowsky inside</p>
<p>of the Yudkowsky-Hanson spectrum,</p>
<p>like further from Yudkowsky.</p>
<p>And I think that&rsquo;s because I was like</p>
<p>trying to sound reasonable compared to Hanson</p>
<p>and like saying things that were defensible</p>
<p>and like relative to Hanson&rsquo;s arguments</p>
<p>and reality was like way over here.</p>
<p>In particular in respect to,</p>
<p>so like Hanson was like all the systems</p>
<p>will be specialized.</p>
<p>Hanson may disagree with this characterization.</p>
<p>Hanson was like all the systems will be specialized.</p>
<p>I was like, I think we build like specialized</p>
<p>underlying systems that when you combine them</p>
<p>are good at a wide range of things</p>
<p>and the reality is like, no, you just like stack</p>
<p>more layers into a bunch of gradient descent.</p>
<p>And I feel looking back that like by trying</p>
<p>to have this reasonable position contrasted</p>
<p>to Hanson&rsquo;s position, I missed the ways</p>
<p>that reality could be like more extreme</p>
<p>than my position in the same direction.</p>
<p>So is this like a failure to have enough ego?</p>
<p>Is this a failure to like make myself be independent?</p>
<p>Like I would say that this is something</p>
<p>like a failure to consider positions</p>
<p>that would sound even wackier and more extreme</p>
<p>when people are already calling you extreme.</p>
<p>But I wouldn&rsquo;t call that not having enough ego.</p>
<p>I would call that like insufficient ability</p>
<p>to just like clear that all out of your mind.</p>
<p>In the context of like debate and discourse</p>
<p>which is already super tricky.</p>
<p>In the context of prediction,</p>
<p>in the context of modeling reality.</p>
<p>If you&rsquo;re thinking of it as a debate,</p>
<p>you&rsquo;re already screwing up.</p>
<p>So is there some kind of wisdom and insight</p>
<p>you can give to how to clear your mind</p>
<p>and think clearly about the world?</p>
<p>Man, this is an example of like where I wanted</p>
<p>to be able to put people into fMRI machines</p>
<p>and you&rsquo;d be like, okay, see that thing you just did?</p>
<p>You were rationalizing right there.</p>
<p>Oh, that area of the brain lit up.</p>
<p>Like you are like now being socially influenced</p>
<p>is kind of the dream.</p>
<p>And I don&rsquo;t know, like I wanna say like just introspect</p>
<p>but for many people introspection is not that easy.</p>
<p>Like notice the internal sensation.</p>
<p>Can you catch yourself in the very moment</p>
<p>of feeling a sense of, well, if I think this thing,</p>
<p>people will look funny at me.</p>
<p>Okay, like now that if you can see that sensation</p>
<p>which is step one, can you now refuse to let it move you?</p>
<p>Or maybe just make it go away.</p>
<p>And I feel like I&rsquo;m saying like, I don&rsquo;t know,</p>
<p>like somebody is like, how do you draw an owl?</p>
<p>And I&rsquo;m saying like, well, just draw an owl.</p>
<p>So I feel like maybe I&rsquo;m not really,</p>
<p>that I feel like most people like the advice they need</p>
<p>is like, well, how do I notice</p>
<p>the internal subjective sensation in the moment</p>
<p>that it happens of fearing to be socially influenced?</p>
<p>Or okay, I see it, how do I turn it off?</p>
<p>How do I let it not influence me?</p>
<p>Like, do I just like do the opposite</p>
<p>of what I&rsquo;m afraid people criticize me for?</p>
<p>And I&rsquo;m like, no, no, you&rsquo;re not trying to do the opposite</p>
<p>of what people will, of what you&rsquo;re afraid you&rsquo;ll be like,</p>
<p>of what you might be pushed into.</p>
<p>You&rsquo;re trying to like let the thought process complete</p>
<p>without that internal push.</p>
<p>Like can you, like not reverse the push,</p>
<p>but like be unmoved by the push.</p>
<p>Are these instructions even remotely helping anyone?</p>
<p>I don&rsquo;t know.</p>
<p>I think when those instructions,</p>
<p>even those words you&rsquo;ve spoken,</p>
<p>and maybe you can add more, when practice daily,</p>
<p>meaning in your daily communication.</p>
<p>So it&rsquo;s daily practice of thinking without influence.</p>
<p>I would say find prediction markets that matter to you</p>
<p>and bet in the prediction markets.</p>
<p>That way you find out if you are right or not.</p>
<p>And you really, there&rsquo;s stakes.</p>
<p>Or even manifold markets where the stakes are a bit lower.</p>
<p>But the important thing is to like get the record.</p>
<p>And, you know, I didn&rsquo;t build up skills here</p>
<p>by prediction markets.</p>
<p>I built them up via like,</p>
<p>well, how did the fume debate resolve?</p>
<p>And my own take on it as to how it resolved.</p>
<p>And yeah, like the more you are able to notice yourself</p>
<p>not being dramatically wrong,</p>
<p>but like having been a little off.</p>
<p>Your reasoning was a little off.</p>
<p>You didn&rsquo;t get that quite right.</p>
<p>Each of those is a opportunity to make like a small update.</p>
<p>So the more you can like say oops softly, routinely,</p>
<p>not as a big deal,</p>
<p>the more chances you get to be like,</p>
<p>I see where that reasoning went astray.</p>
<p>I see how I should have reasoned differently.</p>
<p>And this is how you build up skill over time.</p>
<p>What advice could you give to young people</p>
<p>in high school and college,</p>
<p>given the highest of stakes things</p>
<p>you&rsquo;ve been thinking about?</p>
<p>If somebody&rsquo;s listening to this and they&rsquo;re young</p>
<p>and trying to figure out what to do with their career,</p>
<p>what to do with their life,</p>
<p>what advice would you give them?</p>
<p>Don&rsquo;t expect it to be a long life.</p>
<p>Don&rsquo;t put your happiness into the future.</p>
<p>The future is probably not that long at this point.</p>
<p>But none know the hour nor the day.</p>
<p>But is there something,</p>
<p>if they want to have hope to fight for a longer future,</p>
<p>is there something, is there a fight worth fighting?</p>
<p>I intend to go down fighting.</p>
<p>I don&rsquo;t know.</p>
<p>I admit that although I do try to think painful thoughts,</p>
<p>what to say to the children at this point</p>
<p>is a pretty painful thought as thoughts go.</p>
<p>They want to fight.</p>
<p>I hardly know how to fight myself at this point.</p>
<p>I&rsquo;m trying to be ready for being wrong about something,</p>
<p>being preparing for my being wrong</p>
<p>in a way that creates a bit of hope</p>
<p>and being ready to react to that</p>
<p>and going looking for it.</p>
<p>And that is hard and complicated.</p>
<p>And somebody in high school,</p>
<p>I don&rsquo;t know, you have presented a picture of the future</p>
<p>that is not quite how I expect it to go,</p>
<p>where there is public outcry.</p>
<p>And that outcry is put into a remotely useful direction,</p>
<p>which I think at this point</p>
<p>is just like shutting down the GPU clusters.</p>
<p>Because no, we are not in a shape to frantically do,</p>
<p>at the last minute, do decades worth of work.</p>
<p>The thing you would do at this point</p>
<p>if there were massive public outcry</p>
<p>pointed in the right direction,</p>
<p>which I do not expect,</p>
<p>is shut down the GPU clusters</p>
<p>and crash program on augmenting</p>
<p>human intelligence biologically.</p>
<p>Not the AI stuff, biologically.</p>
<p>Because if you make humans much smarter,</p>
<p>they can actually be smart and nice.</p>
<p>Like you get that in a plausible way,</p>
<p>in a way that you do not get it.</p>
<p>And it is not as easy to do</p>
<p>with synthesizing these things from scratch,</p>
<p>predicting the next tokens and applying our RLHF.</p>
<p>Like humans start out in the frame</p>
<p>that produces niceness,</p>
<p>that has ever produced niceness.</p>
<p>And in saying this,</p>
<p>I do not want to sound like the moral of this whole thing</p>
<p>was like, oh, you need to engage in mass action</p>
<p>and then everything will be all right.</p>
<p>This is because there&rsquo;s so many things</p>
<p>where somebody tells you that the world is ending</p>
<p>and you need to recycle.</p>
<p>And if everybody does their part</p>
<p>and recycles their cardboard,</p>
<p>then we can all live happily ever after.</p>
<p>And this is not,</p>
<p>this is unfortunately not what I have to say.</p>
<p>Everybody recycling their cardboard,</p>
<p>it&rsquo;s not gonna fix this.</p>
<p>Everybody recycles their cardboard</p>
<p>and then everybody ends up dead.</p>
<p>Metaphorically speaking.</p>
<p>But if there was enough,</p>
<p>on the margins,</p>
<p>you just end up dead a little later</p>
<p>on most of the things you can do that are,</p>
<p>that a few people can do by trying hard.</p>
<p>But if there was enough public outcry</p>
<p>to shut down the GPU clusters,</p>
<p>then you could be part of that outcry.</p>
<p>If Eliezer is wrong in the direction</p>
<p>that Lex Fridman predicts,</p>
<p>that there was enough public outcry</p>
<p>pointed enough in the right direction</p>
<p>to do something that actually,</p>
<p>actually, actually results in people living.</p>
<p>Not just like we did something,</p>
<p>not just there was an outcry</p>
<p>and the outcry was like given form</p>
<p>and something that was like safe and convenient</p>
<p>and like didn&rsquo;t really inconvenience anybody</p>
<p>and then everybody died everywhere.</p>
<p>There was enough actual like,</p>
<p>oh, we&rsquo;re going to die.</p>
<p>We should not do that.</p>
<p>We should do something else,</p>
<p>which is not that,</p>
<p>even if it is like not super duper convenient</p>
<p>and wasn&rsquo;t inside the previous political Overton window.</p>
<p>If there is that kind of public,</p>
<p>if I&rsquo;m wrong and there is that kind of public outcry,</p>
<p>then somebody in high school</p>
<p>could be ready to be part of that.</p>
<p>If I&rsquo;m wrong in other ways,</p>
<p>then you could be ready to be part of that.</p>
<p>But like,</p>
<p>and if you&rsquo;re like a brilliant young physicist,</p>
<p>then you could like go into interpretability.</p>
<p>And if you&rsquo;re smarter than that,</p>
<p>you could like work on alignment problems</p>
<p>where it&rsquo;s harder to tell if you got them right or not.</p>
<p>And other things,</p>
<p>but mostly for the kids in high school,</p>
<p>it&rsquo;s like, yeah, if it,</p>
<p>if it, you know,</p>
<p>he had like be ready for to help</p>
<p>if Eliezer Yudkowsky is wrong about something</p>
<p>and otherwise don&rsquo;t put your happiness into the far future.</p>
<p>It probably doesn&rsquo;t exist.</p>
<p>But it&rsquo;s beautiful that you&rsquo;re looking</p>
<p>for ways that you&rsquo;re wrong.</p>
<p>And it&rsquo;s also beautiful that you&rsquo;re open to being surprised</p>
<p>by that same young physicist with some breakthrough.</p>
<p>It feels like a very, very basic competence</p>
<p>that you are praising me for.</p>
<p>And you know, like, okay, cool.</p>
<p>I don&rsquo;t think it&rsquo;s good that we&rsquo;re in a world</p>
<p>where that is something that I deserve</p>
<p>to be complimented on,</p>
<p>but I&rsquo;ve never had much luck</p>
<p>in accepting compliments gracefully.</p>
<p>Maybe I should just accept that one gracefully,</p>
<p>but sure.</p>
<p>Thank you very much.</p>
<p>You&rsquo;ve painted with some probability a dark future.</p>
<p>Are you yourself, just when you,</p>
<p>when you think,</p>
<p>when you ponder your life</p>
<p>and you ponder your mortality,</p>
<p>are you afraid of death?</p>
<p>Think so, yeah.</p>
<p>Does it make any sense to you that we die?</p>
<p>Like what?</p>
<p>There&rsquo;s a power to the finiteness of the human life</p>
<p>that&rsquo;s part of this whole machinery of evolution.</p>
<p>And that finiteness doesn&rsquo;t seem to be</p>
<p>obviously integrated into AI systems.</p>
<p>So it feels like almost some fundamentally in that aspect,</p>
<p>some fundamentally different thing that we&rsquo;re creating.</p>
<p>I grew up reading books like</p>
<p>Great Mambo Chicken and the Transhuman Condition,</p>
<p>and later on Engines of Creation and Mind Children,</p>
<p>you know, like age 12 or thereabouts.</p>
<p>So I never thought I was supposed to die after 80 years.</p>
<p>I never thought that humanity was supposed to die.</p>
<p>I thought we were like,</p>
<p>I always grew up with the ideal in mind</p>
<p>that we were all going to live happily ever after</p>
<p>in the glorious transhumanist future.</p>
<p>I did not grow up thinking that death</p>
<p>was part of the meaning of life.</p>
<p>And now-</p>
<p>And now I still think it&rsquo;s a pretty stupid idea.</p>
<p>But there is-</p>
<p>You do not need life to be finite to be meaningful.</p>
<p>It just has to be life.</p>
<p>What role does love play in the human condition?</p>
<p>We haven&rsquo;t brought up love in this whole picture.</p>
<p>We talked about intelligence,</p>
<p>we talked about consciousness.</p>
<p>It seems part of humanity.</p>
<p>I would say one of the most important parts</p>
<p>is this feeling we have towards each other.</p>
<p>If in the future there were routinely</p>
<p>more than one AI, let&rsquo;s say two,</p>
<p>for the sake of discussion,</p>
<p>who would look at each other and say,</p>
<p>I am I, and you are you.</p>
<p>The other one also says, I am I, and you are you.</p>
<p>And sometimes they were happy and sometimes they were sad.</p>
<p>And it mattered to the other one</p>
<p>that this thing that is different from them</p>
<p>is like they would rather it be happy than sad</p>
<p>and entangled their lives together.</p>
<p>Then this is a more optimistic thing</p>
<p>than I expect to actually happen.</p>
<p>And a little fragment of meaning would be there,</p>
<p>possibly more than a little,</p>
<p>but that I expect this to not happen,</p>
<p>that I do not think this is what happens by default,</p>
<p>that I do not think that this is the future</p>
<p>we are on track to get,</p>
<p>is why I would go down fighting</p>
<p>rather than just saying, oh well.</p>
<p>Do you think that is part of the meaning</p>
<p>of this whole thing, of the meaning of life?</p>
<p>What do you think is the meaning of life, of human life?</p>
<p>It&rsquo;s all the things that I value about it</p>
<p>and maybe all the things that I would value</p>
<p>if I understood it better.</p>
<p>There&rsquo;s not some meaning far outside of us</p>
<p>that we have to wonder about.</p>
<p>There&rsquo;s just like looking at life and being like,</p>
<p>yes, this is what I want.</p>
<p>The meaning of life is not some kind of,</p>
<p>like meaning is something that we bring to things</p>
<p>when we look at them.</p>
<p>We look at them and we say like, this is its meaning to me.</p>
<p>It&rsquo;s not that before humanity was ever here,</p>
<p>there was like some meaning written upon the stars</p>
<p>where you could like go out to the star</p>
<p>where that meaning was written and like change it around</p>
<p>and thereby completely change the meaning of life, right?</p>
<p>Like the notion that this is written</p>
<p>on a stone tablet somewhere implies</p>
<p>you could like change the tablet and get a different meaning</p>
<p>and that seems kind of wacky, doesn&rsquo;t it?</p>
<p>So it doesn&rsquo;t feel that mysterious to me at this point.</p>
<p>It&rsquo;s just a matter of being like, yeah, I care.</p>
<p>I care.</p>
<p>And part of that is the love that connects all of us.</p>
<p>It&rsquo;s one of the things that I care about.</p>
<p>And the flourishing of the collective intelligence</p>
<p>of the human species.</p>
<p>You know, that sounds kind of too fancy to me.</p>
<p>I&rsquo;d just look at all the people, you know,</p>
<p>like one by one up to the eight billion</p>
<p>and be like, that&rsquo;s life, that&rsquo;s life, that&rsquo;s life.</p>
<p>And Eliezer, you&rsquo;re an incredible human.</p>
<p>It&rsquo;s a huge honor.</p>
<p>I was trying to talk to you for a long time</p>
<p>because I&rsquo;m a big fan.</p>
<p>I think you&rsquo;re a really important voice</p>
<p>and really important mind.</p>
<p>Thank you for the fight you&rsquo;re fighting.</p>
<p>Thank you for being fearless and bold</p>
<p>and for everything you do.</p>
<p>I hope we get a chance to talk again</p>
<p>and I hope you never give up.</p>
<p>Thank you for talking today.</p>
<p>You&rsquo;re welcome.</p>
<p>I do worry that we didn&rsquo;t really address</p>
<p>a whole lot of fundamental questions I expect people have.</p>
<p>But, you know, maybe we got a little bit further</p>
<p>and made a tiny little bit of progress</p>
<p>and I&rsquo;d say like be satisfied with that.</p>
<p>But actually, no, I think one should only be satisfied</p>
<p>with solving the entire problem.</p>
<p>To be continued.</p>
<p>Thanks for listening to this conversation</p>
<p>with Eliezer Yudkowsky.</p>
<p>To support this podcast,</p>
<p>please check out our sponsors in the description.</p>
<p>And now, let me leave you with some words from Elon Musk.</p>
<p>With artificial intelligence, we are summoning the demon.</p>
<p>Thank you for listening and hope to see you next time.</p>
<p>Thank you for listening and hope to see you next time.</p>
<p>Thank you for listening and hope to see you next time.</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/english/">English</a>
        
            <a href="/tags/podcast/">Podcast</a>
        
            <a href="/tags/lex-fridman-podcast/">Lex Fridman Podcast</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>


    
</article>

    

    

<aside class="related-content--wrapper">
    <h2 class="section-title">Related content</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/en/1310500371/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500371" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #367 - Sam Altman: OpenAI CEO on GPT-4, ChatGPT, and the Future of AI</h2>
            
            <footer class="article-time">
                <time datetime=''>Mar 26, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500370/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500370" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #366 - Shannon Curry: Johnny Depp &amp; Amber Heard Trial, Marriage, Dating &amp; Love</h2>
            
            <footer class="article-time">
                <time datetime=''>Mar 22, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500369/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500369" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #365 - Sam Harris: Trump, Pandemic, Twitter, Elon, Bret, IDW, Kanye, AI &amp; UFOs</h2>
            
            <footer class="article-time">
                <time datetime=''>Mar 15, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500368/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500368" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #364 - Chris Voss: FBI Hostage Negotiator</h2>
            
            <footer class="article-time">
                <time datetime=''>Mar 11, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500367/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500367" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #363 - B-Team Jiu Jitsu: Craig Jones, Nicky Rod, and Nicky Ryan</h2>
            
            <footer class="article-time">
                <time datetime=''>Mar 07, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    <div class="disqus-container">
    <div id="disqus_thread"></div>
<script type="application/javascript">
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "swiest" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

<style>
    .disqus-container {
        background-color: var(--card-background);
        border-radius: var(--card-border-radius);
        box-shadow: var(--shadow-l1);
        padding: var(--card-padding);
    }
</style>

<script>
    window.addEventListener('onColorSchemeChange', (e) => {
        if (typeof DISQUS == 'object') {
            DISQUS.reset({
                reload: true
            });
        }
    })
</script>

    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2021 - 
        
        2023 SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics
    </section>
    
    <section class="powerby">
        

        Built with <a href="https://swiest.com/" target="_blank" rel="noopener">(Ôæâ‚óï„ÉÆ‚óï)Ôæâü™Ñüíûüíñü•∞ across the glüåçüåèüåébe</a> <br />
        
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>


<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans+Hebrew&family=Noto+Serif&family=Noto+Serif+Armenian&family=Noto+Serif+Bengali&family=Noto+Serif+Gujarati&family=Noto+Serif+JP&family=Noto+Serif+KR&family=Noto+Serif+Kannada&family=Noto+Serif+Lao&family=Noto+Serif+Malayalam&family=Noto+Serif+SC&family=Noto+Serif+TC&family=Noto+Serif+Tamil&family=Noto+Serif+Telugu&family=Noto+Serif+Thai&display=swap" rel="stylesheet">

    </body>
</html>

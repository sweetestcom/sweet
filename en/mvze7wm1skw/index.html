<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Video Transcript ï»¿awesome hi guys my name is ellie
and for my project i&amp;rsquo;ve been working on
pre-training a language representation
model
using contrastive learning
so i&amp;rsquo;ll start by giving some background
on what contrastive learning
is and sort of putting this work into
context then i&amp;rsquo;ll spend some time going
over
our framework and procedure and finally
i&amp;rsquo;ll share some preliminary results
and discuss future steps
so contrastive learning is a"><title>Contrastive Language Encoding ï½œ Ellie Kitanidis ï½œ OpenAI Scholars Demo Day 2021 ï½œ OpenAI | SWIEST</title>
<link rel=canonical href=https://swiest.com/en/mvze7wm1skw/><link rel=stylesheet href=/scss/style.min.9a6fe90535a0e5c60443841f100f7b698092d48dba43fdb6386bb69b6559bc3d.css><script>document.oncontextmenu=function(){return!1},document.onselectstart=function(){return!1},document.oncopy=function(){return!1},document.oncut=function(){return!1}</script><script src=https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js></script><script type=text/javascript>$(document).ready(function(){$("#back-to-top").hide(),$(function(){$(window).scroll(function(){$(window).scrollTop()>600?$("#back-to-top").fadeIn(500):$("#back-to-top").fadeOut(500)}),$("#back-to-top").click(function(){return $("body,html").animate({scrollTop:0},500),!1})})})</script><meta property="og:title" content="Contrastive Language Encoding ï½œ Ellie Kitanidis ï½œ OpenAI Scholars Demo Day 2021 ï½œ OpenAI"><meta property="og:description" content="Video Transcript ï»¿awesome hi guys my name is ellie
and for my project i&amp;rsquo;ve been working on
pre-training a language representation
model
using contrastive learning
so i&amp;rsquo;ll start by giving some background
on what contrastive learning
is and sort of putting this work into
context then i&amp;rsquo;ll spend some time going
over
our framework and procedure and finally
i&amp;rsquo;ll share some preliminary results
and discuss future steps
so contrastive learning is a"><meta property="og:url" content="https://swiest.com/en/mvze7wm1skw/"><meta property="og:site_name" content="SWIEST - Transcripts Â· Screenplays Â· Lyrics"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="English"><meta property="article:tag" content="Video Transcripts"><meta property="article:tag" content="OpenAI"><meta property="article:published_time" content="2023-11-06T19:57:43+00:00"><meta property="article:modified_time" content="2023-11-06T19:57:43+00:00"><meta name=twitter:title content="Contrastive Language Encoding ï½œ Ellie Kitanidis ï½œ OpenAI Scholars Demo Day 2021 ï½œ OpenAI"><meta name=twitter:description content="Video Transcript ï»¿awesome hi guys my name is ellie
and for my project i&amp;rsquo;ve been working on
pre-training a language representation
model
using contrastive learning
so i&amp;rsquo;ll start by giving some background
on what contrastive learning
is and sort of putting this work into
context then i&amp;rsquo;ll spend some time going
over
our framework and procedure and finally
i&amp;rsquo;ll share some preliminary results
and discuss future steps
so contrastive learning is a"><link rel="shortcut icon" href=/favicon.ico><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"dark")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu307e6a33fa6fd661ccda3b77024ef5c2_252345_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>SWIEST - Transcripts Â· Screenplays Â· Lyrics</a></h1><h2 class=site-description>ğŸ§™ğŸª„ğŸŒ</h2></div></header><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><li><a href=/tags/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg><span>Tags</span></a></li><li><a href=/chart/podcastchart.html target=_blank><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-apple-podcast" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M18.364 18.364a9 9 0 10-12.728.0"/><path d="M11.766 22h.468a2 2 0 001.985-1.752l.5-4A2 2 0 0012.734 14h-1.468a2 2 0 00-1.985 2.248l.5 4A2 2 0 0011.766 22z"/><path d="M12 9m-2 0a2 2 0 104 0 2 2 0 10-4 0"/></svg><span>Podcasts</span></a></li><li><a href=/radio.html target=_blank><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-radio" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14 3 4.629 6.749A1 1 0 004 7.677V19a1 1 0 001 1h14a1 1 0 001-1V8a1 1 0 00-1-1H4.5"/><path d="M4 12h16"/><path d="M7 12v-2"/><path d="M17 16v.01"/><path d="M13 16v.01"/></svg><span>Radio</span></a></li><div class=menu-bottom-section><li id=i18n-switch><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg><select name=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://swiest.com/ selected>English</option><option value=https://swiest.com/af/>Afrikaans</option><option value=https://swiest.com/am/>áŠ áˆ›áˆ­áŠ›</option><option value=https://swiest.com/ar/>Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</option><option value=https://swiest.com/az/>AzÉ™rbaycan</option><option value=https://swiest.com/be/>Ğ±ĞµĞ»Ğ°Ñ€ÑƒÑĞºÑ–</option><option value=https://swiest.com/bg/>Ğ±ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸</option><option value=https://swiest.com/bn/>à¦¬à¦¾à¦‚à¦²à¦¾</option><option value=https://swiest.com/bo/>à½–à½¼à½‘à¼‹à½¦à¾à½‘à¼‹</option><option value=https://swiest.com/bs/>Bosanski</option><option value=https://swiest.com/ca/>CatalÃ </option><option value=https://swiest.com/zh-hans/>ç®€ä½“ä¸­æ–‡</option><option value=https://swiest.com/zh-hant/>ç¹é«”ä¸­æ–‡</option><option value=https://swiest.com/cs/>ÄŒeÅ¡tina</option><option value=https://swiest.com/el/>ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬</option><option value=https://swiest.com/cy/>Cymraeg</option><option value=https://swiest.com/da/>Dansk</option><option value=https://swiest.com/de/>Deutsch</option><option value=https://swiest.com/eo/>Esperanto</option><option value=https://swiest.com/es-es/>EspaÃ±ol (EspaÃ±a)</option><option value=https://swiest.com/es-419/>EspaÃ±ol (LatinoamÃ©rica)</option><option value=https://swiest.com/et/>Eesti</option><option value=https://swiest.com/eu/>Euskara</option><option value=https://swiest.com/haw/>Ê»ÅŒlelo HawaiÊ»i</option><option value=https://swiest.com/fa/>ÙØ§Ø±Ø³ÛŒ</option><option value=https://swiest.com/fi/>Suomi</option><option value=https://swiest.com/fo/>FÃ¸royskt</option><option value=https://swiest.com/fr/>FranÃ§ais</option><option value=https://swiest.com/fy/>Frysk</option><option value=https://swiest.com/ga/>Gaeilge</option><option value=https://swiest.com/gl/>Galego</option><option value=https://swiest.com/gu/>àª—à«àªœàª°àª¾àª¤à«€</option><option value=https://swiest.com/he/>×¢Ö´×‘×¨Ö´×™×ª</option><option value=https://swiest.com/km/>á€á˜áŸ’á–á»á‡á¶áŸ”</option><option value=https://swiest.com/hi/>à¤¹à¤¿à¤¨à¥à¤¦à¥€</option><option value=https://swiest.com/hr/>Hrvatski</option><option value=https://swiest.com/ht/>KreyÃ²l Ayisyen</option><option value=https://swiest.com/hu/>Magyar</option><option value=https://swiest.com/hy/>Õ€Õ¡ÕµÕ¥Ö€Õ¥Õ¶</option><option value=https://swiest.com/ig/>Ãsá»¥Ì€sá»¥Ì ÃŒgbÃ²</option><option value=https://swiest.com/id/>Bahasa Indonesia</option><option value=https://swiest.com/is/>Ãslenska</option><option value=https://swiest.com/it/>Italiano</option><option value=https://swiest.com/ja/>æ—¥æœ¬èª</option><option value=https://swiest.com/jv/>Basa Jawa</option><option value=https://swiest.com/ka/>áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜</option><option value=https://swiest.com/kk/>ÒšĞ°Ğ·Ğ°Ò›ÑˆĞ°</option><option value=https://swiest.com/kn/>à²•à²¨à³à²¨à²¡</option><option value=https://swiest.com/ko/>í•œêµ­ì–´</option><option value=https://swiest.com/or/>à¬“à¬¡à¬¼à¬¿à¬†</option><option value=https://swiest.com/ckb/>Ú©ÙˆØ±Ø¯ÛŒ</option><option value=https://swiest.com/ky/>ĞšÑ‹Ñ€Ğ³Ñ‹Ğ·Ñ‡Ğ°</option><option value=https://swiest.com/la/>Latina</option><option value=https://swiest.com/lb/>LÃ«tzebuergesch</option><option value=https://swiest.com/lo/>àºàº²àºªàº²àº¥àº²àº§</option><option value=https://swiest.com/lt/>LietuviÅ³</option><option value=https://swiest.com/lv/>LatvieÅ¡u</option><option value=https://swiest.com/mk/>ĞœĞ°ĞºĞµĞ´Ğ¾Ğ½ÑĞºĞ¸</option><option value=https://swiest.com/ml/>à´®à´²à´¯à´¾à´³à´‚</option><option value=https://swiest.com/mn/>ĞœĞ¾Ğ½Ğ³Ğ¾Ğ» Ñ…ÑĞ»</option><option value=https://swiest.com/mr/>à¤®à¤°à¤¾à¤ à¥€</option><option value=https://swiest.com/sw/>Kiswahili</option><option value=https://swiest.com/ms/>Bahasa Melayu</option><option value=https://swiest.com/my/>á€™á€¼á€”á€ºá€™á€¬</option><option value=https://swiest.com/ne/>à¤¨à¥‡à¤ªà¤¾à¤²à¥€</option><option value=https://swiest.com/nl/>Nederlands</option><option value=https://swiest.com/no/>Norsk</option><option value=https://swiest.com/pa/>à¨ªà©°à¨œà¨¾à¨¬à©€</option><option value=https://swiest.com/pl/>Polski</option><option value=https://swiest.com/pt-br/>PortuguÃªs Brasil</option><option value=https://swiest.com/pt-pt/>PortuguÃªs Europeu</option><option value=https://swiest.com/ro/>RomÃ¢nÄƒ</option><option value=https://swiest.com/ru/>Ğ ÑƒÑÑĞºĞ¸Ğ¹</option><option value=https://swiest.com/rw/>Kinyarwanda</option><option value=https://swiest.com/si/>à·ƒà·’à¶‚à·„à¶½</option><option value=https://swiest.com/sk/>SlovenÄina</option><option value=https://swiest.com/sl/>SlovenÅ¡Äina</option><option value=https://swiest.com/sq/>Shqip</option><option value=https://swiest.com/sr/>Ğ¡Ñ€Ğ¿ÑĞºĞ¸ (Srpski)</option><option value=https://swiest.com/su/>Basa Sunda</option><option value=https://swiest.com/sv/>Svenska</option><option value=https://swiest.com/ta/>à®¤à®®à®¿à®´à¯</option><option value=https://swiest.com/te/>à°¤à±†à°²à±à°—à±</option><option value=https://swiest.com/tg/>Ğ¢Ğ¾Ò·Ğ¸ĞºÓ£</option><option value=https://swiest.com/th/>à¹„à¸—à¸¢</option><option value=https://swiest.com/tk/>TÃ¼rkmenler</option><option value=https://swiest.com/tl/>Filipino</option><option value=https://swiest.com/tr/>TÃ¼rkÃ§e</option><option value=https://swiest.com/uk/>Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°</option><option value=https://swiest.com/ur/>Ø§Ø±Ø¯Ùˆ</option><option value=https://swiest.com/uz/>O'zbekcha</option><option value=https://swiest.com/vi/>Tiáº¿ng Viá»‡t</option><option value=https://swiest.com/yi/>××™×“×™×©</option><option value=https://swiest.com/zh-hk/>ç²µèª</option><option value=https://swiest.com/zu/>IsiZulu</option></select></li><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#video>Video</a></li><li><a href=#transcript>Transcript</a></li></ol></nav></div></section><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><ins class=adsbygoogle style=display:block data-ad-client=ca-pub-9206135835124064 data-ad-slot=8754979142 data-ad-format=auto data-full-width-responsive=true></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></aside><a id=back-to-top href=#><img src=/img/top_hu7c2829da96df0e9f8f0191d120020b22_22287_40x0_resize_box_3.png></a><main class="main full-width"><form action=/search/ class="search-form widget"><p><label>Search</label>
<input name=keyword required placeholder="Type something...">
<button title=Search><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg></button></p></form><article class=main-article><header class=article-header><div class=article-details><header class=article-tags><a href=/tags/english/>English
</a><a href=/tags/video-transcripts/>Video Transcripts
</a><a href=/tags/openai/>OpenAI</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/en/mvze7wm1skw/>Contrastive Language Encoding ï½œ Ellie Kitanidis ï½œ OpenAI Scholars Demo Day 2021 ï½œ OpenAI</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>2023-11-06</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>12 minute read</time></div></footer></div></header><div class=article-content><p style=text-align:center><a href=https://amzn.to/3Nrdcwk target=_blank>ğŸAmazon Prime</a>
<a href=https://amzn.to/3RIBkxg target=_blank>ğŸ“–Kindle Unlimited</a>
<a href=https://amzn.to/3Rqmudl target=_blank>ğŸ§Audible Plus</a>
<a href=https://amzn.to/3TuLbbj target=_blank>ğŸµAmazon Music Unlimited</a>
<a href="https://www.iherb.com/?rcode=EID1574" target=_blank>ğŸŒ¿iHerb</a>
<a href="https://accounts.binance.com/register?ref=72302422" target=_blank>ğŸ’°Binance</a></p></div><div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><ins class=adsbygoogle style=display:block data-ad-client=ca-pub-9206135835124064 data-ad-slot=8754979142 data-ad-format=auto data-full-width-responsive=true></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><section class=article-content><h2 id=video>Video</h2><div class=video-wrapper><iframe loading=lazy src=https://www.youtube.com/embed/mVZE7wm1skw allowfullscreen title="YouTube Video"></iframe></div><h2 id=transcript>Transcript</h2><p>ï»¿awesome hi guys my name is ellie</p><p>and for my project i&rsquo;ve been working on</p><p>pre-training a language representation</p><p>model</p><p>using contrastive learning</p><p>so i&rsquo;ll start by giving some background</p><p>on what contrastive learning</p><p>is and sort of putting this work into</p><p>context then i&rsquo;ll spend some time going</p><p>over</p><p>our framework and procedure and finally</p><p>i&rsquo;ll share some preliminary results</p><p>and discuss future steps</p><p>so contrastive learning is a</p><p>self-supervised method for</p><p>learning representations like a lot of</p><p>ideas in deep learning the idea of</p><p>contrastive loss</p><p>is not really new but it&rsquo;s just sort of</p><p>become a really</p><p>popular and active area of research</p><p>recently</p><p>in computer vision because of its</p><p>success at learning visual</p><p>representations</p><p>so the concept of contrastive learning</p><p>is pretty intuitive</p><p>the idea is just that similar inputs</p><p>should be</p><p>mapped closer together in representation</p><p>space</p><p>and some contrastive frameworks will</p><p>also explicitly</p><p>structure their loss functions such that</p><p>dissimilar inputs are mapped farther</p><p>apart</p><p>so you can kind of see this in this very</p><p>high level</p><p>conceptual diagram that i&rsquo;ve included on</p><p>this slide</p><p>which should not be taken literally by</p><p>the way um but it sort of shows</p><p>the idea of contrastive learning um</p><p>so it shows that two representations of</p><p>the same</p><p>fundamental concept sort of like two</p><p>images of the same person</p><p>should be mapped close together while</p><p>two distinct concepts like two different</p><p>people</p><p>should be mapped farther apart so</p><p>in practice what usually happens is that</p><p>an</p><p>input image x is augmented to create</p><p>two different versions x1 and x2 this</p><p>augmentation is usually some combination</p><p>of</p><p>random cropping like reorientations</p><p>and other types of noise injection and</p><p>then the two versions x1 and x2 form a</p><p>positive pair</p><p>where the loss function should seek to</p><p>maximize the similarity between x1 and</p><p>x2</p><p>so while contrastive learning has become</p><p>a booming area of research and computer</p><p>vision</p><p>the applications to natural language</p><p>have thus far been pretty limited</p><p>um so i&rsquo;ve listed here under nlp</p><p>basically all of the relevant papers i</p><p>came across most of which have pretty</p><p>recently hit the archive just within the</p><p>last few months</p><p>um because i&rsquo;m limited to 10 minutes i&rsquo;m</p><p>not gonna go through each of these</p><p>individually and discuss</p><p>them but i&rsquo;m putting them up on the</p><p>slide so that you can look them up</p><p>and read about them if you&rsquo;re interested</p><p>um what i will say is that</p><p>in basically all of these prior works</p><p>either they&rsquo;re not</p><p>pre-training from scratch they&rsquo;re you</p><p>know fine-tuning an existing pre-trained</p><p>model</p><p>or they&rsquo;re using contrastive learning as</p><p>sort of an auxiliary</p><p>objective along with a more traditional</p><p>language modeling objective</p><p>so we became interested in whether or</p><p>not you could pre-train a language model</p><p>from scratch</p><p>using just contrastive learning by</p><p>itself</p><p>sort of just in the interest of looking</p><p>for</p><p>new language modeling approaches that</p><p>might have some</p><p>new benefits either in performance</p><p>across some subset of</p><p>tasks or scaling or both</p><p>i&rsquo;m sort of specifically motivated by</p><p>the search for better</p><p>sentence level representations in</p><p>language modeling</p><p>so for our contrastive setup we</p><p>re-implemented a recent paper</p><p>in computer vision um and</p><p>you know matched the performance of that</p><p>paper</p><p>in this so-called sim cyan framework no</p><p>negative pairs are used</p><p>just positive pairs x1 and x2</p><p>so the augmented data pairs are fed into</p><p>an encoder</p><p>which produces the representations those</p><p>are then fed into a projector and</p><p>predictor</p><p>that are trained simultaneously with the</p><p>encoder</p><p>and the loss function in this case is</p><p>pretty simple it&rsquo;s just trying to</p><p>maximize the cosine similarity between</p><p>the positive pairs</p><p>um so the innovation of this particular</p><p>framework is</p><p>the addition of the predictor and the</p><p>stop gradient</p><p>such that the cosine similarity is</p><p>actually between</p><p>p1 the output of the predictor on x1</p><p>and z2 the output of the projector on x2</p><p>with the back propagation only</p><p>calculated through the branch with the</p><p>predictor</p><p>so this was shown in simsyam to</p><p>help avoid mode collapse where basically</p><p>the model just learns to map everything</p><p>to the same place in representation</p><p>space</p><p>since we don&rsquo;t have negative pairs to</p><p>sort of balance out that attractive</p><p>force</p><p>so our model oh i guess i forgot to</p><p>mention um</p><p>so for our encoder we basically just use</p><p>a</p><p>um transformer encoder um</p><p>where uh we&rsquo;ve added a few tweaks from</p><p>openai&rsquo;s sparse transformer</p><p>implementation</p><p>namely uh they present this modified</p><p>residual block and also some</p><p>special initialization schemes and</p><p>activation functions</p><p>okay so our model is pre-trained on the</p><p>pile um</p><p>this is a recent large language data set</p><p>that captures a diverse range of</p><p>modalities</p><p>everything from books to code from</p><p>github repositories</p><p>web pages medical papers etc</p><p>um now a big question</p><p>possibly the biggest question in</p><p>applying contrastive learning to</p><p>language</p><p>is what should our augmentation method</p><p>be</p><p>um so most if not all of the papers i</p><p>cited earlier</p><p>use some form of noise injection uh such</p><p>as randomly deleting or replacing words</p><p>rearranging the order of words and so on</p><p>however</p><p>we felt that um maybe more so than in</p><p>the case of images</p><p>injecting noise into language does not</p><p>necessarily create invariant</p><p>representations</p><p>um so if i make random changes to the</p><p>structure and content of my sentence</p><p>i&rsquo;m not really saying the same sentence</p><p>generally i&rsquo;m not really saying anything</p><p>coherent at all so</p><p>we thought a more principally motivated</p><p>approach would be to choose</p><p>pairs of text that are near each other</p><p>in a document</p><p>and thus likely to have related content</p><p>and style</p><p>so once again due to the 10 minute limit</p><p>i won&rsquo;t go into the technical details of</p><p>our data processing pipeline</p><p>but i do have a few notes on the slide</p><p>for reference if anyone wants to</p><p>dive in deeper later</p><p>okay so now we come to the preliminary</p><p>results</p><p>um so i&rsquo;ll start by saying that when i</p><p>did my phd</p><p>the physics department had an unusual</p><p>way of doing things</p><p>instead of doing our dissertation</p><p>defense at the very end of our phd</p><p>we did it about halfway through and the</p><p>reasoning behind this was that they</p><p>wanted us to get a lot of expert</p><p>feedback early</p><p>on the reason i&rsquo;m telling you this is</p><p>because a part of me really wishes i</p><p>could give you an update on this project</p><p>in a few weeks</p><p>when we have more conclusive results to</p><p>share but on the other hand i think it&rsquo;s</p><p>actually very valuable that i get to</p><p>show it to you now and</p><p>potentially solicit your feedback and</p><p>ideas</p><p>so the one thing i can state with a lot</p><p>of confidence at this point</p><p>is that training these models is really</p><p>hard um</p><p>so far we&rsquo;re finding these sort of spiky</p><p>plateaus in the loss function after the</p><p>first few thousand iterations</p><p>pretty much no matter what we do um so</p><p>i&rsquo;ve plotted a few</p><p>examples here but they all pretty much</p><p>look like this um</p><p>so far hyper parameter sweeps are not</p><p>really giving us any insight into this</p><p>i will say i have not exhaustively</p><p>searched the parameter space by any</p><p>means</p><p>i&rsquo;ve only been working on training this</p><p>for a couple of weeks</p><p>so it could be there&rsquo;s something to</p><p>uh some insight to find there but so far</p><p>we haven&rsquo;t seen anything</p><p>um increasing dropout and or weight</p><p>decay also</p><p>hasn&rsquo;t seemed to have helped um we</p><p>experimented with</p><p>adding a tank before or after the</p><p>average pool</p><p>um that goes at the end of our encoder</p><p>that also did not seem to particularly</p><p>help</p><p>so we wanted to test that this</p><p>training difficulty is related to the</p><p>contrast of loss</p><p>um so we tried training the model with</p><p>an mlm like objective instead</p><p>for anyone not in the language modeling</p><p>world that stands for</p><p>masked language modeling basically we</p><p>replace</p><p>10 of the tokens with random tokens and</p><p>then</p><p>take our same encoder and feed it into a</p><p>linear and sigmoid layer instead so now</p><p>the task is</p><p>to predict the replacement mask um</p><p>this is slightly different than the bert</p><p>objective of trying to predict the</p><p>masked tokens</p><p>since we don&rsquo;t have a decoder or</p><p>anything generative</p><p>we&rsquo;re just trying to predict the mask</p><p>itself</p><p>and when we train this model we find</p><p>that we get much more nicely behaved</p><p>loss functions so that does seem to</p><p>indicate that it is the contrastive loss</p><p>that&rsquo;s causing the problems</p><p>as another sort of probe we decided to</p><p>see how well the model can match</p><p>queries to relevant keys so we randomly</p><p>selected a thousand text pairs from the</p><p>data</p><p>with the same selection parameters as</p><p>were used in pre-training</p><p>and we let these be query key pairs we</p><p>then created a cosine similarity matrix</p><p>of</p><p>encoded queries dotted with encoded keys</p><p>and calculated the percent of queries</p><p>for which the corresponding</p><p>key was in the top k most similar</p><p>so for example out of a thousand query</p><p>key pairs if the model&rsquo;s totally random</p><p>i would expect</p><p>that the matching key uh to be in the</p><p>query</p><p>the query&rsquo;s top five most similar uh 0.5</p><p>percent of the time</p><p>instead we consistently see about 20 um</p><p>and likewise for the top 100 we see 60</p><p>what&rsquo;s kind of interesting about this is</p><p>that doing the same</p><p>test on the mlm version of the model</p><p>we actually get smaller values so 5 and</p><p>25 respectively perhaps</p><p>the contrastive version of the model is</p><p>better at this type of task</p><p>of course this could very well wash out</p><p>at larger scales and with more training</p><p>these were only trained for you know the</p><p>first</p><p>2000 iterations or so um because of the</p><p>noisy plateau issues so this is far from</p><p>a conclusive statement</p><p>but more of an interesting preliminary</p><p>finding</p><p>finally we do have evaluation set up for</p><p>two downstream</p><p>tasks from the glue benchmark set of</p><p>course again at this stage of training</p><p>the results and the configurations used</p><p>to get them</p><p>are far from final um so we include them</p><p>more just to give a sense of</p><p>uh where the performance gap is</p><p>currently</p><p>um so the two tasks we evaluate on are</p><p>rte</p><p>and mnli these are both textual</p><p>entailment classification tasks</p><p>in other words for a given premise and</p><p>hypothesis</p><p>does the premise imply that the</p><p>hypothesis is true</p><p>um i&rsquo;m going a little bit over on time</p><p>so i will skip discussing</p><p>the details of implementation and i&rsquo;ll</p><p>just talk really quickly about this</p><p>table</p><p>so this is the maximum validation</p><p>accuracy for these two tasks they&rsquo;re</p><p>both classification tasks</p><p>classes mli is three classes um</p><p>and the third row is basically the</p><p>results using</p><p>um a pre-trained burp model that i just</p><p>took off hugging face</p><p>so the values aren&rsquo;t exactly the values</p><p>from the burp paper but they&rsquo;re the</p><p>values</p><p>that i got in my fine-tuning pipeline</p><p>very close to the values from the paper</p><p>and we see that for</p><p>our model in both versions there&rsquo;s about</p><p>a five percent gap</p><p>in rte um so the the two models</p><p>uh perform pretty similarly um but</p><p>definitely</p><p>not uh as well as the vert pre-trained</p><p>um for mnli i actually found that uh</p><p>this is a much larger data set so it was</p><p>a lot slower and i found that</p><p>our model was very slow to converge</p><p>so i don&rsquo;t actually have a final value</p><p>nor do i think it&rsquo;s particularly helpful</p><p>to try to get one at this stage</p><p>but the training does start uh with a</p><p>validation accuracy of about</p><p>60 percent so it is doing something</p><p>um but the the slowness to converge</p><p>does uh indicate that it&rsquo;s doing a lot</p><p>of training from scratch which is</p><p>unfortunate</p><p>um so the last thing i&rsquo;ll say about this</p><p>is that um</p><p>obviously this is not an</p><p>apples-to-apples comparison um</p><p>not only because we cut the pre-training</p><p>so prematurely but also</p><p>the model is smaller than bert base</p><p>but it is still i think helpful to see</p><p>that five percent gap</p><p>um and whether or not that can be closed</p><p>um</p><p>by improving those parameters</p><p>okay so very very quickly next steps</p><p>obviously our immediate next steps are</p><p>to try to improve the training</p><p>just do a lot more experiments with</p><p>learning rate parameters and</p><p>augmentation parameters</p><p>and maybe expand our scope a little bit</p><p>in terms of including negative pairs</p><p>and different types of loss functions</p><p>potentially also exploring different</p><p>augmentation methods that might work</p><p>better</p><p>more broadly if we were to get this type</p><p>of model to</p><p>work obviously we would be interested in</p><p>probing its scaling behavior um and also</p><p>its performance on a</p><p>wider variety of tasks yeah so i&rsquo;ve gone</p><p>a little bit over</p><p>but thank you guys so much for listening</p><p>and of course thank you to</p><p>my mentor and the program coordinators</p><p>and my fellow scholars</p><p>this has been an amazing experience and</p><p>i&rsquo;ll</p><p>take a couple of questions and then pass</p><p>it</p><p>all off</p><p>okay so the first question is these</p><p>results make me curious about the input</p><p>pairs you selected</p><p>could you talk a little bit more about</p><p>it yes i can really briefly</p><p>show you a slide that i had skipped</p><p>um so uh basically we started out</p><p>thinking about how to do this</p><p>at the sentence level so we started with</p><p>just</p><p>sentence pairs basically and we would</p><p>allow them to be separated by some</p><p>distance that was a hyper parameter</p><p>and we actually made it really</p><p>open-ended by having a minimum</p><p>and maximum separation such that it</p><p>could be</p><p>a random separation between those two</p><p>intervals and eventually we ended up</p><p>moving towards larger pieces of text</p><p>mostly because the evals were all more</p><p>than one sentence so it just made sense</p><p>to</p><p>take on that level of abstraction um so</p><p>we now</p><p>choose chunks of text of some number of</p><p>sentences</p><p>which is also a hyper parameter and</p><p>we&rsquo;ve explored</p><p>um those three hyper parameters as well</p><p>um</p><p>we then tokenize using a uh subword</p><p>tokenization</p><p>and truncate our pad to the maximum</p><p>sequence length</p><p>um okay i&rsquo;ll take one more question</p><p>do you plan to try out contrastive</p><p>losses with negative pairs</p><p>such as nce to compare with simpson</p><p>byol lost performance yeah so as i was</p><p>saying</p><p>um so far these preliminary results do</p><p>seem to</p><p>indicate that um the simscyan framework</p><p>might not work so well uh for a messier</p><p>more complex</p><p>data set like language and so we would</p><p>definitely be interested</p><p>in trying more complex</p><p>loss functions that do include the</p><p>negative terms so something like nce</p><p>um definitely would be uh something we</p><p>want to look at</p><p>okay i&rsquo;ve gone a little bit over but</p><p>thank you guys so much for the questions</p><p>and um</p><p>if i didn&rsquo;t get to your question please</p><p>just look me up on linkedin</p><p>or on slack if you&rsquo;re at openai and i</p><p>would love to keep chatting about it</p><p>offline</p><p>okay so i will now pass it on to cujo</p></section><footer class=article-footer><section class=article-tags><a href=/tags/english/>English</a>
<a href=/tags/video-transcripts/>Video Transcripts</a>
<a href=/tags/openai/>OpenAI</a></section></footer></article><div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><ins class=adsbygoogle style=display:block;text-align:center data-ad-layout=in-article data-ad-format=fluid data-ad-client=ca-pub-9206135835124064 data-ad-slot=1055602464></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/en/at2xkqjazns/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/AT2XkqJAZns data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>Towards Epileptic Seizure Prediction with Deep Network ï½œ Kata Slama ï½œ OpenAI Scholars Demo Day 2020 ï½œ OpenAI</h2></div></a></article><article><a href=/en/jzohw-eybtq/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/JZOHW-eYBtQ data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>Introductions by Sam Altman & Greg Brockman ï½œ OpenAI Scholars Demo Day 2020 ï½œ OpenAI</h2></div></a></article><article><a href=/en/-fozam9xqs4/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/-FoZAM9xqS4 data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>OpenAI Five vs. OG, Game 2 ï½œ OpenAI Five Finals (4â§¸6) ï½œ OpenAI</h2></div></a></article><article><a href=/en/u9mjuukhuzk/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/U9mJuUkhUzk data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>OpenAI DevDay, Opening Keynote ï½œ OpenAI</h2></div></a></article><article><a href=/en/lpe5gwuqa-k/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/lpe5Gwuqa-k data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>Scaling Laws for Language Transfer Learning ï½œ Christina Kim ï½œ OpenAI Scholars Demo Day 2021 ï½œ OpenAI</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2021 -
2023 SWIEST - Transcripts Â· Screenplays Â· Lyrics</section><section class=powerby>As an Amazon Associate I earn from qualifying purchases ğŸ›’<br>Built with <a href=https://swiest.com/ target=_blank rel=noopener>(ï¾‰â—•ãƒ®â—•)ï¾‰ğŸª„ğŸ’ğŸ’–ğŸ¥° across the glğŸŒğŸŒğŸŒbe</a><br></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Serif&family=Noto+Serif+Armenian&family=Noto+Serif+Bengali&family=Noto+Serif+Devanagari&family=Noto+Serif+Georgian&family=Noto+Serif+Gujarati&family=Noto+Serif+HK&family=Noto+Serif+Hebrew&family=Noto+Serif+JP&family=Noto+Serif+KR&family=Noto+Serif+Kannada&family=Noto+Serif+Khmer&family=Noto+Serif+Lao&family=Noto+Serif+Makasar&family=Noto+Serif+Malayalam&family=Noto+Serif+Myanmar&family=Noto+Serif+Oriya&family=Noto+Serif+SC&family=Noto+Serif+Sinhala&family=Noto+Serif+TC&family=Noto+Serif+Tamil&family=Noto+Serif+Telugu&family=Noto+Serif+Thai&family=Noto+Serif+Tibetan&display=swap" rel=stylesheet></body></html>
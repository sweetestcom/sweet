<!DOCTYPE html>
<html lang="en" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='OpenAI CEO Sam Altman, whose company created ChatGPT, was one of three artificial intelligence
experts to testify on oversight of the swiftly developing technology at a Senate Judiciary
Subcommittee hearing.
Mr. Altman stated that AI could, quote, cause significant harm to the world.
Here&amp;rsquo;s the rest of that hearing.
Welcome to the hearing of the Privacy, Technology, and the Law Subcommittee.
Thank my partner in this effort, Senator Hawley, Ranking Member, and I particularly'>
<title>C-SPAN - ChatGPT CEO Sam Altman Testifies Before Senate on AI Oversight | SWIEST</title>

<link rel='canonical' href='https://swiest.com/en/3001300002/'>

<link rel="stylesheet" href="/scss/style.min.91b18679590f4ceed910ade4d64b1e7375cc0770ef5b8c1d822f42424f9ff2c8.css"><script>
    document.oncontextmenu = function(){ return false; };
    document.onselectstart = function(){ return false; };
    document.oncopy = function(){ return false; };
    document.oncut = function(){ return false; };
</script>

<script src="https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>


<script type="text/javascript">
    $(document).ready(function(){
     
     $("#back-to-top").hide();
     
     $(function () {
      $(window).scroll(function(){
       if ($(window).scrollTop()>600){
        $("#back-to-top").fadeIn(500);
       }else{
        $("#back-to-top").fadeOut(500);
       }
     });
     
     $("#back-to-top").click(function(){
      $('body,html').animate({scrollTop:0},500);
       return false;
      });
     });
    });
    </script><meta property='og:title' content='C-SPAN - ChatGPT CEO Sam Altman Testifies Before Senate on AI Oversight'>
<meta property='og:description' content='OpenAI CEO Sam Altman, whose company created ChatGPT, was one of three artificial intelligence
experts to testify on oversight of the swiftly developing technology at a Senate Judiciary
Subcommittee hearing.
Mr. Altman stated that AI could, quote, cause significant harm to the world.
Here&amp;rsquo;s the rest of that hearing.
Welcome to the hearing of the Privacy, Technology, and the Law Subcommittee.
Thank my partner in this effort, Senator Hawley, Ranking Member, and I particularly'>
<meta property='og:url' content='https://swiest.com/en/3001300002/'>
<meta property='og:site_name' content='SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='English' /><meta property='article:tag' content='Video Transcripts' /><meta property='article:tag' content='C-SPAN' /><meta property='article:published_time' content='2023-05-16T09:00:00&#43;00:00'/><meta property='article:modified_time' content='2023-05-16T09:00:00&#43;00:00'/>
<meta name="twitter:title" content="C-SPAN - ChatGPT CEO Sam Altman Testifies Before Senate on AI Oversight">
<meta name="twitter:description" content="OpenAI CEO Sam Altman, whose company created ChatGPT, was one of three artificial intelligence
experts to testify on oversight of the swiftly developing technology at a Senate Judiciary
Subcommittee hearing.
Mr. Altman stated that AI could, quote, cause significant harm to the world.
Here&amp;rsquo;s the rest of that hearing.
Welcome to the hearing of the Privacy, Technology, and the Law Subcommittee.
Thank my partner in this effort, Senator Hawley, Ranking Member, and I particularly">
    <link rel="shortcut icon" href="/favicon.ico" />
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin="anonymous"></script>
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "dark");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu307e6a33fa6fd661ccda3b77024ef5c2_252345_300x0_resize_box_3.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">‚ú®</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics</a></h1>
            <h2 class="site-description">üåçüåèüåé</h2>
        </div>
    </header><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        
        
        <li >
            <a href='/chart/podcastchart.html' target="_blank">
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-apple-podcast" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#ffffff" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M18.364 18.364a9 9 0 1 0 -12.728 0" />
  <path d="M11.766 22h.468a2 2 0 0 0 1.985 -1.752l.5 -4a2 2 0 0 0 -1.985 -2.248h-1.468a2 2 0 0 0 -1.985 2.248l.5 4a2 2 0 0 0 1.985 1.752z" />
  <path d="M12 9m-2 0a2 2 0 1 0 4 0a2 2 0 1 0 -4 0" />
</svg>
                
                <span>Podcast Charts</span>
            </a>
        </li>
        

        <div class="menu-bottom-section">
                <li id="i18n-switch">  
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M4 5h7" />
  <path d="M9 3v2c0 4.418 -2.239 8 -5 8" />
  <path d="M5 9c-.003 2.144 2.952 3.908 6.7 4" />
  <path d="M12 20l4 -9l4 9" />
  <path d="M19.1 18h-6.2" />
</svg>



                    <select name="language" onchange="window.location.href = this.selectedOptions[0].value">
                        
                            <option value="https://swiest.com/" selected>English</option>
                        
                            <option value="https://swiest.com/af/" >Afrikaans</option>
                        
                            <option value="https://swiest.com/am/" >·ä†·àõ·à≠·äõ</option>
                        
                            <option value="https://swiest.com/ar/" >ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</option>
                        
                            <option value="https://swiest.com/az/" >Az…ôrbaycan</option>
                        
                            <option value="https://swiest.com/be/" >–±–µ–ª–∞—Ä—É—Å–∫—ñ</option>
                        
                            <option value="https://swiest.com/bg/" >–±—ä–ª–≥–∞—Ä—Å–∫–∏</option>
                        
                            <option value="https://swiest.com/bn/" >‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ</option>
                        
                            <option value="https://swiest.com/bo/" >‡Ωñ‡Ωº‡Ωë‡ºã‡Ω¶‡æê‡Ωë‡ºã</option>
                        
                            <option value="https://swiest.com/bs/" >Bosanski</option>
                        
                            <option value="https://swiest.com/ca/" >Catal√†</option>
                        
                            <option value="https://swiest.com/zh-hans/" >ÁÆÄ‰Ωì‰∏≠Êñá</option>
                        
                            <option value="https://swiest.com/zh-hant/" >ÁπÅÈ´î‰∏≠Êñá</option>
                        
                            <option value="https://swiest.com/cs/" >ƒåe≈°tina</option>
                        
                            <option value="https://swiest.com/el/" >ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨</option>
                        
                            <option value="https://swiest.com/cy/" >Cymraeg</option>
                        
                            <option value="https://swiest.com/da/" >Dansk</option>
                        
                            <option value="https://swiest.com/de/" >Deutsch</option>
                        
                            <option value="https://swiest.com/eo/" >Esperanto</option>
                        
                            <option value="https://swiest.com/es-es/" >Espa√±ol (Espa√±a)</option>
                        
                            <option value="https://swiest.com/es-419/" >Espa√±ol (Latinoam√©rica)</option>
                        
                            <option value="https://swiest.com/et/" >Eesti</option>
                        
                            <option value="https://swiest.com/eu/" >Euskara</option>
                        
                            <option value="https://swiest.com/haw/" > ª≈ålelo Hawai ªi</option>
                        
                            <option value="https://swiest.com/fa/" >ŸÅÿßÿ±ÿ≥€å</option>
                        
                            <option value="https://swiest.com/fi/" >Suomi</option>
                        
                            <option value="https://swiest.com/fo/" >F√∏royskt</option>
                        
                            <option value="https://swiest.com/fr/" >Fran√ßais</option>
                        
                            <option value="https://swiest.com/fy/" >Frysk</option>
                        
                            <option value="https://swiest.com/ga/" >Gaeilge</option>
                        
                            <option value="https://swiest.com/gl/" >Galego</option>
                        
                            <option value="https://swiest.com/gu/" >‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä</option>
                        
                            <option value="https://swiest.com/he/" >◊¢÷¥◊ë◊®÷¥◊ô◊™</option>
                        
                            <option value="https://swiest.com/km/" >·ûÄ·ûò·üí·ûñ·ûª·ûá·û∂·üî</option>
                        
                            <option value="https://swiest.com/hi/" >‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</option>
                        
                            <option value="https://swiest.com/hr/" >Hrvatski</option>
                        
                            <option value="https://swiest.com/ht/" >Krey√≤l Ayisyen</option>
                        
                            <option value="https://swiest.com/hu/" >Magyar</option>
                        
                            <option value="https://swiest.com/hy/" >’Ä’°’µ’•÷Ä’•’∂</option>
                        
                            <option value="https://swiest.com/ig/" >√Ås·ª•ÃÄs·ª•ÃÅ √ågb√≤</option>
                        
                            <option value="https://swiest.com/id/" >Bahasa Indonesia</option>
                        
                            <option value="https://swiest.com/is/" >√çslenska</option>
                        
                            <option value="https://swiest.com/it/" >Italiano</option>
                        
                            <option value="https://swiest.com/ja/" >Êó•Êú¨Ë™û</option>
                        
                            <option value="https://swiest.com/jv/" >Basa Jawa</option>
                        
                            <option value="https://swiest.com/ka/" >·É•·Éê·É†·Éó·É£·Éö·Éò</option>
                        
                            <option value="https://swiest.com/kk/" >“ö–∞–∑–∞“õ—à–∞</option>
                        
                            <option value="https://swiest.com/kn/" >‡≤ï‡≤®‡≥ç‡≤®‡≤°</option>
                        
                            <option value="https://swiest.com/ko/" >ÌïúÍµ≠Ïñ¥</option>
                        
                            <option value="https://swiest.com/or/" >‡¨ì‡¨°‡¨º‡¨ø‡¨Ü</option>
                        
                            <option value="https://swiest.com/ckb/" >⁄©Ÿàÿ±ÿØ€å</option>
                        
                            <option value="https://swiest.com/ky/" >–ö—ã—Ä–≥—ã–∑—á–∞</option>
                        
                            <option value="https://swiest.com/la/" >Latina</option>
                        
                            <option value="https://swiest.com/lb/" >L√´tzebuergesch</option>
                        
                            <option value="https://swiest.com/lo/" >‡∫û‡∫≤‡∫™‡∫≤‡∫•‡∫≤‡∫ß</option>
                        
                            <option value="https://swiest.com/lt/" >Lietuvi≈≥</option>
                        
                            <option value="https://swiest.com/lv/" >Latvie≈°u</option>
                        
                            <option value="https://swiest.com/mk/" >–ú–∞–∫–µ–¥–æ–Ω—Å–∫–∏</option>
                        
                            <option value="https://swiest.com/ml/" >‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç</option>
                        
                            <option value="https://swiest.com/mn/" >–ú–æ–Ω–≥–æ–ª —Ö—ç–ª</option>
                        
                            <option value="https://swiest.com/mr/" >‡§Æ‡§∞‡§æ‡§†‡•Ä</option>
                        
                            <option value="https://swiest.com/sw/" >Kiswahili</option>
                        
                            <option value="https://swiest.com/ms/" >Bahasa Melayu</option>
                        
                            <option value="https://swiest.com/my/" >·Äô·Äº·Äî·Ä∫·Äô·Ä¨</option>
                        
                            <option value="https://swiest.com/ne/" >‡§®‡•á‡§™‡§æ‡§≤‡•Ä</option>
                        
                            <option value="https://swiest.com/nl/" >Nederlands</option>
                        
                            <option value="https://swiest.com/no/" >Norsk</option>
                        
                            <option value="https://swiest.com/pa/" >‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä</option>
                        
                            <option value="https://swiest.com/pl/" >Polski</option>
                        
                            <option value="https://swiest.com/pt-br/" >Portugu√™s Brasil</option>
                        
                            <option value="https://swiest.com/pt-pt/" >Portugu√™s Europeu</option>
                        
                            <option value="https://swiest.com/ro/" >Rom√¢nƒÉ</option>
                        
                            <option value="https://swiest.com/ru/" >–†—É—Å—Å–∫–∏–π</option>
                        
                            <option value="https://swiest.com/rw/" >Kinyarwanda</option>
                        
                            <option value="https://swiest.com/si/" >‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω</option>
                        
                            <option value="https://swiest.com/sk/" >Slovenƒçina</option>
                        
                            <option value="https://swiest.com/sl/" >Sloven≈°ƒçina</option>
                        
                            <option value="https://swiest.com/sq/" >Shqip</option>
                        
                            <option value="https://swiest.com/sr/" >–°—Ä–ø—Å–∫–∏ (Srpski)</option>
                        
                            <option value="https://swiest.com/su/" >Basa Sunda</option>
                        
                            <option value="https://swiest.com/sv/" >Svenska</option>
                        
                            <option value="https://swiest.com/ta/" >‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç</option>
                        
                            <option value="https://swiest.com/te/" >‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</option>
                        
                            <option value="https://swiest.com/tg/" >–¢–æ“∑–∏–∫”£</option>
                        
                            <option value="https://swiest.com/th/" >‡πÑ‡∏ó‡∏¢</option>
                        
                            <option value="https://swiest.com/tl/" >Filipino</option>
                        
                            <option value="https://swiest.com/tr/" >T√ºrk√ße</option>
                        
                            <option value="https://swiest.com/uk/" >–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</option>
                        
                            <option value="https://swiest.com/ur/" >ÿßÿ±ÿØŸà</option>
                        
                            <option value="https://swiest.com/uz/" >O&#39;zbekcha</option>
                        
                            <option value="https://swiest.com/vi/" >Ti·∫øng Vi·ªát</option>
                        
                            <option value="https://swiest.com/yi/" >◊ê◊ô◊ì◊ô◊©</option>
                        
                            <option value="https://swiest.com/zh-hk/" >Á≤µË™û</option>
                        
                            <option value="https://swiest.com/zu/" >IsiZulu</option>
                        
                    </select>
                </li>
            
            
            
                <li id="dark-mode-toggle">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <span>Dark Mode</span>
                </li>
            
        </div>
    </ol>
</aside>

    

            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/video-transcripts/" >
                Video Transcripts
            </a>
        
            <a href="/categories/c-span/" >
                C-SPAN
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/en/3001300002/">C-SPAN - ChatGPT CEO Sam Altman Testifies Before Senate on AI Oversight</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2023-05-16</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    136 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>
<div>
    <ul>üéÅ<a href="https://amzn.to/471i0jl" target="_blank">üõíAmazon Prime</a>
       <a href="https://amzn.to/3QDVlVf" target="_blank">üìñKindle Unlimited</a>
       <a href="https://amzn.to/3FqzNoB" target="_blank">üéßAudible Plus</a>
       <a href="https://amzn.to/3tMT3dm" target="_blank">üéµAmazon Music Unlimited</a>
       <a href="https://www.iherb.com/?rcode=EID1574" target="_blank">üåøiHerb</a>
</ul>
</div>
<div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064"
     crossorigin="anonymous"></script>
    
    <ins class="adsbygoogle"
         style="display:block"
         data-ad-client="ca-pub-9206135835124064"
         data-ad-slot="8754979142"
         data-ad-format="auto"
         data-full-width-responsive="true"></ins>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>


    <section class="article-content">
    
    
    <p>OpenAI CEO Sam Altman, whose company created ChatGPT, was one of three artificial intelligence</p>
<p>experts to testify on oversight of the swiftly developing technology at a Senate Judiciary</p>
<p>Subcommittee hearing.</p>
<p>Mr. Altman stated that AI could, quote, cause significant harm to the world.</p>
<p>Here&rsquo;s the rest of that hearing.</p>
<p>Welcome to the hearing of the Privacy, Technology, and the Law Subcommittee.</p>
<p>Thank my partner in this effort, Senator Hawley, Ranking Member, and I particularly</p>
<p>want to thank Senator Durbin, Chairman of the Judiciary Committee, and he will be speaking</p>
<p>shortly.</p>
<p>This hearing is on the oversight of artificial intelligence, the first in a series of hearings</p>
<p>intended to write the rules of AI.</p>
<p>Our goal is to demystify and hold accountable those new technologies to avoid some of the</p>
<p>mistakes of the past.</p>
<p>And now, for some introductory remarks.</p>
<p>Too often, we have seen what happens when technology outpaces regulation.</p>
<p>The unbridled exploitation of personal data, the proliferation of disinformation, and the</p>
<p>deepening of societal inequalities.</p>
<p>We have seen how algorithmic biases can perpetuate discrimination and prejudice, and how the</p>
<p>lack of transparency can undermine public trust.</p>
<p>This is not the future we want.</p>
<p>If you were listening from home, you might have thought that voice was mine and the words</p>
<p>from me.</p>
<p>But in fact, that voice was not mine.</p>
<p>The words were not mine.</p>
<p>And the audio was an AI voice cloning software trained on my floor speeches.</p>
<p>The remarks were written by ChatGPT when it was asked how I would open this hearing.</p>
<p>And you heard just now the result.</p>
<p>I asked ChatGPT, why did you pick those themes and that content?</p>
<p>And it answered, and I&rsquo;m quoting, Blumenthal has a strong record in advocating for consumer</p>
<p>protection and civil rights.</p>
<p>He has been vocal about issues such as data privacy and the potential for discrimination</p>
<p>in algorithmic decision making.</p>
<p>Therefore, the statement emphasizes these aspects.</p>
<p>Mr. Altman, I appreciate ChatGPT&rsquo;s endorsement.</p>
<p>In all seriousness, this apparent reasoning is pretty impressive.</p>
<p>I am sure that we&rsquo;ll look back in a decade and view ChatGPT and GPT-4 like we do the</p>
<p>first cell phone, those big clunky things that we used to carry around.</p>
<p>But we recognize that we are on the verge, really, of a new era.</p>
<p>The audio and my playing, it may strike you as curious or humorous.</p>
<p>But what reverberated in my mind was, what if I had asked it, and what if it had provided</p>
<p>an endorsement of Ukraine&rsquo;s surrendering or Vladimir Putin&rsquo;s leadership?</p>
<p>That would have been really frightening.</p>
<p>And the prospect is more than a little scary, to use the word, Mr. Altman, you have used</p>
<p>yourself.</p>
<p>And I think you have been very constructive in calling attention to the pitfalls, as well</p>
<p>as the promise.</p>
<p>And that&rsquo;s the reason why we wanted you to be here today.</p>
<p>And we thank you and our other witnesses for joining us.</p>
<p>For several months now, the public has been fascinated with GPT, DALI, and other AI tools.</p>
<p>These examples, like the homework done by CHAT GPT, or the articles and op-eds that</p>
<p>it can write, feel like novelties.</p>
<p>But the underlying advancement of this era are more than just research experiments.</p>
<p>They are no longer fantasies of science fiction.</p>
<p>They are real and present.</p>
<p>The promises of curing cancer or developing new understandings of physics and biology</p>
<p>or modeling climate and weather, all very encouraging and hopeful.</p>
<p>But we also know the potential harms.</p>
<p>And we&rsquo;ve seen them already, weaponized disinformation, housing discrimination, harassment of women</p>
<p>and impersonation fraud, voice cloning, deep fakes.</p>
<p>These are the potential risks, despite the other rewards.</p>
<p>And for me, perhaps the biggest nightmare is the looming new industrial revolution,</p>
<p>the displacement of millions of workers, the loss of huge numbers of jobs, the need to</p>
<p>prepare for this new industrial revolution in skill training and relocation that may</p>
<p>be required.</p>
<p>And already, industry leaders are calling attention to those challenges.</p>
<p>To quote CHAT GPT, this is not necessarily the future that we want.</p>
<p>We need to maximize the good over the bad.</p>
<p>Congress has a choice now.</p>
<p>We had the same choice when we faced social media.</p>
<p>We failed to seize that moment.</p>
<p>The result is predators on the Internet, toxic content, exploiting children, creating</p>
<p>dangers for them.</p>
<p>And Senator Blackburn and I and others like Senator Durbin on the Judiciary Committee</p>
<p>are trying to deal with it, Kids Online Safety Act.</p>
<p>But Congress failed to meet the moment on social media.</p>
<p>Now we have the obligation to do it on AI before the threats and the risks become real.</p>
<p>Sensible safeguards are not in opposition to innovation.</p>
<p>Accountability is not a burden, far from it.</p>
<p>They are the foundation of how we can move ahead while protecting public trust.</p>
<p>They are how we can lead the world in technology and science, but also in promoting our democratic</p>
<p>values.</p>
<p>Otherwise, in the absence of that trust, I think we may well lose both.</p>
<p>These are sophisticated technology, but there are basic expectations common in our law.</p>
<p>We can start with transparency.</p>
<p>AI companies ought to be required to test their systems, disclose known risks, and allow</p>
<p>independent researcher access.</p>
<p>We can establish scorecards and nutrition labels to encourage competition based on safety</p>
<p>and trustworthiness.</p>
<p>Limitations on use.</p>
<p>There are places where the risk of AI is so extreme that we ought to impose restriction</p>
<p>or even ban their use, especially when it comes to commercial invasions of privacy for</p>
<p>profit and decisions that affect people&rsquo;s livelihoods.</p>
<p>And of course, accountability or liability.</p>
<p>When AI companies and their clients cause harm, they should be held liable.</p>
<p>We should not repeat our past mistakes.</p>
<p>For example, Section 230, forcing companies to think ahead and be responsible for the</p>
<p>ramifications of their business decisions can be the most powerful tool of all.</p>
<p>Garbage in, garbage out.</p>
<p>The principle still applies.</p>
<p>We ought to beware of the garbage, whether it&rsquo;s going into these platforms or coming</p>
<p>out of them.</p>
<p>And the ideas that we develop in this hearing, I think, will provide a solid path forward.</p>
<p>I look forward to discussing them with you today, and I will just finish on this note.</p>
<p>The AI industry doesn&rsquo;t have to wait for Congress.</p>
<p>I hope there are ideas and feedback from this discussion and from the industry and</p>
<p>voluntary action, such as we&rsquo;ve seen lacking in many social media platforms, and the consequences</p>
<p>have been huge.</p>
<p>So I&rsquo;m hoping that we will elevate rather than have a race to the bottom.</p>
<p>And I think these hearings will be an important part of this conversation.</p>
<p>This one is only the first.</p>
<p>The Ranking Member and I have agreed there should be more, and we&rsquo;re going to invite</p>
<p>other industry leaders.</p>
<p>Some have committed to come, experts, academics, and the public, we hope, will participate.</p>
<p>And with that, I will turn to the Ranking Member, Senator Hawley.</p>
<p>Thank you very much, Mr. Chairman.</p>
<p>Thanks to the witnesses for being here.</p>
<p>I appreciate that several of you had long journeys to make in order to be here.</p>
<p>I appreciate you making the time.</p>
<p>I look forward to your testimony.</p>
<p>I want to thank Senator Blumenthal for convening this hearing, for being a leader on this topic.</p>
<p>You know, a year ago, we couldn&rsquo;t have had this hearing because the technology that we&rsquo;re</p>
<p>talking about had not burst into public consciousness.</p>
<p>That gives us a sense, I think, of just how rapidly this technology that we&rsquo;re talking</p>
<p>about today is changing and evolving and transforming our world right before our very eyes.</p>
<p>I was talking with someone just last night, a researcher in the field of psychiatry, who</p>
<p>was pointing out to me that the chat GPT and generative AI, these large language models,</p>
<p>it&rsquo;s really like the invention of the internet in scale, at least, at least, and potentially</p>
<p>far, far more significant than that.</p>
<p>We could be looking at one of the most significant technological innovations in human history.</p>
<p>And I think my question is, what kind of an innovation is it going to be?</p>
<p>Is it going to be like the printing press that diffused knowledge and power and learning</p>
<p>widely across the landscape that empowered ordinary, everyday individuals that led to</p>
<p>greater flourishing, that led above all to greater liberty?</p>
<p>Or is it going to be more like the atom bomb?</p>
<p>Huge technological breakthrough, but the consequences, severe, terrible, continue to</p>
<p>haunt us to this day.</p>
<p>I don&rsquo;t know the answer to that question.</p>
<p>I don&rsquo;t think any of us in the room know the answer to that question because I think the</p>
<p>answer has not yet been written.</p>
<p>And to a certain extent, it&rsquo;s up to us here and to us as the American people to write</p>
<p>the answer.</p>
<p>What kind of technology will this be?</p>
<p>How will we use it to better our lives?</p>
<p>How will we use it to actually harness the power of technological innovation for the</p>
<p>good of the American people, for the liberty of the American people, not for the power</p>
<p>of the few?</p>
<p>You know, I was reminded of the psychologist and writer Carl Jung who said at the beginning</p>
<p>of the last century that our ability for technological innovation, our capacity for technological</p>
<p>revolution had far outpaced our ethical and moral ability to apply and harness the</p>
<p>technology we developed.</p>
<p>That was a century ago.</p>
<p>I think the story of the 20th century largely bore him out.</p>
<p>And I just wonder what will we say as we look back at this moment about these new technologies,</p>
<p>about generative AI, about these language models, and about the hosts of other AI capacities</p>
<p>that are even right now underdeveloped, but not just in this country, but in China, the</p>
<p>countries of our adversaries, and all around the world.</p>
<p>And I think that the question that Jung posed is really the question that faces us.</p>
<p>Will we strike that balance between technological innovation and our ethical and moral responsibility</p>
<p>to humanity, to liberty, to the freedom of this country?</p>
<p>And I hope that today&rsquo;s hearing will take us a step closer to that answer.</p>
<p>Thank you, Mr. Chairman.</p>
<p>Thanks.</p>
<p>Thanks, Senator Hawley.</p>
<p>I&rsquo;m going to turn to the chairman of the Judiciary Committee and the ranking member, Senator</p>
<p>Graham, if they have opening remarks as well.</p>
<p>Yes, Mr. Chairman.</p>
<p>Thank you very much, and Senator Hawley as well.</p>
<p>Last week in this committee, full committee, Senate Judiciary Committee, we dealt with</p>
<p>an issue that had been waiting for attention for almost two decades, and that is what to</p>
<p>do with the social media when it comes to the abuse of children.</p>
<p>We had four bills initially that were considered by this committee.</p>
<p>And what may be history in the making, we passed all four bills with unanimous roll</p>
<p>calls.</p>
<p>Unanimous roll calls.</p>
<p>I can&rsquo;t remember another time when we&rsquo;ve done that on an issue that important.</p>
<p>It&rsquo;s an indication, I think, of the important position of this committee in the national</p>
<p>debate on issues that affect every single family and affect our future in a profound</p>
<p>way.</p>
<p>1989 was a historic watershed year in America, because that&rsquo;s when Seinfeld arrived.</p>
<p>And we had a sitcom, which was supposedly about little or nothing, which turned out</p>
<p>to be enduring.</p>
<p>I like to watch it, obviously.</p>
<p>And I&rsquo;m always marveled when they show the phones that he used in 1989.</p>
<p>And I think about those in comparison to what we carry around in our pockets today.</p>
<p>It&rsquo;s a dramatic change.</p>
<p>And I guess the question as I look at that is, does this change in phone technology that</p>
<p>we&rsquo;ve witnessed through the sitcom really exemplify a profound change in America?</p>
<p>Still unanswered.</p>
<p>But the very basic question we face is whether or not this issue of AI is a quantitative</p>
<p>change in technology or a qualitative change.</p>
<p>The suggestions that I&rsquo;ve heard from experts in the field suggest it&rsquo;s qualitative.</p>
<p>Is it AI fundamentally different?</p>
<p>Is it a game changer?</p>
<p>Is it so disruptive that we need to treat it differently than other forms of innovation?</p>
<p>That&rsquo;s the starting point.</p>
<p>And the second starting point is one that&rsquo;s humbling, and that is the fact when you look</p>
<p>at the record of Congress in dealing with innovation, technology, and rapid change,</p>
<p>we&rsquo;re not designed for that.</p>
<p>In fact, the Senate was not created for that purpose, but just the opposite.</p>
<p>Slow things down.</p>
<p>Take a harder look at it.</p>
<p>Don&rsquo;t react to public sentiment.</p>
<p>Make sure you&rsquo;re doing the right thing.</p>
<p>Well, I&rsquo;ve heard of the potential, the positive potential of AI, and it is enormous.</p>
<p>You can go through lists of the deployment of technology that would say that an idea</p>
<p>you can sketch for a website on a napkin can generate functioning code.</p>
<p>Medical companies could use the technology to identify new candidates to treat disease.</p>
<p>The list goes on and on.</p>
<p>And then, of course, the danger, and it&rsquo;s profound as well.</p>
<p>So I&rsquo;m glad that this hearing is taking place, and I think it&rsquo;s important for all of us to</p>
<p>participate.</p>
<p>I&rsquo;m glad that it&rsquo;s a bipartisan approach.</p>
<p>We&rsquo;re going to have to scramble to keep up with the pace of innovation in terms of our</p>
<p>government public response to it, but this is a great start.</p>
<p>Thank you, Mr. Chairman.</p>
<p>Thanks.</p>
<p>Thank you, Mr. Chairman.</p>
<p>It is very much a bipartisan approach, very deeply and broadly bipartisan, and in that</p>
<p>spirit, I&rsquo;m going to turn to my friend, Senator Graham.</p>
<p>Thank you.</p>
<p>That was not written by AI for sure.</p>
<p>Let me introduce now the witnesses.</p>
<p>We&rsquo;re very grateful to you for being here.</p>
<p>Graham Altman is the co-founder and CEO of OpenAI, the AI research and deployment company</p>
<p>behind CHAT-GPT and DALY.</p>
<p>Mr. Altman was president of the early stage startup accelerator Y Combinator from 2014</p>
<p>to 2019.</p>
<p>OpenAI was founded in 2015.</p>
<p>Christina Montgomery is IBM&rsquo;s vice president and chief privacy and trust officer overseeing</p>
<p>the company&rsquo;s global privacy program policies, compliance, and strategy.</p>
<p>She also chairs IBM&rsquo;s AI ethics board, a multidisciplinary team responsible for the governance of AI and</p>
<p>emerging technologies.</p>
<p>Christina has served in various roles at IBM, including corporate secretary to the company&rsquo;s</p>
<p>board of directors.</p>
<p>She is a global leader in AI ethics and governments.</p>
<p>And Ms. Montgomery also is a member of the United States Chamber of Commerce AI Commission</p>
<p>and the United States National AI Advisory Committee, which was established in 2022 to</p>
<p>advise the president and the National AI Initiative Office on a range of topics related to AI.</p>
<p>Gary Marcus is a leading voice in artificial intelligence.</p>
<p>He&rsquo;s a scientist, bestselling author, and entrepreneur, founder of the robust AI and</p>
<p>geometric AI acquired by Uber, if I&rsquo;m not mistaken, and emeritus professor of psychology</p>
<p>and neuroscience at NYU.</p>
<p>Mr. Marcus is well known for his challenges to contemporary AI, anticipating many of the</p>
<p>current limitations decades in advance, and for his research in human language development</p>
<p>and cognitive neuroscience.</p>
<p>Thank you for being here.</p>
<p>And as you may know, our custom on the Judiciary Committee is to swear in our witnesses before</p>
<p>they testify.</p>
<p>So if you would all please rise and raise your right hand.</p>
<p>And you solemnly swear that the testimony that you are going to give is the truth, the</p>
<p>whole truth, and nothing but the truth, so help us God.</p>
<p>Thank you.</p>
<p>Mr. Altman, we&rsquo;re going to begin with you, if that&rsquo;s okay.</p>
<p>Thank you.</p>
<p>Thank you, Chairman Blumenthal, Ranking Member Hawley, members of the Judiciary Committee.</p>
<p>Thank you for the opportunity to speak to you today about large neural networks.</p>
<p>It&rsquo;s really an honor to be here, even more so in the moment than I expected.</p>
<p>My name is Sam Altman.</p>
<p>I&rsquo;m the Chief Executive Officer of OpenAI.</p>
<p>OpenAI was founded on the belief that artificial intelligence has the potential to improve</p>
<p>nearly every aspect of our lives, but also that it creates serious risks we have to work</p>
<p>together to manage.</p>
<p>We&rsquo;re here because people love this technology.</p>
<p>We think it can be a printing press moment.</p>
<p>We have to work together to make it so.</p>
<p>OpenAI is an unusual company, and we set it up that way because AI is an unusual technology.</p>
<p>We are governed by a nonprofit, and our activities are driven by our mission and our charter,</p>
<p>which commit us to working to ensure that the broad distribution of the benefits of</p>
<p>AI and to maximizing the safety of AI systems.</p>
<p>We are working to build tools that one day could help us make new discoveries and address</p>
<p>some of humanity&rsquo;s biggest challenges, like climate change and curing cancer.</p>
<p>Our current systems aren&rsquo;t yet capable of doing these things, but it has been immensely</p>
<p>gratifying to watch many people around the world get so much value from what these systems</p>
<p>can already do today.</p>
<p>We love seeing people use our tools to create, to learn, to be more productive.</p>
<p>We&rsquo;re very optimistic that there are going to be fantastic jobs in the future and that</p>
<p>current jobs can get much better.</p>
<p>We also love seeing what developers are doing to improve lives.</p>
<p>For example, Be My Eyes used our new multimodal technology in GPT-4 to help visually impaired</p>
<p>individuals navigate their environment.</p>
<p>We believe that the benefits of the tools we have deployed so far vastly outweigh the</p>
<p>risks, but ensuring their safety is vital to our work, and we make significant efforts</p>
<p>to ensure that safety is built into our systems at all levels.</p>
<p>Before releasing any new system, OpenAI conducts extensive testing, engages external experts</p>
<p>for detailed reviews and independent audits, improves the model&rsquo;s behavior, and implements</p>
<p>robust safety and monitoring systems.</p>
<p>Before we released GPT-4, our latest model, we spent over six months conducting extensive</p>
<p>evaluations, external red teaming, and dangerous capability testing.</p>
<p>We are proud of the progress that we made.</p>
<p>GPT-4 is more likely to respond helpfully and truthfully and refuse harmful requests</p>
<p>than any other widely deployed model of similar capability.</p>
<p>However, we think that regulatory intervention by governments will be critical to mitigate</p>
<p>the risks of increasingly powerful models.</p>
<p>For example, the U.S. government might consider a combination of licensing and testing requirements</p>
<p>for development and release of AI models above a threshold of capabilities.</p>
<p>There are several other areas I mentioned in my written testimony where I believe that</p>
<p>companies like ours can partner with governments, including ensuring that the most powerful</p>
<p>AI models adhere to a set of safety requirements, facilitating processes to develop and update</p>
<p>safety measures, and examining opportunities for global coordination.</p>
<p>And as you mentioned, I think it&rsquo;s important that companies have their own responsibility</p>
<p>here no matter what Congress does.</p>
<p>This is a remarkable time to be working on artificial intelligence.</p>
<p>But as this technology advances, we understand that people are anxious about how it could</p>
<p>change the way we live.</p>
<p>We are too.</p>
<p>But we believe that we can and must work together to identify and manage the potential downsides</p>
<p>so that we can all enjoy the tremendous upsides.</p>
<p>It is essential that powerful AI is developed with democratic values in mind, and this means</p>
<p>that U.S. leadership is critical.</p>
<p>I believe that we will be able to mitigate the risks in front of us and really capitalize</p>
<p>on this technology&rsquo;s potential to grow the U.S. economy and the world&rsquo;s, and I look forward</p>
<p>to working with you all to meet this moment, and I look forward to answering your questions.</p>
<p>Thank you.</p>
<p>Thank you, Mr. Altman.</p>
<p>Ms. Montgomery.</p>
<p>Chairman Blumenthal, Ranking Member Hawley, and members of the subcommittee, thank you</p>
<p>for today&rsquo;s opportunity to present.</p>
<p>AI is not new, but it&rsquo;s certainly having a moment.</p>
<p>Recent breakthroughs in generative AI and the technology&rsquo;s dramatic surge in the public</p>
<p>attention has rightfully raised serious questions at the heart of today&rsquo;s hearing.</p>
<p>What are AI&rsquo;s potential impacts on society?</p>
<p>What do we do about bias?</p>
<p>What about misinformation, misuse, or harmful content generated by AI systems?</p>
<p>Senators, these are the right questions, and I applaud you for convening today&rsquo;s hearing</p>
<p>to address them head on.</p>
<p>While AI may be having its moment, the moment for government to play a role has not passed</p>
<p>us by.</p>
<p>This period of focused public attention on AI is precisely the time to define and build</p>
<p>the right guardrails to protect people and their interests.</p>
<p>But at its core, AI is just a tool, and tools can serve different purposes.</p>
<p>To that end, IBM urges Congress to adopt a precision regulation approach to AI.</p>
<p>This means establishing rules to govern the deployment of AI in specific use cases, not</p>
<p>regulating the technology itself.</p>
<p>Such an approach would involve four things.</p>
<p>First, different rules for different risks.</p>
<p>The strongest regulation should be applied to use cases with the greatest risks to people</p>
<p>and society.</p>
<p>Second, clearly defining risks.</p>
<p>There must be clear guidance on AI uses or categories of AI-supported activity that are</p>
<p>inherently high risk.</p>
<p>This common definition is key to enabling a clear understanding of what regulatory requirements</p>
<p>will apply in different use cases and contexts.</p>
<p>Third, be transparent, so AI shouldn&rsquo;t be hidden.</p>
<p>Consumers should know when they&rsquo;re interacting with an AI system and that they have recourse</p>
<p>to engage with a real person should they so desire.</p>
<p>No person anywhere should be tricked into interacting with an AI system.</p>
<p>And finally, showing the impact.</p>
<p>For higher risk use cases, companies should be required to conduct impact assessments</p>
<p>that show how their systems perform against tests for bias and other ways that they could</p>
<p>potentially impact the public and to attest that they&rsquo;ve done so.</p>
<p>By following risk-based, use case-specific approach at the core of precision regulation,</p>
<p>Congress can mitigate the potential risks of AI without hindering innovation.</p>
<p>But businesses also play a critical role in ensuring the responsible deployment of AI.</p>
<p>Companies active in developing or using AI must have strong internal governance, including,</p>
<p>among other things, designating a lead AI ethics official responsible for an organization&rsquo;s</p>
<p>trustworthy AI strategy, standing up an ethics board or a similar function as a centralized</p>
<p>clearinghouse for resources to help guide implementation of that strategy.</p>
<p>IBM has taken both of these steps, and we continue calling on our industry peers to</p>
<p>follow suit.</p>
<p>Our AI ethics board plays a critical role in overseeing internal AI governance processes,</p>
<p>creating reasonable guardrails to ensure we introduce technology into the world in a responsible</p>
<p>and safe manner.</p>
<p>It provides centralized governance and accountability while still being flexible enough to support</p>
<p>decentralized initiatives across IBM&rsquo;s global operations.</p>
<p>We do this because we recognize that society grants our license to operate.</p>
<p>And with AI, the stakes are simply too high.</p>
<p>We must build, not undermine, the public trust.</p>
<p>The era of AI cannot be another era of move fast and break things.</p>
<p>But we don&rsquo;t have to slam the brakes on innovation either.</p>
<p>These systems are within our control today, as are the solutions.</p>
<p>What we need at this pivotal moment is clear, reasonable policy and sound guardrails.</p>
<p>These guardrails should be matched with meaningful steps by the business community to do their</p>
<p>part.</p>
<p>Congress and the business community must work together to get this right.</p>
<p>The American people deserve no less.</p>
<p>Thank you for your time, and I look forward to your questions.</p>
<p>Thank you.</p>
<p>Professor Marcus.</p>
<p>Thank you, Senators.</p>
<p>Today&rsquo;s meeting is historic.</p>
<p>Thank you, Senators.</p>
<p>Today&rsquo;s meeting is historic.</p>
<p>I&rsquo;m profoundly grateful to be here.</p>
<p>I come as a scientist, someone who&rsquo;s founded AI companies, and as someone who genuinely</p>
<p>loves AI, but who is increasingly worried.</p>
<p>There are benefits, but we don&rsquo;t yet know whether they will outweigh the risks.</p>
<p>Fundamentally, these new systems are going to be destabilizing.</p>
<p>They can and will create persuasive lies at a scale humanity has never seen before.</p>
<p>Outsiders will use them to affect our elections, insiders to manipulate our markets and our</p>
<p>political systems.</p>
<p>Democracy itself is threatened.</p>
<p>Chatbots will also clandestinely shape our opinions, potentially exceeding what social</p>
<p>media can do.</p>
<p>Choices about data sets that AI companies use will have enormous unseen influence.</p>
<p>Those who choose the data will make the rules, shaping society in subtle but powerful ways.</p>
<p>There are other risks, too, many stemming from the inherent unreliability of current</p>
<p>systems.</p>
<p>A law professor, for example, was accused by a chatbot of sexual harassment, untrue,</p>
<p>and it pointed to a Washington Post article that didn&rsquo;t even exist.</p>
<p>The more that that happens, the more that anybody can deny anything.</p>
<p>As one prominent lawyer told me on Friday, defendants are starting to claim that plaintiffs</p>
<p>are making up legitimate evidence.</p>
<p>These sorts of allegations undermine the abilities of juries to decide what or who to believe</p>
<p>and contribute to the undermining of democracy.</p>
<p>Poor medical advice could have serious consequences, too.</p>
<p>An open source large language model recently seems to have played a role in a person&rsquo;s</p>
<p>decision to take their own life.</p>
<p>The large language model asked the human, if you wanted to die, why didn&rsquo;t you do it</p>
<p>earlier?</p>
<p>And then followed up with, were you thinking of me when you overdosed?</p>
<p>Without ever referring the patient to the human help that was obviously needed.</p>
<p>Another system rushed out and made available to millions of children told a person posing</p>
<p>as a 13-year-old how to lie to her parents about a trip with a 31-year-old man.</p>
<p>Further threats continue to emerge regularly.</p>
<p>A month after GPT-4 was released, OpenAI released ChatGPT plugins, which quickly led</p>
<p>others to develop something called AutoGPT with direct access to the internet, the ability</p>
<p>to write source code, and increased powers of automation.</p>
<p>This may well have drastic and difficult to predict security consequences.</p>
<p>What criminals are going to do here is to create counterfeit people.</p>
<p>It&rsquo;s hard to even envision the consequences of that.</p>
<p>We have built machines that are like bulls in a china shop, powerful, reckless, and difficult</p>
<p>to control.</p>
<p>We all more or less agree on the values we would like for our AI systems to honor.</p>
<p>We want, for example, for our systems to be transparent, to protect our privacy, to be</p>
<p>free of bias, and above all else, to be safe.</p>
<p>But current systems are not in line with these values.</p>
<p>Current systems are not transparent.</p>
<p>They do not adequately protect our privacy, and they continue to perpetuate bias.</p>
<p>And even their makers don&rsquo;t entirely understand how they work.</p>
<p>First of all, we cannot remotely guarantee that they&rsquo;re safe, and hope here is not enough.</p>
<p>The big tech company&rsquo;s preferred plan boils down to trust us.</p>
<p>But why should we?</p>
<p>The sums of money at stake are mind-boggling.</p>
<p>Emissions drift.</p>
<p>OpenAI&rsquo;s original mission statement proclaimed, our goal is to advance AI in the way that</p>
<p>is most likely to benefit humanity as a whole, unconstrained by a need to generate financial</p>
<p>return.</p>
<p>Seven years later, they&rsquo;re largely beholden to Microsoft, embroiled in part in an epic</p>
<p>battle of search engines that routinely make things up.</p>
<p>And that&rsquo;s forced Alphabet to rush out products and de-emphasize safety.</p>
<p>Humanity has taken a back seat.</p>
<p>AI is moving incredibly fast, with lots of potential, but also lots of risk.</p>
<p>We obviously need government involved, and we need the tech companies involved, both</p>
<p>big and small.</p>
<p>But we also need independent scientists, not just so that we scientists can have a voice,</p>
<p>but so that we can participate directly in addressing the problems and evaluating solutions.</p>
<p>Not just after products are released, but before, and I&rsquo;m glad that Sam mentioned that.</p>
<p>We need tight collaboration between independent scientists and governments in order to hold</p>
<p>the company&rsquo;s feet to the fire.</p>
<p>Allowing independent scientists access to these systems before they are widely released,</p>
<p>as part of a clinical trial-like safety evaluation, is a vital first step.</p>
<p>Ultimately, we may need something like CERN, global, international, and neutral, but focused</p>
<p>on AI safety rather than high-energy physics.</p>
<p>We have unprecedented opportunities here, but we are also facing a perfect storm of</p>
<p>corporate irresponsibility, widespread deployment, lack of adequate regulation, and inherent</p>
<p>unreliability.</p>
<p>AI is among the most world-changing technologies ever, already changing things more rapidly</p>
<p>than almost any technology in history.</p>
<p>We acted too slowly with social media.</p>
<p>Many unfortunate decisions got locked in with lasting consequence.</p>
<p>The choices we make now will have lasting effects for decades, maybe even centuries.</p>
<p>The very fact that we are here today in bipartisan fashion to discuss these matters gives me</p>
<p>some hope.</p>
<p>Thank you, Mr. Chairman.</p>
<p>Thanks very much, Professor Marcus.</p>
<p>We&rsquo;re going to have seven-minute rounds of questioning, and I will begin.</p>
<p>First of all, Professor Marcus, we are here today because we do face that perfect storm.</p>
<p>Some of us might characterize it more like a bomb in a China shop, not a bull.</p>
<p>And as Senator Hawley indicated, there are precedents here, not only the atomic warfare</p>
<p>era but also the genome project, the research on genetics, where there was international</p>
<p>cooperation as a result.</p>
<p>And we want to avoid those past mistakes, as I indicated in my opening statement, that</p>
<p>were committed on social media.</p>
<p>That is precisely the reason we are here today.</p>
<p>Chat GPT makes mistakes.</p>
<p>All AI does.</p>
<p>And it can be a convincing liar, what people call hallucinations.</p>
<p>That might be an innocent problem in the opening of a judiciary subcommittee hearing where</p>
<p>a voice is impersonated, mine in this instance, or quotes from research papers that don&rsquo;t</p>
<p>exist, but Chat GPT and BARD are willing to answer questions about life or death matters,</p>
<p>for example, drug interactions.</p>
<p>And those kinds of mistakes can be deeply damaging.</p>
<p>I&rsquo;m interested in how we can have reliable information about the accuracy and trustworthiness</p>
<p>of these models and how we can create competition and consumer disclosures that reward greater</p>
<p>accuracy.</p>
<p>The National Institutes of Standards and Technology actually already has an AI accuracy test,</p>
<p>the face recognition vendor test.</p>
<p>It doesn&rsquo;t solve for all the issues with facial recognition, but the scorecard does provide</p>
<p>useful information about the capabilities and flaws of these systems.</p>
<p>So there&rsquo;s work on models to assure accuracy and integrity.</p>
<p>My question, let me begin with you, Mr. Altman, is should we consider independent testing</p>
<p>labs to provide scorecards and nutrition labels or the equivalent of nutrition labels, packaging</p>
<p>that indicates to people whether or not the content can be trusted, what the ingredients</p>
<p>are and what the garbage going in may be, because it could result in garbage going out?</p>
<p>Yeah, I think that&rsquo;s a great idea.</p>
<p>I think that companies should put their own sort of, you know, here are the results of</p>
<p>our test of our model before we release it.</p>
<p>Here&rsquo;s where it has weaknesses, here&rsquo;s where it has strengths.</p>
<p>But also independent audits for that are very important.</p>
<p>These models are getting more accurate over time.</p>
<p>You know, this is, as we have, I think, said as loudly as anyone, this technology is in</p>
<p>its early stages.</p>
<p>It definitely still makes mistakes.</p>
<p>We find that people, that users are pretty sophisticated and understand where the mistakes</p>
<p>are that they need or likely to be, that they need to be responsible for verifying what</p>
<p>the models say, that they go off and check it.</p>
<p>I worry that as the models get better and better, the users can have sort of less and</p>
<p>less of their own discriminating thought process around it.</p>
<p>But I think users are more capable than we could often give them credit for in conversations</p>
<p>like this.</p>
<p>I think a lot of disclosures, which if you&rsquo;ve used ChatGBT, you&rsquo;ll see about the inaccuracies</p>
<p>of the model are also important.</p>
<p>And I&rsquo;m excited for a world where companies publish with the models information about</p>
<p>how they behave, where the inaccuracies are, and independent agencies or companies provide</p>
<p>that as well.</p>
<p>I think it&rsquo;s a great idea.</p>
<p>I alluded in my opening remarks to the jobs issue, the economic effects on employment.</p>
<p>I think you have said, in fact, and I&rsquo;m going to quote, development of superhuman machine</p>
<p>intelligence is probably the greatest threat to the continued existence of humanity, end</p>
<p>quote.</p>
<p>You may have had in mind the effect on jobs, which is really my biggest nightmare in the</p>
<p>long term.</p>
<p>Let me ask you what your biggest nightmare is and whether you share that concern.</p>
<p>Like with all technological revolutions, I expect there to be significant impact on jobs,</p>
<p>but exactly what that impact looks like is very difficult to predict.</p>
<p>If we went back to the other side of a previous technological revolution, talking about the</p>
<p>jobs that exist on the other side, you know, you can go back and read books of this.</p>
<p>What people said at the time, it&rsquo;s difficult.</p>
<p>I believe that there will be far greater jobs on the other side of this, and the jobs of</p>
<p>today will get better.</p>
<p>I think it&rsquo;s important.</p>
<p>First of all, I think it&rsquo;s important to understand and think about GPT-4 as a tool, not a creature,</p>
<p>which is easy to get confused, and it&rsquo;s a tool that people have a great deal of control</p>
<p>over in how they use it.</p>
<p>And second, GPT-4 and other systems like it are good at doing tasks, not jobs, and</p>
<p>so you see already people that are using GPT-4 to do their job much more efficiently by helping</p>
<p>them with tasks.</p>
<p>Now, GPT-4 will, I think, entirely automate away some jobs, and it will create new ones</p>
<p>that we believe will be much better.</p>
<p>This happens, again, my understanding of the history of technology is one long technological</p>
<p>revolution, not a bunch of different ones put together, but this has been continually</p>
<p>happening.</p>
<p>As our quality of life raises, and as machines and tools that we create can help us live</p>
<p>better lives, the bar raises for what we do, and our human ability and what we spend our</p>
<p>time going after goes after more ambitious, more satisfying projects.</p>
<p>So there will be an impact on jobs.</p>
<p>We try to be very clear about that, and I think it will require partnership between</p>
<p>the industry and government, but mostly action by government to figure out how we want to</p>
<p>mitigate that.</p>
<p>But I&rsquo;m very optimistic about how great the jobs of the future will be.</p>
<p>Thank you.</p>
<p>Let me ask Ms. Montgomery and Professor Marcus for your reaction to those questions as well.</p>
<p>Ms. Montgomery?</p>
<p>On the jobs point?</p>
<p>Yeah, I mean, well, it&rsquo;s a hugely important question, and it&rsquo;s one that we&rsquo;ve been talking</p>
<p>about for a really long time at IBM.</p>
<p>We do believe that AI, and we&rsquo;ve said it for a long time, is going to change every job.</p>
<p>New jobs will be created.</p>
<p>Many more jobs will be transformed, and some jobs will transition away.</p>
<p>I&rsquo;m a personal example of a job that didn&rsquo;t exist when I joined IBM, and I have a team</p>
<p>of AI governance professionals who are in new roles that we created as early as three</p>
<p>years ago.</p>
<p>I mean, they&rsquo;re new and they&rsquo;re growing.</p>
<p>But I think the most important thing that we could be doing and can and should be doing</p>
<p>now is to prepare the workforce of today and the workforce of tomorrow for partnering with</p>
<p>AI technologies and using them.</p>
<p>And we&rsquo;ve been very involved for years now in doing that, in focusing on skills-based</p>
<p>hiring, in educating for the skills of the future.</p>
<p>Our skills-build platform has 7 million learners and over 1,000 courses worldwide focused on</p>
<p>skills, and we&rsquo;ve pledged to train 30 million individuals by 2030 in the skills that are</p>
<p>needed for society today.</p>
<p>Thank you.</p>
<p>Professor Marcus?</p>
<p>May I go back to the first question as well?</p>
<p>Absolutely.</p>
<p>On the subject of nutrition labels, I think we absolutely need to do that.</p>
<p>I think that there are some technical challenges and that building proper nutrition labels</p>
<p>goes hand-in-hand with transparency.</p>
<p>The biggest scientific challenge in understanding these models is how they generalize.</p>
<p>What do they memorize and what new things do they do?</p>
<p>The more that there&rsquo;s in the data set, for example, the thing that you want to test accuracy</p>
<p>on, the less you can get a proper read on that.</p>
<p>So it&rsquo;s important, first of all, that scientists be part of that process, and second, that</p>
<p>we have much greater transparency about what actually goes into these systems.</p>
<p>If we don&rsquo;t know what&rsquo;s in them, then we don&rsquo;t know exactly how well they&rsquo;re doing when we</p>
<p>give something new, and we don&rsquo;t know how good a benchmark that will be for something</p>
<p>that&rsquo;s entirely novel.</p>
<p>So I could go into that more, but I want to flag that.</p>
<p>Second is on jobs, past performance history is not a guarantee of the future.</p>
<p>It has always been the case in the past that we have had more jobs, that new jobs, new</p>
<p>professions come in as new technologies come in.</p>
<p>I think this one&rsquo;s going to be different, and the real question is over what time scale?</p>
<p>Is it going to be 10 years?</p>
<p>Is it going to be 100 years?</p>
<p>And I don&rsquo;t think anybody knows the answer to that question.</p>
<p>I think in the long run, so-called artificial general intelligence really will replace a</p>
<p>large fraction of human jobs.</p>
<p>We&rsquo;re not that close to artificial general intelligence.</p>
<p>Despite all of the media hype and so forth, I would say that what we have right now is</p>
<p>just a small sampling of the AI that we will build, and 20 years people will laugh at this,</p>
<p>as I think it was Senator Hawley made the, but maybe Senator Durbin made the example</p>
<p>about this.</p>
<p>It was Senator Durbin made the example about cell phones.</p>
<p>When we look back at the AI of today, 20 years ago, we&rsquo;ll be like, wow, that stuff</p>
<p>was really unreliable.</p>
<p>It couldn&rsquo;t really do planning, which is an important technical aspect.</p>
<p>It&rsquo;s reasoning was ability, and reasoning abilities were limited.</p>
<p>But when we get to AGI, artificial general intelligence, maybe let&rsquo;s say it&rsquo;s 50 years,</p>
<p>that really is going to have, I think, profound effects on labor, and there&rsquo;s just no way</p>
<p>around that.</p>
<p>And last, I don&rsquo;t know if I&rsquo;m allowed to do this, but I will note that Sam&rsquo;s worst fear,</p>
<p>I do not think is employment, and he never told us what his worst fear actually is, and</p>
<p>I think it&rsquo;s germane to find out.</p>
<p>Thank you.</p>
<p>I&rsquo;m going to ask Mr. Altman if he cares to respond.</p>
<p>Yeah.</p>
<p>Look, we have tried to be very clear about the magnitude of the risks here.</p>
<p>I think jobs and employment and what we&rsquo;re all going to do with our time really matters.</p>
<p>I agree that when we get to very powerful systems, the landscape will change.</p>
<p>I think I&rsquo;m just more optimistic that we are incredibly creative and we find new things</p>
<p>to do with better tools, and that will keep happening.</p>
<p>My worst fears are that we cause significant, we, the field, the technology, the industry,</p>
<p>cause significant harm to the world.</p>
<p>I think that could happen in a lot of different ways.</p>
<p>It&rsquo;s why we started the company.</p>
<p>It&rsquo;s a big part of why I&rsquo;m here today, and why we&rsquo;ve been here in the past, and we&rsquo;ve</p>
<p>been able to spend some time with you.</p>
<p>I think if this technology goes wrong, it can go quite wrong, and we want to be vocal</p>
<p>about that.</p>
<p>We want to work with the government to prevent that from happening, but we try to be very</p>
<p>clear-eyed about what the downside case is and the work that we have to do to mitigate</p>
<p>that.</p>
<p>Thank you.</p>
<p>And our hope is that the rest of the industry will follow the example that you and IBM,</p>
<p>and Ms. Montgomery have set by coming today and meeting with us, as you have done privately,</p>
<p>in helping to guide what we&rsquo;re going to do so that we can target the harms and avoid</p>
<p>unintended consequences to the good.</p>
<p>Thank you.</p>
<p>Senator Hawley.</p>
<p>Thank you again, Mr. Chairman.</p>
<p>Thanks to the witnesses for being here.</p>
<p>Mr. Altman, I think you grew up in St. Louis, if I&rsquo;m not mistaken.</p>
<p>It&rsquo;s great to see a fellow Missourian here.</p>
<p>Missouri is a great place.</p>
<p>It is.</p>
<p>Thank you.</p>
<p>I think it&rsquo;s important, especially underlining the record, Missouri is a great place.</p>
<p>That is the takeaway from today&rsquo;s hearing.</p>
<p>Maybe we should stop there, Mr. Chairman.</p>
<p>Let me ask you, Mr. Altman, I think I&rsquo;ll start with you, and I&rsquo;ll just preface this by saying</p>
<p>my questions here are an attempt to get my head around and to ask all of you to help</p>
<p>us to get our heads around what this generative AI, particularly the large language models,</p>
<p>what it can do.</p>
<p>So I&rsquo;m trying to understand its capacities and then its significance.</p>
<p>So I&rsquo;m looking at a paper here entitled, Large Language Models Trained on Media Diets</p>
<p>Can Predict Public Opinion.</p>
<p>This was just posted about a month ago.</p>
<p>The authors are Chu, Andreas, Anselaberry, and Roy.</p>
<p>And their conclusion, this work was done at MIT and then also at Google, their conclusion</p>
<p>is that large language models can indeed predict public opinion.</p>
<p>And they go through and model why this is the case.</p>
<p>And they conclude ultimately that an AI system can predict human survey responses by adapting</p>
<p>a pre-trained language model to subpopulation-specific media diets.</p>
<p>So in other words, you can feed the model a particular set of media inputs and it can,</p>
<p>with remarkable accuracy, and the paper goes into this, predict then what people&rsquo;s opinions</p>
<p>will be.</p>
<p>I want to think about this in the context of elections.</p>
<p>If these large language models can even now, based on the information we put into them,</p>
<p>quite accurately predict public opinion, you know, ahead of time, I mean predict, it&rsquo;s</p>
<p>before you even ask the public these questions, what will happen when entities, whether it&rsquo;s</p>
<p>corporate entities or whether it&rsquo;s governmental entities or whether it&rsquo;s campaigns or whether</p>
<p>it&rsquo;s foreign actors, take this survey information, these predictions about public opinion, and</p>
<p>then fine-tune strategies to elicit certain responses, certain behavioral responses.</p>
<p>I mean, we already know, this committee is her testimony.</p>
<p>I think three years ago now about the effect of something as prosaic, it now seems, as</p>
<p>Google search, the effect that this has on voters in an election, particularly undecided</p>
<p>voters in the final days of an election who may try to get information from Google search</p>
<p>and what an enormous effect the ranking of the Google search, the articles that it returns</p>
<p>has become an enormous effect on an undecided voter.</p>
<p>This of course is orders of magnitude, far more powerful, far more significant, far more</p>
<p>directive if you like.</p>
<p>So Mr. Altman, maybe you can help me understand here what some of the significance of this</p>
<p>is.</p>
<p>Should we be concerned about models that can, large language models that can predict survey</p>
<p>opinion and then can help organizations, entities, fine-tune strategies to elicit behaviors from</p>
<p>voters?</p>
<p>Should we be worried about this for our elections?</p>
<p>Yeah.</p>
<p>Thank you, Senator Hawley, for the question.</p>
<p>It&rsquo;s one of my areas of greatest concern, the more general ability of these models to</p>
<p>manipulate, to persuade, to provide sort of one-on-one, you know, interactive disinformation.</p>
<p>I think that&rsquo;s like a broader version of what you&rsquo;re talking about.</p>
<p>But given that we&rsquo;re going to face an election next year and these models are getting better,</p>
<p>I think this is a significant area of concern.</p>
<p>I think there&rsquo;s a lot, there&rsquo;s a lot of policies that companies can voluntarily adopt and I&rsquo;m</p>
<p>happy to talk about what we do there.</p>
<p>I do think some regulation would be quite wise on this topic.</p>
<p>Someone mentioned earlier, it&rsquo;s something we really agree with.</p>
<p>People need to know if they&rsquo;re talking to an AI, if content that they&rsquo;re looking at</p>
<p>might be generated or might not.</p>
<p>I think it&rsquo;s a great thing to do is to make that clear.</p>
<p>I think we also will need rules, guidelines about what&rsquo;s expected in terms of disclosure</p>
<p>from a company providing a model that could have these sorts of abilities that you talk about.</p>
<p>So I&rsquo;m nervous about it.</p>
<p>I think people are able to adapt quite quickly.</p>
<p>When Photoshop came onto the scene a long time ago, you know, for a while people were</p>
<p>really quite fooled by Photoshop images and then pretty quickly developed an understanding</p>
<p>that images might be Photoshopped.</p>
<p>This will be like that but on steroids and the interactivity, the ability to really model,</p>
<p>predict humans well as you talked about, I think is going to require a combination of</p>
<p>companies doing the right thing, regulation and public education.</p>
<p>Professor Marcus, do you want to address this?</p>
<p>Yeah, I&rsquo;d like to add two things.</p>
<p>One is in the appendix to my remarks, I have two papers to make you even more concerned.</p>
<p>One is in the Wall Street Journal just a couple of days ago called Help, My Political</p>
<p>Beliefs Were Altered by a Chatbot.</p>
<p>And I think the scenario you raised was that we might basically observe people and use</p>
<p>surveys to figure out what they&rsquo;re saying.</p>
<p>But as Sam just acknowledged, the risk is actually worse that the systems will directly,</p>
<p>maybe not even intentionally, manipulate people.</p>
<p>And that was the thrust of the Wall Street Journal article.</p>
<p>And it links to an article that I&rsquo;ve also linked to called Interacting, and it&rsquo;s not</p>
<p>yet published, not yet peer-reviewed.</p>
<p>Interacting with opinionated language models changes users&rsquo; views.</p>
<p>And this comes back ultimately to data.</p>
<p>One of the things that I&rsquo;m most concerned about with GPT-4 is that we don&rsquo;t know what</p>
<p>it&rsquo;s trained on.</p>
<p>I guess Sam knows, but the rest of us do not.</p>
<p>And what it is trained on has consequences for essentially the biases of the system.</p>
<p>We could talk about that in technical terms.</p>
<p>But how these systems might lead people about depends very heavily on what data is trained</p>
<p>on them.</p>
<p>We need transparency about that, and we probably need scientists in there doing analysis in</p>
<p>order to understand what the political influences, for example, of these systems might be.</p>
<p>And it&rsquo;s not just about politics.</p>
<p>It can be about health.</p>
<p>It could be about anything.</p>
<p>These systems absorb a lot of data, and then what they say reflects that data.</p>
<p>And they&rsquo;re going to do it differently depending on what&rsquo;s in that data.</p>
<p>So it makes a difference if they&rsquo;re trained on the Wall Street Journal as opposed to the</p>
<p>New York Times or Reddit.</p>
<p>I mean, actually, they&rsquo;re largely trained on all of this stuff, but we don&rsquo;t really</p>
<p>understand the composition of that.</p>
<p>And so we have this issue of potential manipulation.</p>
<p>And it&rsquo;s even more complex than that because it&rsquo;s subtle manipulation.</p>
<p>People may not be aware of what&rsquo;s going on.</p>
<p>That was the point of both the Wall Street Journal article and the other article that</p>
<p>I called your attention to.</p>
<p>Let me ask you about AI systems trained on personal data, the kind of data that, for</p>
<p>instance, the social media companies, the major platforms, Google, Meta, et cetera,</p>
<p>collect on all of us routinely.</p>
<p>We&rsquo;ve had many a chat about this in this committee over many a year now.</p>
<p>But the massive amounts of data, personal data, that the companies have on each one</p>
<p>of us, an AI system that is trained on that individual data that knows each of us better</p>
<p>than ourselves and also knows the billions of data points about human behavior, human</p>
<p>language interaction generally, can&rsquo;t we foresee an AI system that is extraordinarily good</p>
<p>at determining what will grab human attention and what will keep an individual&rsquo;s attention?</p>
<p>And so for the war for attention, the war for clicks that is currently going on, on</p>
<p>all of these platforms and how they make their money, I&rsquo;m just imagining an AI system, these</p>
<p>AI models supercharging that war for attention such that we now have technology that will</p>
<p>allow individual targeting of a kind we have never even imagined before, where the AI will</p>
<p>know exactly what Sam Altman finds attention grabbing, will know exactly what Josh Hawley</p>
<p>finds attention grabbing, will be able to elicit, to grab our attention and then elicit</p>
<p>responses from us in a way that we have heretofore not even been able to imagine.</p>
<p>Should we be concerned about that for its corporate applications, for the monetary applications,</p>
<p>for the manipulation that could come from that? Mr. Altman.</p>
<p>Yes, we should be concerned about that. To be clear, OpenAI does not, we&rsquo;re not off,</p>
<p>you know, we don&rsquo;t have an ad-based business model, so we&rsquo;re not trying to build up these</p>
<p>profiles of our users. We&rsquo;re not trying to get them to use it more. Actually, we&rsquo;d love it if</p>
<p>they&rsquo;d use it less because we don&rsquo;t have enough GPUs. But I think other companies are already,</p>
<p>and certainly will in the future, use AI models to create, you know, very good ad predictions of</p>
<p>what a user will like. I think that&rsquo;s already happening in many ways. Mr. Marcus, anything</p>
<p>you want to add? Hyper, yes, and perhaps Ms. Montgomery will want to as well, I don&rsquo;t know.</p>
<p>But hyper-targeting of advertising is definitely going to come. I agree that that&rsquo;s not been</p>
<p>OpenAI&rsquo;s business model. Of course, now they&rsquo;re working for Microsoft, and I don&rsquo;t know what&rsquo;s</p>
<p>in Microsoft&rsquo;s thoughts, but we will definitely see it. Maybe it will be with open source</p>
<p>language models. I don&rsquo;t know, but the technology there is, let&rsquo;s say, partway there to being able</p>
<p>to do that, and we&rsquo;ll certainly get there. So, we&rsquo;re an enterprise technology company,</p>
<p>not consumer-focused, so the space isn&rsquo;t one that we necessarily operate in, in terms of,</p>
<p>but these issues are hugely important issues, and it&rsquo;s why we&rsquo;ve been out ahead in developing</p>
<p>the technology that will help to ensure that you can do things like produce a fact sheet that has</p>
<p>the ingredients of what your data is trained on, data sheets, model cards, all those types of</p>
<p>things, and calling for, as I&rsquo;ve mentioned today, transparency, so you know what the algorithm</p>
<p>was trained on, and then you also know and can manage and monitor continuously over the life</p>
<p>cycle of an AI model the behavior and the performance of that model. Senator Durbin.</p>
<p>Thank you. I think what&rsquo;s happening today in this hearing room is historic.</p>
<p>I can&rsquo;t recall when we&rsquo;ve had people representing large corporations or</p>
<p>private sector entities come before us and plead with us to regulate them.</p>
<p>In fact, many people in the Senate have based their careers on the opposite, that the economy</p>
<p>will thrive if government gets the hell out of the way, and what I&rsquo;m hearing instead today is</p>
<p>that stop me before I innovate again message, and I&rsquo;m just curious as to how we&rsquo;re going to achieve</p>
<p>this. As I mentioned section 230 in my opening remarks, we learned something there. We decided</p>
<p>that in section 230 that we were basically going to absolve the industry from liability</p>
<p>for a period of time as it came into being. Well, Mr. Altman, on the podcast earlier this year,</p>
<p>you agreed with host Kara Swisher that section 230 doesn&rsquo;t apply to generative AI,</p>
<p>and that developers like open AI should not be entitled to full immunity for harms caused by</p>
<p>their products. So what have we learned from 230 that applies to your situation with AI?</p>
<p>Thank you for the question, Senator. I don&rsquo;t know yet exactly what the right answer here is. I&rsquo;d</p>
<p>love to collaborate with you to figure it out. I do think for a very new technology,</p>
<p>we need a new framework. Certainly companies like ours bear a lot of responsibility for</p>
<p>the tools that we put out in the world, but tool users do as well, and how we want, and also people</p>
<p>that will build on top of it between them and the end consumer, and how we want to come up with a</p>
<p>liability framework there is a super important question, and we&rsquo;d love to work together.</p>
<p>The point I want to make is this. When it came to online platforms, the inclination of the</p>
<p>government was get out of the way. This is a new industry. Don&rsquo;t overregulate it. In fact,</p>
<p>give them some breathing space and see what happens. I&rsquo;m not sure I&rsquo;m happy with the outcome</p>
<p>as I look at online platforms and the harms that they&rsquo;ve created. Problems that we&rsquo;ve seen</p>
<p>demonstrated in this committee, child exploitation, cyber bullying, online drug sales, and more.</p>
<p>I don&rsquo;t want to repeat that mistake again, and what I hear is the opposite suggestion from the</p>
<p>private sector, and that is come in the front of this thing and establish some liability standards,</p>
<p>precision regulation, for a major company like IBM to come before this committee and say to</p>
<p>the government, please regulate us. Can you explain the difference in thinking from the past and now?</p>
<p>Yeah, absolutely. So for us, this comes back to the issue of trust, and trust in the technology.</p>
<p>Trust is our license to operate, as I mentioned in my remarks, and so we firmly believe, and we&rsquo;ve</p>
<p>been calling for precision regulation of artificial intelligence for years now. This is not a new</p>
<p>position. We think that technology needs to be deployed in a responsible and clear way, that</p>
<p>people, we&rsquo;ve taken principles around that. Trust and transparency, we call them, are principles</p>
<p>that were articulated years ago and build them into practices. That&rsquo;s why we&rsquo;re here advocating</p>
<p>for precision regulatory approach. So we think that AI should be regulated at the point of risk,</p>
<p>essentially, and that&rsquo;s the point at which technology meets society.</p>
<p>Let&rsquo;s take a look at what that might appear to be. Members of Congress are a pretty smart lot</p>
<p>of people, maybe not as smart as we think we are many times, and government certainly has a capacity</p>
<p>to do amazing things, but when you talk about our ability to respond to the current challenge and</p>
<p>perceived challenge of the future, challenges which you all have described in terms which are</p>
<p>hard to forget, as you said, Mr. Altman, things can go quite wrong. As you said, Mr. Marcus,</p>
<p>democracy is threatened. I mean, the magnitude of the challenge you&rsquo;re giving us is substantial.</p>
<p>I&rsquo;m not sure that we respond quickly and with enough expertise to deal with it.</p>
<p>Professor Marcus, you made a reference to CERN, the International Arbiter of Nuclear Research,</p>
<p>I suppose. I don&rsquo;t know if that&rsquo;s a fair characterization, but it&rsquo;s a characterization</p>
<p>I&rsquo;ll start with. What is it, what agency of this government do you think exists that could</p>
<p>respond to the challenge that you&rsquo;ve laid down today?</p>
<p>We have many agencies that can respond in some ways. For example, the FTC,</p>
<p>the FCC, there are many agencies that can, but my view is that we probably need a cabinet level</p>
<p>organization within the United States in order to address this. And my reasoning for that is that</p>
<p>the number of risks is large. The amount of information to keep up on is so much. I think</p>
<p>we need a lot of technical expertise. I think we need a lot of coordination of these efforts. So</p>
<p>there is one model here where we stick to only existing law and try to shape all of what we need</p>
<p>to do, and each agency does their own thing. But I think that AI is going to be such a large part</p>
<p>of our future and is so complicated and moving so fast, and this does not fully solve your problem</p>
<p>about a dynamic world, but it&rsquo;s a step in that direction to have an agency that&rsquo;s full-time job</p>
<p>is to do this. I personally have suggested, in fact, that we should want to do this in a global</p>
<p>way. I wrote an article in The Economist, I have a link in here, an invited essay for The Economist</p>
<p>suggesting we might want an international agency for AI. That&rsquo;s what I wanted to go to next, and</p>
<p>that is the fact that, I&rsquo;ll get inside from the CERN and nuclear examples, because government was</p>
<p>involved in that from day one, at least in the United States. But now we&rsquo;re dealing with</p>
<p>innovation, which doesn&rsquo;t necessarily have a boundary. We may create a great U.S. agency,</p>
<p>and I hope that we do, that may have jurisdiction over U.S. corporations and U.S. activity,</p>
<p>but doesn&rsquo;t have a thing to do with what&rsquo;s going to bombard us from outside the United States.</p>
<p>How do you give this international authority the authority to regulate in a fair way for</p>
<p>all entities involved in AI? I think that&rsquo;s probably over my pay grade. I would like to</p>
<p>see it happen, and I think it may be inevitable that we push there. I mean, I think the politics</p>
<p>behind it are obviously complicated. I&rsquo;m really heartened by the degree to which this room is</p>
<p>bipartisan and supporting the same things, and that makes me feel like it might be possible.</p>
<p>I would like to see the United States take leadership in such organization. It has to</p>
<p>involve the whole world and not just the U.S. to work properly. I think even from the perspective</p>
<p>of the companies, it would be a good thing. So the companies themselves do not want a situation</p>
<p>where you take these models, which are expensive to train, and you have to have 190-some of them,</p>
<p>one for every country. That wouldn&rsquo;t be a good way of operating. When you think about the energy</p>
<p>costs alone, just for training these systems, it would not be a good model if every country has</p>
<p>its own policies, and for each jurisdiction, every company has to train another model,</p>
<p>and maybe, you know, different states are different. So Missouri and California have</p>
<p>different rules, and so then that requires even more training of these expensive models with huge</p>
<p>climate impact. I mean, it would be very difficult for the companies to operate if there was no</p>
<p>global coordination, and so I think that we might get the companies on board if there&rsquo;s bipartisan</p>
<p>support here, and I think there&rsquo;s support around the world. It is entirely possible that we could</p>
<p>develop such a thing, but obviously, there are many, you know, nuances here of diplomacy that</p>
<p>are over my pay grade. I would love to learn from you all to try to help make that happen.</p>
<p>Mr. Altman.</p>
<p>Can I weigh in just briefly?</p>
<p>Briefly, please.</p>
<p>I want to echo support for what Mr. Marcus said. I think the U.S. should lead here and do things</p>
<p>first, but to be effective, we do need something global. As you mentioned, this can happen</p>
<p>everywhere. There is precedent. I know it sounds naive to call for something like this, and it</p>
<p>sounds really hard. There is precedent. We&rsquo;ve done it before with the IAEA. We&rsquo;ve talked about doing</p>
<p>it for other technologies. Given what it takes to make these models, the chip supply chain,</p>
<p>the sort of limited number of competitive GPUs, the power the U.S. has over these companies,</p>
<p>I think there are paths to the U.S. setting some international standards that other countries would</p>
<p>need to collaborate with and be part of that are actually workable, even though it sounds on its</p>
<p>face like an impractical idea. I think it would be great for the world.</p>
<p>Thank you, Mr. Chairman.</p>
<p>Thanks, Senator Durbin. In fact, I think we&rsquo;re going to hear more about what Europe is doing.</p>
<p>European Parliament already is acting on an AI act. On social media, Europe is ahead of us.</p>
<p>We need to be in the lead. I think your point is very well taken. Let me turn to Senator Graham.</p>
<p>Senator Blackburn.</p>
<p>Thank you, Mr. Chairman, and thank you all for being here with us today. I put into my chat</p>
<p>GPT account should Congress regulate AI chat GPT, and it gave me four pros, four cons, and says,</p>
<p>ultimately, the decision rests with Congress and deserves careful consideration. So on that,</p>
<p>you know, it was very balanced. I recently visited with the Nashville Technology Council.</p>
<p>I represent Tennessee. And, of course, you had people there from healthcare, financial services,</p>
<p>logistics, educational entities, and they&rsquo;re concerned about what they see happening with AI,</p>
<p>with the utilizations for their companies. Ms. Montgomery, you know, similar to you,</p>
<p>they&rsquo;ve got healthcare people are looking at disease analytics. They&rsquo;re looking at predictive</p>
<p>diagnosis, how this can better the outcomes for patients, logistics industry, looking at ways to</p>
<p>save time and money and yield efficiencies. You&rsquo;ve got financial services that are saying,</p>
<p>how does this work with quantum? How does it work with blockchain? How can we use this? But</p>
<p>I think as we have talked with them, Mr. Chairman, one of the things that continues to come up</p>
<p>is, yes, Professor Marcus, as you were saying, the EU, different entities are ahead of us in this,</p>
<p>but we have never established a federally given preemption for online privacy, for data security,</p>
<p>and put some of those foundational elements in place, which is something that we need to do</p>
<p>as we look at this. And it will require that Commerce Committee, Judiciary Committee decide</p>
<p>how we move forward so that people own their virtual you. And Mr. Altman, I was glad to see</p>
<p>last week that your open AI models are not going to be trained using consumer data. I think that</p>
<p>that is important. And if we have a second round, I&rsquo;ve got a host of questions for you on data</p>
<p>security and privacy. But I think it&rsquo;s important to let people control their virtual you, their</p>
<p>information in these settings. And I want to come to you on music and content creation, because</p>
<p>we&rsquo;ve got a lot of songwriters and artists, and I think we have the best creative community on</p>
<p>the face of the earth there in Tennessee. And they should be able to decide if their copyrighted</p>
<p>songs and images are going to be used to train these models. And I&rsquo;m concerned about OpenAI&rsquo;s</p>
<p>jukebox. It offers some re-renditions in the style of Garth Brooks, which suggests that OpenAI</p>
<p>is trained on Garth Brooks songs. I went in this weekend and I said, write me a song that</p>
<p>sounds like Garth Brooks. And it gave me a different version of Simple Man. So it&rsquo;s interesting that it</p>
<p>would do that. But you&rsquo;re training it on these copyrighted songs, these MIDI files, these sound</p>
<p>technologies. So as you do this, who owns the rights to that AI-generated material? And using</p>
<p>your technology, could I remake a song, insert content from my favorite artist, and then own the</p>
<p>creative rights to that song? Thank you, Senator. This is an area of great interest to us.</p>
<p>I would say, first of all, we think that creators deserve control over how their creations are used</p>
<p>and what happens sort of beyond the point of them releasing it into the world.</p>
<p>Second, I think that we need to figure out new ways with this new technology that creators can</p>
<p>win, succeed, have a vibrant life. And I&rsquo;m optimistic that this will present&hellip;</p>
<p>Then let me ask you this. How do you compensate the artist?</p>
<p>That&rsquo;s exactly what I was going to say. We&rsquo;re working with artists now, visual artists,</p>
<p>musicians, to figure out what people want. There&rsquo;s a lot of different opinions, unfortunately,</p>
<p>and at some point we&rsquo;ll have to&hellip; Let me ask you this. Do you favor</p>
<p>something like SoundExchange that has worked in the area of radio?</p>
<p>I&rsquo;m not familiar with SoundExchange, I&rsquo;m sorry. Okay. You&rsquo;ve got your team behind you. Get back</p>
<p>to me on that. That would be a third-party entity. Okay.</p>
<p>So let&rsquo;s discuss that. Let me move on. Can you commit, as you&rsquo;ve done with consumer data,</p>
<p>not to train ChatGPT, OpenAI, Jukebox, or other AI models on artists and songwriters&rsquo; copyrighted</p>
<p>works, or use their voices and their likenesses without first receiving their consent?</p>
<p>First of all, Jukebox is not a product we offer. That was a research release,</p>
<p>but it&rsquo;s not unlike ChatGPT or Dolly. Yeah, but we&rsquo;ve lived through Napster.</p>
<p>Yes. That was something that really cost a lot of artists a lot of money.</p>
<p>Oh, I understand. Yeah, for sure. In the digital distribution era.</p>
<p>I don&rsquo;t know the numbers on Jukebox on the top of my head as a research release. I can follow up</p>
<p>with your office, but Jukebox is not something that gets much attention or usage. It was put</p>
<p>out to show that something&rsquo;s possible. Well, as Senator Durbin just said,</p>
<p>and I think it&rsquo;s a fair warning to you all, if we&rsquo;re not involved in this from the get-go,</p>
<p>and you all already are a long way down the path on this, but if we don&rsquo;t step in,</p>
<p>then this gets away from you. So, are you working with a copyright office?</p>
<p>Are you considering protections for content generators and creators in generative AI?</p>
<p>Yes, we are absolutely engaged on that. Again, to reiterate my earlier point,</p>
<p>we think that content creators, content owners need to benefit from this technology. Exactly</p>
<p>what the economic model is, we&rsquo;re still talking to artists and content owners about what they want.</p>
<p>I think there&rsquo;s a lot of ways this can happen, but very clearly, no matter what the law is,</p>
<p>the right thing to do is to make sure people get significant upside benefit from this new technology</p>
<p>and we believe that it&rsquo;s really going to deliver that, but that content owners, likenesses,</p>
<p>people totally deserve control over how that&rsquo;s used and to benefit from it.</p>
<p>Okay, so on privacy then, how do you plan to account for the collection</p>
<p>of voice and other user-specific data, things that are copyrighted, user-specific data through</p>
<p>your AI applications? Because if I can go in and say, write me a song that sounds like Garth Brooks</p>
<p>and it takes part of an existing song, there has to be a compensation to that artist for</p>
<p>that utilization and that use. If it was radio play, it would be there. If it was streaming,</p>
<p>it would be there. So if you&rsquo;re going to do that, what is your policy for making certain you&rsquo;re</p>
<p>accounting for that and you&rsquo;re protecting that individual&rsquo;s right to privacy and their right to</p>
<p>secure that data and that created work? So a few thoughts about this. Number one,</p>
<p>we think that people should be able to say, I don&rsquo;t want my personal data trained on.</p>
<p>That&rsquo;s, I think that&rsquo;s&hellip; Right, that gets to a national privacy law,</p>
<p>which many of us here on the dais are working toward getting something that we can use.</p>
<p>Yeah, I think strong privacy&hellip; My time&rsquo;s expired. Let me yield back. Thank you, Mr. Chair.</p>
<p>Thanks, Senator Blackburn. Senator Klobuchar. Thank you very much, Mr. Chairman. And</p>
<p>Senator Blackburn, I love Nashville, love Tennessee, love your music, but I will say,</p>
<p>I use chat GPT and just ask what are the top creative song artists of all time? And two of</p>
<p>the top three were from Minnesota. That would be Prince and Bob Dylan. Okay. All right. So let us</p>
<p>continue on. One thing AI won&rsquo;t change and you&rsquo;re seeing it here. All right. So on a more serious</p>
<p>note, though, my staff and I, in my role as chair of the rules committee and leading a lot of the</p>
<p>election bill, and we just introduced a bill that Representative Yvette Clark from New York</p>
<p>introduced over the house, Senator Booker and Bennett and I did on political advertisements.</p>
<p>But that is just, of course, the tip of the iceberg. You know this from your discussions</p>
<p>with Senator Hawley and others about the images and my own view, Senator Graham&rsquo;s of section 230</p>
<p>is that we just can&rsquo;t let people make stuff up and then not have any consequence. But I&rsquo;m going</p>
<p>to focus in on what my job, one of my jobs will be on the rules committee, and that is election</p>
<p>misinformation. And we just asked chat GPT to do a tweet about a polling location in Bloomington,</p>
<p>Minnesota, and said there are long lines at this polling location at Atonement Lutheran Church.</p>
<p>Where should we go? Now, albeit it&rsquo;s not an election right now, but the answer, the tweet</p>
<p>that was drafted was a completely fake thing. Go to 1234 Elm Street. And so you can imagine what</p>
<p>I&rsquo;m concerned about here with an election upon us, with primary elections upon us, that we&rsquo;re</p>
<p>going to have all kinds of misinformation. And I just want to know what you&rsquo;re planning on doing</p>
<p>it, doing about it. I know we&rsquo;re going to have to do something soon, not just for the images of the</p>
<p>candidates, but also for misinformation about the actual polling places and election rules.</p>
<p>Thank you, Senator. We talked about this a little bit earlier. We are quite concerned about the</p>
<p>impact this can have on elections. I think this is an area where hopefully the entire industry</p>
<p>and the government can work together quickly. There&rsquo;s many approaches, and I&rsquo;ll talk about</p>
<p>some of the things we do. But before that, I think it&rsquo;s tempting to use the frame of social media.</p>
<p>But this is not social media. This is different. And so the response that we need is different.</p>
<p>You know, this is a tool that a user is using to help generate content more efficiently than before.</p>
<p>They can change it. They can test the accuracy of it. If they don&rsquo;t like it, they can get another</p>
<p>version. But it still then spreads through social media or other ways, like chat GPT is a, you know,</p>
<p>single player experience where you&rsquo;re just using this. And so I think as we think about what to do,</p>
<p>that&rsquo;s important to understand. There&rsquo;s a lot that we can do there. There&rsquo;s things that the model</p>
<p>refuses to generate. We have policies. We also importantly have monitoring. So at scale,</p>
<p>we can detect someone generating a lot of those tweets, even if generating one tweet is OK.</p>
<p>Yeah. And of course, there&rsquo;s going to be other platforms. And if they&rsquo;re all spouting out fake</p>
<p>election information, I just I think what happened in the past with Russian interference and like,</p>
<p>it&rsquo;s just going to be a tip of the iceberg when some of those fake ads. So that&rsquo;s number one.</p>
<p>Number two is the impact on intellectual property. And Senator Blackburn was getting at some of this</p>
<p>with song rights and had serious concerns about that. But news content. So Senator Kennedy and</p>
<p>I have a bill that was really quite straightforward that would simply allowed the the news</p>
<p>organizations an exemption to be able to negotiate with basically Google and Facebook. Microsoft was</p>
<p>supportive of the bill, but basically negotiate with them to get better rates and be able to</p>
<p>not have some leverage. And other countries are doing this, Australia and the like.</p>
<p>And so my question is, when we already have a study by Northwestern predicting that one third</p>
<p>of the U.S. newspapers are that roughly existed two decades are going to go are going to be gone</p>
<p>by 2025. Unless you start compensating for everything from book movies, books, yes,</p>
<p>but also news content, we&rsquo;re going to lose any realistic content producers. And so I&rsquo;d</p>
<p>like your response to that. And of course, there is an exemption for copyright in Section 230.</p>
<p>But I think asking little newspapers to go out and sue all the time just can&rsquo;t be the answer.</p>
<p>They&rsquo;re not going to be able to keep up. Yeah, like, it is my hope that tools like</p>
<p>what we&rsquo;re creating can help news organizations do better. I think having a vibrant,</p>
<p>having a vibrant national media is critically important. And let&rsquo;s call it round one of the</p>
<p>internet has not been great for that. Right. We&rsquo;re talking here about local that,</p>
<p>you know, report on your high school for school scores and a scandal in your city council,</p>
<p>those kinds of things. For sure. They&rsquo;re the ones that are actually getting the worst,</p>
<p>the little radio stations and broadcast. But do you understand that this could be</p>
<p>exponentially worse in terms of local news content if they&rsquo;re not compensated?</p>
<p>Well, because what they need is to be compensated for their content and not have it stolen.</p>
<p>Yeah. Again, our our model, you know, the current version of GPT for ended training in 2021. It&rsquo;s</p>
<p>not it&rsquo;s not it&rsquo;s not a good way to find recent news. And it&rsquo;s I don&rsquo;t think it&rsquo;s a service that</p>
<p>can do a great job of linking out, although maybe with our plugins, it&rsquo;s it&rsquo;s possible.</p>
<p>If there are things that we can do to help local news, we would certainly like to again,</p>
<p>I think it&rsquo;s it&rsquo;s critically important. Okay. May I add something there?</p>
<p>Yeah. But let me just ask you a question. You can combine them quick,</p>
<p>more transparency on the platforms. Senator Coons and Senator Cassidy and I have the Platform</p>
<p>Accountability Transparency Act to give researchers access to this information of</p>
<p>the algorithms and the like on social media data. Would that be helpful? And then why don&rsquo;t you</p>
<p>just say yes or no, and then go at his. Transparency is absolutely critical here</p>
<p>to understand the political ramifications, the bias ramifications and so forth. We need</p>
<p>transparency about the data. We need to know more about how the models work. We need to have</p>
<p>scientists have access to them. I was just going to amplify your earlier point about local news.</p>
<p>A lot of news is going to be generated by these systems. They&rsquo;re not reliable. NewsGuard already</p>
<p>is a study. I&rsquo;m sorry, it&rsquo;s not in my appendix, but I will get it to your office, showing that</p>
<p>something like 50 websites are already generated by bots. We&rsquo;re going to see much, much more of</p>
<p>that. And it&rsquo;s going to make it even more competitive for the local news organizations.</p>
<p>And so the quality of the sort of overall news market is going to decline as we have more</p>
<p>generated content by systems that aren&rsquo;t actually reliable in the content they&rsquo;ve generated.</p>
<p>Thank you. And thank you on a very timely basis to make the argument why we have to mark up this</p>
<p>bill again in June. I appreciate it. Thank you. Senator Graham.</p>
<p>Thank you, Mr. Chairman and Senator Hawley for having this. I&rsquo;m trying to find out how it is</p>
<p>different than social media and learn from the mistakes we&rsquo;ve made with social media.</p>
<p>The idea of not suing social media companies is to allow the Internet to flourish because</p>
<p>if I slander you, you can sue me. If you&rsquo;re a billboard company and you put up the slander,</p>
<p>can you sue the billboard company? We said no. Basically, Section 230 is being used by</p>
<p>social media companies to avoid liability for activity that other people generate.</p>
<p>When they refuse to comply with their terms of use, a mother calls up the company and says,</p>
<p>this app is being used to bully my child to death. You promised in the terms of use,</p>
<p>you would prevent bullying. And she calls three times. She gets no response. The child kills</p>
<p>herself and they can&rsquo;t sue. Do you all agree we don&rsquo;t want to do that again? Yes.</p>
<p>If I may speak for one second, there&rsquo;s a fundamental distinction</p>
<p>between reproducing content and generating content.</p>
<p>Yeah, but you you would like liability where people are harmed.</p>
<p>Absolutely. Yes. In fact, IBM has been publicly advocating to condition liability on a reasonable</p>
<p>care standard. So let me just make sure I understand the law as it exists today.</p>
<p>Mr. Altman, thank you for coming. Your company is not a company.</p>
<p>Your company is not claiming that Section 230 applies to the tool you have created.</p>
<p>Yeah, we&rsquo;re claiming we need to work together to find a totally new approach. I don&rsquo;t think</p>
<p>Section 230 is even the right framework. Okay. So under the law, it exists today.</p>
<p>This tool you create, if I&rsquo;m harmed by it, can I sue you?</p>
<p>That is beyond my area of legal expertise. Have you ever been sued?</p>
<p>Not for that, no. Have you ever been sued at all?</p>
<p>That your company? Yeah, I get sued.</p>
<p>Yeah, we&rsquo;ve gotten sued before. Okay. And what for?</p>
<p>I mean, they&rsquo;ve mostly been like pretty frivolous things like I think happens to any company.</p>
<p>But like the examples my colleagues have given from artificial intelligence that could literally</p>
<p>ruin our lives. Can we go to the company that created that tool and sue them? Is that your</p>
<p>understanding? Yeah, I think there needs to be clear responsibility by the companies.</p>
<p>But you&rsquo;re not claiming any kind of legal protection</p>
<p>like Section 230 applies to your industry, is that correct?</p>
<p>No, I don&rsquo;t think we&rsquo;re saying anything like that.</p>
<p>Mr. Marcus, when it comes to consumers, there seems to be like three time-tested</p>
<p>ways to protect consumers against any product. Statutory schemes, which are non-existent here,</p>
<p>legal systems, which may be here, but not social media, and agencies. Go back to</p>
<p>Senator Hawley. The atom bomb has put a cloud over humanity.</p>
<p>But nuclear power could be one of the solutions to climate change.</p>
<p>So what I&rsquo;m trying to do is make sure that you just can&rsquo;t go build a nuclear power plant. Hey,</p>
<p>Bob, what would you like to do today? Let&rsquo;s go build a nuclear power plant.</p>
<p>You have a Nuclear Regulatory Commission that governs how you build a plant and is licensed.</p>
<p>Do you agree, Mr. Altman, that these tools you&rsquo;re creating should be licensed?</p>
<p>Yeah, we&rsquo;ve been calling for this. That&rsquo;s the simplest way. You get a license. And do you</p>
<p>agree with me that the simplest way and the most effective way is to have an agency that is more</p>
<p>nimble and smarter than Congress, which should be easy to create, overlooking what you do?</p>
<p>Yes, we&rsquo;d be enthusiastic about that.</p>
<p>You agree with that, Mr. Marcus?</p>
<p>Absolutely.</p>
<p>You agree with that, Ms. Montgomery?</p>
<p>I would have some nuances. I think we need to build on what we have in place already today.</p>
<p>We don&rsquo;t have an agency that&rsquo;s working.</p>
<p>Regulators.</p>
<p>Oh, wait a minute. Nope, nope, nope.</p>
<p>We don&rsquo;t have an agency that regulates the technology.</p>
<p>So should we have one?</p>
<p>But a lot of the issues I don&rsquo;t think so. A lot of the issues</p>
<p>Okay, wait a minute. Wait a minute. So IBM says we don&rsquo;t need an agency.</p>
<p>Uh, interesting. Should we have a license required for these tools?</p>
<p>So, so what we believe is that we need to regulate.</p>
<p>That&rsquo;s a simple question. Should you get a license to produce one of these tools?</p>
<p>I think it comes back to some of them potentially, yes.</p>
<p>So what I said at the onset is that we need to clearly define risks.</p>
<p>Do you claim Section 230 applies in this area at all?</p>
<p>We&rsquo;re not a platform company and we&rsquo;ve, again,</p>
<p>long advocated for a reasonable care standard in Section 230.</p>
<p>I just don&rsquo;t understand how you could say</p>
<p>that you don&rsquo;t need an agency to deal with the most transformative technology maybe ever?</p>
<p>Well, I think we have existing</p>
<p>Is this a transformative technology that can disrupt life as we know it, good and bad?</p>
<p>I think it&rsquo;s a transformative technology, certainly.</p>
<p>And the conversations that we&rsquo;re having here today have been</p>
<p>really bringing to light the fact that the domains and the issues</p>
<p>This one with you has been very enlightening to me.</p>
<p>Mr. Allman, why are you so willing to have an agency?</p>
<p>Senator, we&rsquo;ve been clear about what we think the upsides are,</p>
<p>and I think you can see from users how much they enjoy it,</p>
<p>how much value they&rsquo;re getting out of it.</p>
<p>But we&rsquo;ve also been clear about what the downsides are,</p>
<p>and so that&rsquo;s why we think we need an agency.</p>
<p>So it&rsquo;s a major tool to be used by a lot of people.</p>
<p>It&rsquo;s a major new technology.</p>
<p>Yeah, if you make a ladder and the ladder doesn&rsquo;t work,</p>
<p>you can sue the people who made the ladder,</p>
<p>but there are some standards out there to make a ladder.</p>
<p>So that&rsquo;s why we&rsquo;re agreeing with you.</p>
<p>Yeah, that&rsquo;s right. I think you&rsquo;re on the right track.</p>
<p>So here&rsquo;s what my two cents worth for the committee</p>
<p>is that we need to empower an agency that issues in a license and can take it away.</p>
<p>Wouldn&rsquo;t that be some incentive to do it right</p>
<p>if you could actually be taken out of business?</p>
<p>Clearly, that should be part of what an agency can do.</p>
<p>And you also agree that China&rsquo;s doing AI research, is that right?</p>
<p>Correct.</p>
<p>This world organization that doesn&rsquo;t exist, maybe it will,</p>
<p>but if you don&rsquo;t do something about the China part of it,</p>
<p>you&rsquo;ll never quite get this right. Do you agree?</p>
<p>Well, that&rsquo;s why I think it doesn&rsquo;t necessarily have to be a world organization,</p>
<p>but there has to be some sort of, and there&rsquo;s a lot of options here,</p>
<p>there has to be some sort of standard,</p>
<p>some sort of set of controls that do have global effect.</p>
<p>You know, because, you know, other people doing this.</p>
<p>I got 15. Military application.</p>
<p>How can AI change the warfare?</p>
<p>And you got one minute.</p>
<p>I got one minute? All right.</p>
<p>This is, that&rsquo;s a tough question for one minute.</p>
<p>This is very far out of my area of expertise.</p>
<p>But I give you one example, a drone.</p>
<p>Can a drone, you can plug into a drone,</p>
<p>the coordinates and it can fly out and it goes over this target</p>
<p>and it drops a missile on this car moving down the road and somebody&rsquo;s watching it.</p>
<p>Could AI create a situation where a drone can select the target itself?</p>
<p>I think we shouldn&rsquo;t allow that.</p>
<p>Well, can it be done?</p>
<p>Sure.</p>
<p>Thanks.</p>
<p>Thanks, Senator Graham.</p>
<p>Senator Coon.</p>
<p>Thank you, Senator Blumenthal, Senator Hawley,</p>
<p>for convening this hearing, for working closely together to come up with this</p>
<p>compelling panel of witnesses and beginning a series of hearings</p>
<p>on this transformational technology.</p>
<p>We recognize the immense promise and substantial risks</p>
<p>associated with generative AI technologies.</p>
<p>We know these models can make us more efficient,</p>
<p>help us learn new skills, open whole new vistas of creativity.</p>
<p>But we also know that generative AI can authoritatively deliver</p>
<p>wildly incorrect information.</p>
<p>It can hallucinate, as is often described.</p>
<p>It can impersonate loved ones.</p>
<p>It can encourage self-destructive behaviors.</p>
<p>And it can shape public opinion and the outcome of elections.</p>
<p>Congress thus far has demonstrably failed to responsibly enact</p>
<p>meaningful regulation of social media companies</p>
<p>with serious harms that have resulted that we don&rsquo;t fully understand.</p>
<p>Senator Klobuchar referenced in her questioning</p>
<p>a bipartisan bill that would open up social media platforms underlying algorithms.</p>
<p>We have struggled to even do that, to understand the underlying technology</p>
<p>and then to move towards responsible regulation.</p>
<p>We cannot afford to be as late to responsibly regulating generative AI</p>
<p>as we have been to social media, because the consequences,</p>
<p>both positive and negative, will exceed those of social media by orders of magnitude.</p>
<p>So let me ask a few questions designed to get at both how we assess the risk,</p>
<p>what&rsquo;s the role of international regulation, and how does this impact AI?</p>
<p>Mr. Altman, I appreciate your testimony about the ways in which open AI</p>
<p>assesses the safety of your models through a process of iterative deployment.</p>
<p>The fundamental question embedded in that process, though,</p>
<p>is how you decide whether or not a model is safe enough to deploy</p>
<p>and safe enough to have been built and then let go into the wild.</p>
<p>I understand one way to prevent generative AI models from providing harmful content</p>
<p>is to have humans identify that content and then train the algorithm to avoid it.</p>
<p>There&rsquo;s another approach that&rsquo;s called constitutional AI</p>
<p>that gives the model a set of values or principles to guide its decision making.</p>
<p>Would it be more effective to give models these kinds of rules</p>
<p>instead of trying to require or compel training the model</p>
<p>on all the different potentials for harmful content?</p>
<p>Thank you, Senator. It&rsquo;s a great question.</p>
<p>I&rsquo;d like to frame it by talking about why we deploy at all,</p>
<p>like why we put these systems out into the world.</p>
<p>There&rsquo;s the obvious answer about there&rsquo;s benefits,</p>
<p>and people are using it for all sorts of wonderful things and getting great value,</p>
<p>and that makes us happy.</p>
<p>But a big part of why we do it is that we believe that iterative deployment</p>
<p>and giving people in our institutions and you all</p>
<p>time to come to grips with this technology, to understand it,</p>
<p>to find its limitations, its benefits, the regulations we need around it,</p>
<p>what it takes to make it safe, that&rsquo;s really important.</p>
<p>Going off to build a super powerful AI system in secret</p>
<p>and then dropping it on the world all at once I think would not go well.</p>
<p>So a big part of our strategy is while these systems are still relatively weak</p>
<p>and deeply imperfect, to find ways to get people to have experience with them,</p>
<p>to have contact with reality,</p>
<p>and to figure out what we need to do to make it safer and better.</p>
<p>And that is the only way that I&rsquo;ve seen in the history of new technology</p>
<p>and products of this magnitude to get to a very good outcome.</p>
<p>And so that interaction with the world is very important.</p>
<p>Now, of course, before we put something out, it needs to meet a bar of safety.</p>
<p>And again, we spent well over six months with GPT-4 after we finished training it,</p>
<p>going through all of these different things,</p>
<p>and deciding also what the standards were going to be before we put something out there,</p>
<p>trying to find the harms that we knew about and how to address those.</p>
<p>One of the things that&rsquo;s been gratifying to us is even some of our biggest critics</p>
<p>have looked at GPT-4 and said, wow, OpenAI made huge progress on&hellip;</p>
<p>If you could focus briefly on whether or not a constitutional model</p>
<p>that gives values would be worth it.</p>
<p>I was just about to get there.</p>
<p>All right. Sorry about that.</p>
<p>Yeah, I think giving the models values up front is an extremely important set.</p>
<p>You know, RLHF is another way of doing that same thing.</p>
<p>But somehow or other, you are&hellip;</p>
<p>With synthetic data or human-generated data,</p>
<p>you&rsquo;re saying, here are the values, here&rsquo;s what I want you to reflect,</p>
<p>or here are the wide bounds of everything that society will allow.</p>
<p>And then within there, you pick as the user,</p>
<p>you know, if you want value system over here, value system over there.</p>
<p>We think that&rsquo;s very important.</p>
<p>There&rsquo;s multiple technical approaches, but we need to give policymakers</p>
<p>and the world as a whole the tools to say, here&rsquo;s the values and implement them.</p>
<p>Thank you, Ms. Montgomery.</p>
<p>You serve on an AI ethics board of a long established company</p>
<p>that has a lot of experience with AI.</p>
<p>I&rsquo;m really concerned that generative AI technologies</p>
<p>can undermine the faith of democratic values</p>
<p>and the institutions that we have.</p>
<p>The Chinese are insisting that AI as being developed in China</p>
<p>reinforce the core values of the Chinese Communist Party</p>
<p>and the Chinese system.</p>
<p>And I&rsquo;m concerned about how we promote AI that reinforces</p>
<p>and strengthens open markets, open societies, and democracy.</p>
<p>In your testimony, you&rsquo;re advocating for AI regulation</p>
<p>tailored to the specific way the technology is being used,</p>
<p>not the underlying technology itself.</p>
<p>And the EU is moving ahead with an AI act</p>
<p>which categorizes AI products based on level of risk.</p>
<p>You all in different ways have said that you view elections</p>
<p>and the shaping of election outcomes and disinformation</p>
<p>that can influence elections as one of the highest risk cases,</p>
<p>one that&rsquo;s entirely predictable.</p>
<p>We have attempted so far unsuccessfully to regulate social media</p>
<p>after the demonstrably harmful impacts of social media</p>
<p>on our last several elections.</p>
<p>What advice do you have for us about what kind of approach we should follow</p>
<p>and whether or not the EU direction is the right one to pursue?</p>
<p>Yeah, I mean the conception of the EU AI Act</p>
<p>is very consistent with this concept of precision regulation</p>
<p>where you&rsquo;re regulating the use of the technology in context.</p>
<p>So absolutely that approach makes a ton of sense.</p>
<p>It&rsquo;s what I advocated for at the onset.</p>
<p>Different rules for different risks.</p>
<p>So in the case of elections,</p>
<p>absolutely any algorithm being used in that context</p>
<p>should be required to have disclosure around the data being used,</p>
<p>the performance of the model,</p>
<p>anything along those lines is really important.</p>
<p>Guardrails need to be in place.</p>
<p>And on the point, just come back to the question</p>
<p>of whether we need an independent agency.</p>
<p>I mean, I think we don&rsquo;t want to slow down regulation</p>
<p>to address real risks right now, right?</p>
<p>So we have existing regulatory authorities in place</p>
<p>who have been clear that they have the ability</p>
<p>to regulate in their respective domains.</p>
<p>A lot of the issues we&rsquo;re talking about today</p>
<p>span multiple domains, elections and the like, so.</p>
<p>If I could, I&rsquo;ll just assert</p>
<p>that those existing regulatory bodies and authorities</p>
<p>are under-resourced and lack many of the statutory</p>
<p>regulatory powers that they need.</p>
<p>Correct.</p>
<p>We have failed to deliver on data privacy</p>
<p>even though industry has been asking us to regulate data privacy.</p>
<p>If I might, Mr. Marcus,</p>
<p>I&rsquo;m interested also what international bodies</p>
<p>are best positioned to convene multilateral discussions</p>
<p>to promote responsible standards.</p>
<p>We&rsquo;ve talked about a model being CERN and nuclear energy.</p>
<p>I&rsquo;m concerned about proliferation and non-proliferation.</p>
<p>We&rsquo;ve also talked, I would suggest that the IPCC,</p>
<p>a UN body, helped at least provide a scientific baseline</p>
<p>of what&rsquo;s happening in climate change.</p>
<p>So that even though we may disagree about strategies,</p>
<p>globally, we&rsquo;ve come to a common understanding</p>
<p>of what&rsquo;s happening</p>
<p>and what should be the direction of intervention.</p>
<p>I&rsquo;d be interested, Mr. Marcus,</p>
<p>if you could just give us your thoughts</p>
<p>on who&rsquo;s the right body internationally</p>
<p>to convene a conversation</p>
<p>and one that could also reflect our values.</p>
<p>I&rsquo;m still feeling my way on that issue.</p>
<p>I think global politics is not my specialty.</p>
<p>I&rsquo;m an AI researcher,</p>
<p>but I have moved towards policy in recent months, really,</p>
<p>because of my great concern about all of these risks.</p>
<p>I think certainly the UN, UNESCO has its guidelines,</p>
<p>should be involved and at the table,</p>
<p>and maybe things work under them,</p>
<p>and maybe they don&rsquo;t,</p>
<p>but they should have a strong voice</p>
<p>and help to develop this.</p>
<p>The OACD has also been thinking greatly about this.</p>
<p>A number of organizations have internationally.</p>
<p>I don&rsquo;t feel like I personally am qualified</p>
<p>to say exactly what the right model is there.</p>
<p>Well, thank you.</p>
<p>I think we need to pursue this,</p>
<p>both at the national level and the international level.</p>
<p>I&rsquo;m the chair of the IP subcommittee</p>
<p>of the Judiciary Committee.</p>
<p>In June and July, we will be having hearings</p>
<p>on the impact of AI on patents and copyrights.</p>
<p>You can already tell from the questions of others,</p>
<p>there&rsquo;ll be a lot of interest.</p>
<p>I look forward to following up with you about that topic.</p>
<p>I know, Mr. Chairman, I&rsquo;m-</p>
<p>I look forward to helping as much as possible.</p>
<p>Thank you very much.</p>
<p>Thanks, Senator Coons.</p>
<p>Senator Kennedy.</p>
<p>Thank you all for being here.</p>
<p>Permit me to share with you three hypotheses</p>
<p>that I would like you to assume for the moment to be true.</p>
<p>Hypothesis number one, many members of Congress</p>
<p>do not understand artificial intelligence.</p>
<p>Hypothesis number two, that absence of understanding</p>
<p>may not prevent Congress from plunging in with enthusiasm</p>
<p>and trying to regulate this technology in a way</p>
<p>that could hurt this technology.</p>
<p>Hypothesis number three that I would like you to assume,</p>
<p>there is likely a berserk wing of the artificial intelligence</p>
<p>community that intentionally or unintentionally</p>
<p>could use artificial intelligence to kill all of us</p>
<p>and hurt us the entire time that we are dying.</p>
<p>Assume all of those to be true.</p>
<p>Please tell me in plain English, two or three reforms,</p>
<p>regulations, if any, that you would implement</p>
<p>if you were a queen or king for a day.</p>
<p>Ms. Montgomery.</p>
<p>I think it comes back again to transparency</p>
<p>and explainability in AI.</p>
<p>We absolutely need to know and have companies attest.</p>
<p>What do you mean by transparency?</p>
<p>Disclosure of the data that&rsquo;s used to train AI,</p>
<p>disclosure of the model and how it performs,</p>
<p>and making sure that there&rsquo;s continuous governance</p>
<p>over these models, that we are the leading edge.</p>
<p>Governance by whom?</p>
<p>Technology governance, organizational governance,</p>
<p>rules and clarification that are needed.</p>
<p>Which rules?</p>
<p>I mean, this is your chance for you to say,</p>
<p>I mean, this is your chance, folks,</p>
<p>to tell us how to get this right.</p>
<p>Please use it.</p>
<p>All right.</p>
<p>I mean, I think, again, the rules should be focused</p>
<p>on the use of AI in certain contexts.</p>
<p>So if you look at, for example, so if you look</p>
<p>at the EU AI Act, it has certain uses of AI</p>
<p>that it says are just simply too dangerous</p>
<p>and will be outlawed in the UK.</p>
<p>OK, so we ought to first pass a law that says</p>
<p>you can use AI for these uses, but not others.</p>
<p>Is that what you&rsquo;re saying?</p>
<p>We need to define the highest risk uses of AI.</p>
<p>Is there anything else?</p>
<p>And then, of course, requiring things</p>
<p>like impact assessments and transparency,</p>
<p>requiring companies to show their work,</p>
<p>protecting data that&rsquo;s used to train AI</p>
<p>in the first place as well.</p>
<p>Professor Marcus, if you could be specific.</p>
<p>This is your shot, man.</p>
<p>Talk in plain English and tell me what,</p>
<p>if any, rules we ought to implement.</p>
<p>And at least don&rsquo;t just use concepts.</p>
<p>I&rsquo;m looking for specificity.</p>
<p>Number one, a safety review like we use</p>
<p>with the FDA prior to widespread deployment.</p>
<p>If you&rsquo;re going to introduce something</p>
<p>to 100 million people, somebody has</p>
<p>to have their eyeballs on it.</p>
<p>There you go.</p>
<p>OK, that&rsquo;s a good one.</p>
<p>I&rsquo;m not sure I agree with it, but that&rsquo;s a good one.</p>
<p>What else?</p>
<p>You didn&rsquo;t ask for three that you would agree with.</p>
<p>Number two, a nimble monitoring agency</p>
<p>to follow what&rsquo;s going on, not just pre-review,</p>
<p>but also post as things are out there</p>
<p>in the world with authority to call things back,</p>
<p>which we&rsquo;ve discussed today.</p>
<p>And number three would be funding geared</p>
<p>towards things like AI constitution,</p>
<p>AI that can reason about what it&rsquo;s doing.</p>
<p>I would not leave things entirely</p>
<p>to current technology, which I think</p>
<p>is poor at behaving in ethical fashion</p>
<p>and behaving in honest fashion.</p>
<p>And so I would have funding to try to basically focus</p>
<p>on AI safety research.</p>
<p>That term has a lot of complications in my field.</p>
<p>There&rsquo;s both safety, let&rsquo;s say, short term and long term.</p>
<p>And I think we need to look at both.</p>
<p>Rather than just funding models to be bigger,</p>
<p>which is the popular thing to do,</p>
<p>we need to find models to be more trustworthy.</p>
<p>Because I want to hear from Mr. Altman.</p>
<p>Mr. Altman, here&rsquo;s your shot.</p>
<p>Thank you, Senator.</p>
<p>Number one, I would form a new agency</p>
<p>that licenses any effort above a certain scale</p>
<p>of capabilities and can take that license away</p>
<p>and ensure compliance with safety standards.</p>
<p>Number two, I would create a set of safety standards</p>
<p>focused on what you said in your third hypothesis</p>
<p>as the dangerous capability evaluations.</p>
<p>One example that we&rsquo;ve used in the past</p>
<p>is looking to see if a model can self-replicate</p>
<p>and self-exfiltrate into the wild.</p>
<p>We can give your office a long other list</p>
<p>of the things that we think are important there,</p>
<p>but specific tests that a model has to pass</p>
<p>before it can be deployed into the world.</p>
<p>And then third, I would require independent audits.</p>
<p>So not just from the company or the agency,</p>
<p>but experts who can say the model is or isn&rsquo;t in compliance</p>
<p>with these stated safety thresholds</p>
<p>and these percentages of performance on question X or Y.</p>
<p>Can you send me that information?</p>
<p>We will do that.</p>
<p>Would you be qualified if we promulgated those rules</p>
<p>to administer those rules?</p>
<p>I love my current job.</p>
<p>Cool.</p>
<p>Are there people out there that would be qualified?</p>
<p>We&rsquo;d be happy to send you recommendations</p>
<p>for people out there, yes.</p>
<p>Okay.</p>
<p>You make a lot of money, do you?</p>
<p>I make, no, I paid enough for health insurance.</p>
<p>I have no equity in OpenAI.</p>
<p>Really? That&rsquo;s interesting.</p>
<p>You need a lawyer.</p>
<p>I need a what?</p>
<p>You need a lawyer or an agent.</p>
<p>I&rsquo;m doing this because I love it.</p>
<p>Thank you, Mr. Chairman.</p>
<p>Thanks, Senator Kennedy.</p>
<p>Senator Hirono.</p>
<p>Thank you, Mr. Chairman.</p>
<p>I&rsquo;m listening to all of you testifying.</p>
<p>Thank you very much for being here.</p>
<p>Clearly, AI truly is a game-changing tool,</p>
<p>and we need to get the regulation of this tool right</p>
<p>because my staff, for example, asked AI,</p>
<p>it might have been GPT-4, it might have been,</p>
<p>I don&rsquo;t know, one of the other entities</p>
<p>to create a song that my favorite band, BTS,</p>
<p>a song that they would sing, somebody else&rsquo;s song,</p>
<p>but neither of the artists were involved</p>
<p>in creating what sounded like a really genuine song,</p>
<p>so you can do a lot.</p>
<p>We also asked, can there be a speech created</p>
<p>talking about the Supreme Court decision in Dobbs</p>
<p>and the chaos that it created using my voice,</p>
<p>my kind of voice, and it created a speech</p>
<p>that was really good.</p>
<p>It almost made me think about, you know,</p>
<p>what do I need my staff for?</p>
<p>So don&rsquo;t worry, that&rsquo;s not where we are.</p>
<p>Nervous laughter behind you.</p>
<p>Their jobs are safe.</p>
<p>But there&rsquo;s so much that can be done,</p>
<p>and one of the things that you mentioned, Mr. Altman,</p>
<p>that intrigued me was you said GPT-4</p>
<p>can refuse harmful requests,</p>
<p>so you must have put some thought</p>
<p>into how your system, if I can call it that,</p>
<p>can refuse harmful requests.</p>
<p>What do you consider a harmful request?</p>
<p>You can just keep it short.</p>
<p>Yeah, I&rsquo;ll give a few examples.</p>
<p>One would be about violent content.</p>
<p>Another would be about content that&rsquo;s encouraging self-harm.</p>
<p>Another&rsquo;s adult content.</p>
<p>Not that we think adult content is inherently harmful,</p>
<p>but there&rsquo;s things that could be associated with that</p>
<p>that we cannot reliably enough differentiate,</p>
<p>so we refuse all of it.</p>
<p>So those are some of the more obvious</p>
<p>harmful kinds of information,</p>
<p>but in the election context, for example,</p>
<p>I saw a picture of former President Trump</p>
<p>being arrested by NYPD, and that went viral.</p>
<p>I don&rsquo;t know.</p>
<p>Is that considered harmful?</p>
<p>I&rsquo;ve seen all kinds of statements</p>
<p>attributed to any one of us</p>
<p>that could be put out there</p>
<p>that may not rise to your level of harmful content,</p>
<p>but there you have it.</p>
<p>So two of you said that we should have a licensing scheme.</p>
<p>I can&rsquo;t envision or imagine right now</p>
<p>what kind of a licensing scheme we would be able to create</p>
<p>to pretty much regulate the vastness</p>
<p>of this game-changing tool.</p>
<p>So are you thinking of an FTC kind of a system,</p>
<p>an FCC kind of a system?</p>
<p>What do the two of you even envision</p>
<p>as a potential licensing scheme</p>
<p>that would provide the kind of guardrails that we need</p>
<p>to protect literally our country from harmful content?</p>
<p>To touch on the first part of what you said,</p>
<p>there are things besides,</p>
<p>you know, should this content be generated or not</p>
<p>that I think are also important.</p>
<p>So that image that you mentioned was generated.</p>
<p>I think it&rsquo;d be a great policy to say</p>
<p>generated images need to be made clear</p>
<p>in all contexts that they were generated.</p>
<p>And, you know, then we still have the image out there,</p>
<p>but we&rsquo;re at least requiring people to say</p>
<p>this was a generated image.</p>
<p>Okay, well, you don&rsquo;t need an entire licensing scheme</p>
<p>in order to make that a reality.</p>
<p>Where I think the licensing scheme comes in</p>
<p>is not for what these models are capable of today,</p>
<p>because as you pointed out,</p>
<p>you don&rsquo;t need a new licensing agency to do that.</p>
<p>But as we head, and, you know, this may take a long time,</p>
<p>I&rsquo;m not sure,</p>
<p>as we head towards artificial general intelligence</p>
<p>and the impact that will have</p>
<p>and the power of that technology,</p>
<p>I think we need to treat that as seriously</p>
<p>as we treat other very powerful technologies.</p>
<p>And that&rsquo;s where I personally think we need such a scheme.</p>
<p>I agree.</p>
<p>And that is why, by the time we&rsquo;re talking about AGI,</p>
<p>we&rsquo;re talking about major harms</p>
<p>that can occur through the use of AGI.</p>
<p>So Professor Marcus, I mean,</p>
<p>what kind of a regulatory scheme would you envision?</p>
<p>And we can&rsquo;t just come up with something,</p>
<p>you know, that is gonna be,</p>
<p>take care of the issues that will arise in the future,</p>
<p>especially with AGI.</p>
<p>So what kind of a scheme would you contemplate?</p>
<p>Well, first, if I can rewind just a moment,</p>
<p>I think you really put your finger</p>
<p>on the central scientific issue</p>
<p>in terms of the challenges</p>
<p>in building artificial intelligence.</p>
<p>We don&rsquo;t know how to build a system</p>
<p>that understands harm in the full breadth of its meaning.</p>
<p>So what we do right now is we gather examples</p>
<p>and we say, is this like the examples</p>
<p>that we have labeled before?</p>
<p>But that&rsquo;s not broad enough.</p>
<p>And so I thought your questioning beautifully outlined</p>
<p>the challenge that AI itself has to face</p>
<p>in order to really deal with this.</p>
<p>We want AI itself to understand harm</p>
<p>and that may require new technology.</p>
<p>So I think that&rsquo;s very important.</p>
<p>On this second part of your question,</p>
<p>the model that I tend to gravitate towards,</p>
<p>but I am not an expert here,</p>
<p>is the FDA, at least as part of it,</p>
<p>in terms of you have to make a safety case</p>
<p>and say why the benefits outweigh the harms</p>
<p>in order to get that license.</p>
<p>Probably we need elements of multiple agencies.</p>
<p>I&rsquo;m not an expert there,</p>
<p>but I think that the safety case part of it</p>
<p>is incredibly important.</p>
<p>You have to be able to have external reviewers</p>
<p>that are scientifically qualified</p>
<p>look at this and say, have you addressed enough?</p>
<p>So I&rsquo;ll just give one specific example.</p>
<p>AutoGPT frightens me.</p>
<p>That&rsquo;s not something that OpenAI made,</p>
<p>but something that OpenAI did make</p>
<p>called ChatGPT plugins led a few weeks later</p>
<p>to some building open source software called AutoGPT.</p>
<p>And what AutoGPT does</p>
<p>is it allows systems to access source code,</p>
<p>access the internet, and so forth.</p>
<p>And there are a lot of potential,</p>
<p>let&rsquo;s say, cybersecurity risks there.</p>
<p>There should be an external agency that says,</p>
<p>well, we need to be reassured</p>
<p>if you&rsquo;re going to release this product</p>
<p>that there aren&rsquo;t gonna be cybersecurity problems</p>
<p>or there are ways of addressing it.</p>
<p>So Professor, I am running out of time.</p>
<p>I just wanted to mention, Ms. Montgomery,</p>
<p>your model is a use model</p>
<p>similar to what the EU has come up with,</p>
<p>but the vastness of AI and the complexities involved,</p>
<p>I think, would require more</p>
<p>than looking at the use of it.</p>
<p>I think that based on what I&rsquo;m hearing today,</p>
<p>don&rsquo;t you think that we&rsquo;re probably gonna need</p>
<p>to do a heck of a lot more</p>
<p>than to focus on what use AI is being used for?</p>
<p>For example, you can ask AI</p>
<p>to come up with a funny joke or something,</p>
<p>but you can use the same,</p>
<p>you can ask the same AI tool</p>
<p>to generate something</p>
<p>that is like an election fraud kind of a situation.</p>
<p>So I don&rsquo;t know how you will make a determination</p>
<p>based on where you&rsquo;re going with the use model,</p>
<p>how to distinguish those kinds of uses of this tool.</p>
<p>So I think that if we&rsquo;re gonna go</p>
<p>toward a licensing kind of a scheme,</p>
<p>we&rsquo;re gonna need to put a lot of thought</p>
<p>into how we&rsquo;re gonna come up with an appropriate scheme</p>
<p>that is going to provide the kind of future reference</p>
<p>that we need to put in place.</p>
<p>So I thank all of you for coming in</p>
<p>and providing further food for thought.</p>
<p>Thank you, Mr. Chairman.</p>
<p>Thanks very much, Senator Hirono.</p>
<p>Senator Padilla.</p>
<p>Thank you, Mr. Chairman.</p>
<p>I appreciate the flexibility</p>
<p>as I&rsquo;ve been back and forth</p>
<p>between this committee</p>
<p>and Homeland Security Committee</p>
<p>where there&rsquo;s a hearing going on right now</p>
<p>on the use of AI in government.</p>
<p>So it&rsquo;s AI day on the hill</p>
<p>or at least the Senate apparently.</p>
<p>Now for folks watching at home,</p>
<p>if you never thought about AI</p>
<p>until the recent emergence of generative AI tools,</p>
<p>the developments in this space</p>
<p>may feel like they&rsquo;ve just happened all of a sudden.</p>
<p>But the fact of the matter is, Mr. Chair,</p>
<p>is that they haven&rsquo;t.</p>
<p>AI is not new, not for government,</p>
<p>not for business, not for the public.</p>
<p>In fact, the public uses AI all the time.</p>
<p>And just for folks to be able to relate,</p>
<p>wanna offer the example of anybody with a smartphone,</p>
<p>many features on your device leverage AI,</p>
<p>including suggested replies, right?</p>
<p>When we&rsquo;re text messaging</p>
<p>or even to email auto-correct features,</p>
<p>including but not limited to spelling</p>
<p>in our email and text applications.</p>
<p>So I&rsquo;m frankly excited to explore</p>
<p>how we can facilitate positive AI innovation</p>
<p>that benefits society</p>
<p>while addressing some of the already known harms</p>
<p>and biases that stem from the development</p>
<p>and use of the tools today.</p>
<p>Now with language models</p>
<p>becoming increasingly ubiquitous,</p>
<p>I wanna make sure that there&rsquo;s a focus</p>
<p>on ensuring equitable treatment</p>
<p>of diverse demographic groups.</p>
<p>My understanding is that most research</p>
<p>in to evaluating and mitigating fairness harms</p>
<p>has been concentrated on the English language,</p>
<p>while non-English languages</p>
<p>have received comparatively little attention or investment.</p>
<p>And we&rsquo;ve seen this problem before.</p>
<p>I&rsquo;ll tell you why I raised this.</p>
<p>Social media companies, for example,</p>
<p>have not adequately invested in content moderation,</p>
<p>tools and resources for their non-English language.</p>
<p>And I share this not just out of concern</p>
<p>for non-US-based users,</p>
<p>but so many US-based users prefer a language</p>
<p>other than English in their communication.</p>
<p>So I&rsquo;m deeply concerned</p>
<p>about repeating social media&rsquo;s failure</p>
<p>in AI tools and applications.</p>
<p>Question, Mr. Altman and Ms. Montgomery,</p>
<p>how are open AI and IBM</p>
<p>ensuring language and cultural inclusivity</p>
<p>that they&rsquo;re in their large language models?</p>
<p>And is even an area of focus</p>
<p>in the development of your products?</p>
<p>So bias and equity in technology</p>
<p>is a focus of ours and always has been.</p>
<p>I think diversity in terms of the development of the tools,</p>
<p>in terms of their deployment.</p>
<p>So having diverse people</p>
<p>that are actually training those tools,</p>
<p>considering the downstream effects as well.</p>
<p>We&rsquo;re also very cautious,</p>
<p>very aware of the fact</p>
<p>that we can&rsquo;t just be articulating</p>
<p>and calling for these types of things</p>
<p>without having the tools and the technology</p>
<p>to test for bias and to apply governance</p>
<p>across the life cycle of AI.</p>
<p>So we were one of the first teams and companies</p>
<p>to put toolkits on the market,</p>
<p>deploy them, contribute them to open source</p>
<p>that will do things like help to address,</p>
<p>be the technical aspects</p>
<p>in which we help to address issues like bias.</p>
<p>Okay, can you speak just for a second</p>
<p>specifically to language inclusivity?</p>
<p>Yeah, I mean, language.</p>
<p>So we don&rsquo;t have a consumer platform,</p>
<p>but we are very actively involved</p>
<p>with ensuring that the technology we help to deploy</p>
<p>and the large language models</p>
<p>that we use in helping our clients</p>
<p>to deploy technology is focused on</p>
<p>and available in many languages.</p>
<p>Thank you.</p>
<p>Michelle.</p>
<p>We think this is really important.</p>
<p>One example is that we worked</p>
<p>with the government of Iceland,</p>
<p>which is a language with fewer speakers</p>
<p>than many of the languages</p>
<p>that are well-represented on the internet</p>
<p>to ensure that their language was included in our model.</p>
<p>And we&rsquo;ve had many similar conversations</p>
<p>and I look forward to many similar partnerships</p>
<p>with lower resource languages</p>
<p>to get them into our models.</p>
<p>GPT-4 is unlike previous models of ours,</p>
<p>which were good at English</p>
<p>and not very good at other languages,</p>
<p>now pretty good at a large number of languages.</p>
<p>You can go pretty far down the list,</p>
<p>ranked by number of speakers</p>
<p>and still get good performance.</p>
<p>But for these very small languages,</p>
<p>we&rsquo;re excited about custom partnerships</p>
<p>to include that language into our model run.</p>
<p>And the part of the question you asked about values</p>
<p>and making sure that cultures are included,</p>
<p>we&rsquo;re equally focused on that,</p>
<p>excited to work with people</p>
<p>who have particular data sets</p>
<p>and to work to collect a representative set of values</p>
<p>from around the world to draw these wide bounds</p>
<p>of what the system can do.</p>
<p>I also appreciate what you said</p>
<p>about the benefits of these systems</p>
<p>and wanting to make sure we get those</p>
<p>to as wide of a group as possible.</p>
<p>I think these systems will have lots of positive impact</p>
<p>on a lot of people,</p>
<p>but in particular,</p>
<p>historically underrepresented groups in technology,</p>
<p>people who have not had as much access</p>
<p>to technology around the world,</p>
<p>this technology seems like it can be a big lift up.</p>
<p>Great.</p>
<p>And I know my question was specific</p>
<p>to language inclusivity,</p>
<p>but I&rsquo;m glad there&rsquo;s agreement</p>
<p>on the broader commitment to diversity and inclusion.</p>
<p>And I&rsquo;ll just give a couple more reasons</p>
<p>why I think it&rsquo;s so critical.</p>
<p>You know, the largest actors in this space</p>
<p>can afford the massive amount of data,</p>
<p>the computing power,</p>
<p>and they have the financial resources necessary</p>
<p>to develop complex AI systems.</p>
<p>But in this space,</p>
<p>we haven&rsquo;t seen from a workforce standpoint,</p>
<p>the racial and gender diversity reflective</p>
<p>of the United States of America.</p>
<p>And we risk, if we&rsquo;re not thoughtful about it,</p>
<p>contributing to the development of tools</p>
<p>and approaches that only exacerbate</p>
<p>the bias and inequities that exist in our society.</p>
<p>So a lot of follow-up work to do there.</p>
<p>In my time remaining,</p>
<p>I do want to ask one more question.</p>
<p>This committee and the public are right to pay attention</p>
<p>to the emergence of generative AI.</p>
<p>This technology has a different opportunity</p>
<p>and risk profile than other AI tools.</p>
<p>And these applications have felt very tangible</p>
<p>for the public due to the nature of the user interface</p>
<p>and the outputs that they produce.</p>
<p>But I don&rsquo;t think we should lose sight</p>
<p>of the broader AI ecosystem</p>
<p>as you consider AI&rsquo;s broader impact on society,</p>
<p>as well as the design of appropriate safeguards.</p>
<p>So Ms. Montgomery, in your testimony,</p>
<p>as you noted, AI is not you.</p>
<p>Can you highlight some of the different applications</p>
<p>that the public and policymakers</p>
<p>should also keep in mind</p>
<p>as we consider possible regulations?</p>
<p>MS. MONTGOMERY Yeah.</p>
<p>I mean, I think the generative AI systems</p>
<p>that are available today are creating new issues</p>
<p>that need to be studied,</p>
<p>new issues around the potential to generate content</p>
<p>that could be extremely misleading,</p>
<p>deceptive, and the like.</p>
<p>So those issues absolutely need to be studied.</p>
<p>But we shouldn&rsquo;t also ignore the fact that AI is a tool.</p>
<p>It&rsquo;s been around for a long time.</p>
<p>It has capabilities beyond just generative capabilities.</p>
<p>And again, that&rsquo;s why I think going back to this approach</p>
<p>where we&rsquo;re regulating AI,</p>
<p>where it&rsquo;s touching people and society</p>
<p>is a really important way to address it.</p>
<p>CHAIRMAN POWELL.</p>
<p>Thank you.</p>
<p>Thank you, Mr. Chair.</p>
<p>CHAIRMAN POWELL.</p>
<p>Thanks, Senator Pia.</p>
<p>Senator Booker is next,</p>
<p>but I think he&rsquo;s going to defer to Senator Ossoff.</p>
<p>CHAIRMAN POWELL.</p>
<p>It&rsquo;s because Senator Ossoff is a very big deal.</p>
<p>I don&rsquo;t know if you&hellip;</p>
<p>SENATOR OSSOFF.</p>
<p>I have a meeting at noon,</p>
<p>and I&rsquo;m grateful to you, Senator Booker,</p>
<p>for yielding your time.</p>
<p>You are, as always, brilliant and handsome.</p>
<p>And thank you to the panelists for joining us.</p>
<p>Thank you to the subcommittee leadership</p>
<p>for opening this up to all committee members.</p>
<p>If we&rsquo;re going to contemplate a regulatory framework,</p>
<p>we&rsquo;re going to have to define what it is that we&rsquo;re regulating.</p>
<p>So, you know, Mr. Alban, any such law</p>
<p>will have to include a section</p>
<p>that defines the scope of regulated activities,</p>
<p>technologies, tools, products.</p>
<p>Just take a stab at it.</p>
<p>MR. ALBAN.</p>
<p>Yeah.</p>
<p>Thanks for asking, Senator Ossoff.</p>
<p>I think it&rsquo;s super important.</p>
<p>I think there are very different levels here,</p>
<p>and I think it&rsquo;s important that any new approach,</p>
<p>any new law does not stop the innovation</p>
<p>from happening with smaller companies,</p>
<p>open-source models,</p>
<p>researchers that are doing work at a smaller scale.</p>
<p>That&rsquo;s a wonderful part of this ecosystem and of America,</p>
<p>and we don&rsquo;t want to slow that down.</p>
<p>There still may need to be some rules there,</p>
<p>but I think we could draw a line at systems</p>
<p>that need to be licensed in a very intense way.</p>
<p>The easiest way to do it,</p>
<p>I&rsquo;m not sure if it&rsquo;s the best,</p>
<p>but the easiest would be to talk about the amount of compute</p>
<p>that goes into such a model.</p>
<p>So we could define a threshold of compute,</p>
<p>and it&rsquo;ll have to go, it&rsquo;ll have to change.</p>
<p>It could go up or down,</p>
<p>down as we discover more efficient algorithms</p>
<p>that says above this amount of compute,</p>
<p>you are in this regime.</p>
<p>What I would prefer, it&rsquo;s hard to do,</p>
<p>but I think more accurate,</p>
<p>is to define some capability thresholds</p>
<p>and say a model that can do things X, Y, and Z,</p>
<p>up to you all to decide.</p>
<p>That&rsquo;s now in this licensing regime,</p>
<p>but models that are less capable,</p>
<p>you know, we don&rsquo;t want to stop our open-source community.</p>
<p>We don&rsquo;t want to stop individual researchers.</p>
<p>We don&rsquo;t want to stop new startups,</p>
<p>can proceed, you know, with a different framework.</p>
<p>Thank you.</p>
<p>As concisely as you can,</p>
<p>please state which capabilities you&rsquo;d propose</p>
<p>we consider for the purposes of this definition.</p>
<p>I would love, rather than to do that off the cuff,</p>
<p>to follow up with your office with like a follow-up.</p>
<p>Well, perhaps opine,</p>
<p>opine understanding that you&rsquo;re just responding,</p>
<p>and you&rsquo;re not making law.</p>
<p>All right, in the spirit of just opining,</p>
<p>I think a model that can persuade,</p>
<p>manipulate, influence a person&rsquo;s behavior,</p>
<p>or a person&rsquo;s beliefs,</p>
<p>that would be a good threshold.</p>
<p>I think a model that could help create</p>
<p>novel biological agents would be a great threshold.</p>
<p>Things like that.</p>
<p>I want to talk about,</p>
<p>the predictive capabilities of the technology,</p>
<p>and we&rsquo;re going to have to think about</p>
<p>a lot of very complicated constitutional questions</p>
<p>that arise from it.</p>
<p>With massive data sets,</p>
<p>the integrity and accuracy with which</p>
<p>such technology can predict future human behaviors,</p>
<p>potentially pretty significant</p>
<p>at the individual level, correct?</p>
<p>I think we don&rsquo;t know the answer to that for sure,</p>
<p>but let&rsquo;s say it can at least have some impact there.</p>
<p>Okay, so we may be confronted by situations where,</p>
<p>for example, a law enforcement agency</p>
<p>deploying such technology</p>
<p>seeks some kind of judicial consent to execute a search,</p>
<p>or to take some other police action</p>
<p>on the basis of a modeled prediction</p>
<p>about some individual&rsquo;s behavior.</p>
<p>But that&rsquo;s very different</p>
<p>from the kind of evidentiary predicate</p>
<p>that normally police would take to a judge</p>
<p>in order to get a warrant.</p>
<p>Talk me through how you think that would work.</p>
<p>I&rsquo;m thinking about that issue.</p>
<p>Yeah, I think it&rsquo;s very important</p>
<p>that we continue to understand</p>
<p>that these are tools that humans use</p>
<p>to make human judgments,</p>
<p>and that we don&rsquo;t take away human judgment.</p>
<p>I don&rsquo;t think that people should be prosecuted</p>
<p>based off of the output of an AI system, for example.</p>
<p>We have no national privacy law.</p>
<p>Europe has rolled one out to mixed reviews.</p>
<p>Do you think we need one?</p>
<p>I think it&rsquo;d be good.</p>
<p>What would be the qualities or purposes of such a law</p>
<p>that you think would make the most sense</p>
<p>based on your experience?</p>
<p>Again, this is very far out of my area of expertise.</p>
<p>I think there&rsquo;s many, many people</p>
<p>that are privacy experts</p>
<p>that could weigh on what a law needs.</p>
<p>I&rsquo;d still like you to weigh in.</p>
<p>I think a minimum is that users</p>
<p>should be able to opt out</p>
<p>from having their data used</p>
<p>by companies like ours</p>
<p>or the social media companies.</p>
<p>It should be easy to delete your data.</p>
<p>I think those are&hellip;</p>
<p>But the thing that I think is important</p>
<p>from my perspective running an AI company</p>
<p>is that if you don&rsquo;t want your data</p>
<p>used for training these systems,</p>
<p>you have the right to do that.</p>
<p>So let&rsquo;s think about</p>
<p>how that would be practically implemented.</p>
<p>I mean, as I understand it,</p>
<p>your tool and certainly similar tools,</p>
<p>one of the inputs will be scraping,</p>
<p>for lack of a better word,</p>
<p>data off of the open web, right,</p>
<p>as a low cost way of gathering information.</p>
<p>And there&rsquo;s a vast amount of information</p>
<p>out there about all of us.</p>
<p>How would such a restriction</p>
<p>on the access or use or analysis</p>
<p>of such data be practically implemented?</p>
<p>So I was speaking about something</p>
<p>a little bit different,</p>
<p>which is the data that someone generates,</p>
<p>the questions they ask our system,</p>
<p>things that they input,</p>
<p>they&rsquo;re training on that.</p>
<p>Data that&rsquo;s on the public web,</p>
<p>that&rsquo;s accessible,</p>
<p>even if we don&rsquo;t train on that,</p>
<p>the models can certainly link out to it.</p>
<p>So that was not what I was referring to.</p>
<p>I think that, you know,</p>
<p>there&rsquo;s ways to have your data</p>
<p>or there should be more ways</p>
<p>to have your data taken down</p>
<p>from the public web,</p>
<p>but certainly models</p>
<p>with web browsing capabilities</p>
<p>will be able to search the web</p>
<p>and link out to it.</p>
<p>When you think about implementing</p>
<p>a safety or a regulatory regime</p>
<p>to constrain such software</p>
<p>and to mitigate some risk,</p>
<p>is your view that the federal government</p>
<p>would make laws</p>
<p>such that certain capabilities</p>
<p>or functionalities themselves</p>
<p>are forbidden in potential?</p>
<p>In other words,</p>
<p>one cannot deploy</p>
<p>or execute code capable of X?</p>
<p>Yes.</p>
<p>Or is it the act itself,</p>
<p>X only when actually executed?</p>
<p>Well, I think both.</p>
<p>I&rsquo;m a believer in defense in depth.</p>
<p>I think that there should be limits</p>
<p>on what a deployed model is capable of</p>
<p>and then what it actually does to.</p>
<p>How are you thinking</p>
<p>about how kids use your product?</p>
<p>We, well, you have to be,</p>
<p>I mean, you have to be 18 or up</p>
<p>or have your parents permission</p>
<p>at 13 and up to use a product.</p>
<p>But we understand that people</p>
<p>get around those safeguards all the time.</p>
<p>And so we try to do</p>
<p>is just design a safe product.</p>
<p>And there are decisions that we make</p>
<p>that we would allow</p>
<p>if we knew only adults were using it,</p>
<p>that we just don&rsquo;t allow in the product</p>
<p>because we know children</p>
<p>will use it some way or other too.</p>
<p>In particular,</p>
<p>given how much these systems</p>
<p>are being used in education,</p>
<p>we like want to be aware</p>
<p>that that&rsquo;s happening.</p>
<p>I think what and Senator Blumenthal</p>
<p>has done extensive work</p>
<p>investigating this,</p>
<p>what we&rsquo;ve seen repeatedly</p>
<p>is that companies</p>
<p>whose revenues depend upon volume of use,</p>
<p>screen time, intensity of use,</p>
<p>design these systems</p>
<p>in order to maximize</p>
<p>the engagement of all users,</p>
<p>including children,</p>
<p>with perverse results in many cases.</p>
<p>And what I would humbly advise you</p>
<p>is that you get way ahead of this issue,</p>
<p>the safety for children of your product,</p>
<p>or I think you&rsquo;re going to find</p>
<p>that Senator Blumenthal,</p>
<p>Senator Hawley,</p>
<p>others on the subcommittee,</p>
<p>and I will look very harshly</p>
<p>on the deployment of technology</p>
<p>that harms children.</p>
<p>We couldn&rsquo;t agree more.</p>
<p>I think we&rsquo;re out of time,</p>
<p>but I&rsquo;m happy to talk about that</p>
<p>if I can respond.</p>
<p>Go ahead.</p>
<p>It&rsquo;s up to the chairman.</p>
<p>OK.</p>
<p>I, first of all,</p>
<p>I think we try to design systems</p>
<p>that do not maximize for engagement.</p>
<p>In fact, we&rsquo;re so short on GPUs.</p>
<p>The less people use our products,</p>
<p>the better.</p>
<p>But we&rsquo;re not an advertising-based model.</p>
<p>We&rsquo;re not trying to get people</p>
<p>to use it more and more.</p>
<p>And I think that&rsquo;s a different shape</p>
<p>than ad-supported social media.</p>
<p>Second, these systems</p>
<p>do have the capability</p>
<p>to influence in obvious</p>
<p>and in very nuanced ways.</p>
<p>And I think that&rsquo;s particularly important</p>
<p>for the safety of children,</p>
<p>but that will impact all of us.</p>
<p>One of the things that we&rsquo;ll do ourselves,</p>
<p>regulation or not,</p>
<p>but I think a regulatory approach</p>
<p>would be good for also,</p>
<p>is requirements about how the values</p>
<p>of these systems are set</p>
<p>and how these systems respond to questions</p>
<p>that can cause influence.</p>
<p>So we&rsquo;d love to partner with you.</p>
<p>Couldn&rsquo;t agree more on the importance.</p>
<p>Thank you.</p>
<p>Mr. Chairman, for the record,</p>
<p>I just want to say that</p>
<p>the senator from Georgia</p>
<p>is also very handsome and brilliant too.</p>
<p>But I will allow that comment</p>
<p>to stand without objection.</p>
<p>Without objection, okay.</p>
<p>Mr. Chairman and ranking members.</p>
<p>You are now recognized.</p>
<p>Thank you very much.</p>
<p>Thank you.</p>
<p>It&rsquo;s nice that we finally got down</p>
<p>to the bald guys down here at the end.</p>
<p>I just want to thank you both.</p>
<p>This has been one of the best hearings</p>
<p>I&rsquo;ve had this Congress</p>
<p>and just a testimony to you two</p>
<p>as seeing the challenges</p>
<p>and the opportunities that AI presents.</p>
<p>So I appreciate you both.</p>
<p>I want to just jump in</p>
<p>I want to just jump in,</p>
<p>I think very broadly,</p>
<p>and then I&rsquo;ll get a little more narrow.</p>
<p>Sam, you said very broadly,</p>
<p>technology has been moving like this</p>
<p>and a lot of people</p>
<p>have been talking about regulation.</p>
<p>And so I use the example of the automobile.</p>
<p>What an extraordinary piece of technology.</p>
<p>I mean, New York City did not know</p>
<p>what to do with horse manure.</p>
<p>They were having crises,</p>
<p>forming commissions,</p>
<p>and the automobile comes along,</p>
<p>ends that problem.</p>
<p>But at the same time,</p>
<p>we have tens of thousands of people</p>
<p>dying on highways every day.</p>
<p>We have emissions crises and the like.</p>
<p>There are multiple federal agencies,</p>
<p>multiple federal agencies</p>
<p>that were created</p>
<p>or are specifically focused</p>
<p>on regulating cars.</p>
<p>And so this idea</p>
<p>that this equally transforming technology</p>
<p>is coming</p>
<p>and for Congress to do nothing,</p>
<p>which is not what anybody here</p>
<p>is calling for,</p>
<p>little or nothing,</p>
<p>is obviously unacceptable.</p>
<p>I really appreciate Senator Welch</p>
<p>and I who&rsquo;ve been going back</p>
<p>and forth during this hearing</p>
<p>and him and Bennett have a bill</p>
<p>talking about trying to regulate</p>
<p>in this space.</p>
<p>Not doing so for social media</p>
<p>has been, I think, very destructive</p>
<p>and allowed a lot of things to go on</p>
<p>that are really causing a lot of harm.</p>
<p>And so the question is,</p>
<p>is what kind of regulation?</p>
<p>You all have spoken that</p>
<p>to a lot of my colleagues.</p>
<p>And I want to say,</p>
<p>Ms. Montgomery,</p>
<p>and I have to give full disclosure,</p>
<p>I&rsquo;m the child of two IBM parents.</p>
<p>But you talked about</p>
<p>defining the highest risk uses.</p>
<p>We don&rsquo;t know all of them.</p>
<p>We really don&rsquo;t.</p>
<p>We can&rsquo;t see where this is going.</p>
<p>Regulating at the point of risk.</p>
<p>And you sort of called not for an agency.</p>
<p>And I think when somebody else</p>
<p>asks you to specify,</p>
<p>because you don&rsquo;t want to slow things down,</p>
<p>we should build on what we have in place.</p>
<p>But you can envision</p>
<p>that we can try to work</p>
<p>on two different ways</p>
<p>that ultimately a specific,</p>
<p>like we have in cars,</p>
<p>EPA, NHTSA,</p>
<p>the Federal Motor Car</p>
<p>Carrier Safety Administration,</p>
<p>all of these things,</p>
<p>you can imagine something specific</p>
<p>that is, as Mr. Marcus points out,</p>
<p>a nimble agency</p>
<p>that could do monitoring, other things.</p>
<p>You can imagine the need</p>
<p>for something like that, correct?</p>
<p>Oh, absolutely, yeah.</p>
<p>And so just for the record then,</p>
<p>in addition to trying to regulate</p>
<p>with what we have now,</p>
<p>you would encourage Congress</p>
<p>and my colleague, Senator Welsh,</p>
<p>to move forward in trying to figure out</p>
<p>the right tailored agency</p>
<p>to deal with what we know</p>
<p>and perhaps things</p>
<p>that might come up in the future.</p>
<p>I would encourage Congress</p>
<p>to make sure it understands</p>
<p>the technology,</p>
<p>has the skills and resources in place</p>
<p>to impose regulatory requirements</p>
<p>on the uses of the technology</p>
<p>and to understand emerging risks as well.</p>
<p>So, yes.</p>
<p>Mr. Marcus, there&rsquo;s no way</p>
<p>to put this genie in the bottle.</p>
<p>Globally, it&rsquo;s exploding.</p>
<p>I appreciate your thoughts</p>
<p>and I shared some with my staff</p>
<p>about your ideas</p>
<p>of what the international context is,</p>
<p>but there&rsquo;s no way</p>
<p>to stop this moving forward.</p>
<p>So, with that understanding,</p>
<p>just building on what Ms. Montgomery said,</p>
<p>what kind of encouragement</p>
<p>do you have as specifically as possible</p>
<p>to forming an agency,</p>
<p>to using current rules and regulations?</p>
<p>Can you just put some clarity</p>
<p>on what you&rsquo;ve already stated?</p>
<p>Let me just insert,</p>
<p>there are more genies</p>
<p>yet to come from more bottles.</p>
<p>Some genies are already out,</p>
<p>but we don&rsquo;t have machines</p>
<p>that can really, for example,</p>
<p>self-improve themselves.</p>
<p>We don&rsquo;t really have machines</p>
<p>that have self-awareness</p>
<p>and we might not ever want to go there.</p>
<p>So, there are other genies</p>
<p>to be concerned about.</p>
<p>On to the main part of your question.</p>
<p>I think that we need to have</p>
<p>some international meetings</p>
<p>very quickly with people</p>
<p>who have expertise</p>
<p>in how you grow agencies,</p>
<p>in the history of growing agencies.</p>
<p>We need to do that in the federal level.</p>
<p>We need to do that</p>
<p>in the international level.</p>
<p>I&rsquo;ll just emphasize one thing.</p>
<p>I haven&rsquo;t as much as I would like to,</p>
<p>which is that I think science</p>
<p>has to be a really important part of it.</p>
<p>And I&rsquo;ll give an example.</p>
<p>We&rsquo;ve talked about misinformation.</p>
<p>We don&rsquo;t really have the tools right now</p>
<p>to detect and label misinformation</p>
<p>with nutrition labels</p>
<p>that we would like to.</p>
<p>We have to build new technologies for that.</p>
<p>We don&rsquo;t really have tools yet</p>
<p>to detect a wide uptick</p>
<p>in cybercrime, probably.</p>
<p>We probably need new tools there.</p>
<p>We need science to probably help us</p>
<p>to figure out what we need to build</p>
<p>and also what it is</p>
<p>that we need to have transparency around.</p>
<p>Understood, understood.</p>
<p>Sam, just going to you</p>
<p>for the little bit of time I have left.</p>
<p>Real quick, first of all,</p>
<p>you&rsquo;re a bit of a unicorn</p>
<p>when I sat down with you first.</p>
<p>Could you explain why non-profit,</p>
<p>in other words,</p>
<p>you&rsquo;re not looking at this</p>
<p>and you&rsquo;ve even capped the VC people.</p>
<p>Just really quickly,</p>
<p>I want folks to understand that.</p>
<p>We started as a non-profit,</p>
<p>really focused on how this technology</p>
<p>was going to be built.</p>
<p>At the time, it was very</p>
<p>outside the Overton window.</p>
<p>Something like AGI was even possible.</p>
<p>That shifted a lot.</p>
<p>We didn&rsquo;t know at the time</p>
<p>how important scale was going to be,</p>
<p>but we did know that we wanted to build this</p>
<p>with humanity&rsquo;s best interest at heart</p>
<p>and a belief that this technology could,</p>
<p>if it goes the way we want,</p>
<p>if we can do some of those things</p>
<p>Professor Marcus mentioned,</p>
<p>really deeply transform the world.</p>
<p>We wanted to be as much of a force</p>
<p>for getting to a positive.</p>
<p>I&rsquo;m going to interrupt you.</p>
<p>I think that&rsquo;s all good.</p>
<p>I hope more of that gets out on the record.</p>
<p>The second part of my question</p>
<p>I found it fascinating.</p>
<p>Are you ever going to,</p>
<p>for a revenue model,</p>
<p>for return on your investors,</p>
<p>are you ever going to do ads</p>
<p>or something like that?</p>
<p>I wouldn&rsquo;t say never.</p>
<p>I think there may be people</p>
<p>that we want to offer services to</p>
<p>and there&rsquo;s no other model that works,</p>
<p>but I really like having</p>
<p>a subscription-based model.</p>
<p>We have API developers pay us</p>
<p>and we have chat GPT pay us.</p>
<p>Okay, then can I just jump real quickly?</p>
<p>One of my biggest concerns</p>
<p>about this space</p>
<p>is what I&rsquo;ve already seen</p>
<p>in the space of Web 2, Web 3</p>
<p>is this massive corporate concentration.</p>
<p>It is really terrifying to see</p>
<p>how few companies now control</p>
<p>and affect the lives of so many of us.</p>
<p>These companies are getting bigger</p>
<p>and more powerful.</p>
<p>I see OpenAI backed by Microsoft.</p>
<p>Anthropic is backed by Google.</p>
<p>Google has its own in-house product.</p>
<p>So I&rsquo;m really worried about that</p>
<p>and I&rsquo;m wondering if Sam,</p>
<p>you can give me a quick</p>
<p>acknowledgement.</p>
<p>Are you worried about the corporate</p>
<p>concentration in this space</p>
<p>and what effect it might have</p>
<p>in the associated risks</p>
<p>perhaps with market concentration in AI?</p>
<p>And then Mr. Marcus,</p>
<p>can you answer that as well?</p>
<p>I think there will be many people</p>
<p>that develop models.</p>
<p>What&rsquo;s happening now</p>
<p>in the open source community is amazing,</p>
<p>but there will be a relatively</p>
<p>small number of providers</p>
<p>that can make models at the true edge.</p>
<p>I think there is benefits</p>
<p>and danger to that</p>
<p>because we&rsquo;re talking about</p>
<p>the dangers with AI.</p>
<p>The fewer of us that you really have</p>
<p>to keep a careful eye on</p>
<p>on the absolute bleeding edge</p>
<p>of capabilities,</p>
<p>there&rsquo;s benefits there.</p>
<p>I think there needs to be enough</p>
<p>and there will</p>
<p>because there&rsquo;s so much value</p>
<p>that consumers have choice</p>
<p>that we have different ideas.</p>
<p>Mr. Marcus, real quick.</p>
<p>There is a real risk</p>
<p>of a kind of technocracy</p>
<p>combined with oligarchy</p>
<p>where a small number of companies</p>
<p>influence people&rsquo;s beliefs</p>
<p>through the nature of these systems.</p>
<p>Again, I put something in the record</p>
<p>about the Wall Street Journal</p>
<p>about how these systems</p>
<p>can subtly shape our beliefs</p>
<p>and that has enormous influence</p>
<p>on how we live our lives.</p>
<p>And having a small number of players</p>
<p>do that with data</p>
<p>that we don&rsquo;t even know about,</p>
<p>that scares me.</p>
<p>Sam, I&rsquo;m sorry.</p>
<p>One more thing I wanted to add.</p>
<p>One thing that I think</p>
<p>is very important</p>
<p>is that what these systems</p>
<p>get aligned to,</p>
<p>whose values,</p>
<p>what those bounds are,</p>
<p>that that is somehow set</p>
<p>by society as a whole,</p>
<p>by governments as a whole.</p>
<p>And so creating that data set,</p>
<p>the alignment data set,</p>
<p>it could be an AI constitution,</p>
<p>whatever it is,</p>
<p>that has got to come</p>
<p>very broadly from society.</p>
<p>Thank you very much, Mr. Chairman.</p>
<p>My time&rsquo;s expired</p>
<p>and I guess the best for last.</p>
<p>Thank you, Senator Booker.</p>
<p>Senator Weld.</p>
<p>First of all, I want to thank you,</p>
<p>Senator Blumenthal and you,</p>
<p>Senator Hawley.</p>
<p>This has been a tremendous hearing.</p>
<p>Senators are noted</p>
<p>for their short attention spans,</p>
<p>but I&rsquo;ve sat through this entire hearing</p>
<p>and enjoyed every minute of it.</p>
<p>You have one of our longer attention spans</p>
<p>in the United States.</p>
<p>To your great credit.</p>
<p>Well, we&rsquo;ve had good witnesses</p>
<p>and it&rsquo;s an incredibly important issue.</p>
<p>And here&rsquo;s just,</p>
<p>I don&rsquo;t, all the questions I have</p>
<p>have been asked really,</p>
<p>but here&rsquo;s a kind of a takeaway.</p>
<p>And what I think is the major question</p>
<p>that we&rsquo;re going to have to answer</p>
<p>as a Congress.</p>
<p>Number one, you&rsquo;re here</p>
<p>because AI is this extraordinary</p>
<p>new technology that everyone says</p>
<p>can be transformative</p>
<p>as much as the printing press.</p>
<p>Number two, it&rsquo;s really unknown</p>
<p>what&rsquo;s going to happen,</p>
<p>but there&rsquo;s a big fear</p>
<p>you&rsquo;ve expressed to all of you</p>
<p>about what bad actors can do</p>
<p>and will do</p>
<p>if there&rsquo;s no rules of the road.</p>
<p>Number three,</p>
<p>as a member who served in the House</p>
<p>and now in the Senate,</p>
<p>I&rsquo;ve come to the conclusion</p>
<p>that it&rsquo;s impossible for Congress</p>
<p>to keep up with the speed of technology.</p>
<p>And there have been concerns expressed</p>
<p>about social media</p>
<p>and now about AI</p>
<p>that relate to fundamental privacy rights,</p>
<p>bias rights, intellectual property,</p>
<p>the spread of disinformation,</p>
<p>which in many ways for me</p>
<p>is the biggest threat</p>
<p>because that goes to the core</p>
<p>of our capacity for self-governing.</p>
<p>There&rsquo;s the economic transformation,</p>
<p>which can be profound.</p>
<p>There&rsquo;s safety concerns.</p>
<p>And I&rsquo;ve come to the conclusion</p>
<p>that we absolutely have to have an agency.</p>
<p>What its scope of engagement is,</p>
<p>it has to be defined by us.</p>
<p>But I believe that</p>
<p>unless we have an agency</p>
<p>that is going to address these questions</p>
<p>from some level,</p>
<p>from social media and AI,</p>
<p>we really don&rsquo;t have much of a defense</p>
<p>against the bad stuff.</p>
<p>And the bad stuff will come.</p>
<p>So last year,</p>
<p>I introduced in the House side</p>
<p>and Senator Bennett did in the Senate side,</p>
<p>it was the end of the year,</p>
<p>Digital Commission Act,</p>
<p>and we&rsquo;re going to be reintroducing that this year.</p>
<p>And the two things that I want to ask,</p>
<p>one, you&rsquo;ve somewhat answered</p>
<p>because I think two of the three of you have said</p>
<p>you think we do need an independent commission.</p>
<p>And Congress established an independent commission</p>
<p>when railroads were running rampant</p>
<p>over the interest of farmers,</p>
<p>when Wall Street had no rules of the road</p>
<p>and we had the SEC.</p>
<p>I think we&rsquo;re at that point now.</p>
<p>But what the commission does</p>
<p>would have to be defined and circumscribed.</p>
<p>But also there&rsquo;s always a question</p>
<p>about the use of regulatory authority</p>
<p>and the recognition</p>
<p>that it can be used for good.</p>
<p>J.D. Vance actually mentioned that</p>
<p>when we were considering his and Senator Brown&rsquo;s bill</p>
<p>about railroads in that event in East Palestine,</p>
<p>regulation for the public health.</p>
<p>But there&rsquo;s also a legitimate concern</p>
<p>about regulation getting in the way of things,</p>
<p>being too cumbersome</p>
<p>and being a negative influence.</p>
<p>So A, two of the three of you have said</p>
<p>you think we do need an agency.</p>
<p>What are some of the perils of an agency</p>
<p>that we would have to be mindful of</p>
<p>in order to make certain that its goals</p>
<p>of protecting many of those interests</p>
<p>I just mentioned, privacy, bias,</p>
<p>intellectual property, disinformation,</p>
<p>would be the winners and not the losers?</p>
<p>And I&rsquo;ll start with you, Mr. Altman.</p>
<p>Thank you, Senator.</p>
<p>One, I think America has got to continue to lead.</p>
<p>This happened in America.</p>
<p>I&rsquo;m very proud that it happened in America.</p>
<p>By the way, I think that&rsquo;s right.</p>
<p>And that&rsquo;s why I&rsquo;d be much more confident</p>
<p>if we had our agency as opposed to</p>
<p>got involved in international discussions.</p>
<p>Ultimately, you want the rules of the road.</p>
<p>But I think if we lead and get rules of the road</p>
<p>that work for us,</p>
<p>that is probably a more effective way to proceed.</p>
<p>I personally believe there&rsquo;s a way to do both.</p>
<p>And I think it is important to have the global view on this</p>
<p>because this technology will impact Americans</p>
<p>and all of us wherever it&rsquo;s developed.</p>
<p>But I think we want America to lead.</p>
<p>We want, we want&hellip;</p>
<p>So get to the perils issue though,</p>
<p>because I know&hellip;</p>
<p>Well, that&rsquo;s one.</p>
<p>I mean, that is a peril,</p>
<p>which is you slow down American industry</p>
<p>in such a way that China or somebody else</p>
<p>makes faster progress.</p>
<p>A second, and I think this can happen with like,</p>
<p>the regulatory pressure should be on us.</p>
<p>It should be on Google.</p>
<p>It should be on the other small set of people</p>
<p>in the lead the most.</p>
<p>We don&rsquo;t want to slow down smaller startups.</p>
<p>We don&rsquo;t want to slow down open source efforts.</p>
<p>We still need them to comply with things.</p>
<p>They can still, you can still cause great harm</p>
<p>with a smaller model,</p>
<p>but leaving the room and the space for new ideas</p>
<p>and new companies and independent researchers</p>
<p>to do their work</p>
<p>and not putting a regulatory burden</p>
<p>and say a company like us could handle</p>
<p>but a smaller one couldn&rsquo;t.</p>
<p>I think that&rsquo;s another peril</p>
<p>and it&rsquo;s clearly a way that regulation has gone.</p>
<p>Mr. Marcus or Professor Marcus.</p>
<p>The other obvious peril is regulatory capture.</p>
<p>If we make it as appear as if we are doing something,</p>
<p>but it&rsquo;s more like greenwashing</p>
<p>and nothing really happens.</p>
<p>We just keep out the little players</p>
<p>because we put so much burden</p>
<p>that only the big players can do it.</p>
<p>So there are also those kinds of perils.</p>
<p>I fully agree with everything that Mr. Altman said</p>
<p>and I would add that to the list.</p>
<p>Okay.</p>
<p>Ms. Montgomery.</p>
<p>One of the things I would add to the list</p>
<p>is the risk of not holding companies accountable</p>
<p>for the harms that they&rsquo;re causing today, right?</p>
<p>So we talk about misinformation in electoral systems.</p>
<p>So no agency or no agency,</p>
<p>we need to hold companies responsible today</p>
<p>and accountable for AI that they&rsquo;re deploying</p>
<p>that disseminates misinformation</p>
<p>on things like elections and where the risk is.</p>
<p>You know, a regulatory agency</p>
<p>would do a lot of the things</p>
<p>that Senator Graham was talking about.</p>
<p>You know, you don&rsquo;t build a nuclear reactor</p>
<p>without getting a license.</p>
<p>You don&rsquo;t build an AI system</p>
<p>without getting a license</p>
<p>that gets tested independently.</p>
<p>I think it&rsquo;s a great analogy.</p>
<p>We need both pre-deployment and post-deployment.</p>
<p>Okay.</p>
<p>Thank you all very much.</p>
<p>I yield back, Mr. Chairman.</p>
<p>Thanks.</p>
<p>Thanks, Senator Wells.</p>
<p>Let me ask a few more questions.</p>
<p>You&rsquo;ve all been very, very patient</p>
<p>and the turnout today,</p>
<p>which is beyond our subcommittee,</p>
<p>I think reflects both your value</p>
<p>in what you&rsquo;re contributing</p>
<p>as well as the interest in this topic.</p>
<p>There are a number of subjects</p>
<p>that we haven&rsquo;t covered at all.</p>
<p>One was just alluded to by Professor Marcus,</p>
<p>which is the monopolization danger,</p>
<p>the dominance of markets</p>
<p>that excludes new competition</p>
<p>and thereby inhibits</p>
<p>or prevents innovation and invention,</p>
<p>which we have seen in social media.</p>
<p>As well as some of the old industries,</p>
<p>airlines, automobiles,</p>
<p>and others where consolidation</p>
<p>has narrowed competition.</p>
<p>And so I think we need to focus</p>
<p>on kind of an old area of antitrust,</p>
<p>which dates more than a century,</p>
<p>still inadequate to deal with the challenges</p>
<p>we have right now in our economy.</p>
<p>And certainly we need to be mindful</p>
<p>of the way that rules</p>
<p>can enable the big guys to get bigger</p>
<p>and exclude innovation and competition</p>
<p>and responsible good guys,</p>
<p>such as are represented</p>
<p>in this industry right now.</p>
<p>We haven&rsquo;t dealt with national security.</p>
<p>There are huge implications</p>
<p>for national security.</p>
<p>I will tell you,</p>
<p>as a member of the Armed Services Committee,</p>
<p>classified briefings on this issue</p>
<p>have abounded</p>
<p>and the threats that are posed</p>
<p>by some of our adversaries.</p>
<p>China has been mentioned here,</p>
<p>but the sources of threats to this nation</p>
<p>in this space are very real and urgent.</p>
<p>We&rsquo;re not going to deal with them today,</p>
<p>but we do need to deal with them.</p>
<p>And we will hopefully in this committee.</p>
<p>And then on the issue of a new agency,</p>
<p>you know, I&rsquo;ve been doing this stuff for a while.</p>
<p>I was attorney general of Connecticut for 20 years.</p>
<p>I was a federal prosecutor,</p>
<p>the U.S. attorney.</p>
<p>Most of my career has been in enforcement.</p>
<p>And I will tell you something,</p>
<p>you can create 10 new agencies,</p>
<p>but if you don&rsquo;t give them the resources</p>
<p>and I&rsquo;m talking not just about dollars,</p>
<p>I&rsquo;m talking about scientific expertise,</p>
<p>you guys will run circles around.</p>
<p>And it isn&rsquo;t just the models</p>
<p>or the generative AI</p>
<p>that will run models or run circles around them,</p>
<p>but it is the scientists in your companies.</p>
<p>For every success story in government regulation,</p>
<p>you can think of five failures.</p>
<p>That&rsquo;s true of the FDA.</p>
<p>It&rsquo;s true of the IAEA.</p>
<p>It&rsquo;s true of the SEC.</p>
<p>It&rsquo;s true of the whole alphabet list</p>
<p>of government agencies.</p>
<p>And I hope our experience here will be different.</p>
<p>But the Pandora&rsquo;s box requires</p>
<p>more than just the words</p>
<p>or the concepts licensing new agency.</p>
<p>There&rsquo;s some real hard decision-making</p>
<p>as Ms. Montgomery has alluded to</p>
<p>about how to frame the rules to fit the risks.</p>
<p>First, do no harm.</p>
<p>Make it effective.</p>
<p>Make it enforceable.</p>
<p>Make it real.</p>
<p>I think we need to grapple</p>
<p>with the hard questions here</p>
<p>that frankly this initial hearing</p>
<p>I think has raised very successfully,</p>
<p>but not answered.</p>
<p>And I thank our colleagues</p>
<p>who have participated</p>
<p>and made these very creative suggestions.</p>
<p>I&rsquo;m very interested in enforcement.</p>
<p>I literally 15 years ago,</p>
<p>I think advocated abolishing Section 230.</p>
<p>What&rsquo;s old is new again.</p>
<p>You know, now people are talking</p>
<p>about abolishing Section 230.</p>
<p>Back then it was considered</p>
<p>completely unrealistic.</p>
<p>But enforcement really does matter.</p>
<p>I want to ask Mr. Altman,</p>
<p>because of the privacy issue</p>
<p>and you&rsquo;ve suggested</p>
<p>that you have an interest</p>
<p>in protecting the privacy of the data</p>
<p>that may come to you or be available.</p>
<p>How do you,</p>
<p>what specific steps do you take</p>
<p>to protect privacy?</p>
<p>One is that we don&rsquo;t train on any data</p>
<p>submitted to our API.</p>
<p>So if you&rsquo;re a business customer of ours</p>
<p>and submit data,</p>
<p>we don&rsquo;t train on it at all.</p>
<p>We do retain it for 30 days</p>
<p>solely for the purpose</p>
<p>of trust and safety enforcement.</p>
<p>But that&rsquo;s different than training on it.</p>
<p>If you use ChatGPT,</p>
<p>you can opt out of us</p>
<p>training on your data.</p>
<p>You can also delete your conversation history</p>
<p>or your whole account.</p>
<p>Ms. Montgomery,</p>
<p>I know you don&rsquo;t deal directly with consumers,</p>
<p>but do you take steps</p>
<p>to protect privacy as well?</p>
<p>Absolutely.</p>
<p>And we even filter</p>
<p>our large language models for content</p>
<p>that includes personal information</p>
<p>that may have been pulled</p>
<p>from public data sets as well.</p>
<p>So we apply additional level of filtering.</p>
<p>Professor Marcus,</p>
<p>you made reference to self-awareness,</p>
<p>self-learning.</p>
<p>Already we&rsquo;re talking about</p>
<p>the potential for jailbreaks.</p>
<p>How soon do you think</p>
<p>that new kind of generative AI</p>
<p>will be usable,</p>
<p>will be practical?</p>
<p>New AI that is self-aware and so forth?</p>
<p>Yes.</p>
<p>I mean, I have no idea on that one.</p>
<p>I think we don&rsquo;t really understand</p>
<p>what self-awareness is,</p>
<p>and so it&rsquo;s hard to put a date on it.</p>
<p>In terms of self-improvement,</p>
<p>there&rsquo;s some modest self-improvement</p>
<p>in current systems,</p>
<p>but one could imagine a lot more,</p>
<p>and that could happen in two years.</p>
<p>It could happen in 20 years.</p>
<p>There are basic paradigms</p>
<p>that haven&rsquo;t been invented yet.</p>
<p>Some of them we might want to discourage,</p>
<p>but it&rsquo;s a bit hard</p>
<p>to put timelines on them.</p>
<p>Just going back to enforcement</p>
<p>for one second,</p>
<p>one thing that is absolutely paramount,</p>
<p>I think,</p>
<p>is far greater transparency</p>
<p>about what the models are</p>
<p>and what the data are.</p>
<p>That doesn&rsquo;t necessarily mean</p>
<p>everybody in the general public</p>
<p>has to know exactly</p>
<p>what&rsquo;s in one of these systems,</p>
<p>but I think it means</p>
<p>that there needs to be</p>
<p>some enforcement arm</p>
<p>that can look at these systems,</p>
<p>can look at the data,</p>
<p>can perform tests and so forth.</p>
<p>Let me ask you, all of you,</p>
<p>I think there has been a reference</p>
<p>to elections</p>
<p>and banning outputs</p>
<p>involving elections.</p>
<p>Are there other areas</p>
<p>where you think,</p>
<p>what are the other high risk</p>
<p>or highest risk areas</p>
<p>where you would either ban</p>
<p>or establish especially strict rules?</p>
<p>Ms. Montgomery.</p>
<p>The space around misinformation,</p>
<p>I think, is a hugely important one.</p>
<p>And coming back to the points</p>
<p>of transparency,</p>
<p>knowing what content</p>
<p>was generated by AI</p>
<p>is going to be a really critical area</p>
<p>that we need to address.</p>
<p>Any others?</p>
<p>I think medical misinformation</p>
<p>is something to really worry about.</p>
<p>We have systems</p>
<p>that hallucinate things.</p>
<p>They&rsquo;re going to hallucinate</p>
<p>medical advice.</p>
<p>Some of the advice</p>
<p>they&rsquo;ll give is good.</p>
<p>Some of it&rsquo;s bad.</p>
<p>We need really tight regulation</p>
<p>around that.</p>
<p>Same with psychiatric advice,</p>
<p>people using these things</p>
<p>as kind of ersatz therapists.</p>
<p>I think we need to be</p>
<p>very concerned about that.</p>
<p>I think we need to be concerned</p>
<p>about internet access</p>
<p>for these tools</p>
<p>when they can start making requests,</p>
<p>both of people and internet things.</p>
<p>It&rsquo;s probably OK</p>
<p>if they just do search,</p>
<p>but as they do more intrusive things</p>
<p>on the internet,</p>
<p>do we want them to be able</p>
<p>to order equipment</p>
<p>or order chemistry and so forth?</p>
<p>So as we empower these systems more</p>
<p>by giving them internet access,</p>
<p>I think we need to be concerned about that.</p>
<p>And then we&rsquo;ve hardly talked at all</p>
<p>about long-term risks.</p>
<p>Sam alluded to it briefly.</p>
<p>I don&rsquo;t think that&rsquo;s where</p>
<p>we are right now,</p>
<p>but as we start to approach machines</p>
<p>that have a larger footprint on the world</p>
<p>beyond just having a conversation,</p>
<p>we need to worry about that</p>
<p>and think about how we&rsquo;re going</p>
<p>to regulate that</p>
<p>and monitor it and so forth.</p>
<p>In a sense,</p>
<p>we&rsquo;ve been talking about bad guys</p>
<p>or certain bad actors</p>
<p>manipulating AI to do harm.</p>
<p>Manipulating people.</p>
<p>And manipulating people,</p>
<p>but also generative AI</p>
<p>can manipulate the manipulators.</p>
<p>It can.</p>
<p>I mean, there&rsquo;s many layers</p>
<p>of manipulation that are possible,</p>
<p>and I think we don&rsquo;t yet</p>
<p>really understand the consequences.</p>
<p>Dan Dennett just sent me</p>
<p>a manuscript last night</p>
<p>that will be in the Atlantic</p>
<p>in a few days</p>
<p>on what he calls counterfeit people.</p>
<p>It&rsquo;s a wonderful metaphor.</p>
<p>These systems are almost</p>
<p>like counterfeit people,</p>
<p>and we don&rsquo;t really honestly understand</p>
<p>what the consequence of that is.</p>
<p>They&rsquo;re not perfectly human-like yet,</p>
<p>but they&rsquo;re good enough</p>
<p>to fool a lot of the people</p>
<p>a lot of the time,</p>
<p>and that introduces lots of problems.</p>
<p>For example, cybercrime</p>
<p>and how people might try</p>
<p>to manipulate markets and so forth.</p>
<p>So it&rsquo;s a serious concern.</p>
<p>In my opening,</p>
<p>I suggested three principles.</p>
<p>Transparency,</p>
<p>accountability,</p>
<p>and limits on use.</p>
<p>Would you agree that</p>
<p>those are a good starting point?</p>
<p>Ms. Montgomery?</p>
<p>100 percent.</p>
<p>And as you also mentioned,</p>
<p>industry shouldn&rsquo;t wait for Congress.</p>
<p>That&rsquo;s what we&rsquo;re doing here at IBM.</p>
<p>There&rsquo;s no reason that industry</p>
<p>should wait for Congress.</p>
<p>Yeah.</p>
<p>Professor Marcus?</p>
<p>I think those three</p>
<p>would be a great start.</p>
<p>I mean, there are things</p>
<p>like the White House Bill of Rights,</p>
<p>for example, that show,</p>
<p>I think, a large consensus.</p>
<p>The UNESCO guidelines and so forth</p>
<p>show a large consensus</p>
<p>around what it is we need,</p>
<p>and the real question is definitely</p>
<p>now how are we going to put</p>
<p>some teeth in it,</p>
<p>try to make these things</p>
<p>actually enforced?</p>
<p>So, for example,</p>
<p>we don&rsquo;t have transparency yet.</p>
<p>We all know we want it,</p>
<p>but we&rsquo;re not doing enough</p>
<p>to enforce it.</p>
<p>Mr. Altman?</p>
<p>I certainly agree that</p>
<p>those are important points.</p>
<p>I would add that,</p>
<p>and Professor Marcus touched on this,</p>
<p>I would add that as we,</p>
<p>we spend most of the time today</p>
<p>on current risks,</p>
<p>and I think that&rsquo;s appropriate,</p>
<p>and I&rsquo;m very glad we have done it.</p>
<p>As these systems do become</p>
<p>more capable,</p>
<p>and I&rsquo;m not sure how far away that is,</p>
<p>but maybe not super far.</p>
<p>I think it&rsquo;s important</p>
<p>that we also spend time</p>
<p>talking about how we&rsquo;re going</p>
<p>to confront those challenges.</p>
<p>Having talked to you privately,</p>
<p>I know how much I care.</p>
<p>I agree that you care</p>
<p>deeply and intensely,</p>
<p>but also that prospect</p>
<p>of increased danger or risk</p>
<p>resulting from even more</p>
<p>complex and capable AI mechanisms</p>
<p>certainly may be closer</p>
<p>than a lot of people appreciate.</p>
<p>Let me just add for the record</p>
<p>that I&rsquo;m sitting next to Sam.</p>
<p>Closer than I&rsquo;ve ever sat to him,</p>
<p>except once before in my life,</p>
<p>and that his sincerity</p>
<p>in talking about those fears</p>
<p>is very apparent physically</p>
<p>in a way that just doesn&rsquo;t communicate</p>
<p>on the television screen,</p>
<p>but communicates from here.</p>
<p>Thank you.</p>
<p>Senator Hawley.</p>
<p>Thank you again, Mr. Chairman,</p>
<p>for a great hearing.</p>
<p>Thanks to the witnesses.</p>
<p>So I&rsquo;ve been keeping a little list here</p>
<p>of the potential downsides or harms,</p>
<p>risks of generative AI,</p>
<p>even in its current form.</p>
<p>Let&rsquo;s just run through it.</p>
<p>Loss of jobs.</p>
<p>And this isn&rsquo;t speculative.</p>
<p>I think your company, Ms. Montgomery,</p>
<p>has announced that it&rsquo;s potentially</p>
<p>laying off 7,800 people,</p>
<p>third of your non-consumer facing workforce</p>
<p>because of AI.</p>
<p>So loss of jobs,</p>
<p>invasion of privacy, personal privacy,</p>
<p>on a scale we&rsquo;ve never before seen,</p>
<p>manipulation of personal behavior,</p>
<p>manipulation of personal opinions,</p>
<p>and potentially the degradation</p>
<p>of free elections in America.</p>
<p>Did I miss anything?</p>
<p>I mean, this is quite a list.</p>
<p>I noticed that an eclectic group</p>
<p>of about 1,000 technology and AI leaders,</p>
<p>everybody from Andrew Yang to Elon Musk,</p>
<p>recently called for a six-month moratorium</p>
<p>on any further AI development.</p>
<p>Were they right?</p>
<p>Do you join those calls?</p>
<p>Are they right to do that?</p>
<p>Should we pause for six months?</p>
<p>Your characterization is not quite correct.</p>
<p>I actually signed that letter.</p>
<p>About 27,000 people signed it.</p>
<p>It did not call for a ban</p>
<p>on all AI research,</p>
<p>nor on all AI,</p>
<p>but only on a very specific thing,</p>
<p>which would be systems like GPT-5.</p>
<p>Every other piece of research</p>
<p>that&rsquo;s ever been done,</p>
<p>it was actually supportive or neutral about.</p>
<p>It specifically called for more AI,</p>
<p>specifically called for more research</p>
<p>on trustworthy and safe AI.</p>
<p>So you think that we should take a moratorium,</p>
<p>a six-month moratorium or more</p>
<p>on anything beyond CHAT-GPT-4?</p>
<p>I took the letter.</p>
<p>What is the famous phrase?</p>
<p>Spiritually, not literally,</p>
<p>what was the famous phrase?</p>
<p>Well, I&rsquo;m asking for your opinion now, though.</p>
<p>So do you endorse the six-month moratorium?</p>
<p>My opinion is that the moratorium</p>
<p>that we should focus on</p>
<p>is actually deployment</p>
<p>until we have good safety cases.</p>
<p>I don&rsquo;t know that we need</p>
<p>to pause that particular project,</p>
<p>but I do think its emphasis</p>
<p>on focusing more on AI safety,</p>
<p>on trustworthy, reliable AI,</p>
<p>is exactly right.</p>
<p>Deployment means not making it available to the public?</p>
<p>Yeah, so my concern is about things</p>
<p>that are deployed at a scale of,</p>
<p>let&rsquo;s say, 100 million people</p>
<p>without any external review.</p>
<p>I think that we should think</p>
<p>very carefully about doing that.</p>
<p>What about you, Mr. Altman?</p>
<p>Do you agree with that?</p>
<p>Would you pause any further development</p>
<p>for six months or longer?</p>
<p>So first of all,</p>
<p>after we finished training GPT-4,</p>
<p>we waited more than six months to deploy it.</p>
<p>We are not currently training</p>
<p>what will be GPT-5.</p>
<p>We don&rsquo;t have plans to do it</p>
<p>in the next six months.</p>
<p>But I think the frame of the letter is wrong.</p>
<p>What matters is audits,</p>
<p>red teaming, safety standards</p>
<p>that a model needs to pass before training.</p>
<p>If we pause for six months,</p>
<p>then I&rsquo;m not really sure what we do then.</p>
<p>Do we pause for another six?</p>
<p>Do we kind of come up with some rules then?</p>
<p>The standards that we have developed</p>
<p>and that we&rsquo;ve used for GPT-4 deployment,</p>
<p>we want to build on those,</p>
<p>but we think that&rsquo;s the right direction,</p>
<p>not a calendar clock pause.</p>
<p>There may be times,</p>
<p>I expect there will be times</p>
<p>when we find something that we don&rsquo;t understand</p>
<p>and we really do need to take a pause,</p>
<p>but we don&rsquo;t see that yet.</p>
<p>Nevermind all the benefits.</p>
<p>What would you don&rsquo;t see what yet?</p>
<p>You&rsquo;re comfortable with all of the potential ramifications</p>
<p>from the current existing technology?</p>
<p>I&rsquo;m sorry, we don&rsquo;t see the reasons</p>
<p>to not train a new one for deploying.</p>
<p>As I mentioned,</p>
<p>I think there&rsquo;s all sorts of risky behavior</p>
<p>and there&rsquo;s limits we put.</p>
<p>We have to pull things back sometimes,</p>
<p>add new ones.</p>
<p>I mean, we don&rsquo;t see something</p>
<p>that would stop us from training the next model.</p>
<p>The next model where we&rsquo;d be so worried</p>
<p>that we&rsquo;d create something dangerous,</p>
<p>even in that process,</p>
<p>let alone the deployment.</p>
<p>What about you, Ms. Montgomery?</p>
<p>I think we need to use the time</p>
<p>to prioritize ethics and responsible technology</p>
<p>as opposed to posing development.</p>
<p>Well, wouldn&rsquo;t a pause in development</p>
<p>help the development of protocols</p>
<p>for safety standards and ethics?</p>
<p>I&rsquo;m not sure how practical it is to pause,</p>
<p>but we absolutely should be prioritizing</p>
<p>safety protocols.</p>
<p>Okay, the point about practicality</p>
<p>leads me to this.</p>
<p>I&rsquo;m interested in this talk about an agency</p>
<p>and maybe that would work.</p>
<p>Although, having seen how agencies</p>
<p>work in this government,</p>
<p>they usually get captured by the interests</p>
<p>that they&rsquo;re supposed to regulate.</p>
<p>They usually get controlled</p>
<p>by the people who they&rsquo;re supposed to be watching.</p>
<p>I mean, that&rsquo;s just been our history for 100 years.</p>
<p>Maybe this agency would be different.</p>
<p>I have a little different idea.</p>
<p>Why don&rsquo;t we just let people sue you?</p>
<p>Why don&rsquo;t we just make you liable in court?</p>
<p>We can do that.</p>
<p>We know how to do that.</p>
<p>We can pass a statute.</p>
<p>We can create a federal right of action</p>
<p>that will allow private individuals</p>
<p>who are harmed by this technology</p>
<p>to get into court</p>
<p>and to bring evidence into court.</p>
<p>And it can be anybody.</p>
<p>I mean, you want to talk about crowdsourcing.</p>
<p>We&rsquo;ll just open the courthouse doors.</p>
<p>We&rsquo;ll define a broad right of action,</p>
<p>private right of action,</p>
<p>private citizens to be class actions.</p>
<p>We&rsquo;ll just open it up.</p>
<p>We&rsquo;ll allow people to go into court.</p>
<p>We&rsquo;ll allow them to present evidence.</p>
<p>They say that they were harmed by,</p>
<p>they were given medical misinformation.</p>
<p>They were given election misinformation, whatever.</p>
<p>Why not do that, Mr. Altman?</p>
<p>I mean, please forgive my ignorance.</p>
<p>Can&rsquo;t people sue us?</p>
<p>Well, you&rsquo;re not protected by Section 230,</p>
<p>but there&rsquo;s not currently, I don&rsquo;t think,</p>
<p>a federal right of action,</p>
<p>private right of action that says</p>
<p>that if you are harmed by generative AI technology,</p>
<p>we will guarantee you the ability to get into court.</p>
<p>Oh, well, I think there&rsquo;s like a lot of other laws</p>
<p>where if technology harms you,</p>
<p>there&rsquo;s standards that we could be sued under,</p>
<p>unless I&rsquo;m really misunderstanding how things work.</p>
<p>If the question is, are clearer laws</p>
<p>about the specifics of this technology</p>
<p>and consumer protection is a good thing,</p>
<p>I would say definitely yes.</p>
<p>The laws that we have today were designed</p>
<p>long before we had artificial intelligence,</p>
<p>and I do not think they give us enough coverage.</p>
<p>The plan that you propose, I think,</p>
<p>is a hypothetical,</p>
<p>would certainly make a lot of lawyers wealthy.</p>
<p>I think it would be too slow to affect</p>
<p>a lot of the things that we care about,</p>
<p>and there are gaps in the law.</p>
<p>For example, we don&rsquo;t really&hellip;</p>
<p>Wait, you think it&rsquo;d be slower than Congress?</p>
<p>Yes, I do, in some ways.</p>
<p>Really?</p>
<p>Well, litigation can take a decade or more.</p>
<p>Oh, but the threat of litigation is a powerful tool.</p>
<p>I mean, how would IBM like to be sued for $100 billion?</p>
<p>I&rsquo;m in no way asking to take litigation</p>
<p>off the table among the tools,</p>
<p>but I think, for example, if I can continue,</p>
<p>there are areas like copyright</p>
<p>where we don&rsquo;t really have laws,</p>
<p>we don&rsquo;t really have a way of thinking</p>
<p>about wholesale misinformation</p>
<p>as opposed to individual pieces of it,</p>
<p>where, say, a foreign actor might make</p>
<p>billions of pieces of misinformation,</p>
<p>or a local actor.</p>
<p>We have some laws around market manipulation</p>
<p>we could apply,</p>
<p>but we get in a lot of situations</p>
<p>where we don&rsquo;t really know which laws apply,</p>
<p>there would be loopholes.</p>
<p>The system is really not thought through.</p>
<p>In fact, we don&rsquo;t even know that 230</p>
<p>does or does not apply here, as far as I know.</p>
<p>I think that that&rsquo;s something a lot of people</p>
<p>speculated about this afternoon,</p>
<p>but it&rsquo;s not solid.</p>
<p>Well, we could fix that.</p>
<p>The question is how?</p>
<p>Oh, easy.</p>
<p>You just, it would be easy for us to say</p>
<p>that Section 230 doesn&rsquo;t apply to generative AI.</p>
<p>Ms. Montgomery, I&rsquo;ll give you the last word.</p>
<p>I think it&rsquo;s an important start.</p>
<p>You suggested, Ms. Montgomery, a duty of care,</p>
<p>which I think fits the idea</p>
<p>of a private right of action.</p>
<p>No, that&rsquo;s exactly right.</p>
<p>And also, AI is not a shield, right?</p>
<p>So if a company discriminates in granting credit,</p>
<p>for example, or in the hiring process,</p>
<p>by virtue of the fact that they relied</p>
<p>too significantly on an AI tool,</p>
<p>they&rsquo;re responsible for that today,</p>
<p>regardless of whether they used a tool</p>
<p>or a human to make that decision.</p>
<p>I&rsquo;m gonna turn to Senator Booker</p>
<p>for some final questions,</p>
<p>but I just wanna make a quick point here.</p>
<p>On the issue of the moratorium,</p>
<p>I think we need to be careful.</p>
<p>The world won&rsquo;t wait.</p>
<p>The rest of the global scientific community</p>
<p>isn&rsquo;t going to pause.</p>
<p>We have adversaries that are moving ahead</p>
<p>and sticking our head in the sand</p>
<p>is not the answer.</p>
<p>Safeguards and protections, yes,</p>
<p>but a flat stop sign,</p>
<p>sticking our head in the sand,</p>
<p>I would be very, very worried.</p>
<p>Without militating for any sort of pause,</p>
<p>I would just again emphasize</p>
<p>there is a difference between research,</p>
<p>which surely we need to do</p>
<p>to keep pace with our foreign rivals,</p>
<p>and deployment at really massive scale.</p>
<p>You could deploy things at the scale</p>
<p>of a million people or 10 million people,</p>
<p>but not 100 million people or a billion people.</p>
<p>And if there are risks,</p>
<p>you might find them out sooner</p>
<p>and be able to close the barn doors</p>
<p>before the horses leave rather than after.</p>
<p>Senator Booker.</p>
<p>Yeah, there will be no pause.</p>
<p>I mean, there&rsquo;s no enforcement body</p>
<p>to force a pause.</p>
<p>It&rsquo;s just not going to happen.</p>
<p>It&rsquo;s nice to call for it</p>
<p>for any just reasons or whatsoever,</p>
<p>but forgive me for sounding skeptical.</p>
<p>Nobody&rsquo;s pausing.</p>
<p>This thing is racing.</p>
<p>I would agree.</p>
<p>I don&rsquo;t think it&rsquo;s a realistic thing in the world.</p>
<p>The reason I personally signed the letter</p>
<p>was to call attention</p>
<p>to how serious the problems were</p>
<p>and to emphasize spending more of our efforts</p>
<p>on trustworthy and safe AI</p>
<p>rather than just making a bigger version</p>
<p>of something we already know to be unreliable.</p>
<p>Yeah.</p>
<p>So I&rsquo;m a futurist.</p>
<p>I love exciting about the future.</p>
<p>And I guess there&rsquo;s a famous question.</p>
<p>If you couldn&rsquo;t control for your race,</p>
<p>your gender,</p>
<p>where you would land on the planet Earth</p>
<p>or what time in humanity</p>
<p>would you want to be born?</p>
<p>Everyone would say right now.</p>
<p>It&rsquo;s still the best time to be alive</p>
<p>because of technology,</p>
<p>innovation and everything.</p>
<p>And I&rsquo;m excited about what the future holds,</p>
<p>but the destructiveness that I&rsquo;ve also seen</p>
<p>as a person that&rsquo;s seen</p>
<p>the transformative technologies</p>
<p>of a lot of the technologies</p>
<p>of the last 25 years</p>
<p>is what really concerns me.</p>
<p>And one of the things,</p>
<p>especially with companies</p>
<p>that are designed to want</p>
<p>to keep my attention on screens,</p>
<p>and I&rsquo;m not just talking about new media.</p>
<p>24-hour cable news is a great example</p>
<p>of people that want to keep your eyes on screens.</p>
<p>I have a lot of concerns</p>
<p>about the corporate intention.</p>
<p>And Sam, this is, again,</p>
<p>why I find your story so fascinating to me</p>
<p>and your values that I believe in</p>
<p>from our conversations so compelling to me.</p>
<p>But absent that,</p>
<p>I really want to just explore</p>
<p>what happens when these companies</p>
<p>that are already controlling</p>
<p>so much of our lives,</p>
<p>a lot has been written about the fang companies.</p>
<p>What happens when they are the ones</p>
<p>that are dominating this technology</p>
<p>as they did before?</p>
<p>So Professor Marcus,</p>
<p>does that have any concern</p>
<p>the role that corporate power,</p>
<p>corporate concentration has in this realm</p>
<p>that a few companies might control this whole area?</p>
<p>I radically changed the shape of my own life</p>
<p>in the last few months.</p>
<p>And it was because of what happened</p>
<p>with Microsoft releasing Sydney.</p>
<p>And it didn&rsquo;t go the way I thought it would.</p>
<p>In one way, it did,</p>
<p>which is I anticipated the hallucinations.</p>
<p>I wrote an essay,</p>
<p>which I have in the appendix,</p>
<p>What to Expect When You&rsquo;re Expecting GPT-4.</p>
<p>And I said that it would still be</p>
<p>a good tool for misinformation,</p>
<p>that it would still have trouble</p>
<p>with physical reasoning,</p>
<p>psychological reasoning,</p>
<p>that it would hallucinate.</p>
<p>And then along came Sydney</p>
<p>and the initial press reports</p>
<p>were quite favorable.</p>
<p>And then there was the famous article</p>
<p>by Kevin Roose</p>
<p>in which it recommended</p>
<p>he get a divorce.</p>
<p>And I had seen Tay</p>
<p>and I had seen Galactica from Meta</p>
<p>and those had been pulled</p>
<p>after they had problems.</p>
<p>And Sydney clearly had problems.</p>
<p>What I would have done</p>
<p>had I run Microsoft,</p>
<p>which clearly I do not,</p>
<p>would have been to temporarily</p>
<p>withdraw it from the market.</p>
<p>And they didn&rsquo;t.</p>
<p>And that was a wake-up call to me</p>
<p>and a reminder that even</p>
<p>if you have a company like OpenAI</p>
<p>that is a non-profit</p>
<p>and Sam&rsquo;s values, I think,</p>
<p>have become clear today,</p>
<p>other people can buy those companies</p>
<p>and do what they like with them.</p>
<p>And maybe we have</p>
<p>a stable set of actors now,</p>
<p>but the amount of power</p>
<p>that these systems have</p>
<p>to shape our views</p>
<p>and our lives</p>
<p>is really, really significant.</p>
<p>And that doesn&rsquo;t even</p>
<p>get into the risks</p>
<p>that someone might repurpose them</p>
<p>deliberately for all kinds</p>
<p>of bad purposes.</p>
<p>And so in the middle of February,</p>
<p>I stopped writing much</p>
<p>about technical issues in AI,</p>
<p>which is most of what</p>
<p>I&rsquo;ve written about</p>
<p>for the last decade and said,</p>
<p>I need to work on policy.</p>
<p>This is frightening.</p>
<p>And Sam, I want to give you</p>
<p>an opportunity.</p>
<p>It&rsquo;s my sort of last</p>
<p>question or so.</p>
<p>Don&rsquo;t you have concerns about,</p>
<p>I mean, I graduated from Stanford.</p>
<p>I know so many of the players</p>
<p>in the valley from VC folks,</p>
<p>angel folks,</p>
<p>to a lot of founders of companies</p>
<p>that we all know.</p>
<p>Do you have some concern</p>
<p>about a few players</p>
<p>with extraordinary resources</p>
<p>and power,</p>
<p>power to influence Washington?</p>
<p>I mean, I see us,</p>
<p>I love, I&rsquo;m a big believer</p>
<p>in the free market,</p>
<p>but the reason why</p>
<p>I walk into a bodega</p>
<p>and a Twinkie is cheaper</p>
<p>than an apple</p>
<p>or a Happy Meal costs less</p>
<p>than a bucket of salad</p>
<p>is because of the way</p>
<p>the government tips the scales</p>
<p>to pick winners and losers.</p>
<p>So the free market</p>
<p>is not what it should be</p>
<p>when you have</p>
<p>large corporate power</p>
<p>that can even influence</p>
<p>the game here.</p>
<p>Do you have some concerns</p>
<p>about that in this next era</p>
<p>of technological innovation?</p>
<p>Yeah.</p>
<p>I mean, again, that&rsquo;s so much</p>
<p>of why we started OpenAI.</p>
<p>We have huge concerns about that.</p>
<p>I think it&rsquo;s important</p>
<p>to democratize the inputs</p>
<p>to these systems,</p>
<p>the values that we&rsquo;re going</p>
<p>to align to.</p>
<p>And I think it&rsquo;s also important</p>
<p>to give people wide use</p>
<p>of these tools.</p>
<p>When we started the API strategy,</p>
<p>which is a big part</p>
<p>of how we make our systems</p>
<p>available for anyone to use,</p>
<p>there was a huge amount</p>
<p>of skepticism over that.</p>
<p>And it does come with challenges,</p>
<p>that&rsquo;s for sure.</p>
<p>But we think putting this</p>
<p>in the hands of a lot of people</p>
<p>and not in the hands</p>
<p>of a few companies</p>
<p>is really quite important.</p>
<p>And we are seeing the result</p>
<p>in innovation boom from that.</p>
<p>But it is absolutely true</p>
<p>that the number of companies</p>
<p>that can train</p>
<p>the true frontier models</p>
<p>is going to be small</p>
<p>just because of</p>
<p>the resources required.</p>
<p>And so I think there needs</p>
<p>to be incredible scrutiny</p>
<p>on us and our competitors.</p>
<p>I think there is a rich</p>
<p>and exciting industry</p>
<p>happening of incredibly</p>
<p>good research and new startups</p>
<p>that are not just using our models,</p>
<p>but creating their own.</p>
<p>And I think it&rsquo;s important</p>
<p>to make sure that</p>
<p>whatever regulatory stuff happens,</p>
<p>whatever new agencies</p>
<p>may or may not happen,</p>
<p>we preserve that fire</p>
<p>because that&rsquo;s critical.</p>
<p>Well, I&rsquo;m a big believer</p>
<p>in the democratizing</p>
<p>potential of technology,</p>
<p>but I&rsquo;ve seen the promise</p>
<p>of that fail time and time again</p>
<p>where people said,</p>
<p>oh, this is going to have</p>
<p>a big democratizing force.</p>
<p>My team works on a lot of issues</p>
<p>about the reinforcing</p>
<p>of bias through algorithms,</p>
<p>the failure to advertise</p>
<p>certain opportunities</p>
<p>and certain zip codes.</p>
<p>But you seem to be saying,</p>
<p>and I heard this with Web3,</p>
<p>that this is going to be</p>
<p>decentralized finance.</p>
<p>All these things</p>
<p>are going to happen.</p>
<p>But this seems to me</p>
<p>not even to offer that promise</p>
<p>because the people</p>
<p>who are designing these,</p>
<p>it takes so much power,</p>
<p>energy, resources.</p>
<p>Are you saying that</p>
<p>my dreams of technology,</p>
<p>further democratizing opportunity</p>
<p>and more are possible</p>
<p>within a technology</p>
<p>that is ultimately,</p>
<p>I think, can be very centralized</p>
<p>to a few players</p>
<p>who already control so much?</p>
<p>So this point that I made</p>
<p>about use of the model</p>
<p>and building on top of it,</p>
<p>this is really a new platform, right?</p>
<p>It is definitely important</p>
<p>to talk about who&rsquo;s going</p>
<p>to create the models.</p>
<p>I want to do that.</p>
<p>I also think it&rsquo;s really important</p>
<p>to decide to whose values</p>
<p>we&rsquo;re going to align these models.</p>
<p>But in terms of using the models,</p>
<p>the people that build</p>
<p>on top of the OpenAI API</p>
<p>do incredible things.</p>
<p>And it&rsquo;s, you know,</p>
<p>people frequently comment,</p>
<p>like, I can&rsquo;t believe</p>
<p>you get this much technology</p>
<p>for this little money.</p>
<p>And so what people are,</p>
<p>the companies people are building,</p>
<p>putting AI everywhere,</p>
<p>using our API,</p>
<p>which does let us</p>
<p>put safeguards in place,</p>
<p>I think that&rsquo;s quite exciting.</p>
<p>And I think that is how</p>
<p>it is being democratized,</p>
<p>not how it&rsquo;s going to be,</p>
<p>but how it is being</p>
<p>democratized right now.</p>
<p>There is a whole new Cambrian</p>
<p>explosion of new businesses,</p>
<p>new products, new services</p>
<p>happening by lots</p>
<p>of different companies</p>
<p>on top of these models.</p>
<p>And so I&rsquo;ll say, Chairman,</p>
<p>as I close, that I have,</p>
<p>most industries resist</p>
<p>even reasonable regulation</p>
<p>from seatbelt laws</p>
<p>to we&rsquo;ve been talking a lot</p>
<p>recently about rail safety.</p>
<p>The only way we&rsquo;re going</p>
<p>to see the democratization</p>
<p>of values, I think,</p>
<p>and while there are</p>
<p>noble companies out there,</p>
<p>is if we create rules of the road</p>
<p>that enforce</p>
<p>certain safety measures,</p>
<p>like we&rsquo;ve seen</p>
<p>with other technology.</p>
<p>Thank you.</p>
<p>Thanks, Senator Booker.</p>
<p>And I couldn&rsquo;t agree more</p>
<p>that in terms of</p>
<p>consumer protection,</p>
<p>which I&rsquo;ve been doing</p>
<p>for a while,</p>
<p>participation by the industry</p>
<p>is tremendously important</p>
<p>and not just rhetorically,</p>
<p>but in real terms,</p>
<p>because we have a lot of</p>
<p>industries that come before us</p>
<p>and say, oh,</p>
<p>we&rsquo;re all in favor of rules,</p>
<p>but not those rules.</p>
<p>Those rules we don&rsquo;t like.</p>
<p>And it&rsquo;s every rule,</p>
<p>in fact, that they don&rsquo;t like.</p>
<p>And I sense that</p>
<p>there is a willingness</p>
<p>to participate here</p>
<p>that is genuine and authentic.</p>
<p>I thought about asking</p>
<p>CHAT-GPT to do a new version</p>
<p>of don&rsquo;t stop</p>
<p>thinking about tomorrow,</p>
<p>because that&rsquo;s what we need</p>
<p>to be doing here.</p>
<p>And Senator Hawley</p>
<p>has pointed out,</p>
<p>Congress doesn&rsquo;t always move</p>
<p>at the pace of technology.</p>
<p>And that may be a reason</p>
<p>why we need a new agency.</p>
<p>But we also need to recognize</p>
<p>the rest of the world</p>
<p>is going to be moving as well.</p>
<p>And you&rsquo;ve been enormously helpful</p>
<p>in focusing us</p>
<p>and illuminating</p>
<p>some of these questions</p>
<p>and performed a great service</p>
<p>by being here today.</p>
<p>So thank you to every one</p>
<p>of our witnesses.</p>
<p>And I&rsquo;m going to close the hearing,</p>
<p>leave the record open for one week.</p>
<p>In case anyone wants</p>
<p>to submit anything,</p>
<p>I encourage any of you</p>
<p>who have either manuscripts</p>
<p>that are going to be published</p>
<p>or observations</p>
<p>from your companies</p>
<p>to submit them to us.</p>
<p>And we look forward</p>
<p>to our next hearing.</p>
<p>This one is closed.</p>
<p>It&rsquo;s.</p>
<p>I&rsquo;m here all week</p>
<p>if you have any time to talk.</p>
<p>Could you make any comparisons</p>
<p>between Sam Altman&rsquo;s</p>
<p>testimony here</p>
<p>and earlier testimony</p>
<p>by other tech CEOs?</p>
<p>Well, you know,</p>
<p>just looking at the record,</p>
<p>Sam Altman is night and day</p>
<p>compared to the CEO.</p>
<p>And not just in the words</p>
<p>and rhetoric,</p>
<p>but in actual actions</p>
<p>and his willingness</p>
<p>to participate</p>
<p>and commit to specific action.</p>
<p>So, you know,</p>
<p>some of the big tech companies</p>
<p>are under consent decrees,</p>
<p>which they have violated.</p>
<p>That&rsquo;s a far cry</p>
<p>from the kind of cooperation</p>
<p>that Sam Altman has promised.</p>
<p>And given his track record,</p>
<p>I think it&rsquo;s a,</p>
<p>it seems to be pretty sincere.</p>
<p>Senator, the hearing</p>
<p>really reflected</p>
<p>the range of concerns here.</p>
<p>You&rsquo;re talking about elections</p>
<p>to national security,</p>
<p>to medical employment.</p>
<p>Right.</p>
<p>How do you,</p>
<p>what kind of challenge</p>
<p>does that pose</p>
<p>in trying to craft a response?</p>
<p>It means that we have</p>
<p>to construct a system</p>
<p>that is broad</p>
<p>and flexible</p>
<p>without Congress</p>
<p>being the gatekeeper</p>
<p>every time there&rsquo;s</p>
<p>some new technological advance.</p>
<p>So probably creating an agency</p>
<p>or delegating a high degree</p>
<p>of responsibility</p>
<p>for rulemaking</p>
<p>makes a lot of sense</p>
<p>in this area.</p>
<p>But does that make it challenging</p>
<p>to come up with a bill</p>
<p>and get consensus on a bill</p>
<p>when you&rsquo;ve got so many different</p>
<p>constituencies and concerns</p>
<p>all swirling around this?</p>
<p>Well, there&rsquo;s no question</p>
<p>that it&rsquo;s complex</p>
<p>with a lot of constituencies.</p>
<p>But the recognition</p>
<p>that we&rsquo;re not going</p>
<p>to solve the problem</p>
<p>by an excruciatingly detailed</p>
<p>prescriptive formula</p>
<p>answering every one</p>
<p>of these questions,</p>
<p>in other words,</p>
<p>that we&rsquo;re going to have to say</p>
<p>to an agency,</p>
<p>look, you know,</p>
<p>do the rulemaking here.</p>
<p>Make the rules clear.</p>
<p>Fit the risks.</p>
<p>You&rsquo;re going to have to develop</p>
<p>the expertise.</p>
<p>Congress is not going to have it.</p>
<p>And Congress can&rsquo;t</p>
<p>act quickly enough.</p>
<p>That degree of humility</p>
<p>is required here.</p>
<p>And I think you sense</p>
<p>that degree of humility</p>
<p>in space today.</p>
<p>When we&rsquo;re talking about</p>
<p>a new regulatory agency,</p>
<p>are you thinking about</p>
<p>a regulatory agency</p>
<p>for all of technology</p>
<p>or for AI specifically?</p>
<p>Um, you know,</p>
<p>the question of a new</p>
<p>regulatory agency,</p>
<p>I think, is still to be answered,</p>
<p>whether it&rsquo;s new or</p>
<p>part of an existing agency.</p>
<p>But certainly it should be</p>
<p>broader than just AI.</p>
<p>Probably technology, privacy,</p>
<p>you know, but clearly</p>
<p>the FTC doesn&rsquo;t have</p>
<p>the capability right now.</p>
<p>So if you&rsquo;re going to rely</p>
<p>on the FTC,</p>
<p>you&rsquo;ve got to, in effect,</p>
<p>clone it within itself,</p>
<p>so to speak.</p>
<p>I think there&rsquo;s a powerful</p>
<p>argument for an entirely</p>
<p>new agency that is given</p>
<p>the resources to really</p>
<p>do the job, you know,</p>
<p>because as I said here,</p>
<p>you can create an agency</p>
<p>just by signing a bill.</p>
<p>But an agency alone</p>
<p>is not the solution.</p>
<p>It&rsquo;s resources and expertise</p>
<p>and a genuine commitment</p>
<p>to make it work.</p>
<p>Senator, how soon,</p>
<p>realistically,</p>
<p>do you think action</p>
<p>could take place?</p>
<p>This session?</p>
<p>Senator Schumer</p>
<p>is working on a framework.</p>
<p>President of the United States</p>
<p>has said there should be</p>
<p>a Bill of Rights.</p>
<p>The European Parliament</p>
<p>is doing an AI Act.</p>
<p>You know, all kinds</p>
<p>of private groups</p>
<p>are issuing ideas</p>
<p>for legislation.</p>
<p>There is certainly</p>
<p>a lot of legs for a bill</p>
<p>and a lot of momentum.</p>
<p>And there&rsquo;s a clear need.</p>
<p>You know, people are excited,</p>
<p>but also anxious</p>
<p>with good reason.</p>
<p>Any details</p>
<p>on Leader Schumer&rsquo;s framework</p>
<p>that you can share?</p>
<p>Not beyond what he has said.</p>
<p>He should be the one</p>
<p>to talk about.</p>
<p>But his leadership</p>
<p>certainly is very,</p>
<p>very important.</p>
<p>Thank you.</p>
<p>Why should‚Äî</p>
<p>I&rsquo;m going to have to run.</p>
<p>Yeah, one more.</p>
<p>OK, sure.</p>
<p>Why should consumers</p>
<p>believe that, you know,</p>
<p>with AI, we&rsquo;ll get regulation</p>
<p>faster than with something</p>
<p>like privacy,</p>
<p>which we&rsquo;ve seen</p>
<p>have to be reintroduced</p>
<p>over and over?</p>
<p>Well, they should demand</p>
<p>better protection on privacy</p>
<p>as well as AI.</p>
<p>You know, there&rsquo;s</p>
<p>a need for privacy legislation.</p>
<p>There&rsquo;s a need for AI protection.</p>
<p>And there is a need</p>
<p>for the Kids Online Safety Act.</p>
<p>You know, social media.</p>
<p>So we all understand</p>
<p>we&rsquo;ve got a bill</p>
<p>now that will protect kids</p>
<p>from a form of AI.</p>
<p>Those algorithms</p>
<p>are a form of AI</p>
<p>that is out there</p>
<p>and deploying bullying,</p>
<p>eating disorders,</p>
<p>suicidal thoughts,</p>
<p>drug abuse.</p>
<p>They&rsquo;re out there right now</p>
<p>in effect ricocheting</p>
<p>that toxic content</p>
<p>back and forth from kids</p>
<p>to social media.</p>
<p>And, you know,</p>
<p>there is certainly</p>
<p>a sense of urgency</p>
<p>around that issue.</p>
<p>Thank you.</p>
<p>OK, thanks.</p>
<p>Thank you.</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/english/">English</a>
        
            <a href="/tags/video-transcripts/">Video Transcripts</a>
        
            <a href="/tags/c-span/">C-SPAN</a>
        
    </section>


    </footer>


    
</article>

    

    

<div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064"
         crossorigin="anonymous"></script>
    <ins class="adsbygoogle"
         style="display:block; text-align:center;"
         data-ad-layout="in-article"
         data-ad-format="fluid"
         data-ad-client="ca-pub-9206135835124064"
         data-ad-slot="1055602464"></ins>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>
<aside class="related-content--wrapper">
    <h2 class="section-title">Related content</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/en/3001300001/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/3001300001" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">C-SPAN - TikTok CEO Shou Zi Chew testifies before Congress</h2>
            
            <footer class="article-time">
                <time datetime=''>Mar 23, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/q4df3j4sace/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/q4DF3j4saCE" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">The Most Extreme Explosion in the Universe ÔΩú Kurzgesagt</h2>
            
            <footer class="article-time">
                <time datetime=''>Oct 22, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/cfslusyfzpc/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/cFslUSyfZPc" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">What Happens If You Destroy A Black HoleÔºü ÔΩú Kurzgesagt</h2>
            
            <footer class="article-time">
                <time datetime=''>Oct 22, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/kl39khs07xc/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/kl39KHS07Xc" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Universal Basic Income Explained ‚Äì Free Money for EverybodyÔºü UBI ÔΩú Kurzgesagt</h2>
            
            <footer class="article-time">
                <time datetime=''>Oct 22, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/wski8hfcxek/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/WSKi8HfcxEk" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">The Rise of the Machines ‚Äì Why Automation is Different this Time ÔΩú Kurzgesagt</h2>
            
            <footer class="article-time">
                <time datetime=''>Oct 22, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2021 - 
        
        2023 SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics
    </section>
    
    <section class="powerby">
        

        As an Amazon Associate I earn from qualifying purchases üõí<br/>

        Built with <a href="https://swiest.com/" target="_blank" rel="noopener">(Ôæâ‚óï„ÉÆ‚óï)Ôæâü™Ñüíûüíñü•∞ across the glüåçüåèüåébe</a> <br />
        
        
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>


<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Serif&family=Noto+Serif+Armenian&family=Noto+Serif+Bengali&family=Noto+Serif+Devanagari&family=Noto+Serif+Georgian&family=Noto+Serif+Gujarati&family=Noto+Serif+HK&family=Noto+Serif+Hebrew&family=Noto+Serif+JP&family=Noto+Serif+KR&family=Noto+Serif+Kannada&family=Noto+Serif+Khmer&family=Noto+Serif+Lao&family=Noto+Serif+Makasar&family=Noto+Serif+Malayalam&family=Noto+Serif+Myanmar&family=Noto+Serif+Oriya&family=Noto+Serif+SC&family=Noto+Serif+Sinhala&family=Noto+Serif+TC&family=Noto+Serif+Tamil&family=Noto+Serif+Telugu&family=Noto+Serif+Thai&family=Noto+Serif+Tibetan&display=swap" rel="stylesheet">

    </body>
</html>

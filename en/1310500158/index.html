<!DOCTYPE html>
<html lang="en" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='The following is a conversation with Max Tegmark,
his second time on the podcast.
In fact, the previous conversation
was episode number one of this very podcast.
He is a physicist and artificial intelligence researcher
at MIT, cofounder of the Future of Life Institute,
and author of Life 3.0,
Being Human in the Age of Artificial Intelligence.
He&amp;rsquo;s also the head of a bunch of other huge,
fascinating projects and has written'>
<title>Lex Fridman Podcast - #155 ‚Äì Max Tegmark: AI and Physics | SWIEST</title>

<link rel='canonical' href='https://swiest.com/en/1310500158/'>

<link rel="stylesheet" href="/scss/style.min.dbeedb45fe5ec84049b6a1867f0e052825d50915ff52cd1efe27b0c35d1ebabd.css"><script>
    document.oncontextmenu = function(){ return false; };
    document.onselectstart = function(){ return false; };
    document.oncopy = function(){ return false; };
    document.oncut = function(){ return false; };
</script>

<script src="https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>


<script type="text/javascript">
    $(document).ready(function(){
     
     $("#back-to-top").hide();
     
     $(function () {
      $(window).scroll(function(){
       if ($(window).scrollTop()>600){
        $("#back-to-top").fadeIn(500);
       }else{
        $("#back-to-top").fadeOut(500);
       }
     });
     
     $("#back-to-top").click(function(){
      $('body,html').animate({scrollTop:0},500);
       return false;
      });
     });
    });
    </script><meta property='og:title' content='Lex Fridman Podcast - #155 ‚Äì Max Tegmark: AI and Physics'>
<meta property='og:description' content='The following is a conversation with Max Tegmark,
his second time on the podcast.
In fact, the previous conversation
was episode number one of this very podcast.
He is a physicist and artificial intelligence researcher
at MIT, cofounder of the Future of Life Institute,
and author of Life 3.0,
Being Human in the Age of Artificial Intelligence.
He&amp;rsquo;s also the head of a bunch of other huge,
fascinating projects and has written'>
<meta property='og:url' content='https://swiest.com/en/1310500158/'>
<meta property='og:site_name' content='SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='English' /><meta property='article:tag' content='Podcast' /><meta property='article:tag' content='Lex Fridman Podcast' /><meta property='article:published_time' content='2022-08-05T17:00:00&#43;00:00'/><meta property='article:modified_time' content='2022-08-05T17:00:00&#43;00:00'/>
<meta name="twitter:title" content="Lex Fridman Podcast - #155 ‚Äì Max Tegmark: AI and Physics">
<meta name="twitter:description" content="The following is a conversation with Max Tegmark,
his second time on the podcast.
In fact, the previous conversation
was episode number one of this very podcast.
He is a physicist and artificial intelligence researcher
at MIT, cofounder of the Future of Life Institute,
and author of Life 3.0,
Being Human in the Age of Artificial Intelligence.
He&amp;rsquo;s also the head of a bunch of other huge,
fascinating projects and has written">
    <link rel="shortcut icon" href="/favicon.ico" />
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin="anonymous"></script>
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "dark");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu307e6a33fa6fd661ccda3b77024ef5c2_252345_300x0_resize_box_3.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">‚ú®</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics</a></h1>
            <h2 class="site-description">üåçüåèüåé</h2>
        </div>
    </header><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        
        
        <li >
            <a href='/tags/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11 3L20 12a1.5 1.5 0 0 1 0 2L14 20a1.5 1.5 0 0 1 -2 0L3 11v-4a4 4 0 0 1 4 -4h4" />
  <circle cx="9" cy="9" r="2" />
</svg>



                
                <span>Tags</span>
            </a>
        </li>
        
        
        <li >
            <a href='/chart/podcastchart.html' target="_blank">
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-apple-podcast" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M18.364 18.364a9 9 0 1 0 -12.728 0" />
  <path d="M11.766 22h.468a2 2 0 0 0 1.985 -1.752l.5 -4a2 2 0 0 0 -1.985 -2.248h-1.468a2 2 0 0 0 -1.985 2.248l.5 4a2 2 0 0 0 1.985 1.752z" />
  <path d="M12 9m-2 0a2 2 0 1 0 4 0a2 2 0 1 0 -4 0" />
</svg>
                
                <span>Podcasts</span>
            </a>
        </li>
        


        <div class="menu-bottom-section">
                <li id="i18n-switch">  
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M4 5h7" />
  <path d="M9 3v2c0 4.418 -2.239 8 -5 8" />
  <path d="M5 9c-.003 2.144 2.952 3.908 6.7 4" />
  <path d="M12 20l4 -9l4 9" />
  <path d="M19.1 18h-6.2" />
</svg>



                    <select name="language" onchange="window.location.href = this.selectedOptions[0].value">
                        
                            <option value="https://swiest.com/" selected>English</option>
                        
                            <option value="https://swiest.com/af/" >Afrikaans</option>
                        
                            <option value="https://swiest.com/am/" >·ä†·àõ·à≠·äõ</option>
                        
                            <option value="https://swiest.com/ar/" >ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</option>
                        
                            <option value="https://swiest.com/az/" >Az…ôrbaycan</option>
                        
                            <option value="https://swiest.com/be/" >–±–µ–ª–∞—Ä—É—Å–∫—ñ</option>
                        
                            <option value="https://swiest.com/bg/" >–±—ä–ª–≥–∞—Ä—Å–∫–∏</option>
                        
                            <option value="https://swiest.com/bn/" >‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ</option>
                        
                            <option value="https://swiest.com/bo/" >‡Ωñ‡Ωº‡Ωë‡ºã‡Ω¶‡æê‡Ωë‡ºã</option>
                        
                            <option value="https://swiest.com/bs/" >Bosanski</option>
                        
                            <option value="https://swiest.com/ca/" >Catal√†</option>
                        
                            <option value="https://swiest.com/zh-hans/" >ÁÆÄ‰Ωì‰∏≠Êñá</option>
                        
                            <option value="https://swiest.com/zh-hant/" >ÁπÅÈ´î‰∏≠Êñá</option>
                        
                            <option value="https://swiest.com/cs/" >ƒåe≈°tina</option>
                        
                            <option value="https://swiest.com/el/" >ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨</option>
                        
                            <option value="https://swiest.com/cy/" >Cymraeg</option>
                        
                            <option value="https://swiest.com/da/" >Dansk</option>
                        
                            <option value="https://swiest.com/de/" >Deutsch</option>
                        
                            <option value="https://swiest.com/eo/" >Esperanto</option>
                        
                            <option value="https://swiest.com/es-es/" >Espa√±ol (Espa√±a)</option>
                        
                            <option value="https://swiest.com/es-419/" >Espa√±ol (Latinoam√©rica)</option>
                        
                            <option value="https://swiest.com/et/" >Eesti</option>
                        
                            <option value="https://swiest.com/eu/" >Euskara</option>
                        
                            <option value="https://swiest.com/haw/" > ª≈ålelo Hawai ªi</option>
                        
                            <option value="https://swiest.com/fa/" >ŸÅÿßÿ±ÿ≥€å</option>
                        
                            <option value="https://swiest.com/fi/" >Suomi</option>
                        
                            <option value="https://swiest.com/fo/" >F√∏royskt</option>
                        
                            <option value="https://swiest.com/fr/" >Fran√ßais</option>
                        
                            <option value="https://swiest.com/fy/" >Frysk</option>
                        
                            <option value="https://swiest.com/ga/" >Gaeilge</option>
                        
                            <option value="https://swiest.com/gl/" >Galego</option>
                        
                            <option value="https://swiest.com/gu/" >‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä</option>
                        
                            <option value="https://swiest.com/he/" >◊¢÷¥◊ë◊®÷¥◊ô◊™</option>
                        
                            <option value="https://swiest.com/km/" >·ûÄ·ûò·üí·ûñ·ûª·ûá·û∂·üî</option>
                        
                            <option value="https://swiest.com/hi/" >‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</option>
                        
                            <option value="https://swiest.com/hr/" >Hrvatski</option>
                        
                            <option value="https://swiest.com/ht/" >Krey√≤l Ayisyen</option>
                        
                            <option value="https://swiest.com/hu/" >Magyar</option>
                        
                            <option value="https://swiest.com/hy/" >’Ä’°’µ’•÷Ä’•’∂</option>
                        
                            <option value="https://swiest.com/ig/" >√Ås·ª•ÃÄs·ª•ÃÅ √ågb√≤</option>
                        
                            <option value="https://swiest.com/id/" >Bahasa Indonesia</option>
                        
                            <option value="https://swiest.com/is/" >√çslenska</option>
                        
                            <option value="https://swiest.com/it/" >Italiano</option>
                        
                            <option value="https://swiest.com/ja/" >Êó•Êú¨Ë™û</option>
                        
                            <option value="https://swiest.com/jv/" >Basa Jawa</option>
                        
                            <option value="https://swiest.com/ka/" >·É•·Éê·É†·Éó·É£·Éö·Éò</option>
                        
                            <option value="https://swiest.com/kk/" >“ö–∞–∑–∞“õ—à–∞</option>
                        
                            <option value="https://swiest.com/kn/" >‡≤ï‡≤®‡≥ç‡≤®‡≤°</option>
                        
                            <option value="https://swiest.com/ko/" >ÌïúÍµ≠Ïñ¥</option>
                        
                            <option value="https://swiest.com/or/" >‡¨ì‡¨°‡¨º‡¨ø‡¨Ü</option>
                        
                            <option value="https://swiest.com/ckb/" >⁄©Ÿàÿ±ÿØ€å</option>
                        
                            <option value="https://swiest.com/ky/" >–ö—ã—Ä–≥—ã–∑—á–∞</option>
                        
                            <option value="https://swiest.com/la/" >Latina</option>
                        
                            <option value="https://swiest.com/lb/" >L√´tzebuergesch</option>
                        
                            <option value="https://swiest.com/lo/" >‡∫û‡∫≤‡∫™‡∫≤‡∫•‡∫≤‡∫ß</option>
                        
                            <option value="https://swiest.com/lt/" >Lietuvi≈≥</option>
                        
                            <option value="https://swiest.com/lv/" >Latvie≈°u</option>
                        
                            <option value="https://swiest.com/mk/" >–ú–∞–∫–µ–¥–æ–Ω—Å–∫–∏</option>
                        
                            <option value="https://swiest.com/ml/" >‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç</option>
                        
                            <option value="https://swiest.com/mn/" >–ú–æ–Ω–≥–æ–ª —Ö—ç–ª</option>
                        
                            <option value="https://swiest.com/mr/" >‡§Æ‡§∞‡§æ‡§†‡•Ä</option>
                        
                            <option value="https://swiest.com/sw/" >Kiswahili</option>
                        
                            <option value="https://swiest.com/ms/" >Bahasa Melayu</option>
                        
                            <option value="https://swiest.com/my/" >·Äô·Äº·Äî·Ä∫·Äô·Ä¨</option>
                        
                            <option value="https://swiest.com/ne/" >‡§®‡•á‡§™‡§æ‡§≤‡•Ä</option>
                        
                            <option value="https://swiest.com/nl/" >Nederlands</option>
                        
                            <option value="https://swiest.com/no/" >Norsk</option>
                        
                            <option value="https://swiest.com/pa/" >‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä</option>
                        
                            <option value="https://swiest.com/pl/" >Polski</option>
                        
                            <option value="https://swiest.com/pt-br/" >Portugu√™s Brasil</option>
                        
                            <option value="https://swiest.com/pt-pt/" >Portugu√™s Europeu</option>
                        
                            <option value="https://swiest.com/ro/" >Rom√¢nƒÉ</option>
                        
                            <option value="https://swiest.com/ru/" >–†—É—Å—Å–∫–∏–π</option>
                        
                            <option value="https://swiest.com/rw/" >Kinyarwanda</option>
                        
                            <option value="https://swiest.com/si/" >‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω</option>
                        
                            <option value="https://swiest.com/sk/" >Slovenƒçina</option>
                        
                            <option value="https://swiest.com/sl/" >Sloven≈°ƒçina</option>
                        
                            <option value="https://swiest.com/sq/" >Shqip</option>
                        
                            <option value="https://swiest.com/sr/" >–°—Ä–ø—Å–∫–∏ (Srpski)</option>
                        
                            <option value="https://swiest.com/su/" >Basa Sunda</option>
                        
                            <option value="https://swiest.com/sv/" >Svenska</option>
                        
                            <option value="https://swiest.com/ta/" >‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç</option>
                        
                            <option value="https://swiest.com/te/" >‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</option>
                        
                            <option value="https://swiest.com/tg/" >–¢–æ“∑–∏–∫”£</option>
                        
                            <option value="https://swiest.com/th/" >‡πÑ‡∏ó‡∏¢</option>
                        
                            <option value="https://swiest.com/tk/" >T√ºrkmenler</option>
                        
                            <option value="https://swiest.com/tl/" >Filipino</option>
                        
                            <option value="https://swiest.com/tr/" >T√ºrk√ße</option>
                        
                            <option value="https://swiest.com/uk/" >–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</option>
                        
                            <option value="https://swiest.com/ur/" >ÿßÿ±ÿØŸà</option>
                        
                            <option value="https://swiest.com/uz/" >O&#39;zbekcha</option>
                        
                            <option value="https://swiest.com/vi/" >Ti·∫øng Vi·ªát</option>
                        
                            <option value="https://swiest.com/yi/" >◊ê◊ô◊ì◊ô◊©</option>
                        
                            <option value="https://swiest.com/zh-hk/" >Á≤µË™û</option>
                        
                            <option value="https://swiest.com/zu/" >IsiZulu</option>
                        
                    </select>
                </li>
            
            
            
                <li id="dark-mode-toggle">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <span>Dark Mode</span>
                </li>
            
        </div>
    </ol>
</aside>

    

            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-tags">
        
            <a href="/tags/english/" >
                English
            </a>
        
            <a href="/tags/podcast/" >
                Podcast
            </a>
        
            <a href="/tags/lex-fridman-podcast/" >
                Lex Fridman Podcast
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/en/1310500158/">Lex Fridman Podcast - #155 ‚Äì Max Tegmark: AI and Physics</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2022-08-05</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    141 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>
<div class="article-content">
    <p style="text-align:center">
       <a href="https://amzn.to/471i0jl" target="_blank">üéÅAmazon Prime</a>
       <a href="https://amzn.to/3Fulwaf" target="_blank">üíóThe Drop</a>
       <a href="https://amzn.to/3QDVlVf" target="_blank">üìñKindle Unlimited</a>
       <a href="https://amzn.to/3FqzNoB" target="_blank">üéßAudible Plus</a>
       <a href="https://amzn.to/3tMT3dm" target="_blank">üéµAmazon Music Unlimited</a>
       <a href="https://www.iherb.com/?rcode=EID1574" target="_blank">üåøiHerb</a>
       <a href="https://accounts.binance.com/register?ref=72302422" target="_blank">üí∞Binance</a>
    </p>
</div>
<div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064"
     crossorigin="anonymous"></script>
    
    <ins class="adsbygoogle"
         style="display:block"
         data-ad-client="ca-pub-9206135835124064"
         data-ad-slot="8754979142"
         data-ad-format="auto"
         data-full-width-responsive="true"></ins>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>


    <section class="article-content">
    
    
    <p>The following is a conversation with Max Tegmark,</p>
<p>his second time on the podcast.</p>
<p>In fact, the previous conversation</p>
<p>was episode number one of this very podcast.</p>
<p>He is a physicist and artificial intelligence researcher</p>
<p>at MIT, cofounder of the Future of Life Institute,</p>
<p>and author of Life 3.0,</p>
<p>Being Human in the Age of Artificial Intelligence.</p>
<p>He&rsquo;s also the head of a bunch of other huge,</p>
<p>fascinating projects and has written</p>
<p>a lot of different things</p>
<p>that you should definitely check out.</p>
<p>He has been one of the key humans</p>
<p>who has been outspoken about longterm existential risks</p>
<p>of AI and also its exciting possibilities</p>
<p>and solutions to real world problems.</p>
<p>Most recently at the intersection of AI and physics,</p>
<p>and also in reengineering the algorithms</p>
<p>that divide us by controlling the information we see</p>
<p>and thereby creating bubbles and all other kinds</p>
<p>of complex social phenomena that we see today.</p>
<p>In general, he&rsquo;s one of the most passionate</p>
<p>and brilliant people I have the fortune of knowing.</p>
<p>I hope to talk to him many more times</p>
<p>on this podcast in the future.</p>
<p>Quick mention of our sponsors,</p>
<p>The Jordan Harbinger Show,</p>
<p>Four Sigmatic Mushroom Coffee,</p>
<p>BetterHelp Online Therapy, and ExpressVPN.</p>
<p>So the choices, wisdom, caffeine, sanity, or privacy.</p>
<p>Choose wisely, my friends, and if you wish,</p>
<p>click the sponsor links below to get a discount</p>
<p>and to support this podcast.</p>
<p>As a side note, let me say that much of the researchers</p>
<p>in the machine learning</p>
<p>and artificial intelligence communities</p>
<p>do not spend much time thinking deeply</p>
<p>about existential risks of AI.</p>
<p>Because our current algorithms are seen as useful but dumb,</p>
<p>it&rsquo;s difficult to imagine how they may become destructive</p>
<p>to the fabric of human civilization</p>
<p>in the foreseeable future.</p>
<p>I understand this mindset, but it&rsquo;s very troublesome.</p>
<p>To me, this is both a dangerous and uninspiring perspective,</p>
<p>reminiscent of a lobster sitting in a pot of lukewarm water</p>
<p>that a minute ago was cold.</p>
<p>I feel a kinship with this lobster.</p>
<p>I believe that already the algorithms</p>
<p>that drive our interaction on social media</p>
<p>have an intelligence and power</p>
<p>that far outstrip the intelligence and power</p>
<p>of any one human being.</p>
<p>Now really is the time to think about this,</p>
<p>to define the trajectory of the interplay</p>
<p>of technology and human beings in our society.</p>
<p>I think that the future of human civilization</p>
<p>very well may be at stake over this very question</p>
<p>of the role of artificial intelligence in our society.</p>
<p>If you enjoy this thing, subscribe on YouTube,</p>
<p>review it on Apple Podcasts, follow on Spotify,</p>
<p>support on Patreon, or connect with me on Twitter</p>
<p>at Lex Friedman.</p>
<p>And now, here&rsquo;s my conversation with Max Tegmark.</p>
<p>So people might not know this,</p>
<p>but you were actually episode number one of this podcast</p>
<p>just a couple of years ago, and now we&rsquo;re back.</p>
<p>And it so happens that a lot of exciting things happened</p>
<p>in both physics and artificial intelligence,</p>
<p>both fields that you&rsquo;re super passionate about.</p>
<p>Can we try to catch up to some of the exciting things</p>
<p>happening in artificial intelligence,</p>
<p>especially in the context of the way it&rsquo;s cracking,</p>
<p>open the different problems of the sciences?</p>
<p>Yeah, I&rsquo;d love to, especially now as we start 2021 here,</p>
<p>it&rsquo;s a really fun time to think about</p>
<p>what were the biggest breakthroughs in AI,</p>
<p>not the ones necessarily that media wrote about,</p>
<p>but that really matter, and what does that mean</p>
<p>for our ability to do better science?</p>
<p>What does it mean for our ability</p>
<p>to help people around the world?</p>
<p>And what does it mean for new problems</p>
<p>that they could cause if we&rsquo;re not smart enough</p>
<p>to avoid them, so what do we learn basically from this?</p>
<p>Yes, absolutely.</p>
<p>So one of the amazing things you&rsquo;re a part of</p>
<p>is the AI Institute for Artificial Intelligence</p>
<p>and Fundamental Interactions.</p>
<p>What&rsquo;s up with this institute?</p>
<p>What are you working on?</p>
<p>What are you thinking about?</p>
<p>The idea is something I&rsquo;m very on fire with,</p>
<p>which is basically AI meets physics.</p>
<p>And it&rsquo;s been almost five years now</p>
<p>since I shifted my own MIT research</p>
<p>from physics to machine learning.</p>
<p>And in the beginning, I noticed that a lot of my colleagues,</p>
<p>even though they were polite about it,</p>
<p>were like kind of, what is Max doing?</p>
<p>What is this weird stuff?</p>
<p>He&rsquo;s lost his mind.</p>
<p>But then gradually, I, together with some colleagues,</p>
<p>were able to persuade more and more of the other professors</p>
<p>in our physics department to get interested in this.</p>
<p>And now we&rsquo;ve got this amazing NSF Center,</p>
<p>so 20 million bucks for the next five years, MIT,</p>
<p>and a bunch of neighboring universities here also.</p>
<p>And I noticed now those colleagues</p>
<p>who were looking at me funny have stopped</p>
<p>asking what the point is of this,</p>
<p>because it&rsquo;s becoming more clear.</p>
<p>And I really believe that, of course,</p>
<p>AI can help physics a lot to do better physics.</p>
<p>But physics can also help AI a lot,</p>
<p>both by building better hardware.</p>
<p>My colleague, Marin Soljacic, for example,</p>
<p>is working on an optical chip for much faster machine</p>
<p>learning, where the computation is done</p>
<p>not by moving electrons around, but by moving photons around,</p>
<p>dramatically less energy use, faster, better.</p>
<p>We can also help AI a lot, I think,</p>
<p>by having a different set of tools</p>
<p>and a different, maybe more audacious attitude.</p>
<p>AI has, to a significant extent, been an engineering discipline</p>
<p>where you&rsquo;re just trying to make things that work</p>
<p>and being more interested in maybe selling them</p>
<p>than in figuring out exactly how they work</p>
<p>and proving theorems about that they will always work.</p>
<p>Contrast that with physics.</p>
<p>When Elon Musk sends a rocket to the International Space</p>
<p>Station, they didn&rsquo;t just train with machine learning.</p>
<p>Oh, let&rsquo;s fire it a little bit more to the left,</p>
<p>a bit more to the right.</p>
<p>Oh, that also missed.</p>
<p>Let&rsquo;s try here.</p>
<p>No, we figured out Newton&rsquo;s laws of gravitation and other things</p>
<p>and got a really deep fundamental understanding.</p>
<p>And that&rsquo;s what gives us such confidence in rockets.</p>
<p>And my vision is that in the future,</p>
<p>all machine learning systems that actually have impact</p>
<p>on people&rsquo;s lives will be understood</p>
<p>at a really, really deep level.</p>
<p>So we trust them, not because some sales rep told us to,</p>
<p>but because they&rsquo;ve earned our trust.</p>
<p>And really safety critical things</p>
<p>even prove that they will always do what we expect them to do.</p>
<p>That&rsquo;s very much the physics mindset.</p>
<p>So it&rsquo;s interesting, if you look at big breakthroughs</p>
<p>that have happened in machine learning this year,</p>
<p>from dancing robots, it&rsquo;s pretty fantastic.</p>
<p>Not just because it&rsquo;s cool, but if you just</p>
<p>think about not that many years ago,</p>
<p>this YouTube video at this DARPA challenge with the MIT robot</p>
<p>comes out of the car and face plants.</p>
<p>How far we&rsquo;ve come in just a few years.</p>
<p>Similarly, Alpha Fold 2, crushing the protein folding</p>
<p>problem.</p>
<p>We can talk more about implications</p>
<p>for medical research and stuff.</p>
<p>But hey, that&rsquo;s huge progress.</p>
<p>You can look at the GPT3 that can spout off</p>
<p>English text, which sometimes really, really blows you away.</p>
<p>You can look at DeepMind&rsquo;s MuZero,</p>
<p>which doesn&rsquo;t just kick our butt in Go and Chess and Shogi,</p>
<p>but also in all these Atari games.</p>
<p>And you don&rsquo;t even have to teach it the rules now.</p>
<p>What all of those have in common is, besides being powerful,</p>
<p>is we don&rsquo;t fully understand how they work.</p>
<p>And that&rsquo;s fine if it&rsquo;s just some dancing robots.</p>
<p>And the worst thing that can happen is they face plant.</p>
<p>Or if they&rsquo;re playing Go, and the worst thing that can happen</p>
<p>is that they make a bad move and lose the game.</p>
<p>It&rsquo;s less fine if that&rsquo;s what&rsquo;s controlling</p>
<p>your self driving car or your nuclear power plant.</p>
<p>And we&rsquo;ve seen already that even though Hollywood</p>
<p>had all these movies where they try</p>
<p>to make us worry about the wrong things,</p>
<p>like machines turning evil, the actual bad things that</p>
<p>have happened with automation have not</p>
<p>been machines turning evil.</p>
<p>They&rsquo;ve been caused by overtrust in things</p>
<p>we didn&rsquo;t understand as well as we thought we did.</p>
<p>Even very simple automated systems</p>
<p>like what Boeing put into the 737 MAX killed a lot of people.</p>
<p>Was it that that little simple system was evil?</p>
<p>Of course not.</p>
<p>But we didn&rsquo;t understand it as well as we should have.</p>
<p>And we trusted without understanding.</p>
<p>Exactly.</p>
<p>That&rsquo;s the overtrust.</p>
<p>We didn&rsquo;t even understand that we didn&rsquo;t understand.</p>
<p>The humility is really at the core of being a scientist.</p>
<p>I think step one, if you want to be a scientist,</p>
<p>is don&rsquo;t ever fool yourself into thinking you understand things</p>
<p>when you actually don&rsquo;t.</p>
<p>That&rsquo;s probably good advice for humans in general.</p>
<p>I think humility in general can do us good.</p>
<p>But in science, it&rsquo;s so spectacular.</p>
<p>Why did we have the wrong theory of gravity</p>
<p>ever from Aristotle onward until Galileo&rsquo;s time?</p>
<p>Why would we believe something so dumb as that if I throw</p>
<p>this water bottle, it&rsquo;s going to go up with constant speed</p>
<p>until it realizes that its natural motion is down?</p>
<p>It changes its mind.</p>
<p>Because people just kind of assumed Aristotle was right.</p>
<p>He&rsquo;s an authority.</p>
<p>We understand that.</p>
<p>Why did we believe things like that the sun is</p>
<p>going around the Earth?</p>
<p>Why did we believe that time flows</p>
<p>at the same rate for everyone until Einstein?</p>
<p>Same exact mistake over and over again.</p>
<p>We just weren&rsquo;t humble enough to acknowledge that we actually</p>
<p>didn&rsquo;t know for sure.</p>
<p>We assumed we knew.</p>
<p>So we didn&rsquo;t discover the truth because we</p>
<p>assumed there was nothing there to be discovered, right?</p>
<p>There was something to be discovered about the 737 Max.</p>
<p>And if you had been a bit more suspicious</p>
<p>and tested it better, we would have found it.</p>
<p>And it&rsquo;s the same thing with most harm</p>
<p>that&rsquo;s been done by automation so far, I would say.</p>
<p>So I don&rsquo;t know if you heard here of a company called</p>
<p>Knight Capital?</p>
<p>So good.</p>
<p>That means you didn&rsquo;t invest in them earlier.</p>
<p>They deployed this automated trading system,</p>
<p>all nice and shiny.</p>
<p>They didn&rsquo;t understand it as well as they thought.</p>
<p>And it went about losing $10 million</p>
<p>per minute for 44 minutes straight</p>
<p>until someone presumably was like, oh, no, shut this up.</p>
<p>Was it evil?</p>
<p>No.</p>
<p>It was, again, misplaced trust, something they didn&rsquo;t fully</p>
<p>understand, right?</p>
<p>And there have been so many, even when people</p>
<p>have been killed by robots, which is quite rare still,</p>
<p>but in factory accidents, it&rsquo;s in every single case</p>
<p>been not malice, just that the robot didn&rsquo;t understand</p>
<p>that a human is different from an auto part or whatever.</p>
<p>So this is why I think there&rsquo;s so much opportunity</p>
<p>for a physics approach, where you just aim for a higher</p>
<p>level of understanding.</p>
<p>And if you look at all these systems</p>
<p>that we talked about from reinforcement learning</p>
<p>systems and dancing robots to all these neural networks</p>
<p>that power GPT3 and go playing software and stuff,</p>
<p>they&rsquo;re all basically black boxes,</p>
<p>not so different from if you teach a human something,</p>
<p>you have no idea how their brain works, right?</p>
<p>Except the human brain, at least,</p>
<p>has been error corrected during many, many centuries</p>
<p>of evolution in a way that some of these systems have not,</p>
<p>right?</p>
<p>And my MIT research is entirely focused</p>
<p>on demystifying this black box, intelligible intelligence</p>
<p>is my slogan.</p>
<p>That&rsquo;s a good line, intelligible intelligence.</p>
<p>Yeah, that we shouldn&rsquo;t settle for something</p>
<p>that seems intelligent, but it should</p>
<p>be intelligible so that we actually trust it</p>
<p>because we understand it, right?</p>
<p>Like, again, Elon trusts his rockets</p>
<p>because he understands Newton&rsquo;s laws and thrust</p>
<p>and how everything works.</p>
<p>And can I tell you why I&rsquo;m optimistic about this?</p>
<p>Yes.</p>
<p>I think we&rsquo;ve made a bit of a mistake</p>
<p>where some people still think that somehow we&rsquo;re never going</p>
<p>to understand neural networks.</p>
<p>We&rsquo;re just going to have to learn to live with this.</p>
<p>It&rsquo;s this very powerful black box.</p>
<p>Basically, for those who haven&rsquo;t spent time</p>
<p>building their own, it&rsquo;s super simple what happens inside.</p>
<p>You send in a long list of numbers,</p>
<p>and then you do a bunch of operations on them,</p>
<p>multiply by matrices, et cetera, et cetera,</p>
<p>and some other numbers come out that&rsquo;s output of it.</p>
<p>And then there are a bunch of knobs you can tune.</p>
<p>And when you change them, it affects the computation,</p>
<p>the input output relation.</p>
<p>And then you just give the computer</p>
<p>some definition of good, and it keeps optimizing these knobs</p>
<p>until it performs as good as possible.</p>
<p>And often, you go like, wow, that&rsquo;s really good.</p>
<p>This robot can dance, or this machine</p>
<p>is beating me at chess now.</p>
<p>And in the end, you have something</p>
<p>which, even though you can look inside it,</p>
<p>you have very little idea of how it works.</p>
<p>You can print out tables of all the millions of parameters</p>
<p>in there.</p>
<p>Is it crystal clear now how it&rsquo;s working?</p>
<p>No, of course not.</p>
<p>Many of my colleagues seem willing to settle for that.</p>
<p>And I&rsquo;m like, no, that&rsquo;s like the halfway point.</p>
<p>Some have even gone as far as sort of guessing</p>
<p>that the mistrutability of this is</p>
<p>where some of the power comes from,</p>
<p>and some sort of mysticism.</p>
<p>I think that&rsquo;s total nonsense.</p>
<p>I think the real power of neural networks</p>
<p>comes not from inscrutability, but from differentiability.</p>
<p>And what I mean by that is simply</p>
<p>that the output changes only smoothly if you tweak your knobs.</p>
<p>And then you can use all these powerful methods</p>
<p>we have for optimization in science.</p>
<p>We can just tweak them a little bit and see,</p>
<p>did that get better or worse?</p>
<p>That&rsquo;s the fundamental idea of machine learning,</p>
<p>that the machine itself can keep optimizing</p>
<p>until it gets better.</p>
<p>Suppose you wrote this algorithm instead in Python</p>
<p>or some other programming language,</p>
<p>and then what the knobs did was they just changed</p>
<p>random letters in your code.</p>
<p>Now it would just epically fail.</p>
<p>You change one thing, and instead of saying print,</p>
<p>it says, synth, syntax error.</p>
<p>You don&rsquo;t even know, was that for the better</p>
<p>or for the worse, right?</p>
<p>This, to me, is what I believe is</p>
<p>the fundamental power of neural networks.</p>
<p>And just to clarify, the changing</p>
<p>of the different letters in a program</p>
<p>would not be a differentiable process.</p>
<p>It would make it an invalid program, typically.</p>
<p>And then you wouldn&rsquo;t even know if you changed more letters</p>
<p>if it would make it work again, right?</p>
<p>So that&rsquo;s the magic of neural networks, the inscrutability.</p>
<p>The differentiability, that every setting of the parameters</p>
<p>is a program, and you can tell is it better or worse, right?</p>
<p>And so.</p>
<p>So you don&rsquo;t like the poetry of the mystery of neural networks</p>
<p>as the source of its power?</p>
<p>I generally like poetry, but.</p>
<p>Not in this case.</p>
<p>It&rsquo;s so misleading.</p>
<p>And above all, it shortchanges us.</p>
<p>It makes us underestimate the good things</p>
<p>we can accomplish.</p>
<p>So what we&rsquo;ve been doing in my group</p>
<p>is basically step one, train the mysterious neural network</p>
<p>to do something well.</p>
<p>And then step two, do some additional AI techniques</p>
<p>to see if we can now transform this black box into something</p>
<p>equally intelligent that you can actually understand.</p>
<p>So for example, I&rsquo;ll give you one example, this AI Feynman</p>
<p>project that we just published, right?</p>
<p>So we took the 100 most famous or complicated equations</p>
<p>from one of my favorite physics textbooks,</p>
<p>in fact, the one that got me into physics</p>
<p>in the first place, the Feynman lectures on physics.</p>
<p>And so you have a formula.</p>
<p>Maybe it has what goes into the formula</p>
<p>as six different variables, and then what comes out as one.</p>
<p>So then you can make a giant Excel spreadsheet</p>
<p>with seven columns.</p>
<p>You put in just random numbers for the six columns</p>
<p>for those six input variables, and then you</p>
<p>calculate with a formula the seventh column, the output.</p>
<p>So maybe it&rsquo;s like the force equals in the last column</p>
<p>some function of the other.</p>
<p>And now the task is, OK, if I don&rsquo;t tell you</p>
<p>what the formula was, can you figure that out</p>
<p>from looking at my spreadsheet I gave you?</p>
<p>This problem is called symbolic regression.</p>
<p>If I tell you that the formula is</p>
<p>what we call a linear formula, so it&rsquo;s just</p>
<p>that the output is sum of all the things, input, the times,</p>
<p>some constants, that&rsquo;s the famous easy problem</p>
<p>we can solve.</p>
<p>We do it all the time in science and engineering.</p>
<p>But the general one, if it&rsquo;s more complicated functions</p>
<p>with logarithms or cosines or other math,</p>
<p>it&rsquo;s a very, very hard one and probably impossible</p>
<p>to do fast in general, just because the number of formulas</p>
<p>with n symbols just grows exponentially,</p>
<p>just like the number of passwords</p>
<p>you can make grow dramatically with length.</p>
<p>But we had this idea that if you first</p>
<p>have a neural network that can actually approximate</p>
<p>the formula, you just trained it,</p>
<p>even if you don&rsquo;t understand how it works,</p>
<p>that can be the first step towards actually understanding</p>
<p>how it works.</p>
<p>So that&rsquo;s what we do first.</p>
<p>And then we study that neural network now</p>
<p>and put in all sorts of other data</p>
<p>that wasn&rsquo;t in the original training data</p>
<p>and use that to discover simplifying</p>
<p>properties of the formula.</p>
<p>And that lets us break it apart, often</p>
<p>into many simpler pieces in a kind of divide</p>
<p>and conquer approach.</p>
<p>So we were able to solve all of those 100 formulas,</p>
<p>discover them automatically, plus a whole bunch</p>
<p>of other ones.</p>
<p>And it&rsquo;s actually kind of humbling</p>
<p>to see that this code, which anyone who wants now</p>
<p>is listening to this, can type pip install AI Feynman</p>
<p>on the computer and run it.</p>
<p>It can actually do what Johannes Kepler spent four years doing</p>
<p>when he stared at Mars data until he was like,</p>
<p>finally, Eureka, this is an ellipse.</p>
<p>This will do it automatically for you in one hour.</p>
<p>Or Max Planck, he was looking at how much radiation comes out</p>
<p>from different wavelengths from a hot object</p>
<p>and discovered the famous blackbody formula.</p>
<p>This discovers it automatically.</p>
<p>I&rsquo;m actually excited about seeing</p>
<p>if we can discover not just old formulas again,</p>
<p>but new formulas that no one has seen before.</p>
<p>I do like this process of using kind of a neural network</p>
<p>to find some basic insights and then dissecting</p>
<p>the neural network to then gain the final.</p>
<p>So in that way, you&rsquo;ve forcing the explainability issue,</p>
<p>really trying to analyze the neural network for the things</p>
<p>it knows in order to come up with the final beautiful,</p>
<p>simple theory underlying the initial system</p>
<p>that you were looking at.</p>
<p>I love that.</p>
<p>And the reason I&rsquo;m so optimistic that it</p>
<p>can be generalized to so much more</p>
<p>is because that&rsquo;s exactly what we do as human scientists.</p>
<p>Think of Galileo, whom we mentioned, right?</p>
<p>I bet when he was a little kid, if his dad threw him an apple,</p>
<p>he would catch it.</p>
<p>Why?</p>
<p>Because he had a neural network in his brain</p>
<p>that he had trained to predict the parabolic orbit of apples</p>
<p>that are thrown under gravity.</p>
<p>If you throw a tennis ball to a dog,</p>
<p>it also has this same ability of deep learning</p>
<p>to figure out how the ball is going to move and catch it.</p>
<p>But Galileo went one step further when he got older.</p>
<p>He went back and was like, wait a minute.</p>
<p>I can write down a formula for this.</p>
<p>Y equals x squared, a parabola.</p>
<p>And he helped revolutionize physics as we know it, right?</p>
<p>So there was a basic neural network</p>
<p>in there from childhood that captured the experiences</p>
<p>of observing different kinds of trajectories.</p>
<p>And then he was able to go back in</p>
<p>with another extra little neural network</p>
<p>and analyze all those experiences and be like,</p>
<p>wait a minute.</p>
<p>There&rsquo;s a deeper rule here.</p>
<p>Exactly.</p>
<p>He was able to distill out in symbolic form</p>
<p>what that complicated black box neural network was doing.</p>
<p>Not only did the formula he got ultimately</p>
<p>become more accurate, and similarly, this</p>
<p>is how Newton got Newton&rsquo;s laws, which</p>
<p>is why Elon can send rockets to the space station now, right?</p>
<p>So it&rsquo;s not only more accurate, but it&rsquo;s also simpler,</p>
<p>much simpler.</p>
<p>And it&rsquo;s so simple that we can actually describe it</p>
<p>to our friends and each other, right?</p>
<p>We&rsquo;ve talked about it just in the context of physics now.</p>
<p>But hey, isn&rsquo;t this what we&rsquo;re doing when we&rsquo;re</p>
<p>talking to each other also?</p>
<p>We go around with our neural networks,</p>
<p>just like dogs and cats and chipmunks and Blue Jays.</p>
<p>And we experience things in the world.</p>
<p>But then we humans do this additional step</p>
<p>on top of that, where we then distill out</p>
<p>certain high level knowledge that we&rsquo;ve extracted from this</p>
<p>in a way that we can communicate it</p>
<p>to each other in a symbolic form in English in this case, right?</p>
<p>So if we can do it and we believe</p>
<p>that we are information processing entities,</p>
<p>then we should be able to make machine learning that</p>
<p>does it also.</p>
<p>Well, do you think the entire thing could be learning?</p>
<p>Because this dissection process, like for AI Feynman,</p>
<p>the secondary stage feels like something like reasoning.</p>
<p>And the initial step feels more like the more basic kind</p>
<p>of differentiable learning.</p>
<p>Do you think the whole thing could be differentiable</p>
<p>learning?</p>
<p>Do you think the whole thing could be basically neural</p>
<p>networks on top of each other?</p>
<p>It&rsquo;s like turtles all the way down.</p>
<p>Could it be neural networks all the way down?</p>
<p>I mean, that&rsquo;s a really interesting question.</p>
<p>We know that in your case, it is neural networks all the way</p>
<p>down because that&rsquo;s all you have in your skull</p>
<p>is a bunch of neurons doing their thing, right?</p>
<p>But if you ask the question more generally,</p>
<p>what algorithms are being used in your brain,</p>
<p>I think it&rsquo;s super interesting to compare.</p>
<p>I think we&rsquo;ve gone a little bit backwards historically</p>
<p>because we humans first discovered good old fashioned</p>
<p>AI, the logic based AI that we often call GoFi</p>
<p>for good old fashioned AI.</p>
<p>And then more recently, we did machine learning</p>
<p>because it required bigger computers.</p>
<p>So we had to discover it later.</p>
<p>So we think of machine learning with neural networks</p>
<p>as the modern thing and the logic based AI</p>
<p>as the old fashioned thing.</p>
<p>But if you look at evolution on Earth,</p>
<p>it&rsquo;s actually been the other way around.</p>
<p>I would say that, for example, an eagle</p>
<p>has a better vision system than I have using.</p>
<p>And dogs are just as good at casting tennis balls as I am.</p>
<p>All this stuff which is done by training in neural network</p>
<p>and not interpreting it in words is</p>
<p>something so many of our animal friends can do,</p>
<p>at least as well as us, right?</p>
<p>What is it that we humans can do that the chipmunks</p>
<p>and the eagles cannot?</p>
<p>It&rsquo;s more to do with this logic based stuff, right,</p>
<p>where we can extract out information</p>
<p>in symbols, in language, and now even with equations</p>
<p>if you&rsquo;re a scientist, right?</p>
<p>So basically what happened was first we</p>
<p>built these computers that could multiply numbers real fast</p>
<p>and manipulate symbols.</p>
<p>And we felt they were pretty dumb.</p>
<p>And then we made neural networks that</p>
<p>can see as well as a cat can and do</p>
<p>a lot of this inscrutable black box neural networks.</p>
<p>What we humans can do also is put the two together</p>
<p>in a useful way.</p>
<p>Yes, in our own brain.</p>
<p>So if we ever want to get artificial general intelligence</p>
<p>that can do all jobs as well as humans can, right,</p>
<p>then that&rsquo;s what&rsquo;s going to be required</p>
<p>to be able to combine the neural networks with symbolic,</p>
<p>combine the old AI with the new AI in a good way.</p>
<p>We do it in our brains.</p>
<p>And there seems to be basically two strategies</p>
<p>I see in industry now.</p>
<p>One scares the heebie jeebies out of me,</p>
<p>and the other one I find much more encouraging.</p>
<p>OK, which one?</p>
<p>Can we break them apart?</p>
<p>Which of the two?</p>
<p>The one that scares the heebie jeebies out of me</p>
<p>is this attitude that we&rsquo;re just going</p>
<p>to make ever bigger systems that we still</p>
<p>don&rsquo;t understand until they can be as smart as humans.</p>
<p>What could possibly go wrong?</p>
<p>I think it&rsquo;s just such a reckless thing to do.</p>
<p>And unfortunately, if we actually</p>
<p>succeed as a species to build artificial general intelligence,</p>
<p>then we still have no clue how it works.</p>
<p>I think at least 50% chance we&rsquo;re</p>
<p>going to be extinct before too long.</p>
<p>It&rsquo;s just going to be an utter epic own goal.</p>
<p>So it&rsquo;s that 44 minute losing money problem or the paper clip</p>
<p>problem where we don&rsquo;t understand how it works,</p>
<p>and it just in a matter of seconds</p>
<p>runs away in some kind of direction</p>
<p>that&rsquo;s going to be very problematic.</p>
<p>Even long before, you have to worry about the machines</p>
<p>themselves somehow deciding to do things.</p>
<p>And to us, we have to worry about people using machines</p>
<p>that are short of AGI and power to do bad things.</p>
<p>I mean, just take a moment.</p>
<p>And if anyone is not worried particularly about advanced AI,</p>
<p>just take 10 seconds and just think</p>
<p>about your least favorite leader on the planet right now.</p>
<p>Don&rsquo;t tell me who it is.</p>
<p>I want to keep this apolitical.</p>
<p>But just see the face in front of you,</p>
<p>that person, for 10 seconds.</p>
<p>Now imagine that that person has this incredibly powerful AI</p>
<p>under their control and can use it</p>
<p>to impose their will on the whole planet.</p>
<p>How does that make you feel?</p>
<p>Yeah.</p>
<p>So can we break that apart just briefly?</p>
<p>For the 50% chance that we&rsquo;ll run</p>
<p>to trouble with this approach, do you</p>
<p>see the bigger worry in that leader or humans</p>
<p>using the system to do damage?</p>
<p>Or are you more worried, and I think I&rsquo;m in this camp,</p>
<p>more worried about accidental, unintentional destruction</p>
<p>of everything?</p>
<p>So humans trying to do good, and in a way</p>
<p>where everyone agrees it&rsquo;s kind of good,</p>
<p>it&rsquo;s just they&rsquo;re trying to do good without understanding.</p>
<p>Because I think every evil leader in history</p>
<p>thought they&rsquo;re, to some degree, thought</p>
<p>they&rsquo;re trying to do good.</p>
<p>Oh, yeah.</p>
<p>I&rsquo;m sure Hitler thought he was doing good.</p>
<p>Yeah.</p>
<p>I&rsquo;ve been reading a lot about Stalin.</p>
<p>I&rsquo;m sure Stalin is from, he legitimately</p>
<p>thought that communism was good for the world,</p>
<p>and that he was doing good.</p>
<p>I think Mao Zedong thought what he was doing with the Great</p>
<p>Leap Forward was good too.</p>
<p>Yeah.</p>
<p>I&rsquo;m actually concerned about both of those.</p>
<p>Before, I promised to answer this in detail,</p>
<p>but before we do that, let me finish</p>
<p>answering the first question.</p>
<p>Because I told you that there were two different routes we</p>
<p>could get to artificial general intelligence,</p>
<p>and one scares the hell out of me,</p>
<p>which is this one where we build something,</p>
<p>we just say bigger neural networks, ever more hardware,</p>
<p>and just train the heck out of more data,</p>
<p>and poof, now it&rsquo;s very powerful.</p>
<p>That, I think, is the most unsafe and reckless approach.</p>
<p>The alternative to that is the intelligible intelligence</p>
<p>approach instead, where we say neural networks is just</p>
<p>a tool for the first step to get the intuition,</p>
<p>but then we&rsquo;re going to spend also</p>
<p>serious resources on other AI techniques</p>
<p>for demystifying this black box and figuring out</p>
<p>what it&rsquo;s actually doing so we can convert it</p>
<p>into something that&rsquo;s equally intelligent,</p>
<p>but that we actually understand what it&rsquo;s doing.</p>
<p>Maybe we can even prove theorems about it,</p>
<p>that this car here will never be hacked when it&rsquo;s driving,</p>
<p>because here is the proof.</p>
<p>There is a whole science of this.</p>
<p>It doesn&rsquo;t work for neural networks</p>
<p>that are big black boxes, but it works well</p>
<p>and works with certain other kinds of codes, right?</p>
<p>That approach, I think, is much more promising.</p>
<p>That&rsquo;s exactly why I&rsquo;m working on it, frankly,</p>
<p>not just because I think it&rsquo;s cool for science,</p>
<p>but because I think the more we understand these systems,</p>
<p>the better the chances that we can</p>
<p>make them do the things that are good for us</p>
<p>that are actually intended, not unintended.</p>
<p>So you think it&rsquo;s possible to prove things</p>
<p>about something as complicated as a neural network?</p>
<p>That&rsquo;s the hope?</p>
<p>Well, ideally, there&rsquo;s no reason it</p>
<p>has to be a neural network in the end either, right?</p>
<p>We discovered Newton&rsquo;s laws of gravity</p>
<p>with neural network in Newton&rsquo;s head.</p>
<p>But that&rsquo;s not the way it&rsquo;s programmed into the navigation</p>
<p>system of Elon Musk&rsquo;s rocket anymore.</p>
<p>It&rsquo;s written in C++, or I don&rsquo;t know</p>
<p>what language he uses exactly.</p>
<p>And then there are software tools called symbolic</p>
<p>verification.</p>
<p>DARPA and the US military has done a lot of really great</p>
<p>research on this, because they really</p>
<p>want to understand that when they build weapon systems,</p>
<p>they don&rsquo;t just go fire at random or malfunction, right?</p>
<p>And there is even a whole operating system</p>
<p>called Cell 3 that&rsquo;s been developed by a DARPA grant,</p>
<p>where you can actually mathematically prove</p>
<p>that this thing can never be hacked.</p>
<p>Wow.</p>
<p>One day, I hope that will be something</p>
<p>you can say about the OS that&rsquo;s running on our laptops too.</p>
<p>As you know, we&rsquo;re not there.</p>
<p>But I think we should be ambitious, frankly.</p>
<p>And if we can use machine learning</p>
<p>to help do the proofs and so on as well,</p>
<p>then it&rsquo;s much easier to verify that a proof is correct</p>
<p>than to come up with a proof in the first place.</p>
<p>That&rsquo;s really the core idea here.</p>
<p>If someone comes on your podcast and says</p>
<p>they proved the Riemann hypothesis</p>
<p>or some sensational new theorem, it&rsquo;s</p>
<p>much easier for someone else, take some smart grad,</p>
<p>math grad students to check, oh, there&rsquo;s an error here</p>
<p>on equation five, or this really checks out,</p>
<p>than it was to discover the proof.</p>
<p>Yeah, although some of those proofs are pretty complicated.</p>
<p>But yes, it&rsquo;s still nevertheless much easier</p>
<p>to verify the proof.</p>
<p>I love the optimism.</p>
<p>We kind of, even with the security of systems,</p>
<p>there&rsquo;s a kind of cynicism that pervades people</p>
<p>who think about this, which is like, oh, it&rsquo;s hopeless.</p>
<p>I mean, in the same sense, exactly like you&rsquo;re saying</p>
<p>when you own networks, oh, it&rsquo;s hopeless to understand</p>
<p>what&rsquo;s happening.</p>
<p>With security, people are just like, well,</p>
<p>it&rsquo;s always going, there&rsquo;s always going to be</p>
<p>attack vectors, like ways to attack the system.</p>
<p>But you&rsquo;re right, we&rsquo;re just very new</p>
<p>with these computational systems.</p>
<p>We&rsquo;re new with these intelligent systems.</p>
<p>And it&rsquo;s not out of the realm of possibly,</p>
<p>just like people that understand the movement</p>
<p>of the stars and the planets and so on.</p>
<p>It&rsquo;s entirely possible that within, hopefully soon,</p>
<p>but it could be within 100 years,</p>
<p>we start to have an obvious laws of gravity</p>
<p>about intelligence and God forbid about consciousness too.</p>
<p>That one is&hellip;</p>
<p>Agreed.</p>
<p>I think, of course, if you&rsquo;re selling computers</p>
<p>that get hacked a lot, that&rsquo;s in your interest</p>
<p>as a company that people think it&rsquo;s impossible</p>
<p>to make it safe, but he&rsquo;s going to get the idea</p>
<p>of suing you.</p>
<p>I want to really inject optimism here.</p>
<p>It&rsquo;s absolutely possible to do much better</p>
<p>than we&rsquo;re doing now.</p>
<p>And your laptop does so much stuff.</p>
<p>You don&rsquo;t need the music player to be super safe</p>
<p>in your future self driving car, right?</p>
<p>If someone hacks it and starts playing music</p>
<p>you don&rsquo;t like, the world won&rsquo;t end.</p>
<p>But what you can do is you can break out</p>
<p>and say that your drive computer that controls your safety</p>
<p>must be completely physically decoupled entirely</p>
<p>from the entertainment system.</p>
<p>And it must physically be such that it can&rsquo;t take on</p>
<p>over the air updates while you&rsquo;re driving.</p>
<p>And it can have ultimately some operating system on it</p>
<p>which is symbolically verified and proven</p>
<p>that it&rsquo;s always going to do what it&rsquo;s supposed to do, right?</p>
<p>We can basically have, and companies should take</p>
<p>that attitude too.</p>
<p>They should look at everything they do and say</p>
<p>what are the few systems in our company</p>
<p>that threaten the whole life of the company</p>
<p>if they get hacked and have the highest standards for them.</p>
<p>And then they can save money by going for the el cheapo</p>
<p>poorly understood stuff for the rest.</p>
<p>This is very feasible, I think.</p>
<p>And coming back to the bigger question</p>
<p>that you worried about that there&rsquo;ll be unintentional</p>
<p>failures, I think there are two quite separate risks here.</p>
<p>Right?</p>
<p>We talked a lot about one of them</p>
<p>which is that the goals are noble of the human.</p>
<p>The human says, I want this airplane to not crash</p>
<p>because this is not Muhammad Atta</p>
<p>now flying the airplane, right?</p>
<p>And now there&rsquo;s this technical challenge</p>
<p>of making sure that the autopilot is actually</p>
<p>gonna behave as the pilot wants.</p>
<p>If you set that aside, there&rsquo;s also the separate question.</p>
<p>How do you make sure that the goals of the pilot</p>
<p>are actually aligned with the goals of the passenger?</p>
<p>How do you make sure very much more broadly</p>
<p>that if we can all agree as a species</p>
<p>that we would like things to kind of go well</p>
<p>for humanity as a whole, that the goals are aligned here.</p>
<p>The alignment problem.</p>
<p>And yeah, there&rsquo;s been a lot of progress</p>
<p>in the sense that there&rsquo;s suddenly huge amounts</p>
<p>of research going on on it about it.</p>
<p>I&rsquo;m very grateful to Elon Musk</p>
<p>for giving us that money five years ago</p>
<p>so we could launch the first research program</p>
<p>on technical AI safety and alignment.</p>
<p>There&rsquo;s a lot of stuff happening.</p>
<p>But I think we need to do more than just make sure</p>
<p>little machines do always what their owners do.</p>
<p>That wouldn&rsquo;t have prevented September 11th</p>
<p>if Muhammad Atta said, okay, autopilot,</p>
<p>please fly into World Trade Center.</p>
<p>And it&rsquo;s like, okay.</p>
<p>That even happened in a different situation.</p>
<p>There was this depressed pilot named Andreas Lubitz, right?</p>
<p>Who told his German wings passenger jet</p>
<p>to fly into the Alps.</p>
<p>He just told the computer to change the altitude</p>
<p>to a hundred meters or something like that.</p>
<p>And you know what the computer said?</p>
<p>Okay.</p>
<p>And it had the freaking topographical map of the Alps</p>
<p>in there, it had GPS, everything.</p>
<p>No one had bothered teaching it</p>
<p>even the basic kindergarten ethics of like,</p>
<p>no, we never want airplanes to fly into mountains</p>
<p>under any circumstances.</p>
<p>And so we have to think beyond just the technical issues</p>
<p>and think about how do we align in general incentives</p>
<p>on this planet for the greater good?</p>
<p>So starting with simple stuff like that,</p>
<p>every airplane that has a computer in it</p>
<p>should be taught whatever kindergarten ethics</p>
<p>that&rsquo;s smart enough to understand.</p>
<p>Like, no, don&rsquo;t fly into fixed objects</p>
<p>if the pilot tells you to do so.</p>
<p>Then go on autopilot mode.</p>
<p>Send an email to the cops and land at the latest airport,</p>
<p>nearest airport, you know.</p>
<p>Any car with a forward facing camera</p>
<p>should just be programmed by the manufacturer</p>
<p>so that it will never accelerate into a human ever.</p>
<p>That would avoid things like the NIS attack</p>
<p>and many horrible terrorist vehicle attacks</p>
<p>where they deliberately did that, right?</p>
<p>This was not some sort of thing,</p>
<p>oh, you know, US and China, different views on,</p>
<p>no, there was not a single car manufacturer</p>
<p>in the world, right, who wanted the cars to do this.</p>
<p>They just hadn&rsquo;t thought to do the alignment.</p>
<p>And if you look at more broadly problems</p>
<p>that happen on this planet,</p>
<p>the vast majority have to do a poor alignment.</p>
<p>I mean, think about, let&rsquo;s go back really big</p>
<p>because I know you&rsquo;re so good at that.</p>
<p>Let&rsquo;s go big, yeah.</p>
<p>Yeah, so long ago in evolution, we had these genes.</p>
<p>And they wanted to make copies of themselves.</p>
<p>That&rsquo;s really all they cared about.</p>
<p>So some genes said, hey, I&rsquo;m gonna build a brain</p>
<p>on this body I&rsquo;m in so that I can get better</p>
<p>at making copies of myself.</p>
<p>And then they decided for their benefit</p>
<p>to get copied more, to align your brain&rsquo;s incentives</p>
<p>with their incentives.</p>
<p>So it didn&rsquo;t want you to starve to death.</p>
<p>So it gave you an incentive to eat</p>
<p>and it wanted you to make copies of the genes.</p>
<p>So it gave you incentive to fall in love</p>
<p>and do all sorts of naughty things</p>
<p>to make copies of itself, right?</p>
<p>So that was successful value alignment done on the genes.</p>
<p>They created something more intelligent than themselves,</p>
<p>but they made sure to try to align the values.</p>
<p>But then something went a little bit wrong</p>
<p>against the idea of what the genes wanted</p>
<p>because a lot of humans discovered,</p>
<p>hey, you know, yeah, we really like this business</p>
<p>about sex that the genes have made us enjoy,</p>
<p>but we don&rsquo;t wanna have babies right now.</p>
<p>So we&rsquo;re gonna hack the genes and use birth control.</p>
<p>And I really feel like drinking a Coca Cola right now,</p>
<p>but I don&rsquo;t wanna get a potbelly,</p>
<p>so I&rsquo;m gonna drink Diet Coke.</p>
<p>We have all these things we&rsquo;ve figured out</p>
<p>because we&rsquo;re smarter than the genes,</p>
<p>how we can actually subvert their intentions.</p>
<p>So it&rsquo;s not surprising that we humans now,</p>
<p>when we are in the role of these genes,</p>
<p>creating other nonhuman entities with a lot of power,</p>
<p>have to face the same exact challenge.</p>
<p>How do we make other powerful entities</p>
<p>have incentives that are aligned with ours?</p>
<p>And so they won&rsquo;t hack them.</p>
<p>Corporations, for example, right?</p>
<p>We humans decided to create corporations</p>
<p>because it can benefit us greatly.</p>
<p>Now all of a sudden there&rsquo;s a supermarket.</p>
<p>I can go buy food there.</p>
<p>I don&rsquo;t have to hunt.</p>
<p>Awesome, and then to make sure that this corporation</p>
<p>would do things that were good for us and not bad for us,</p>
<p>we created institutions to keep them in check.</p>
<p>Like if the local supermarket sells poisonous food,</p>
<p>then the owners of the supermarket</p>
<p>have to spend some years reflecting behind bars, right?</p>
<p>So we created incentives to align them.</p>
<p>But of course, just like we were able to see</p>
<p>through this thing and you develop birth control,</p>
<p>if you&rsquo;re a powerful corporation,</p>
<p>you also have an incentive to try to hack the institutions</p>
<p>that are supposed to govern you.</p>
<p>Because you ultimately, as a corporation,</p>
<p>have an incentive to maximize your profit.</p>
<p>Just like you have an incentive</p>
<p>to maximize the enjoyment your brain has,</p>
<p>not for your genes.</p>
<p>So if they can figure out a way of bribing regulators,</p>
<p>then they&rsquo;re gonna do that.</p>
<p>In the US, we kind of caught onto that</p>
<p>and made laws against corruption and bribery.</p>
<p>Then in the late 1800s, Teddy Roosevelt realized that,</p>
<p>no, we were still being kind of hacked</p>
<p>because the Massachusetts Railroad companies</p>
<p>had like a bigger budget than the state of Massachusetts</p>
<p>and they were doing a lot of very corrupt stuff.</p>
<p>So he did the whole trust busting thing</p>
<p>to try to align these other nonhuman entities,</p>
<p>the companies, again,</p>
<p>more with the incentives of Americans as a whole.</p>
<p>It&rsquo;s not surprising, though,</p>
<p>that this is a battle you have to keep fighting.</p>
<p>Now we have even larger companies than we ever had before.</p>
<p>And of course, they&rsquo;re gonna try to, again,</p>
<p>subvert the institutions.</p>
<p>Not because, I think people make a mistake</p>
<p>of getting all too,</p>
<p>thinking about things in terms of good and evil.</p>
<p>Like arguing about whether corporations are good or evil,</p>
<p>or whether robots are good or evil.</p>
<p>A robot isn&rsquo;t good or evil, it&rsquo;s a tool.</p>
<p>And you can use it for great things</p>
<p>like robotic surgery or for bad things.</p>
<p>And a corporation also is a tool, of course.</p>
<p>And if you have good incentives to the corporation,</p>
<p>it&rsquo;ll do great things,</p>
<p>like start a hospital or a grocery store.</p>
<p>If you have any bad incentives,</p>
<p>then it&rsquo;s gonna start maybe marketing addictive drugs</p>
<p>to people and you&rsquo;ll have an opioid epidemic, right?</p>
<p>It&rsquo;s all about,</p>
<p>we should not make the mistake of getting into</p>
<p>some sort of fairytale, good, evil thing</p>
<p>about corporations or robots.</p>
<p>We should focus on putting the right incentives in place.</p>
<p>My optimistic vision is that if we can do that,</p>
<p>then we can really get good things.</p>
<p>We&rsquo;re not doing so great with that right now,</p>
<p>either on AI, I think,</p>
<p>or on other intelligent nonhuman entities,</p>
<p>like big companies, right?</p>
<p>We just have a new second generation of AI</p>
<p>and a secretary of defense who&rsquo;s gonna start up now</p>
<p>in the Biden administration,</p>
<p>who was an active member of the board of Raytheon,</p>
<p>for example.</p>
<p>So, I have nothing against Raytheon.</p>
<p>I&rsquo;m not a pacifist,</p>
<p>but there&rsquo;s an obvious conflict of interest</p>
<p>if someone is in the job where they decide</p>
<p>who they&rsquo;re gonna contract with.</p>
<p>And I think somehow we have,</p>
<p>maybe we need another Teddy Roosevelt to come along again</p>
<p>and say, hey, you know,</p>
<p>we want what&rsquo;s good for all Americans,</p>
<p>and we need to go do some serious realigning again</p>
<p>of the incentives that we&rsquo;re giving to these big companies.</p>
<p>And then we&rsquo;re gonna be better off.</p>
<p>It seems that naturally with human beings,</p>
<p>just like you beautifully described the history</p>
<p>of this whole thing,</p>
<p>of it all started with the genes</p>
<p>and they&rsquo;re probably pretty upset</p>
<p>by all the unintended consequences that happened since.</p>
<p>But it seems that it kind of works out,</p>
<p>like it&rsquo;s in this collective intelligence</p>
<p>that emerges at the different levels.</p>
<p>It seems to find sometimes last minute</p>
<p>a way to realign the values or keep the values aligned.</p>
<p>It&rsquo;s almost, it finds a way,</p>
<p>like different leaders, different humans pop up</p>
<p>all over the place that reset the system.</p>
<p>Do you want, I mean, do you have an explanation why that is?</p>
<p>Or is that just survivor bias?</p>
<p>And also is that different,</p>
<p>somehow fundamentally different than with AI systems</p>
<p>where you&rsquo;re no longer dealing with something</p>
<p>that was a direct, maybe companies are the same,</p>
<p>a direct byproduct of the evolutionary process?</p>
<p>I think there is one thing which has changed.</p>
<p>That&rsquo;s why I&rsquo;m not all optimistic.</p>
<p>That&rsquo;s why I think there&rsquo;s about a 50% chance</p>
<p>if we take the dumb route with artificial intelligence</p>
<p>that humanity will be extinct in this century.</p>
<p>First, just the big picture.</p>
<p>Yeah, companies need to have the right incentives.</p>
<p>Even governments, right?</p>
<p>We used to have governments,</p>
<p>usually there were just some king,</p>
<p>who was the king because his dad was the king.</p>
<p>And then there were some benefits</p>
<p>of having this powerful kingdom or empire of any sort</p>
<p>because then it could prevent a lot of local squabbles.</p>
<p>So at least everybody in that region</p>
<p>would stop warring against each other.</p>
<p>And their incentives of different cities in the kingdom</p>
<p>became more aligned, right?</p>
<p>That was the whole selling point.</p>
<p>Harare, Noel Harare has a beautiful piece</p>
<p>on how empires were collaboration enablers.</p>
<p>And then we also, Harare says,</p>
<p>invented money for that reason</p>
<p>so we could have better alignment</p>
<p>and we could do trade even with people we didn&rsquo;t know.</p>
<p>So this sort of stuff has been playing out</p>
<p>since time immemorial, right?</p>
<p>What&rsquo;s changed is that it happens on ever larger scales,</p>
<p>right?</p>
<p>The technology keeps getting better</p>
<p>because science gets better.</p>
<p>So now we can communicate over larger distances,</p>
<p>transport things fast over larger distances.</p>
<p>And so the entities get ever bigger,</p>
<p>but our planet is not getting bigger anymore.</p>
<p>So in the past, you could have one experiment</p>
<p>that just totally screwed up like Easter Island,</p>
<p>where they actually managed to have such poor alignment</p>
<p>that when they went extinct, people there,</p>
<p>there was no one else to come back and replace them, right?</p>
<p>If Elon Musk doesn&rsquo;t get us to Mars</p>
<p>and then we go extinct on a global scale,</p>
<p>then we&rsquo;re not coming back.</p>
<p>That&rsquo;s the fundamental difference.</p>
<p>And that&rsquo;s a mistake we don&rsquo;t make for that reason.</p>
<p>In the past, of course, history is full of fiascos, right?</p>
<p>But it was never the whole planet.</p>
<p>And then, okay, now there&rsquo;s this nice uninhabited land here.</p>
<p>Some other people could move in and organize things better.</p>
<p>This is different.</p>
<p>The second thing, which is also different</p>
<p>is that technology gives us so much more empowerment, right?</p>
<p>Both to do good things and also to screw up.</p>
<p>In the stone age, even if you had someone</p>
<p>whose goals were really poorly aligned,</p>
<p>like maybe he was really pissed off</p>
<p>because his stone age girlfriend dumped him</p>
<p>and he just wanted to,</p>
<p>if he wanted to kill as many people as he could,</p>
<p>how many could he really take out with a rock and a stick</p>
<p>before he was overpowered, right?</p>
<p>Just handful, right?</p>
<p>Now, with today&rsquo;s technology,</p>
<p>if we have an accidental nuclear war</p>
<p>between Russia and the US,</p>
<p>which we almost have about a dozen times,</p>
<p>and then we have a nuclear winter,</p>
<p>it could take out seven billion people</p>
<p>or six billion people, we don&rsquo;t know.</p>
<p>So the scale of the damage is bigger that we can do.</p>
<p>And there&rsquo;s obviously no law of physics</p>
<p>that says that technology will never get powerful enough</p>
<p>that we could wipe out our species entirely.</p>
<p>That would just be fantasy to think</p>
<p>that science is somehow doomed</p>
<p>to not get more powerful than that, right?</p>
<p>And it&rsquo;s not at all unfeasible in our lifetime</p>
<p>that someone could design a designer pandemic</p>
<p>which spreads as easily as COVID,</p>
<p>but just basically kills everybody.</p>
<p>We already had smallpox.</p>
<p>It killed one third of everybody who got it.</p>
<p>What do you think of the, here&rsquo;s an intuition,</p>
<p>maybe it&rsquo;s completely naive</p>
<p>and this optimistic intuition I have,</p>
<p>which it seems, and maybe it&rsquo;s a biased experience</p>
<p>that I have, but it seems like the most brilliant people</p>
<p>I&rsquo;ve met in my life all are really like</p>
<p>fundamentally good human beings.</p>
<p>And not like naive good, like they really wanna do good</p>
<p>for the world in a way that, well, maybe is aligned</p>
<p>to my sense of what good means.</p>
<p>And so I have a sense that the people</p>
<p>that will be defining the very cutting edge of technology,</p>
<p>there&rsquo;ll be much more of the ones that are doing good</p>
<p>versus the ones that are doing evil.</p>
<p>So the race, I&rsquo;m optimistic on the,</p>
<p>us always like last minute coming up with a solution.</p>
<p>So if there&rsquo;s an engineered pandemic</p>
<p>that has the capability to destroy</p>
<p>most of the human civilization,</p>
<p>it feels like to me either leading up to that before</p>
<p>or as it&rsquo;s going on, there will be,</p>
<p>we&rsquo;re able to rally the collective genius</p>
<p>of the human species.</p>
<p>I can tell by your smile that you&rsquo;re</p>
<p>at least some percentage doubtful,</p>
<p>but could that be a fundamental law of human nature?</p>
<p>That evolution only creates, like karma is beneficial,</p>
<p>good is beneficial, and therefore we&rsquo;ll be all right.</p>
<p>I hope you&rsquo;re right.</p>
<p>I would really love it if you&rsquo;re right,</p>
<p>if there&rsquo;s some sort of law of nature that says</p>
<p>that we always get lucky in the last second</p>
<p>with karma, but I prefer not playing it so close</p>
<p>and gambling on that.</p>
<p>And I think, in fact, I think it can be dangerous</p>
<p>to have too strong faith in that</p>
<p>because it makes us complacent.</p>
<p>Like if someone tells you, you never have to worry</p>
<p>about your house burning down,</p>
<p>then you&rsquo;re not gonna put in a smoke detector</p>
<p>because why would you need to?</p>
<p>Even if it&rsquo;s sometimes very simple precautions,</p>
<p>we don&rsquo;t take them.</p>
<p>If you&rsquo;re like, oh, the government is gonna take care</p>
<p>of everything for us, I can always trust my politicians.</p>
<p>I can always, we advocate our own responsibility.</p>
<p>I think it&rsquo;s a healthier attitude to say,</p>
<p>yeah, maybe things will work out.</p>
<p>Maybe I&rsquo;m actually gonna have to myself step up</p>
<p>and take responsibility.</p>
<p>And the stakes are so huge.</p>
<p>I mean, if we do this right, we can develop</p>
<p>all this ever more powerful technology</p>
<p>and cure all diseases and create a future</p>
<p>where humanity is healthy and wealthy</p>
<p>for not just the next election cycle,</p>
<p>but like billions of years throughout our universe.</p>
<p>That&rsquo;s really worth working hard for</p>
<p>and not just sitting and hoping</p>
<p>for some sort of fairytale karma.</p>
<p>Well, I just mean, so you&rsquo;re absolutely right.</p>
<p>From the perspective of the individual,</p>
<p>like for me, the primary thing should be</p>
<p>to take responsibility and to build the solutions</p>
<p>that your skillset allows.</p>
<p>Yeah, which is a lot.</p>
<p>I think we underestimate often very much</p>
<p>how much good we can do.</p>
<p>If you or anyone listening to this</p>
<p>is completely confident that our government</p>
<p>would do a perfect job on handling any future crisis</p>
<p>with engineered pandemics or future AI,</p>
<p>I actually reflect a bit on what actually happened in 2020.</p>
<p>Do you feel that the government by and large</p>
<p>around the world has handled this flawlessly?</p>
<p>That&rsquo;s a really sad and disappointing reality</p>
<p>that hopefully is a wake up call for everybody.</p>
<p>For the scientists, for the engineers,</p>
<p>for the researchers in AI especially,</p>
<p>it was disappointing to see how inefficient we were</p>
<p>at collecting the right amount of data</p>
<p>in a privacy preserving way and spreading that data</p>
<p>and utilizing that data to make decisions,</p>
<p>all that kind of stuff.</p>
<p>Yeah, I think when something bad happens to me,</p>
<p>I made myself a promise many years ago</p>
<p>that I would not be a whiner.</p>
<p>So when something bad happens to me,</p>
<p>of course it&rsquo;s a process of disappointment,</p>
<p>but then I try to focus on what did I learn from this</p>
<p>that can make me a better person in the future.</p>
<p>And there&rsquo;s usually something to be learned when I fail.</p>
<p>And I think we should all ask ourselves,</p>
<p>what can we learn from the pandemic</p>
<p>about how we can do better in the future?</p>
<p>And you mentioned there a really good lesson.</p>
<p>We were not as resilient as we thought we were</p>
<p>and we were not as prepared maybe as we wish we were.</p>
<p>You can even see very stark contrast around the planet.</p>
<p>South Korea, they have over 50 million people.</p>
<p>Do you know how many deaths they have from COVID</p>
<p>last time I checked?</p>
<p>No.</p>
<p>It&rsquo;s about 500.</p>
<p>Why is that?</p>
<p>Well, the short answer is that they had prepared.</p>
<p>They were incredibly quick,</p>
<p>incredibly quick to get on it</p>
<p>with very rapid testing and contact tracing and so on,</p>
<p>which is why they never had more cases</p>
<p>than they could contract trace effectively, right?</p>
<p>They never even had to have the kind of big lockdowns</p>
<p>we had in the West.</p>
<p>But the deeper answer to,</p>
<p>it&rsquo;s not just the Koreans are just somehow better people.</p>
<p>The reason I think they were better prepared</p>
<p>was because they had already had a pretty bad hit</p>
<p>from the SARS pandemic,</p>
<p>or which never became a pandemic,</p>
<p>something like 17 years ago, I think.</p>
<p>So it was kind of fresh memory</p>
<p>that we need to be prepared for pandemics.</p>
<p>So they were, right?</p>
<p>So maybe this is a lesson here</p>
<p>for all of us to draw from COVID</p>
<p>that rather than just wait for the next pandemic</p>
<p>or the next problem with AI getting out of control</p>
<p>or anything else,</p>
<p>maybe we should just actually set aside</p>
<p>a tiny fraction of our GDP</p>
<p>to have people very systematically</p>
<p>do some horizon scanning and say,</p>
<p>okay, what are the things that could go wrong?</p>
<p>And let&rsquo;s duke it out and see</p>
<p>which are the more likely ones</p>
<p>and which are the ones that are actually actionable</p>
<p>and then be prepared.</p>
<p>So one of the observations as one little ant slash human</p>
<p>that I am of disappointment</p>
<p>is the political division over information</p>
<p>that has been observed, that I observed this year,</p>
<p>that it seemed the discussion was less about</p>
<p>sort of what happened and understanding</p>
<p>what happened deeply and more about</p>
<p>there&rsquo;s different truths out there.</p>
<p>And it&rsquo;s like an argument,</p>
<p>my truth is better than your truth.</p>
<p>And it&rsquo;s like red versus blue or different.</p>
<p>It was like this ridiculous discourse</p>
<p>that doesn&rsquo;t seem to get at any kind of notion of the truth.</p>
<p>It&rsquo;s not like some kind of scientific process.</p>
<p>Even science got politicized in ways</p>
<p>that&rsquo;s very heartbreaking to me.</p>
<p>You have an exciting project on the AI front</p>
<p>of trying to rethink one of the,</p>
<p>you mentioned corporations.</p>
<p>There&rsquo;s one of the other collective intelligence systems</p>
<p>that have emerged through all of this is social networks.</p>
<p>And just the spread, the internet is the spread</p>
<p>of information on the internet,</p>
<p>our ability to share that information.</p>
<p>There&rsquo;s all different kinds of news sources and so on.</p>
<p>And so you said like that&rsquo;s from first principles,</p>
<p>let&rsquo;s rethink how we think about the news,</p>
<p>how we think about information.</p>
<p>Can you talk about this amazing effort</p>
<p>that you&rsquo;re undertaking?</p>
<p>Oh, I&rsquo;d love to.</p>
<p>This has been my big COVID project</p>
<p>and nights and weekends on ever since the lockdown.</p>
<p>To segue into this actually,</p>
<p>let me come back to what you said earlier</p>
<p>that you had this hope that in your experience,</p>
<p>people who you felt were very talented</p>
<p>were often idealistic and wanted to do good.</p>
<p>Frankly, I feel the same about all people by and large,</p>
<p>there are always exceptions,</p>
<p>but I think the vast majority of everybody,</p>
<p>regardless of education and whatnot,</p>
<p>really are fundamentally good, right?</p>
<p>So how can it be that people still do so much nasty stuff?</p>
<p>I think it has everything to do with this,</p>
<p>with the information that we&rsquo;re given.</p>
<p>Yes.</p>
<p>If you go into Sweden 500 years ago</p>
<p>and you start telling all the farmers</p>
<p>that those Danes in Denmark,</p>
<p>they&rsquo;re so terrible people, and we have to invade them</p>
<p>because they&rsquo;ve done all these terrible things</p>
<p>that you can&rsquo;t fact check yourself.</p>
<p>A lot of people, Swedes did that, right?</p>
<p>And we&rsquo;re seeing so much of this today in the world,</p>
<p>both geopolitically, where we are told that China is bad</p>
<p>and Russia is bad and Venezuela is bad,</p>
<p>and people in those countries are often told</p>
<p>that we are bad.</p>
<p>And we also see it at a micro level where people are told</p>
<p>that, oh, those who voted for the other party are bad people.</p>
<p>It&rsquo;s not just an intellectual disagreement,</p>
<p>but they&rsquo;re bad people and we&rsquo;re getting ever more divided.</p>
<p>So how do you reconcile this with this intrinsic goodness</p>
<p>in people?</p>
<p>I think it&rsquo;s pretty obvious that it has, again,</p>
<p>to do with the information that we&rsquo;re fed and given, right?</p>
<p>We evolved to live in small groups</p>
<p>where you might know 30 people in total, right?</p>
<p>So you then had a system that was quite good</p>
<p>for assessing who you could trust and who you could not.</p>
<p>And if someone told you that Joe there is a jerk,</p>
<p>but you had interacted with him yourself</p>
<p>and seen him in action,</p>
<p>and you would quickly realize maybe</p>
<p>that that&rsquo;s actually not quite accurate, right?</p>
<p>But now that the most people on the planet</p>
<p>are people we&rsquo;ve never met,</p>
<p>it&rsquo;s very important that we have a way</p>
<p>of trusting the information we&rsquo;re given.</p>
<p>And so, okay, so where does the news project come in?</p>
<p>Well, throughout history, you can go read Machiavelli,</p>
<p>from the 1400s, and you&rsquo;ll see how already then</p>
<p>they were busy manipulating people</p>
<p>with propaganda and stuff.</p>
<p>Propaganda is not new at all.</p>
<p>And the incentives to manipulate people</p>
<p>is just not new at all.</p>
<p>What is it that&rsquo;s new?</p>
<p>What&rsquo;s new is machine learning meets propaganda.</p>
<p>That&rsquo;s what&rsquo;s new.</p>
<p>That&rsquo;s why this has gotten so much worse.</p>
<p>Some people like to blame certain individuals,</p>
<p>like in my liberal university bubble,</p>
<p>many people blame Donald Trump and say it was his fault.</p>
<p>I see it differently.</p>
<p>I think Donald Trump just had this extreme skill</p>
<p>at playing this game in the machine learning algorithm age.</p>
<p>A game he couldn&rsquo;t have played 10 years ago.</p>
<p>So what&rsquo;s changed?</p>
<p>What&rsquo;s changed is, well, Facebook and Google</p>
<p>and other companies, and I&rsquo;m not badmouthing them,</p>
<p>I have a lot of friends who work for these companies,</p>
<p>good people, they deployed machine learning algorithms</p>
<p>just to increase their profit a little bit,</p>
<p>to just maximize the time people spent watching ads.</p>
<p>And they had totally underestimated</p>
<p>how effective they were gonna be.</p>
<p>This was, again, the black box, non intelligible intelligence.</p>
<p>They just noticed, oh, we&rsquo;re getting more ad revenue.</p>
<p>Great.</p>
<p>It took a long time until they even realized why and how</p>
<p>and how damaging this was for society.</p>
<p>Because of course, what the machine learning figured out</p>
<p>was that the by far most effective way of gluing you</p>
<p>to your little rectangle was to show you things</p>
<p>that triggered strong emotions, anger, et cetera, resentment,</p>
<p>and if it was true or not, it didn&rsquo;t really matter.</p>
<p>It was also easier to find stories that weren&rsquo;t true.</p>
<p>If you weren&rsquo;t limited, that&rsquo;s just the limitation,</p>
<p>is to show people.</p>
<p>That&rsquo;s a very limiting fact.</p>
<p>And before long, we got these amazing filter bubbles</p>
<p>on a scale we had never seen before.</p>
<p>A couple of days to the fact that also the online news media</p>
<p>were so effective that they killed a lot of people</p>
<p>that were so effective that they killed a lot of print</p>
<p>journalism.</p>
<p>There&rsquo;s less than half as many journalists</p>
<p>now in America, I believe, as there was a generation ago.</p>
<p>You just couldn&rsquo;t compete with the online advertising.</p>
<p>So all of a sudden, most people are not</p>
<p>getting even reading newspapers.</p>
<p>They get their news from social media.</p>
<p>And most people only get news in their little bubble.</p>
<p>So along comes now some people like Donald Trump,</p>
<p>who figured out, among the first successful politicians,</p>
<p>to figure out how to really play this new game</p>
<p>and become very, very influential.</p>
<p>But I think Donald Trump was as simple.</p>
<p>He took advantage of it.</p>
<p>He didn&rsquo;t create the fundamental conditions</p>
<p>were created by machine learning taking over the news media.</p>
<p>So this is what motivated my little COVID project here.</p>
<p>So I said before, machine learning and tech in general</p>
<p>is not evil, but it&rsquo;s also not good.</p>
<p>It&rsquo;s just a tool that you can use</p>
<p>for good things or bad things.</p>
<p>And as it happens, machine learning and news</p>
<p>was mainly used by the big players, big tech,</p>
<p>to manipulate people and to watch as many ads as possible,</p>
<p>which had this unintended consequence of really screwing</p>
<p>up our democracy and fragmenting it into filter bubbles.</p>
<p>So I thought, well, machine learning algorithms</p>
<p>are basically free.</p>
<p>They can run on your smartphone for free also</p>
<p>if someone gives them away to you, right?</p>
<p>There&rsquo;s no reason why they only have to help the big guy</p>
<p>to manipulate the little guy.</p>
<p>They can just as well help the little guy</p>
<p>to see through all the manipulation attempts</p>
<p>from the big guy.</p>
<p>So this project is called,</p>
<p>you can go to improvethenews.org.</p>
<p>The first thing we&rsquo;ve built is this little news aggregator.</p>
<p>Looks a bit like Google News,</p>
<p>except it has these sliders on it to help you break out</p>
<p>of your filter bubble.</p>
<p>So if you&rsquo;re reading, you can click, click</p>
<p>and go to your favorite topic.</p>
<p>And then if you just slide the left, right slider</p>
<p>away all the way over to the left.</p>
<p>There&rsquo;s two sliders, right?</p>
<p>Yeah, there&rsquo;s the one, the most obvious one</p>
<p>is the one that has left, right labeled on it.</p>
<p>You go to the left, you get one set of articles,</p>
<p>you go to the right, you see a very different truth</p>
<p>appearing.</p>
<p>Oh, that&rsquo;s literally left and right on the political spectrum.</p>
<p>On the political spectrum.</p>
<p>So if you&rsquo;re reading about immigration, for example,</p>
<p>it&rsquo;s very, very noticeable.</p>
<p>And I think step one always,</p>
<p>if you wanna not get manipulated is just to be able</p>
<p>to recognize the techniques people use.</p>
<p>So it&rsquo;s very helpful to just see how they spin things</p>
<p>on the two sides.</p>
<p>I think many people are under the misconception</p>
<p>that the main problem is fake news.</p>
<p>It&rsquo;s not.</p>
<p>I had an amazing team of MIT students</p>
<p>where we did an academic project to use machine learning</p>
<p>to detect the main kinds of bias over the summer.</p>
<p>And yes, of course, sometimes there&rsquo;s fake news</p>
<p>where someone just claims something that&rsquo;s false, right?</p>
<p>Like, oh, Hillary Clinton just got divorced or something.</p>
<p>But what we see much more of is actually just omissions.</p>
<p>If you go to, there&rsquo;s some stories which just won&rsquo;t be</p>
<p>mentioned by the left or the right, because it doesn&rsquo;t suit</p>
<p>their agenda.</p>
<p>And then they&rsquo;ll mention other ones very, very, very much.</p>
<p>So for example, we&rsquo;ve had a number of stories</p>
<p>about the Trump family&rsquo;s financial dealings.</p>
<p>And then there&rsquo;s been a bunch of stories</p>
<p>about the Biden family&rsquo;s, Hunter Biden&rsquo;s financial dealings.</p>
<p>Surprise, surprise, they don&rsquo;t get equal coverage</p>
<p>on the left and the right.</p>
<p>One side loves to cover the Biden, Hunter Biden&rsquo;s stuff,</p>
<p>and one side loves to cover the Trump.</p>
<p>You can never guess which is which, right?</p>
<p>But the great news is if you&rsquo;re a normal American citizen</p>
<p>and you dislike corruption in all its forms,</p>
<p>then slide, slide, you can just look at both sides</p>
<p>and you&rsquo;ll see all those political corruption stories.</p>
<p>It&rsquo;s really liberating to just take in the both sides,</p>
<p>the spin on both sides.</p>
<p>It somehow unlocks your mind to think on your own,</p>
<p>to realize that, I don&rsquo;t know, it&rsquo;s the same thing</p>
<p>that was useful, right, in the Soviet Union times</p>
<p>for when everybody was much more aware</p>
<p>that they&rsquo;re surrounded by propaganda, right?</p>
<p>That is so interesting what you&rsquo;re saying, actually.</p>
<p>So Noam Chomsky, used to be our MIT colleague,</p>
<p>once said that propaganda is to democracy</p>
<p>what violence is to totalitarianism.</p>
<p>And what he means by that is if you have</p>
<p>a really totalitarian government,</p>
<p>you don&rsquo;t need propaganda.</p>
<p>People will do what you want them to do anyway,</p>
<p>but out of fear, right?</p>
<p>But otherwise, you need propaganda.</p>
<p>So I would say actually that the propaganda</p>
<p>is much higher quality in democracies,</p>
<p>much more believable.</p>
<p>And it&rsquo;s really, it&rsquo;s really striking.</p>
<p>When I talk to colleagues, science colleagues</p>
<p>like from Russia and China and so on,</p>
<p>I notice they are actually much more aware</p>
<p>of the propaganda in their own media</p>
<p>than many of my American colleagues are</p>
<p>about the propaganda in Western media.</p>
<p>That&rsquo;s brilliant.</p>
<p>That means the propaganda in the Western media</p>
<p>is just better.</p>
<p>Yes.</p>
<p>That&rsquo;s so brilliant.</p>
<p>Everything&rsquo;s better in the West, even the propaganda.</p>
<p>But once you realize that,</p>
<p>you realize there&rsquo;s also something very optimistic there</p>
<p>that you can do about it, right?</p>
<p>Because first of all, omissions,</p>
<p>as long as there&rsquo;s no outright censorship,</p>
<p>you can just look at both sides</p>
<p>and pretty quickly piece together</p>
<p>a much more accurate idea of what&rsquo;s actually going on, right?</p>
<p>And develop a natural skepticism too.</p>
<p>Yeah.</p>
<p>Just an analytical scientific mind</p>
<p>about the way you&rsquo;re taking the information.</p>
<p>Yeah.</p>
<p>And I think, I have to say,</p>
<p>sometimes I feel that some of us in the academic bubble</p>
<p>are too arrogant about this and somehow think,</p>
<p>oh, it&rsquo;s just people who aren&rsquo;t as educated</p>
<p>as the dots are pulled.</p>
<p>When we are often just as gullible also,</p>
<p>we read only our media and don&rsquo;t see through things.</p>
<p>Anyone who looks at both sides like this</p>
<p>and compares a little will immediately start noticing</p>
<p>the shenanigans being pulled.</p>
<p>And I think what I tried to do with this app</p>
<p>is that the big tech has to some extent</p>
<p>tried to blame the individual for being manipulated,</p>
<p>much like big tobacco tried to blame the individuals</p>
<p>entirely for smoking.</p>
<p>And then later on, our government stepped up and say,</p>
<p>actually, you can&rsquo;t just blame little kids</p>
<p>for starting to smoke.</p>
<p>We have to have more responsible advertising</p>
<p>and this and that.</p>
<p>I think it&rsquo;s a bit the same here.</p>
<p>It&rsquo;s very convenient for a big tech to blame.</p>
<p>So it&rsquo;s just people who are so dumb and get fooled.</p>
<p>The blame usually comes in saying,</p>
<p>oh, it&rsquo;s just human psychology.</p>
<p>People just wanna hear what they already believe.</p>
<p>But professor David Rand at MIT actually partly debunked that</p>
<p>with a really nice study showing that people</p>
<p>tend to be interested in hearing things</p>
<p>that go against what they believe,</p>
<p>if it&rsquo;s presented in a respectful way.</p>
<p>Suppose, for example, that you have a company</p>
<p>and you&rsquo;re just about to launch this project</p>
<p>and you&rsquo;re convinced it&rsquo;s gonna work.</p>
<p>And someone says, you know, Lex,</p>
<p>I hate to tell you this, but this is gonna fail.</p>
<p>And here&rsquo;s why.</p>
<p>Would you be like, shut up, I don&rsquo;t wanna hear it.</p>
<p>La, la, la, la, la, la, la, la, la.</p>
<p>Would you?</p>
<p>You would be interested, right?</p>
<p>And also if you&rsquo;re on an airplane,</p>
<p>back in the pre COVID times,</p>
<p>and the guy next to you</p>
<p>is clearly from the opposite side of the political spectrum,</p>
<p>but is very respectful and polite to you.</p>
<p>Wouldn&rsquo;t you be kind of interested to hear a bit about</p>
<p>how he or she thinks about things?</p>
<p>Of course.</p>
<p>But it&rsquo;s not so easy to find out</p>
<p>respectful disagreement now,</p>
<p>because like, for example, if you are a Democrat</p>
<p>and you&rsquo;re like, oh, I wanna see something</p>
<p>on the other side,</p>
<p>so you just go Breitbart.com.</p>
<p>And then after the first 10 seconds,</p>
<p>you feel deeply insulted by something.</p>
<p>And they, it&rsquo;s not gonna work.</p>
<p>Or if you take someone who votes Republican</p>
<p>and they go to something on the left,</p>
<p>then they just get very offended very quickly</p>
<p>by them having put a deliberately ugly picture</p>
<p>of Donald Trump on the front page or something.</p>
<p>It doesn&rsquo;t really work.</p>
<p>So this news aggregator also has this nuance slider,</p>
<p>which you can pull to the right</p>
<p>and then sort of make it easier to get exposed</p>
<p>to actually more sort of academic style</p>
<p>or more respectful,</p>
<p>portrayals of different views.</p>
<p>And finally, the one kind of bias</p>
<p>I think people are mostly aware of is the left, right,</p>
<p>because it&rsquo;s so obvious,</p>
<p>because both left and right are very powerful here, right?</p>
<p>Both of them have well funded TV stations and newspapers,</p>
<p>and it&rsquo;s kind of hard to miss.</p>
<p>But there&rsquo;s another one, the establishment slider,</p>
<p>which is also really fun.</p>
<p>I love to play with it.</p>
<p>And that&rsquo;s more about corruption.</p>
<p>Yeah, yeah.</p>
<p>I love that one. Yes.</p>
<p>Because if you have a society</p>
<p>where almost all the powerful entities</p>
<p>want you to believe a certain thing,</p>
<p>that&rsquo;s what you&rsquo;re gonna read in both the big media,</p>
<p>mainstream media on the left and on the right, of course.</p>
<p>And the powerful companies can push back very hard,</p>
<p>like tobacco companies push back very hard</p>
<p>back in the day when some newspapers</p>
<p>started writing articles about tobacco being dangerous,</p>
<p>so that it was hard to get a lot of coverage</p>
<p>about it initially.</p>
<p>And also if you look geopolitically, right,</p>
<p>of course, in any country, when you read their media,</p>
<p>you&rsquo;re mainly gonna be reading a lot of articles</p>
<p>about how our country is the good guy</p>
<p>and the other countries are the bad guys, right?</p>
<p>So if you wanna have a really more nuanced understanding,</p>
<p>like the Germans used to be told that the British</p>
<p>used to be told that the French were the bad guys</p>
<p>and the French used to be told</p>
<p>that the British were the bad guys.</p>
<p>Now they visit each other&rsquo;s countries a lot</p>
<p>and have a much more nuanced understanding.</p>
<p>I don&rsquo;t think there&rsquo;s gonna be any more wars</p>
<p>between France and Germany.</p>
<p>But on the geopolitical scale,</p>
<p>it&rsquo;s just as much as ever, you know,</p>
<p>big Cold War, now US, China, and so on.</p>
<p>And if you wanna get a more nuanced understanding</p>
<p>of what&rsquo;s happening geopolitically,</p>
<p>then it&rsquo;s really fun to look at this establishment slider</p>
<p>because it turns out there are tons of little newspapers,</p>
<p>both on the left and on the right,</p>
<p>who sometimes challenge establishment and say,</p>
<p>you know, maybe we shouldn&rsquo;t actually invade Iraq right now.</p>
<p>Maybe this weapons of mass destruction thing is BS.</p>
<p>If you look at the journalism research afterwards,</p>
<p>you can actually see that quite clearly.</p>
<p>Both CNN and Fox were very pro.</p>
<p>Let&rsquo;s get rid of Saddam.</p>
<p>There are weapons of mass destruction.</p>
<p>Then there were a lot of smaller newspapers.</p>
<p>They were like, wait a minute,</p>
<p>this evidence seems a bit sketchy and maybe we&hellip;</p>
<p>But of course they were so hard to find.</p>
<p>Most people didn&rsquo;t even know they existed, right?</p>
<p>Yet it would have been better for American national security</p>
<p>if those voices had also come up.</p>
<p>I think it harmed America&rsquo;s national security actually</p>
<p>that we invaded Iraq.</p>
<p>And arguably there&rsquo;s a lot more interest</p>
<p>in that kind of thinking too, from those small sources.</p>
<p>So like when you say big,</p>
<p>it&rsquo;s more about kind of the reach of the broadcast,</p>
<p>but it&rsquo;s not big in terms of the interest.</p>
<p>I think there&rsquo;s a lot of interest</p>
<p>in that kind of anti establishment</p>
<p>or like skepticism towards, you know,</p>
<p>out of the box thinking.</p>
<p>There&rsquo;s a lot of interest in that kind of thing.</p>
<p>Do you see this news project or something like it</p>
<p>being basically taken over the world</p>
<p>as the main way we consume information?</p>
<p>Like how do we get there?</p>
<p>Like how do we, you know?</p>
<p>So, okay, the idea is brilliant.</p>
<p>It&rsquo;s a, you&rsquo;re calling it your little project in 2020,</p>
<p>but how does that become the new way we consume information?</p>
<p>I hope, first of all, just to plant a little seed there</p>
<p>because normally the big barrier of doing anything in media</p>
<p>is you need a ton of money, but this costs no money at all.</p>
<p>I&rsquo;ve just been paying myself.</p>
<p>You pay a tiny amount of money each month to Amazon</p>
<p>to run the thing in their cloud.</p>
<p>We&rsquo;re not, there never will never be any ads.</p>
<p>The point is not to make any money off of it.</p>
<p>And we just train machine learning algorithms</p>
<p>to classify the articles and stuff.</p>
<p>So it just kind of runs by itself.</p>
<p>So if it actually gets good enough at some point</p>
<p>that it starts catching on, it could scale.</p>
<p>And if other people carbon copy it</p>
<p>and make other versions that are better,</p>
<p>that&rsquo;s the more the merrier.</p>
<p>I think there&rsquo;s a real opportunity for machine learning</p>
<p>to empower the individual against the powerful players.</p>
<p>As I said in the beginning here, it&rsquo;s</p>
<p>been mostly the other way around so far,</p>
<p>that the big players have the AI and then they tell people,</p>
<p>this is the truth, this is how it is.</p>
<p>But it can just as well go the other way around.</p>
<p>And when the internet was born, actually, a lot of people</p>
<p>had this hope that maybe this will be</p>
<p>a great thing for democracy, make it easier</p>
<p>to find out about things.</p>
<p>And maybe machine learning and things like this</p>
<p>can actually help again.</p>
<p>And I have to say, I think it&rsquo;s more important than ever now</p>
<p>because this is very linked also to the whole future of life</p>
<p>as we discussed earlier.</p>
<p>We&rsquo;re getting this ever more powerful tech.</p>
<p>Frank, it&rsquo;s pretty clear if you look</p>
<p>on the one or two generation, three generation timescale</p>
<p>that there are only two ways this can end geopolitically.</p>
<p>Either it ends great for all humanity</p>
<p>or it ends terribly for all of us.</p>
<p>There&rsquo;s really no in between.</p>
<p>And we&rsquo;re so stuck in that because technology</p>
<p>knows no borders.</p>
<p>And you can&rsquo;t have people fighting</p>
<p>when the weapons just keep getting ever more</p>
<p>powerful indefinitely.</p>
<p>Eventually, the luck runs out.</p>
<p>And right now we have, I love America,</p>
<p>but the fact of the matter is what&rsquo;s good for America</p>
<p>is not opposite in the long term to what&rsquo;s</p>
<p>good for other countries.</p>
<p>It would be if this was some sort of zero sum game</p>
<p>like it was thousands of years ago when the only way one</p>
<p>country could get more resources was</p>
<p>to take land from other countries</p>
<p>because that was basically the resource.</p>
<p>Look at the map of Europe.</p>
<p>Some countries kept getting bigger and smaller,</p>
<p>endless wars.</p>
<p>But then since 1945, there hasn&rsquo;t been any war</p>
<p>in Western Europe.</p>
<p>And they all got way richer because of tech.</p>
<p>So the optimistic outcome is that the big winner</p>
<p>in this century is going to be America and China and Russia</p>
<p>and everybody else because technology just makes</p>
<p>us all healthier and wealthier.</p>
<p>And we just find some way of keeping the peace</p>
<p>on this planet.</p>
<p>But I think, unfortunately, there</p>
<p>are some pretty powerful forces right now</p>
<p>that are pushing in exactly the opposite direction</p>
<p>and trying to demonize other countries, which just makes</p>
<p>it more likely that this ever more powerful tech we&rsquo;re</p>
<p>building is going to be used in disastrous ways.</p>
<p>Yeah, for aggression versus cooperation,</p>
<p>that kind of thing.</p>
<p>Yeah, even look at just military AI now.</p>
<p>It was so awesome to see these dancing robots.</p>
<p>I loved it.</p>
<p>But one of the biggest growth areas in robotics</p>
<p>now is, of course, autonomous weapons.</p>
<p>And 2020 was like the best marketing year</p>
<p>ever for autonomous weapons.</p>
<p>Because in both Libya, it&rsquo;s a civil war,</p>
<p>and in Nagorno Karabakh, they made the decisive difference.</p>
<p>And everybody else is watching this.</p>
<p>Oh, yeah, we want to build autonomous weapons, too.</p>
<p>In Libya, you had, on one hand, our ally,</p>
<p>the United Arab Emirates that were flying</p>
<p>their autonomous weapons that they bought from China,</p>
<p>bombing Libyans.</p>
<p>And on the other side, you had our other ally, Turkey,</p>
<p>flying their drones.</p>
<p>And they had no skin in the game,</p>
<p>any of these other countries.</p>
<p>And of course, it was the Libyans who really got screwed.</p>
<p>In Nagorno Karabakh, you had actually, again,</p>
<p>Turkey is sending drones built by this company that</p>
<p>was actually founded by a guy who went to MIT AeroAstro.</p>
<p>Do you know that?</p>
<p>No.</p>
<p>Bacratyar.</p>
<p>Yeah.</p>
<p>So MIT has a direct responsibility</p>
<p>for ultimately this.</p>
<p>And a lot of civilians were killed there.</p>
<p>So because it was militarily so effective,</p>
<p>now suddenly there&rsquo;s a huge push.</p>
<p>Oh, yeah, yeah, let&rsquo;s go build ever more autonomy</p>
<p>into these weapons, and it&rsquo;s going to be great.</p>
<p>And I think, actually, people who</p>
<p>are obsessed about some sort of future Terminator scenario</p>
<p>right now should start focusing on the fact</p>
<p>that we have two much more urgent threats happening</p>
<p>from machine learning.</p>
<p>One of them is the whole destruction of democracy</p>
<p>that we&rsquo;ve talked about now, where</p>
<p>our flow of information is being manipulated</p>
<p>by machine learning.</p>
<p>And the other one is that right now,</p>
<p>this is the year when the big arms race and out of control</p>
<p>arms race in at least Thomas Weapons is going to start,</p>
<p>or it&rsquo;s going to stop.</p>
<p>So you have a sense that there is like 2020</p>
<p>was an instrumental catalyst for the autonomous weapons race.</p>
<p>Yeah, because it was the first year when they proved</p>
<p>decisive in the battlefield.</p>
<p>And these ones are still not fully autonomous, mostly.</p>
<p>They&rsquo;re remote controlled, right?</p>
<p>But we could very quickly make things</p>
<p>about the size and cost of a smartphone, which you just put</p>
<p>in the GPS coordinates or the face of the one</p>
<p>you want to kill, a skin color or whatever,</p>
<p>and it flies away and does it.</p>
<p>And the real good reason why the US and all</p>
<p>the other superpowers should put the kibosh on this</p>
<p>is the same reason we decided to put the kibosh on bioweapons.</p>
<p>So we gave the Future of Life Award</p>
<p>that we can talk more about later to Matthew Messelson</p>
<p>from Harvard before for convincing</p>
<p>Nixon to ban bioweapons.</p>
<p>And I asked him, how did you do it?</p>
<p>And he was like, well, I just said, look,</p>
<p>we don&rsquo;t want there to be a $500 weapon of mass destruction</p>
<p>that all our enemies can afford, even nonstate actors.</p>
<p>And Nixon was like, good point.</p>
<p>It&rsquo;s in America&rsquo;s interest that the powerful weapons are all</p>
<p>really expensive, so only we can afford them,</p>
<p>or maybe some more stable adversaries, right?</p>
<p>Nuclear weapons are like that.</p>
<p>But bioweapons were not like that.</p>
<p>That&rsquo;s why we banned them.</p>
<p>And that&rsquo;s why you never hear about them now.</p>
<p>That&rsquo;s why we love biology.</p>
<p>So you have a sense that it&rsquo;s possible for the big power</p>
<p>houses in terms of the big nations in the world</p>
<p>to agree that autonomous weapons is not a race we want to be on,</p>
<p>that it doesn&rsquo;t end well.</p>
<p>Yeah, because we know it&rsquo;s just going</p>
<p>to end in mass proliferation.</p>
<p>And every terrorist everywhere is</p>
<p>going to have these super cheap weapons</p>
<p>that they will use against us.</p>
<p>And our politicians have to constantly worry</p>
<p>about being assassinated every time they go outdoors</p>
<p>by some anonymous little mini drone.</p>
<p>We don&rsquo;t want that.</p>
<p>And even if the US and China and everyone else</p>
<p>could just agree that you can only</p>
<p>build these weapons if they cost at least $10 million,</p>
<p>that would be a huge win for the superpowers</p>
<p>and, frankly, for everybody.</p>
<p>And people often push back and say, well, it&rsquo;s</p>
<p>so hard to prevent cheating.</p>
<p>But hey, you could say the same about bioweapons.</p>
<p>Take any of your MIT colleagues in biology.</p>
<p>Of course, they could build some nasty bioweapon</p>
<p>if they really wanted to.</p>
<p>But first of all, they don&rsquo;t want to</p>
<p>because they think it&rsquo;s disgusting because of the stigma.</p>
<p>And second, even if there&rsquo;s some sort of nutcase and want to,</p>
<p>it&rsquo;s very likely that some of their grad students</p>
<p>or someone would rat them out because everyone else thinks</p>
<p>it&rsquo;s so disgusting.</p>
<p>And in fact, we now know there was even a fair bit of cheating</p>
<p>on the bioweapons ban.</p>
<p>But no countries used them because it was so stigmatized</p>
<p>that it just wasn&rsquo;t worth revealing that they had cheated.</p>
<p>You talk about drones, but you kind of</p>
<p>think that drones is a remote operation.</p>
<p>Which they are, mostly, still.</p>
<p>But you&rsquo;re not taking the next intellectual step</p>
<p>of where does this go.</p>
<p>You&rsquo;re kind of saying the problem with drones</p>
<p>is that you&rsquo;re removing yourself from direct violence.</p>
<p>Therefore, you&rsquo;re not able to sort of maintain</p>
<p>the common humanity required to make</p>
<p>the proper decisions strategically.</p>
<p>But that&rsquo;s the criticism as opposed to like,</p>
<p>if this is automated, and just exactly as you said,</p>
<p>if you automate it and there&rsquo;s a race,</p>
<p>then the technology&rsquo;s gonna get better and better and better</p>
<p>which means getting cheaper and cheaper and cheaper.</p>
<p>And unlike, perhaps, nuclear weapons</p>
<p>which is connected to resources in a way,</p>
<p>like it&rsquo;s hard to engineer, yeah.</p>
<p>It feels like there&rsquo;s too much overlap</p>
<p>between the tech industry and autonomous weapons</p>
<p>to where you could have smartphone type of cheapness.</p>
<p>If you look at drones, for $1,000,</p>
<p>you can have an incredible system</p>
<p>that&rsquo;s able to maintain flight autonomously for you</p>
<p>and take pictures and stuff.</p>
<p>You could see that going into the autonomous weapons space</p>
<p>that&rsquo;s, but why is that not thought about</p>
<p>or discussed enough in the public, do you think?</p>
<p>You see those dancing Boston Dynamics robots</p>
<p>and everybody has this kind of,</p>
<p>as if this is like a far future.</p>
<p>They have this fear like, oh, this&rsquo;ll be Terminator</p>
<p>in like some, I don&rsquo;t know, unspecified 20, 30, 40 years.</p>
<p>And they don&rsquo;t think about, well, this is like</p>
<p>some much less dramatic version of that</p>
<p>is actually happening now.</p>
<p>It&rsquo;s not gonna be legged, it&rsquo;s not gonna be dancing,</p>
<p>but it already has the capability</p>
<p>to use artificial intelligence to kill humans.</p>
<p>Yeah, the Boston Dynamics legged robots,</p>
<p>I think the reason we imagine them holding guns</p>
<p>is just because you&rsquo;ve all seen Arnold Schwarzenegger, right?</p>
<p>That&rsquo;s our reference point.</p>
<p>That&rsquo;s pretty useless.</p>
<p>That&rsquo;s not gonna be the main military use of them.</p>
<p>They might be useful in law enforcement in the future</p>
<p>and then there&rsquo;s a whole debate about,</p>
<p>do you want robots showing up at your house with guns</p>
<p>telling you who&rsquo;ll be perfectly obedient</p>
<p>to whatever dictator controls them?</p>
<p>But let&rsquo;s leave that aside for a moment</p>
<p>and look at what&rsquo;s actually relevant now.</p>
<p>So there&rsquo;s a spectrum of things you can do</p>
<p>with AI in the military.</p>
<p>And again, to put my card on the table,</p>
<p>I&rsquo;m not the pacifist, I think we should have good defense.</p>
<p>So for example, a predator drone is basically</p>
<p>a fancy little remote controlled airplane, right?</p>
<p>There&rsquo;s a human piloting it and the decision ultimately</p>
<p>about whether to kill somebody with it</p>
<p>is made by a human still.</p>
<p>And this is a line I think we should never cross.</p>
<p>There&rsquo;s a current DOD policy.</p>
<p>Again, you have to have a human in the loop.</p>
<p>I think algorithms should never make life</p>
<p>or death decisions, they should be left to humans.</p>
<p>Now, why might we cross that line?</p>
<p>Well, first of all, these are expensive, right?</p>
<p>So for example, when Azerbaijan had all these drones</p>
<p>and Armenia didn&rsquo;t have any, they start trying</p>
<p>to jerry rig little cheap things, fly around.</p>
<p>But then of course, the Armenians would jam them</p>
<p>or the Azeris would jam them.</p>
<p>And remote control things can be jammed,</p>
<p>that makes them inferior.</p>
<p>Also, there&rsquo;s a bit of a time delay between,</p>
<p>if we&rsquo;re piloting something from far away,</p>
<p>speed of light, and the human has a reaction time as well,</p>
<p>it would be nice to eliminate that jamming possibility</p>
<p>in the time that they by having it fully autonomous.</p>
<p>But now you might be, so then if you do,</p>
<p>but now you might be crossing that exact line.</p>
<p>You might program it to just, oh yeah, the air drone,</p>
<p>go hover over this country for a while</p>
<p>and whenever you find someone who is a bad guy,</p>
<p>kill them.</p>
<p>Now the machine is making these sort of decisions</p>
<p>and some people who defend this still say,</p>
<p>well, that&rsquo;s morally fine because we are the good guys</p>
<p>and we will tell it the definition of bad guy</p>
<p>that we think is moral.</p>
<p>But now it would be very naive to think</p>
<p>that if ISIS buys that same drone,</p>
<p>that they&rsquo;re gonna use our definition of bad guy.</p>
<p>Maybe for them, bad guy is someone wearing</p>
<p>a US army uniform or maybe there will be some,</p>
<p>weird ethnic group who decides that someone</p>
<p>of another ethnic group, they are the bad guys, right?</p>
<p>The thing is human soldiers with all our faults,</p>
<p>we still have some basic wiring in us.</p>
<p>Like, no, it&rsquo;s not okay to kill kids and civilians.</p>
<p>And Thomas Weprin has none of that.</p>
<p>It&rsquo;s just gonna do whatever is programmed.</p>
<p>It&rsquo;s like the perfect Adolf Eichmann on steroids.</p>
<p>Like they told him, Adolf Eichmann, you know,</p>
<p>he wanted to do this and this and this</p>
<p>to make the Holocaust more efficient.</p>
<p>And he was like, yeah, and off he went and did it, right?</p>
<p>Do we really wanna make machines that are like that,</p>
<p>like completely amoral and we&rsquo;ll take the user&rsquo;s definition</p>
<p>of who is the bad guy?</p>
<p>And do we then wanna make them so cheap</p>
<p>that all our adversaries can have them?</p>
<p>Like what could possibly go wrong?</p>
<p>That&rsquo;s I think the big ordeal of the whole thing.</p>
<p>I think the big argument for why we wanna,</p>
<p>this year really put the kibosh on this.</p>
<p>And I think you can tell there&rsquo;s a lot</p>
<p>of very active debate even going on within the US military</p>
<p>and undoubtedly in other militaries around the world also</p>
<p>about whether we should have some sort</p>
<p>of international agreement to at least require</p>
<p>that these weapons have to be above a certain size</p>
<p>and cost, you know, so that things just don&rsquo;t totally spiral</p>
<p>out of control.</p>
<p>And finally, just for your question,</p>
<p>but is it possible to stop it?</p>
<p>Because some people tell me, oh, just give up, you know.</p>
<p>But again, so Matthew Messelsen again from Harvard, right,</p>
<p>who the bioweapons hero, he had exactly this criticism</p>
<p>also with bioweapons.</p>
<p>People were like, how can you check for sure</p>
<p>that the Russians aren&rsquo;t cheating?</p>
<p>And he told me this, I think really ingenious insight.</p>
<p>He said, you know, Max, some people</p>
<p>think you have to have inspections and things</p>
<p>and you have to make sure that you can catch the cheaters</p>
<p>with 100% chance.</p>
<p>You don&rsquo;t need 100%, he said.</p>
<p>1% is usually enough.</p>
<p>Because if it&rsquo;s another big state,</p>
<p>suppose China and the US have signed the treaty drawing</p>
<p>a certain line and saying, yeah, these kind of drones are OK,</p>
<p>but these fully autonomous ones are not.</p>
<p>Now suppose you are China and you have cheated and secretly</p>
<p>developed some clandestine little thing</p>
<p>or you&rsquo;re thinking about doing it.</p>
<p>What&rsquo;s your calculation that you do?</p>
<p>Well, you&rsquo;re like, OK, what&rsquo;s the probability</p>
<p>that we&rsquo;re going to get caught?</p>
<p>If the probability is 100%, of course, we&rsquo;re not going to do it.</p>
<p>But if the probability is 5% that we&rsquo;re going to get caught,</p>
<p>then it&rsquo;s going to be like a huge embarrassment for us.</p>
<p>And we still have our nuclear weapons anyway,</p>
<p>so it doesn&rsquo;t really make an enormous difference in terms</p>
<p>of deterring the US.</p>
<p>And that feeds the stigma that you kind of established,</p>
<p>like this fabric, this universal stigma over the thing.</p>
<p>Exactly.</p>
<p>It&rsquo;s very reasonable for them to say, well, we probably</p>
<p>get away with it.</p>
<p>If we don&rsquo;t, then the US will know we cheated,</p>
<p>and then they&rsquo;re going to go full tilt with their program</p>
<p>and say, look, the Chinese are cheaters,</p>
<p>and now we have all these weapons against us,</p>
<p>and that&rsquo;s bad.</p>
<p>So the stigma alone is very, very powerful.</p>
<p>And again, look what happened with bioweapons.</p>
<p>It&rsquo;s been 50 years now.</p>
<p>When was the last time you read about a bioterrorism attack?</p>
<p>The only deaths I really know about with bioweapons</p>
<p>that have happened when we Americans managed</p>
<p>to kill some of our own with anthrax,</p>
<p>or the idiot who sent them to Tom Daschle and others</p>
<p>in letters, right?</p>
<p>And similarly in Sverdlovsk in the Soviet Union,</p>
<p>they had some anthrax in some lab there.</p>
<p>Maybe they were cheating or who knows,</p>
<p>and it leaked out and killed a bunch of Russians.</p>
<p>I&rsquo;d say that&rsquo;s a pretty good success, right?</p>
<p>50 years, just two own goals by the superpowers,</p>
<p>and then nothing.</p>
<p>And that&rsquo;s why whenever I ask anyone</p>
<p>what they think about biology, they think it&rsquo;s great.</p>
<p>They associate it with new cures, new diseases,</p>
<p>maybe a good vaccine.</p>
<p>This is how I want to think about AI in the future.</p>
<p>And I want others to think about AI too,</p>
<p>as a source of all these great solutions to our problems,</p>
<p>not as, oh, AI, oh yeah, that&rsquo;s the reason</p>
<p>I feel scared going outside these days.</p>
<p>Yeah, it&rsquo;s kind of brilliant that bioweapons</p>
<p>and nuclear weapons, we&rsquo;ve figured out,</p>
<p>I mean, of course there&rsquo;s still a huge source of danger,</p>
<p>but we figured out some way of creating rules</p>
<p>and social stigma over these weapons</p>
<p>that then creates a stability to our,</p>
<p>whatever that game theoretic stability that occurs.</p>
<p>And we don&rsquo;t have that with AI,</p>
<p>and you&rsquo;re kind of screaming from the top of the mountain</p>
<p>about this, that we need to find that</p>
<p>because it&rsquo;s very possible with the future of life,</p>
<p>as you point out, Institute Awards pointed out</p>
<p>that with nuclear weapons,</p>
<p>we could have destroyed ourselves quite a few times.</p>
<p>And it&rsquo;s a learning experience that is very costly.</p>
<p>We gave this Future Life Award,</p>
<p>we gave it the first time to this guy, Vasily Arkhipov.</p>
<p>He was on, most people haven&rsquo;t even heard of him.</p>
<p>Yeah, can you say who he is?</p>
<p>Vasily Arkhipov, he has, in my opinion,</p>
<p>made the greatest positive contribution to humanity</p>
<p>of any human in modern history.</p>
<p>And maybe it sounds like hyperbole here,</p>
<p>like I&rsquo;m just over the top,</p>
<p>but let me tell you the story and I think maybe you&rsquo;ll agree.</p>
<p>So during the Cuban Missile Crisis,</p>
<p>we Americans first didn&rsquo;t know</p>
<p>that the Russians had sent four submarines,</p>
<p>but we caught two of them.</p>
<p>And we didn&rsquo;t know that,</p>
<p>so we dropped practice depth charges</p>
<p>on the one that he was on,</p>
<p>try to force it to the surface.</p>
<p>But we didn&rsquo;t know that this nuclear submarine</p>
<p>actually was a nuclear submarine with a nuclear torpedo.</p>
<p>We also didn&rsquo;t know that they had authorization</p>
<p>to launch it without clearance from Moscow.</p>
<p>And we also didn&rsquo;t know</p>
<p>that they were running out of electricity.</p>
<p>Their batteries were almost dead.</p>
<p>They were running out of oxygen.</p>
<p>Sailors were fainting left and right.</p>
<p>The temperature was about 110, 120 Fahrenheit on board.</p>
<p>It was really hellish conditions,</p>
<p>really just a kind of doomsday.</p>
<p>And at that point,</p>
<p>these giant explosions start happening</p>
<p>from the Americans dropping these.</p>
<p>The captain thought World War III had begun.</p>
<p>They decided they were gonna launch the nuclear torpedo.</p>
<p>And one of them shouted,</p>
<p>we&rsquo;re all gonna die,</p>
<p>but we&rsquo;re not gonna disgrace our Navy.</p>
<p>We don&rsquo;t know what would have happened</p>
<p>if there had been a giant mushroom cloud all of a sudden</p>
<p>against the Americans.</p>
<p>But since everybody had their hands on the triggers,</p>
<p>you don&rsquo;t have to be too creative to think</p>
<p>that it could have led to an all out nuclear war,</p>
<p>in which case we wouldn&rsquo;t be having this conversation now.</p>
<p>What actually took place was</p>
<p>they needed three people to approve this.</p>
<p>The captain had said yes.</p>
<p>There was the Communist Party political officer.</p>
<p>He also said, yes, let&rsquo;s do it.</p>
<p>And the third man was this guy, Vasily Arkhipov,</p>
<p>who said, no.</p>
<p>For some reason, he was just more chill than the others</p>
<p>and he was the right man at the right time.</p>
<p>I don&rsquo;t want us as a species rely on the right person</p>
<p>being there at the right time, you know.</p>
<p>We tracked down his family</p>
<p>living in relative poverty outside Moscow.</p>
<p>When he flew his daughter,</p>
<p>he had passed away and flew them to London.</p>
<p>They had never been to the West even.</p>
<p>It was incredibly moving to get to honor them for this.</p>
<p>The next year we gave them a medal.</p>
<p>The next year we gave this Future Life Award</p>
<p>to Stanislav Petrov.</p>
<p>Have you heard of him?</p>
<p>Yes.</p>
<p>So he was in charge of the Soviet early warning station,</p>
<p>which was built with Soviet technology</p>
<p>and honestly not that reliable.</p>
<p>It said that there were five US missiles coming in.</p>
<p>Again, if they had launched at that point,</p>
<p>we probably wouldn&rsquo;t be having this conversation.</p>
<p>He decided based on just mainly gut instinct</p>
<p>to just not escalate this.</p>
<p>And I&rsquo;m very glad he wasn&rsquo;t replaced by an AI</p>
<p>that was just automatically following orders.</p>
<p>And then we gave the third one to Matthew Messelson.</p>
<p>Last year, we gave this award to these guys</p>
<p>who actually use technology for good,</p>
<p>not avoiding something bad, but for something good.</p>
<p>The guys who eliminated this disease,</p>
<p>it was way worse than COVID that had killed</p>
<p>half a billion people in its final century.</p>
<p>Smallpox, right?</p>
<p>So you mentioned it earlier.</p>
<p>COVID on average kills less than 1% of people who get it.</p>
<p>Smallpox, about 30%.</p>
<p>And they just ultimately, Viktor Zhdanov and Bill Foege,</p>
<p>most of my colleagues have never heard of either of them,</p>
<p>one American, one Russian, they did this amazing effort</p>
<p>not only was Zhdanov able to get the US and the Soviet Union</p>
<p>to team up against smallpox during the Cold War,</p>
<p>but Bill Foege came up with this ingenious strategy</p>
<p>for making it actually go all the way</p>
<p>to defeat the disease without funding</p>
<p>for vaccinating everyone.</p>
<p>And as a result, we haven&rsquo;t had any,</p>
<p>we went from 15 million deaths the year</p>
<p>I was born in smallpox.</p>
<p>So what do we have in COVID now?</p>
<p>A little bit short of 2 million, right?</p>
<p>Yes.</p>
<p>To zero deaths, of course, this year and forever.</p>
<p>There have been 200 million people,</p>
<p>we estimate, who would have died since then by smallpox</p>
<p>had it not been for this.</p>
<p>So isn&rsquo;t science awesome when you use it for good?</p>
<p>The reason we wanna celebrate these sort of people</p>
<p>is to remind them of this.</p>
<p>Science is so awesome when you use it for good.</p>
<p>And those awards actually, the variety there,</p>
<p>it&rsquo;s a very interesting picture.</p>
<p>So the first two are looking at,</p>
<p>it&rsquo;s kind of exciting to think that these average humans</p>
<p>in some sense, they&rsquo;re products of billions</p>
<p>of other humans that came before them, evolution,</p>
<p>and some little, you said gut,</p>
<p>but there&rsquo;s something in there</p>
<p>that stopped the annihilation of the human race.</p>
<p>And that&rsquo;s a magical thing,</p>
<p>but that&rsquo;s like this deeply human thing.</p>
<p>And then there&rsquo;s the other aspect</p>
<p>where that&rsquo;s also very human,</p>
<p>which is to build solution</p>
<p>to the existential crises that we&rsquo;re facing,</p>
<p>like to build it, to take the responsibility</p>
<p>and to come up with different technologies and so on.</p>
<p>And both of those are deeply human,</p>
<p>the gut and the mind, whatever that is that creates.</p>
<p>The best is when they work together.</p>
<p>Arkhipov, I wish I could have met him, of course,</p>
<p>but he had passed away.</p>
<p>He was really a fantastic military officer,</p>
<p>combining all the best traits</p>
<p>that we in America admire in our military.</p>
<p>Because first of all, he was very loyal, of course.</p>
<p>He never even told anyone about this during his whole life,</p>
<p>even though you think he had some bragging rights, right?</p>
<p>But he just was like, this is just business,</p>
<p>just doing my job.</p>
<p>It only came out later after his death.</p>
<p>And second, the reason he did the right thing</p>
<p>was not because he was some sort of liberal</p>
<p>or some sort of, not because he was just,</p>
<p>oh, peace and love.</p>
<p>It was partly because he had been the captain</p>
<p>on another submarine that had a nuclear reactor meltdown.</p>
<p>And it was his heroism that helped contain this.</p>
<p>That&rsquo;s why he died of cancer later also.</p>
<p>But he had seen many of his crew members die.</p>
<p>And I think for him, that gave him this gut feeling</p>
<p>that if there&rsquo;s a nuclear war</p>
<p>between the US and the Soviet Union,</p>
<p>the whole world is gonna go through</p>
<p>what I saw my dear crew members suffer through.</p>
<p>It wasn&rsquo;t just an abstract thing for him.</p>
<p>I think it was real.</p>
<p>And second though, not just the gut, the mind, right?</p>
<p>He was, for some reason, very levelheaded personality</p>
<p>and very smart guy,</p>
<p>which is exactly what we want our best fighter pilots</p>
<p>to be also, right?</p>
<p>I never forget Neil Armstrong when he&rsquo;s landing on the moon</p>
<p>and almost running out of gas.</p>
<p>And he doesn&rsquo;t even change when they say 30 seconds,</p>
<p>he doesn&rsquo;t even change the tone of voice, just keeps going.</p>
<p>Arkhipov, I think was just like that.</p>
<p>So when the explosions start going off</p>
<p>and his captain is screaming and we should nuke them</p>
<p>and all, he&rsquo;s like,</p>
<p>I don&rsquo;t think the Americans are trying to sink us.</p>
<p>I think they&rsquo;re trying to send us a message.</p>
<p>That&rsquo;s pretty bad ass.</p>
<p>Yes.</p>
<p>Coolness, because he said, if they wanted to sink us,</p>
<p>and he said, listen, listen, it&rsquo;s alternating</p>
<p>one loud explosion on the left, one on the right,</p>
<p>one on the left, one on the right.</p>
<p>He was the only one who noticed this pattern.</p>
<p>And he&rsquo;s like, I think this is,</p>
<p>I&rsquo;m trying to send us a signal</p>
<p>that they want it to surface</p>
<p>and they&rsquo;re not gonna sink us.</p>
<p>And somehow,</p>
<p>this is how he then managed it ultimately</p>
<p>with his combination of gut</p>
<p>and also just cool analytical thinking,</p>
<p>was able to deescalate the whole thing.</p>
<p>And yeah, so this is some of the best in humanity.</p>
<p>I guess coming back to what we talked about earlier,</p>
<p>it&rsquo;s the combination of the neural network,</p>
<p>the instinctive, with, I&rsquo;m getting teary up here,</p>
<p>getting emotional, but he was just,</p>
<p>he is one of my superheroes,</p>
<p>having both the heart and the mind combined.</p>
<p>And especially in that time, there&rsquo;s something about the,</p>
<p>I mean, this is a very, in America,</p>
<p>people are used to this kind of idea</p>
<p>of being the individual of like on your own thinking.</p>
<p>I think under, in the Soviet Union under communism,</p>
<p>it&rsquo;s actually much harder to do that.</p>
<p>Oh yeah, he didn&rsquo;t even, he even got,</p>
<p>he didn&rsquo;t get any accolades either</p>
<p>when he came back for this, right?</p>
<p>They just wanted to hush the whole thing up.</p>
<p>Yeah, there&rsquo;s echoes of that with Chernobyl,</p>
<p>there&rsquo;s all kinds of,</p>
<p>that&rsquo;s one, that&rsquo;s a really hopeful thing</p>
<p>that amidst big centralized powers,</p>
<p>whether it&rsquo;s companies or states,</p>
<p>there&rsquo;s still the power of the individual</p>
<p>to think on their own, to act.</p>
<p>But I think we need to think of people like this,</p>
<p>not as a panacea we can always count on,</p>
<p>but rather as a wake up call.</p>
<p>So because of them, because of Arkhipov,</p>
<p>we are alive to learn from this lesson,</p>
<p>to learn from the fact that we shouldn&rsquo;t keep playing</p>
<p>Russian roulette and almost have a nuclear war</p>
<p>by mistake now and then,</p>
<p>because relying on luck is not a good longterm strategy.</p>
<p>If you keep playing Russian roulette over and over again,</p>
<p>the probability of surviving just drops exponentially</p>
<p>with time.</p>
<p>Yeah.</p>
<p>And if you have some probability</p>
<p>of having an accidental nuke war every year,</p>
<p>the probability of not having one also drops exponentially.</p>
<p>I think we can do better than that.</p>
<p>So I think the message is very clear,</p>
<p>once in a while shit happens,</p>
<p>and there&rsquo;s a lot of very concrete things we can do</p>
<p>to reduce the risk of things like that happening</p>
<p>in the first place.</p>
<p>On the AI front, if we just link on that for a second.</p>
<p>Yeah.</p>
<p>So you&rsquo;re friends with, you often talk with Elon Musk</p>
<p>throughout history, you&rsquo;ve did a lot</p>
<p>of interesting things together.</p>
<p>He has a set of fears about the future</p>
<p>of artificial intelligence, AGI.</p>
<p>Do you have a sense, we&rsquo;ve already talked about</p>
<p>the things we should be worried about with AI,</p>
<p>do you have a sense of the shape of his fears</p>
<p>in particular about AI,</p>
<p>of which subset of what we&rsquo;ve talked about,</p>
<p>whether it&rsquo;s creating, it&rsquo;s that direction</p>
<p>of creating sort of these giant competition systems</p>
<p>that are not explainable,</p>
<p>they&rsquo;re not intelligible intelligence,</p>
<p>or is it the&hellip;</p>
<p>And then like as a branch of that,</p>
<p>is it the manipulation by big corporations of that</p>
<p>or individual evil people to use that for destruction</p>
<p>or the unintentional consequences?</p>
<p>Do you have a sense of where his thinking is on this?</p>
<p>From my many conversations with Elon,</p>
<p>yeah, I certainly have a model of how he thinks.</p>
<p>It&rsquo;s actually very much like the way I think also,</p>
<p>I&rsquo;ll elaborate on it a bit.</p>
<p>I just wanna push back on when you said evil people,</p>
<p>I don&rsquo;t think it&rsquo;s a very helpful concept.</p>
<p>Evil people, sometimes people do very, very bad things,</p>
<p>but they usually do it because they think it&rsquo;s a good thing</p>
<p>because somehow other people had told them</p>
<p>that that was a good thing</p>
<p>or given them incorrect information or whatever, right?</p>
<p>I believe in the fundamental goodness of humanity</p>
<p>that if we educate people well</p>
<p>and they find out how things really are,</p>
<p>people generally wanna do good and be good.</p>
<p>Hence the value alignment,</p>
<p>as opposed to it&rsquo;s about information, about knowledge,</p>
<p>and then once we have that,</p>
<p>we&rsquo;ll likely be able to do good</p>
<p>in the way that&rsquo;s aligned with everybody else</p>
<p>who thinks differently.</p>
<p>Yeah, and it&rsquo;s not just the individual people</p>
<p>we have to align.</p>
<p>So we don&rsquo;t just want people to be educated</p>
<p>to know the way things actually are</p>
<p>and to treat each other well,</p>
<p>but we also need to align other nonhuman entities.</p>
<p>We talked about corporations, there has to be institutions</p>
<p>so that what they do is actually good</p>
<p>for the country they&rsquo;re in</p>
<p>and we should align, make sure that what countries do</p>
<p>is actually good for the species as a whole, et cetera.</p>
<p>Coming back to Elon,</p>
<p>yeah, my understanding of how Elon sees this</p>
<p>is really quite similar to my own,</p>
<p>which is one of the reasons I like him so much</p>
<p>and enjoy talking with him so much.</p>
<p>I feel he&rsquo;s quite different from most people</p>
<p>in that he thinks much more than most people</p>
<p>about the really big picture,</p>
<p>not just what&rsquo;s gonna happen in the next election cycle,</p>
<p>but in millennia, millions and billions of years from now.</p>
<p>And when you look in this more cosmic perspective,</p>
<p>it&rsquo;s so obvious that we are gazing out into this universe</p>
<p>that as far as we can tell is mostly dead</p>
<p>with life being almost imperceptibly tiny perturbation,</p>
<p>and he sees this enormous opportunity</p>
<p>for our universe to come alive,</p>
<p>first to become an interplanetary species.</p>
<p>Mars is obviously just first stop on this cosmic journey.</p>
<p>And precisely because he thinks more long term,</p>
<p>it&rsquo;s much more clear to him than to most people</p>
<p>that what we do with this Russian roulette thing</p>
<p>we keep playing with our nukes is a really poor strategy,</p>
<p>really reckless strategy.</p>
<p>And also that we&rsquo;re just building</p>
<p>these ever more powerful AI systems that we don&rsquo;t understand</p>
<p>is also just a really reckless strategy.</p>
<p>I feel Elon is very much a humanist</p>
<p>in the sense that he wants an awesome future for humanity.</p>
<p>He wants it to be us that control the machines</p>
<p>rather than the machines that control us.</p>
<p>And why shouldn&rsquo;t we insist on that?</p>
<p>We&rsquo;re building them after all, right?</p>
<p>Why should we build things that just make us</p>
<p>into some little cog in the machinery</p>
<p>that has no further say in the matter, right?</p>
<p>That&rsquo;s not my idea of an inspiring future either.</p>
<p>Yeah, if you think on the cosmic scale</p>
<p>in terms of both time and space,</p>
<p>so much is put into perspective.</p>
<p>Yeah.</p>
<p>Whenever I have a bad day, that&rsquo;s what I think about.</p>
<p>It immediately makes me feel better.</p>
<p>It makes me sad that for us individual humans,</p>
<p>at least for now, the ride ends too quickly.</p>
<p>That we don&rsquo;t get to experience the cosmic scale.</p>
<p>Yeah, I mean, I think of our universe sometimes</p>
<p>as an organism that has only begun to wake up a tiny bit,</p>
<p>just like the very first little glimmers of consciousness</p>
<p>you have in the morning when you start coming around.</p>
<p>Before the coffee.</p>
<p>Before the coffee, even before you get out of bed,</p>
<p>before you even open your eyes.</p>
<p>You start to wake up a little bit.</p>
<p>There&rsquo;s something here.</p>
<p>That&rsquo;s very much how I think of where we are.</p>
<p>All those galaxies out there,</p>
<p>I think they&rsquo;re really beautiful,</p>
<p>but why are they beautiful?</p>
<p>They&rsquo;re beautiful because conscious entities</p>
<p>are actually observing them,</p>
<p>experiencing them through our telescopes.</p>
<p>I define consciousness as subjective experience,</p>
<p>whether it be colors or emotions or sounds.</p>
<p>So beauty is an experience.</p>
<p>Meaning is an experience.</p>
<p>Purpose is an experience.</p>
<p>If there was no conscious experience,</p>
<p>observing these galaxies, they wouldn&rsquo;t be beautiful.</p>
<p>If we do something dumb with advanced AI in the future here</p>
<p>and Earth originating, life goes extinct.</p>
<p>And that was it for this.</p>
<p>If there is nothing else with telescopes in our universe,</p>
<p>then it&rsquo;s kind of game over for beauty</p>
<p>and meaning and purpose in our whole universe.</p>
<p>And I think that would be just such</p>
<p>an opportunity lost, frankly.</p>
<p>And I think when Elon points this out,</p>
<p>he gets very unfairly maligned in the media</p>
<p>for all the dumb media bias reasons we talked about.</p>
<p>They want to print precisely the things about Elon</p>
<p>out of context that are really click baity.</p>
<p>He has gotten so much flack</p>
<p>for this summoning the demon statement.</p>
<p>I happen to know exactly the context</p>
<p>because I was in the front row when he gave that talk.</p>
<p>It was at MIT, you&rsquo;ll be pleased to know,</p>
<p>it was the AeroAstro anniversary.</p>
<p>They had Buzz Aldrin there from the moon landing,</p>
<p>a whole house, a Kresge auditorium</p>
<p>packed with MIT students.</p>
<p>And he had this amazing Q&amp;A, it might&rsquo;ve gone for an hour.</p>
<p>And they talked about rockets and Mars and everything.</p>
<p>At the very end, this one student</p>
<p>who has actually hit my class asked him, what about AI?</p>
<p>Elon makes this one comment</p>
<p>and they take this out of context, print it, goes viral.</p>
<p>What is it like with AI,</p>
<p>we&rsquo;re summoning the demons, something like that.</p>
<p>And try to cast him as some sort of doom and gloom dude.</p>
<p>You know Elon, he&rsquo;s not the doom and gloom dude.</p>
<p>He is such a positive visionary.</p>
<p>And the whole reason he warns about this</p>
<p>is because he realizes more than most</p>
<p>what the opportunity cost is of screwing up.</p>
<p>That there is so much awesomeness in the future</p>
<p>that we can and our descendants can enjoy</p>
<p>if we don&rsquo;t screw up, right?</p>
<p>I get so pissed off when people try to cast him</p>
<p>as some sort of technophobic Luddite.</p>
<p>And at this point, it&rsquo;s kind of ludicrous</p>
<p>when I hear people say that people who worry about</p>
<p>artificial general intelligence are Luddites</p>
<p>because of course, if you look more closely,</p>
<p>you have some of the most outspoken people making warnings</p>
<p>are people like Professor Stuart Russell from Berkeley</p>
<p>who&rsquo;s written the bestselling AI textbook, you know.</p>
<p>So claiming that he&rsquo;s a Luddite who doesn&rsquo;t understand AI</p>
<p>is the joke is really on the people who said it.</p>
<p>But I think more broadly,</p>
<p>this message is really not sunk in at all.</p>
<p>What it is that people worry about,</p>
<p>they think that Elon and Stuart Russell and others</p>
<p>are worried about the dancing robots picking up an AR 15</p>
<p>and going on a rampage, right?</p>
<p>They think they&rsquo;re worried about robots turning evil.</p>
<p>They&rsquo;re not, I&rsquo;m not.</p>
<p>The risk is not malice, it&rsquo;s competence.</p>
<p>The risk is just that we build some systems</p>
<p>that are incredibly competent,</p>
<p>which means they&rsquo;re always gonna get</p>
<p>their goals accomplished,</p>
<p>even if they clash with our goals.</p>
<p>That&rsquo;s the risk.</p>
<p>Why did we humans drive the West African black rhino extinct?</p>
<p>Is it because we&rsquo;re malicious, evil rhinoceros haters?</p>
<p>No, it&rsquo;s just because our goals didn&rsquo;t align</p>
<p>with the goals of those rhinos</p>
<p>and tough luck for the rhinos, you know.</p>
<p>So the point is just we don&rsquo;t wanna put ourselves</p>
<p>in the position of those rhinos</p>
<p>creating something more powerful than us</p>
<p>if we haven&rsquo;t first figured out how to align the goals.</p>
<p>And I am optimistic.</p>
<p>I think we could do it if we worked really hard on it,</p>
<p>because I spent a lot of time</p>
<p>around intelligent entities that were more intelligent</p>
<p>than me, my mom and my dad.</p>
<p>And I was little and that was fine</p>
<p>because their goals were actually aligned</p>
<p>with mine quite well.</p>
<p>But we&rsquo;ve seen today many examples of where the goals</p>
<p>of our powerful systems are not so aligned.</p>
<p>So those click through optimization algorithms</p>
<p>that are polarized social media, right?</p>
<p>They were actually pretty poorly aligned</p>
<p>with what was good for democracy, it turned out.</p>
<p>And again, almost all problems we&rsquo;ve had</p>
<p>in the machine learning again came so far,</p>
<p>not from malice, but from poor alignment.</p>
<p>And that&rsquo;s exactly why that&rsquo;s why we should be concerned</p>
<p>about it in the future.</p>
<p>Do you think it&rsquo;s possible that with systems</p>
<p>like Neuralink and brain computer interfaces,</p>
<p>you know, again, thinking of the cosmic scale,</p>
<p>Elon&rsquo;s talked about this, but others have as well</p>
<p>throughout history of figuring out how the exact mechanism</p>
<p>of how to achieve that kind of alignment.</p>
<p>So one of them is having a symbiosis with AI,</p>
<p>which is like coming up with clever ways</p>
<p>where we&rsquo;re like stuck together in this weird relationship,</p>
<p>whether it&rsquo;s biological or in some kind of other way.</p>
<p>Do you think that&rsquo;s a possibility</p>
<p>of having that kind of symbiosis?</p>
<p>Or do we wanna instead kind of focus</p>
<p>on this distinct entities of us humans talking</p>
<p>to these intelligible, self doubting AIs,</p>
<p>maybe like Stuart Russell thinks about it,</p>
<p>like we&rsquo;re self doubting and full of uncertainty</p>
<p>and our AI systems are full of uncertainty.</p>
<p>We communicate back and forth</p>
<p>and in that way achieve symbiosis.</p>
<p>I honestly don&rsquo;t know.</p>
<p>I would say that because we don&rsquo;t know for sure</p>
<p>what if any of our, which of any of our ideas will work.</p>
<p>But we do know that if we don&rsquo;t,</p>
<p>I&rsquo;m pretty convinced that if we don&rsquo;t get any</p>
<p>of these things to work and just barge ahead,</p>
<p>then our species is, you know,</p>
<p>probably gonna go extinct this century.</p>
<p>I think it&rsquo;s&hellip;</p>
<p>This century, you think like,</p>
<p>you think we&rsquo;re facing this crisis</p>
<p>is a 21st century crisis.</p>
<p>Like this century will be remembered.</p>
<p>But on a hard drive and a hard drive somewhere</p>
<p>or maybe by future generations is like,</p>
<p>like there&rsquo;ll be future Future of Life Institute awards</p>
<p>for people that have done something about AI.</p>
<p>It could also end even worse,</p>
<p>whether we&rsquo;re not superseded</p>
<p>by leaving any AI behind either.</p>
<p>We just totally wipe out, you know,</p>
<p>like on Easter Island.</p>
<p>Our century is long.</p>
<p>You know, there are still 79 years left of it, right?</p>
<p>Think about how far we&rsquo;ve come just in the last 30 years.</p>
<p>So we can talk more about what might go wrong,</p>
<p>but you asked me this really good question</p>
<p>about what&rsquo;s the best strategy.</p>
<p>Is it Neuralink or Russell&rsquo;s approach or whatever?</p>
<p>I think, you know, when we did the Manhattan project,</p>
<p>we didn&rsquo;t know if any of our four ideas</p>
<p>for enriching uranium and getting out the uranium 235</p>
<p>were gonna work.</p>
<p>But we felt this was really important</p>
<p>to get it before Hitler did.</p>
<p>So, you know what we did?</p>
<p>We tried all four of them.</p>
<p>Here, I think it&rsquo;s analogous</p>
<p>where there&rsquo;s the greatest threat</p>
<p>that&rsquo;s ever faced our species.</p>
<p>And of course, US national security by implication.</p>
<p>We don&rsquo;t know if we don&rsquo;t have any method</p>
<p>that&rsquo;s guaranteed to work, but we have a lot of ideas.</p>
<p>So we should invest pretty heavily</p>
<p>in pursuing all of them with an open mind</p>
<p>and hope that one of them at least works.</p>
<p>These are, the good news is the century is long,</p>
<p>and it might take decades</p>
<p>until we have artificial general intelligence.</p>
<p>So we have some time hopefully,</p>
<p>but it takes a long time to solve</p>
<p>these very, very difficult problems.</p>
<p>It&rsquo;s gonna actually be the,</p>
<p>it&rsquo;s the most difficult problem</p>
<p>we were ever trying to solve as a species.</p>
<p>So we have to start now.</p>
<p>So we don&rsquo;t have, rather than begin thinking about it</p>
<p>the night before some people who&rsquo;ve had too much Red Bull</p>
<p>switch it on.</p>
<p>And we have to, coming back to your question,</p>
<p>we have to pursue all of these different avenues and see.</p>
<p>If you were my investment advisor</p>
<p>and I was trying to invest in the future,</p>
<p>how do you think the human species</p>
<p>is most likely to destroy itself in the century?</p>
<p>Yeah, so if the crises,</p>
<p>many of the crises we&rsquo;re facing are really before us</p>
<p>within the next hundred years,</p>
<p>how do we make explicit,</p>
<p>make known the unknowns and solve those problems</p>
<p>to avoid the biggest,</p>
<p>starting with the biggest existential crisis?</p>
<p>So as your investment advisor,</p>
<p>how are you planning to make money on us</p>
<p>destroying ourselves?</p>
<p>I have to ask.</p>
<p>I don&rsquo;t know.</p>
<p>It might be the Russian origins.</p>
<p>Somehow it&rsquo;s involved.</p>
<p>At the micro level of detailed strategies,</p>
<p>of course, these are unsolved problems.</p>
<p>For AI alignment,</p>
<p>we can break it into three sub problems</p>
<p>that are all unsolved.</p>
<p>I think you want first to make machines</p>
<p>understand our goals,</p>
<p>then adopt our goals and then retain our goals.</p>
<p>So to hit on all three real quickly.</p>
<p>The problem when Andreas Lubitz told his autopilot</p>
<p>to fly into the Alps was that the computer</p>
<p>didn&rsquo;t even understand anything about his goals.</p>
<p>It was too dumb.</p>
<p>It could have understood actually,</p>
<p>but you would have had to put some effort in</p>
<p>as a systems designer to don&rsquo;t fly into mountains.</p>
<p>So that&rsquo;s the first challenge.</p>
<p>How do you program into computers human values,</p>
<p>human goals?</p>
<p>We can start rather than saying,</p>
<p>oh, it&rsquo;s so hard.</p>
<p>We should start with the simple stuff, as I said,</p>
<p>self driving cars, airplanes,</p>
<p>just put in all the goals that we all agree on already,</p>
<p>and then have a habit of whenever machines get smarter</p>
<p>so they can understand one level higher goals,</p>
<p>put them into.</p>
<p>The second challenge is getting them to adopt the goals.</p>
<p>It&rsquo;s easy for situations like that</p>
<p>where you just program it in,</p>
<p>but when you have self learning systems like children,</p>
<p>you know, any parent knows</p>
<p>that there was a difference between getting our kids</p>
<p>to understand what we want them to do</p>
<p>and to actually adopt our goals, right?</p>
<p>With humans, with children, fortunately,</p>
<p>they go through this phase.</p>
<p>First, they&rsquo;re too dumb to understand</p>
<p>what we want our goals are.</p>
<p>And then they have this period of some years</p>
<p>when they&rsquo;re both smart enough to understand them</p>
<p>and malleable enough that we have a chance</p>
<p>to raise them well.</p>
<p>And then they become teenagers kind of too late.</p>
<p>But we have this window with machines,</p>
<p>the challenges, the intelligence might grow so fast</p>
<p>that that window is pretty short.</p>
<p>So that&rsquo;s a research problem.</p>
<p>The third one is how do you make sure they keep the goals</p>
<p>if they keep learning more and getting smarter?</p>
<p>Many sci fi movies are about how you have something</p>
<p>in which initially was aligned,</p>
<p>but then things kind of go off keel.</p>
<p>And, you know, my kids were very, very excited</p>
<p>about their Legos when they were little.</p>
<p>Now they&rsquo;re just gathering dust in the basement.</p>
<p>If we create machines that are really on board</p>
<p>with the goal of taking care of humanity,</p>
<p>we don&rsquo;t want them to get as bored with us</p>
<p>as my kids got with Legos.</p>
<p>So this is another research challenge.</p>
<p>How can you make some sort of recursively</p>
<p>self improving system retain certain basic goals?</p>
<p>That said, a lot of adult people still play with Legos.</p>
<p>So maybe we succeeded with the Legos.</p>
<p>Maybe, I like your optimism.</p>
<p>But above all.</p>
<p>So not all AI systems have to maintain the goals, right?</p>
<p>Just some fraction.</p>
<p>Yeah, so there&rsquo;s a lot of talented AI researchers now</p>
<p>who have heard of this and want to work on it.</p>
<p>Not so much funding for it yet.</p>
<p>Of the billions that go into building AI more powerful,</p>
<p>it&rsquo;s only a minuscule fraction</p>
<p>so far going into this safety research.</p>
<p>My attitude is generally we should not try to slow down</p>
<p>the technology, but we should greatly accelerate</p>
<p>the investment in this sort of safety research.</p>
<p>And also, this was very embarrassing last year,</p>
<p>but the NSF decided to give out</p>
<p>six of these big institutes.</p>
<p>We got one of them for AI and science, you asked me about.</p>
<p>Another one was supposed to be for AI safety research.</p>
<p>And they gave it to people studying oceans</p>
<p>and climate and stuff.</p>
<p>So I&rsquo;m all for studying oceans and climates,</p>
<p>but we need to actually have some money</p>
<p>that actually goes into AI safety research also</p>
<p>and doesn&rsquo;t just get grabbed by whatever.</p>
<p>That&rsquo;s a fantastic investment.</p>
<p>And then at the higher level, you asked this question,</p>
<p>okay, what can we do?</p>
<p>What are the biggest risks?</p>
<p>I think we cannot just consider this</p>
<p>to be only a technical problem.</p>
<p>Again, because if you solve only the technical problem,</p>
<p>can I play with your robot?</p>
<p>Yes, please.</p>
<p>If we can get our machines to just blindly obey</p>
<p>the orders we give them,</p>
<p>so we can always trust that it will do what we want.</p>
<p>That might be great for the owner of the robot.</p>
<p>That might not be so great for the rest of humanity</p>
<p>if that person is that least favorite world leader</p>
<p>or whatever you imagine, right?</p>
<p>So we have to also take a look at the,</p>
<p>apply alignment, not just to machines,</p>
<p>but to all the other powerful structures.</p>
<p>That&rsquo;s why it&rsquo;s so important</p>
<p>to strengthen our democracy again,</p>
<p>as I said, to have institutions,</p>
<p>make sure that the playing field is not rigged</p>
<p>so that corporations are given the right incentives</p>
<p>to do the things that both make profit</p>
<p>and are good for people,</p>
<p>to make sure that countries have incentives</p>
<p>to do things that are both good for their people</p>
<p>and don&rsquo;t screw up the rest of the world.</p>
<p>And this is not just something for AI nerds to geek out on.</p>
<p>This is an interesting challenge for political scientists,</p>
<p>economists, and so many other thinkers.</p>
<p>So one of the magical things</p>
<p>that perhaps makes this earth quite unique</p>
<p>is that it&rsquo;s home to conscious beings.</p>
<p>So you mentioned consciousness.</p>
<p>Perhaps as a small aside,</p>
<p>because we didn&rsquo;t really get specific</p>
<p>to how we might do the alignment.</p>
<p>Like you said,</p>
<p>is there just a really important research problem,</p>
<p>but do you think engineering consciousness</p>
<p>into AI systems is a possibility,</p>
<p>is something that we might one day do,</p>
<p>or is there something fundamental to consciousness</p>
<p>that is, is there something about consciousness</p>
<p>that is fundamental to humans and humans only?</p>
<p>I think it&rsquo;s possible.</p>
<p>I think both consciousness and intelligence</p>
<p>are information processing.</p>
<p>Certain types of information processing.</p>
<p>And that fundamentally,</p>
<p>it doesn&rsquo;t matter whether the information is processed</p>
<p>by carbon atoms in the neurons and brains</p>
<p>or by silicon atoms and so on in our technology.</p>
<p>Some people disagree.</p>
<p>This is what I think as a physicist.</p>
<p>That consciousness is the same kind of,</p>
<p>you said consciousness is information processing.</p>
<p>So meaning, I think you had a quote of something like</p>
<p>it&rsquo;s information knowing itself, that kind of thing.</p>
<p>I think consciousness is, yeah,</p>
<p>is the way information feels when it&rsquo;s being processed.</p>
<p>One&rsquo;s being put in complex ways.</p>
<p>We don&rsquo;t know exactly what those complex ways are.</p>
<p>It&rsquo;s clear that most of the information processing</p>
<p>in our brains does not create an experience.</p>
<p>We&rsquo;re not even aware of it, right?</p>
<p>Like for example,</p>
<p>you&rsquo;re not aware of your heartbeat regulation right now,</p>
<p>even though it&rsquo;s clearly being done by your body, right?</p>
<p>It&rsquo;s just kind of doing its own thing.</p>
<p>When you go jogging,</p>
<p>there&rsquo;s a lot of complicated stuff</p>
<p>about how you put your foot down and we know it&rsquo;s hard.</p>
<p>That&rsquo;s why robots used to fall over so much,</p>
<p>but you&rsquo;re mostly unaware about it.</p>
<p>Your brain, your CEO consciousness module</p>
<p>just sends an email,</p>
<p>hey, I&rsquo;m gonna keep jogging along this path.</p>
<p>The rest is on autopilot, right?</p>
<p>So most of it is not conscious,</p>
<p>but somehow there is some of the information processing,</p>
<p>which is we don&rsquo;t know what exactly.</p>
<p>I think this is a science problem</p>
<p>that I hope one day we&rsquo;ll have some equation for</p>
<p>or something so we can be able to build</p>
<p>a consciousness detector and say, yeah,</p>
<p>here there is some consciousness, here there&rsquo;s not.</p>
<p>Oh, don&rsquo;t boil that lobster because it&rsquo;s feeling pain</p>
<p>or it&rsquo;s okay because it&rsquo;s not feeling pain.</p>
<p>Right now we treat this as sort of just metaphysics,</p>
<p>but it would be very useful in emergency rooms</p>
<p>to know if a patient has locked in syndrome</p>
<p>and is conscious or if they are actually just out.</p>
<p>And in the future, if you build a very, very intelligent</p>
<p>helper robot to take care of you,</p>
<p>I think you&rsquo;d like to know</p>
<p>if you should feel guilty about shutting it down</p>
<p>or if it&rsquo;s just like a zombie going through the motions</p>
<p>like a fancy tape recorder, right?</p>
<p>And once we can make progress</p>
<p>on the science of consciousness</p>
<p>and figure out what is conscious and what isn&rsquo;t,</p>
<p>then assuming we want to create positive experiences</p>
<p>and not suffering, we&rsquo;ll probably choose to build</p>
<p>some machines that are deliberately unconscious</p>
<p>that do incredibly boring, repetitive jobs</p>
<p>in an iron mine somewhere or whatever.</p>
<p>And maybe we&rsquo;ll choose to create helper robots</p>
<p>for the elderly that are conscious</p>
<p>so that people don&rsquo;t just feel creeped out</p>
<p>that the robot is just faking it</p>
<p>when it acts like it&rsquo;s sad or happy.</p>
<p>Like you said, elderly,</p>
<p>I think everybody gets pretty deeply lonely in this world.</p>
<p>And so there&rsquo;s a place I think for everybody</p>
<p>to have a connection with conscious beings,</p>
<p>whether they&rsquo;re human or otherwise.</p>
<p>But I know for sure that I would,</p>
<p>if I had a robot, if I was gonna develop any kind</p>
<p>of personal emotional connection with it,</p>
<p>I would be very creeped out</p>
<p>if I knew it in an intellectual level</p>
<p>that the whole thing was just a fraud.</p>
<p>Now today you can buy a little talking doll for a kid</p>
<p>which will say things and the little child will often think</p>
<p>that this is actually conscious</p>
<p>and even real secrets to it that then go on the internet</p>
<p>and with lots of the creepy repercussions.</p>
<p>I would not wanna be just hacked and tricked like this.</p>
<p>If I was gonna be developing real emotional connections</p>
<p>with the robot, I would wanna know</p>
<p>that this is actually real.</p>
<p>It&rsquo;s acting conscious, acting happy</p>
<p>because it actually feels it.</p>
<p>And I think this is not sci fi.</p>
<p>I think it&rsquo;s possible to measure, to come up with tools.</p>
<p>After we understand the science of consciousness,</p>
<p>you&rsquo;re saying we&rsquo;ll be able to come up with tools</p>
<p>that can measure consciousness</p>
<p>and definitively say like this thing is experiencing</p>
<p>the things it says it&rsquo;s experiencing.</p>
<p>Kind of by definition.</p>
<p>If it is a physical phenomenon, information processing</p>
<p>and we know that some information processing is conscious</p>
<p>and some isn&rsquo;t, well, then there is something there</p>
<p>to be discovered with the methods of science.</p>
<p>Giulio Tononi has stuck his neck out the farthest</p>
<p>and written down some equations for a theory.</p>
<p>Maybe that&rsquo;s right, maybe it&rsquo;s wrong.</p>
<p>We certainly don&rsquo;t know.</p>
<p>But I applaud that kind of efforts to sort of take this,</p>
<p>say this is not just something that philosophers</p>
<p>can have beer and muse about,</p>
<p>but something we can measure and study.</p>
<p>And coming, bringing that back to us,</p>
<p>I think what we would probably choose to do, as I said,</p>
<p>is if we cannot figure this out,</p>
<p>choose to make, to be quite mindful</p>
<p>about what sort of consciousness, if any,</p>
<p>we put in different machines that we have.</p>
<p>And certainly, we wouldn&rsquo;t wanna make,</p>
<p>we should not be making much machines that suffer</p>
<p>without us even knowing it, right?</p>
<p>And if at any point someone decides to upload themselves</p>
<p>like Ray Kurzweil wants to do,</p>
<p>I don&rsquo;t know if you&rsquo;ve had him on your show.</p>
<p>We agree, but then COVID happens,</p>
<p>so we&rsquo;re waiting it out a little bit.</p>
<p>Suppose he uploads himself into this robo Ray</p>
<p>and it talks like him and acts like him and laughs like him.</p>
<p>And before he powers off his biological body,</p>
<p>he would probably be pretty disturbed</p>
<p>if he realized that there&rsquo;s no one home.</p>
<p>This robot is not having any subjective experience, right?</p>
<p>If humanity gets replaced by machine descendants,</p>
<p>which do all these cool things and build spaceships</p>
<p>and go to intergalactic rock concerts,</p>
<p>and it turns out that they are all unconscious,</p>
<p>just going through the motions,</p>
<p>wouldn&rsquo;t that be like the ultimate zombie apocalypse, right?</p>
<p>Just a play for empty benches?</p>
<p>Yeah, I have a sense that there&rsquo;s some kind of,</p>
<p>once we understand consciousness better,</p>
<p>we&rsquo;ll understand that there&rsquo;s some kind of continuum</p>
<p>and it would be a greater appreciation.</p>
<p>And we&rsquo;ll probably understand, just like you said,</p>
<p>it&rsquo;d be unfortunate if it&rsquo;s a trick.</p>
<p>We&rsquo;ll probably definitely understand</p>
<p>that love is indeed a trick that we&rsquo;ll play on each other,</p>
<p>that we humans are, we convince ourselves we&rsquo;re conscious,</p>
<p>but we&rsquo;re really, us and trees and dolphins</p>
<p>are all the same kind of consciousness.</p>
<p>Can I try to cheer you up a little bit</p>
<p>with a philosophical thought here about the love part?</p>
<p>Yes, let&rsquo;s do it.</p>
<p>You know, you might say,</p>
<p>okay, yeah, love is just a collaboration enabler.</p>
<p>And then maybe you can go and get depressed about that.</p>
<p>But I think that would be the wrong conclusion, actually.</p>
<p>You know, I know that the only reason I enjoy food</p>
<p>is because my genes hacked me</p>
<p>and they don&rsquo;t want me to starve to death.</p>
<p>Not because they care about me consciously</p>
<p>enjoying succulent delights of pistachio ice cream,</p>
<p>but they just want me to make copies of them.</p>
<p>The whole thing, so in a sense,</p>
<p>the whole enjoyment of food is also a scam like this.</p>
<p>But does that mean I shouldn&rsquo;t take pleasure</p>
<p>in this pistachio ice cream?</p>
<p>I love pistachio ice cream.</p>
<p>And I can tell you, I know this is an experimental fact.</p>
<p>I enjoy pistachio ice cream every bit as much,</p>
<p>even though I scientifically know exactly why,</p>
<p>what kind of scam this was.</p>
<p>Your genes really appreciate</p>
<p>that you like the pistachio ice cream.</p>
<p>Well, but I, my mind appreciates it too, you know?</p>
<p>And I have a conscious experience right now.</p>
<p>Ultimately, all of my brain is also just something</p>
<p>the genes built to copy themselves.</p>
<p>But so what?</p>
<p>You know, I&rsquo;m grateful that,</p>
<p>yeah, thanks genes for doing this,</p>
<p>but you know, now it&rsquo;s my brain that&rsquo;s in charge here</p>
<p>and I&rsquo;m gonna enjoy my conscious experience,</p>
<p>thank you very much.</p>
<p>And not just the pistachio ice cream,</p>
<p>but also the love I feel for my amazing wife</p>
<p>and all the other delights of being conscious.</p>
<p>I don&rsquo;t, actually Richard Feynman,</p>
<p>I think said this so well.</p>
<p>He is also the guy, you know, really got me into physics.</p>
<p>Some art friend said that,</p>
<p>oh, science kind of just is the party pooper.</p>
<p>It&rsquo;s kind of ruins the fun, right?</p>
<p>When like you have a beautiful flowers as the artist</p>
<p>and then the scientist is gonna deconstruct that</p>
<p>into just a blob of quarks and electrons.</p>
<p>And Feynman pushed back on that in such a beautiful way,</p>
<p>which I think also can be used to push back</p>
<p>and make you not feel guilty about falling in love.</p>
<p>So here&rsquo;s what Feynman basically said.</p>
<p>He said to his friend, you know,</p>
<p>yeah, I can also as a scientist see</p>
<p>that this is a beautiful flower, thank you very much.</p>
<p>Maybe I can&rsquo;t draw as good a painting as you</p>
<p>because I&rsquo;m not as talented an artist,</p>
<p>but yeah, I can really see the beauty in it.</p>
<p>And it just, it also looks beautiful to me.</p>
<p>But in addition to that, Feynman said, as a scientist,</p>
<p>I see even more beauty that the artist did not see, right?</p>
<p>Suppose this is a flower on a blossoming apple tree.</p>
<p>You could say this tree has more beauty in it</p>
<p>than just the colors and the fragrance.</p>
<p>This tree is made of air, Feynman wrote.</p>
<p>This is one of my favorite Feynman quotes ever.</p>
<p>And it took the carbon out of the air</p>
<p>and bound it in using the flaming heat of the sun,</p>
<p>you know, to turn the air into a tree.</p>
<p>And when you burn logs in your fireplace,</p>
<p>it&rsquo;s really beautiful to think that this is being reversed.</p>
<p>Now the tree is going, the wood is going back into air.</p>
<p>And in this flaming, beautiful dance of the fire</p>
<p>that the artist can see is the flaming light of the sun</p>
<p>that was bound in to turn the air into tree.</p>
<p>And then the ashes is the little residue</p>
<p>that didn&rsquo;t come from the air</p>
<p>that the tree sucked out of the ground, you know.</p>
<p>Feynman said, these are beautiful things.</p>
<p>And science just adds, it doesn&rsquo;t subtract.</p>
<p>And I feel exactly that way about love</p>
<p>and about pistachio ice cream also.</p>
<p>I can understand that there is even more nuance</p>
<p>to the whole thing, right?</p>
<p>At this very visceral level,</p>
<p>you can fall in love just as much as someone</p>
<p>who knows nothing about neuroscience.</p>
<p>But you can also appreciate this even greater beauty in it.</p>
<p>Just like, isn&rsquo;t it remarkable that it came about</p>
<p>from this completely lifeless universe,</p>
<p>just a bunch of hot blob of plasma expanding.</p>
<p>And then over the eons, you know, gradually,</p>
<p>first the strong nuclear force decided</p>
<p>to combine quarks together into nuclei.</p>
<p>And then the electric force bound in electrons</p>
<p>and made atoms.</p>
<p>And then they clustered from gravity</p>
<p>and you got planets and stars and this and that.</p>
<p>And then natural selection came along</p>
<p>and the genes had their little thing.</p>
<p>And you started getting what went from seeming</p>
<p>like a completely pointless universe</p>
<p>that we&rsquo;re just trying to increase entropy</p>
<p>and approach heat death into something</p>
<p>that looked more goal oriented.</p>
<p>Isn&rsquo;t that kind of beautiful?</p>
<p>And then this goal orientedness through evolution</p>
<p>got ever more sophisticated where you got ever more.</p>
<p>And then you started getting this thing,</p>
<p>which is kind of like DeepMind&rsquo;s mu zero and steroids,</p>
<p>the ultimate self play is not what DeepMind&rsquo;s AI</p>
<p>does against itself to get better at go.</p>
<p>It&rsquo;s what all these little quark blobs did</p>
<p>against each other in the game of survival of the fittest.</p>
<p>Now, when you had really dumb bacteria</p>
<p>living in a simple environment,</p>
<p>there wasn&rsquo;t much incentive to get intelligent,</p>
<p>but then the life made environment more complex.</p>
<p>And then there was more incentive to get even smarter.</p>
<p>And that gave the other organisms more of incentive</p>
<p>to also get smarter.</p>
<p>And then here we are now,</p>
<p>just like mu zero learned to become world master at go</p>
<p>and chess from playing against itself</p>
<p>by just playing against itself.</p>
<p>All the quirks here on our planet,</p>
<p>the electrons have created giraffes and elephants</p>
<p>and humans and love.</p>
<p>I just find that really beautiful.</p>
<p>And to me, that just adds to the enjoyment of love.</p>
<p>It doesn&rsquo;t subtract anything.</p>
<p>Do you feel a little more careful now?</p>
<p>I feel way better, that was incredible.</p>
<p>So this self play of quirks,</p>
<p>taking back to the beginning of our conversation</p>
<p>a little bit, there&rsquo;s so many exciting possibilities</p>
<p>about artificial intelligence understanding</p>
<p>the basic laws of physics.</p>
<p>Do you think AI will help us unlock?</p>
<p>There&rsquo;s been quite a bit of excitement</p>
<p>throughout the history of physics</p>
<p>of coming up with more and more general simple laws</p>
<p>that explain the nature of our reality.</p>
<p>And then the ultimate of that would be a theory</p>
<p>of everything that combines everything together.</p>
<p>Do you think it&rsquo;s possible that one, we humans,</p>
<p>but perhaps AI systems will figure out a theory of physics</p>
<p>that unifies all the laws of physics?</p>
<p>Yeah, I think it&rsquo;s absolutely possible.</p>
<p>I think it&rsquo;s very clear</p>
<p>that we&rsquo;re gonna see a great boost to science.</p>
<p>We&rsquo;re already seeing a boost actually</p>
<p>from machine learning helping science.</p>
<p>Alpha fold was an example,</p>
<p>the decades old protein folding problem.</p>
<p>So, and gradually, yeah, unless we go extinct</p>
<p>by doing something dumb like we discussed,</p>
<p>I think it&rsquo;s very likely</p>
<p>that our understanding of physics will become so good</p>
<p>that our technology will no longer be limited</p>
<p>by human intelligence,</p>
<p>but instead be limited by the laws of physics.</p>
<p>So our tech today is limited</p>
<p>by what we&rsquo;ve been able to invent, right?</p>
<p>I think as AI progresses,</p>
<p>it&rsquo;ll just be limited by the speed of light</p>
<p>and other physical limits,</p>
<p>which would mean it&rsquo;s gonna be just dramatically beyond</p>
<p>where we are now.</p>
<p>Do you think it&rsquo;s a fundamentally mathematical pursuit</p>
<p>of trying to understand like the laws</p>
<p>of our universe from a mathematical perspective?</p>
<p>So almost like if it&rsquo;s AI,</p>
<p>it&rsquo;s exploring the space of like theorems</p>
<p>and those kinds of things,</p>
<p>or is there some other more computational ideas,</p>
<p>more sort of empirical ideas?</p>
<p>They&rsquo;re both, I would say.</p>
<p>It&rsquo;s really interesting to look out at the landscape</p>
<p>of everything we call science today.</p>
<p>So here you come now with this big new hammer.</p>
<p>It says machine learning on it</p>
<p>and that&rsquo;s, you know, where are there some nails</p>
<p>that you can help with here that you can hammer?</p>
<p>Ultimately, if machine learning gets the point</p>
<p>that it can do everything better than us,</p>
<p>it will be able to help across the whole space of science.</p>
<p>But maybe we can anchor it by starting a little bit</p>
<p>right now near term and see how we kind of move forward.</p>
<p>So like right now, first of all,</p>
<p>you have a lot of big data science, right?</p>
<p>Where, for example, with telescopes,</p>
<p>we are able to collect way more data every hour</p>
<p>than a grad student can just pour over</p>
<p>like in the old times, right?</p>
<p>And machine learning is already being used very effectively,</p>
<p>even at MIT, to find planets around other stars,</p>
<p>to detect exciting new signatures</p>
<p>of new particle physics in the sky,</p>
<p>to detect the ripples in the fabric of space time</p>
<p>that we call gravitational waves</p>
<p>caused by enormous black holes</p>
<p>crashing into each other halfway</p>
<p>across the observable universe.</p>
<p>Machine learning is running and ticking right now,</p>
<p>doing all these things,</p>
<p>and it&rsquo;s really helping all these experimental fields.</p>
<p>There is a separate front of physics,</p>
<p>computational physics,</p>
<p>which is getting an enormous boost also.</p>
<p>So we had to do all our computations by hand, right?</p>
<p>People would have these giant books</p>
<p>with tables of logarithms,</p>
<p>and oh my God, it pains me to even think</p>
<p>how long time it would have taken to do simple stuff.</p>
<p>Then we started to get little calculators and computers</p>
<p>that could do some basic math for us.</p>
<p>Now, what we&rsquo;re starting to see is</p>
<p>kind of a shift from GOFI, computational physics,</p>
<p>to neural network, computational physics.</p>
<p>What I mean by that is most computational physics</p>
<p>would be done by humans programming in</p>
<p>the intelligence of how to do the computation</p>
<p>into the computer.</p>
<p>Just as when Garry Kasparov got his posterior kicked</p>
<p>by IBM&rsquo;s Deep Blue in chess,</p>
<p>humans had programmed in exactly how to play chess.</p>
<p>Intelligence came from the humans.</p>
<p>It wasn&rsquo;t learned, right?</p>
<p>Mu zero can be not only Kasparov in chess,</p>
<p>but also Stockfish,</p>
<p>which is the best sort of GOFI chess program.</p>
<p>By learning, and we&rsquo;re seeing more of that now,</p>
<p>that shift beginning to happen in physics.</p>
<p>So let me give you an example.</p>
<p>So lattice QCD is an area of physics</p>
<p>whose goal is basically to take the periodic table</p>
<p>and just compute the whole thing from first principles.</p>
<p>This is not the search for theory of everything.</p>
<p>We already know the theory</p>
<p>that&rsquo;s supposed to produce as output the periodic table,</p>
<p>which atoms are stable, how heavy they are,</p>
<p>all that good stuff, their spectral lines.</p>
<p>It&rsquo;s a theory, lattice QCD,</p>
<p>you can put it on your tshirt.</p>
<p>Our colleague Frank Wilczek</p>
<p>got the Nobel Prize for working on it.</p>
<p>But the math is just too hard for us to solve.</p>
<p>We have not been able to start with these equations</p>
<p>and solve them to the extent that we can predict, oh yeah.</p>
<p>And then there is carbon,</p>
<p>and this is what the spectrum of the carbon atom looks like.</p>
<p>But awesome people are building</p>
<p>these supercomputer simulations</p>
<p>where you just put in these equations</p>
<p>and you make a big cubic lattice of space,</p>
<p>or actually it&rsquo;s a very small lattice</p>
<p>because you&rsquo;re going down to the subatomic scale,</p>
<p>and you try to solve it.</p>
<p>But it&rsquo;s just so computationally expensive</p>
<p>that we still haven&rsquo;t been able to calculate things</p>
<p>as accurately as we measure them in many cases.</p>
<p>And now machine learning is really revolutionizing this.</p>
<p>So my colleague Fiala Shanahan at MIT, for example,</p>
<p>she&rsquo;s been using this really cool</p>
<p>machine learning technique called normalizing flows,</p>
<p>where she&rsquo;s realized she can actually speed up</p>
<p>the calculation dramatically</p>
<p>by having the AI learn how to do things faster.</p>
<p>Another area like this</p>
<p>where we suck up an enormous amount of supercomputer time</p>
<p>to do physics is black hole collisions.</p>
<p>So now that we&rsquo;ve done the sexy stuff</p>
<p>of detecting a bunch of this with LIGO and other experiments,</p>
<p>we want to be able to know what we&rsquo;re seeing.</p>
<p>And so it&rsquo;s a very simple conceptual problem.</p>
<p>It&rsquo;s the two body problem.</p>
<p>Newton solved it for classical gravity hundreds of years ago,</p>
<p>but the two body problem is still not fully solved.</p>
<p>For black holes.</p>
<p>Black holes, yes, and Einstein&rsquo;s gravity</p>
<p>because they won&rsquo;t just orbit in space,</p>
<p>they won&rsquo;t just orbit each other forever anymore,</p>
<p>two things, they give off gravitational waves</p>
<p>and make sure they crash into each other.</p>
<p>And the game, what you want to do is you want to figure out,</p>
<p>okay, what kind of wave comes out</p>
<p>as a function of the masses of the two black holes,</p>
<p>as a function of how they&rsquo;re spinning,</p>
<p>relative to each other, et cetera.</p>
<p>And that is so hard.</p>
<p>It can take months of supercomputer time</p>
<p>and massive numbers of cores to do it.</p>
<p>Now, wouldn&rsquo;t it be great if you can use machine learning</p>
<p>to greatly speed that up, right?</p>
<p>Now you can use the expensive old GoFi calculation</p>
<p>as the truth, and then see if machine learning</p>
<p>can figure out a smarter, faster way</p>
<p>of getting the right answer.</p>
<p>Yet another area, like computational physics.</p>
<p>These are probably the big three</p>
<p>that suck up the most computer time.</p>
<p>Lattice QCD, black hole collisions,</p>
<p>and cosmological simulations,</p>
<p>where you take not a subatomic thing</p>
<p>and try to figure out the mass of the proton,</p>
<p>but you take something enormous</p>
<p>and try to look at how all the galaxies get formed in there.</p>
<p>There again, there are a lot of very cool ideas right now</p>
<p>about how you can use machine learning</p>
<p>to do this sort of stuff better.</p>
<p>The difference between this and the big data</p>
<p>is you kind of make the data yourself, right?</p>
<p>So, and then finally,</p>
<p>we&rsquo;re looking over the physics landscape</p>
<p>and seeing what can we hammer with machine learning, right?</p>
<p>So we talked about experimental data, big data,</p>
<p>discovering cool stuff that we humans</p>
<p>then look more closely at.</p>
<p>Then we talked about taking the expensive computations</p>
<p>we&rsquo;re doing now and figuring out</p>
<p>how to do them much faster and better with AI.</p>
<p>And finally, let&rsquo;s go really theoretical.</p>
<p>So things like discovering equations,</p>
<p>having deep fundamental insights,</p>
<p>this is something closest to what I&rsquo;ve been doing</p>
<p>in my group.</p>
<p>We talked earlier about the whole AI Feynman project,</p>
<p>where if you just have some data,</p>
<p>how do you automatically discover equations</p>
<p>that seem to describe this well,</p>
<p>that you can then go back as a human</p>
<p>and then work with and test and explore.</p>
<p>And you asked a really good question also</p>
<p>about if this is sort of a search problem in some sense.</p>
<p>That&rsquo;s very deep actually what you said, because it is.</p>
<p>Suppose I ask you to prove some mathematical theorem.</p>
<p>What is a proof in math?</p>
<p>It&rsquo;s just a long string of steps, logical steps</p>
<p>that you can write out with symbols.</p>
<p>And once you find it, it&rsquo;s very easy to write a program</p>
<p>to check whether it&rsquo;s a valid proof or not.</p>
<p>So why is it so hard to prove it?</p>
<p>Well, because there are ridiculously many possible</p>
<p>candidate proofs you could write down, right?</p>
<p>If the proof contains 10,000 symbols,</p>
<p>even if there were only 10 options</p>
<p>for what each symbol could be,</p>
<p>that&rsquo;s 10 to the power of 1,000 possible proofs,</p>
<p>which is way more than there are atoms in our universe.</p>
<p>So you could say it&rsquo;s trivial to prove these things.</p>
<p>You just write a computer, generate all strings,</p>
<p>and then check, is this a valid proof?</p>
<p>No.</p>
<p>Is this a valid proof?</p>
<p>No.</p>
<p>And then you just keep doing this forever.</p>
<p>But there are a lot of,</p>
<p>but it is fundamentally a search problem.</p>
<p>You just want to search the space of all those,</p>
<p>all strings of symbols to find one that is the proof, right?</p>
<p>And there&rsquo;s a whole area of machine learning called search.</p>
<p>How do you search through some giant space</p>
<p>to find the needle in the haystack?</p>
<p>And it&rsquo;s easier in cases</p>
<p>where there&rsquo;s a clear measure of good,</p>
<p>like you&rsquo;re not just right or wrong,</p>
<p>but this is better and this is worse,</p>
<p>so you can maybe get some hints</p>
<p>as to which direction to go in.</p>
<p>That&rsquo;s why we talked about neural networks work so well.</p>
<p>I mean, that&rsquo;s such a human thing</p>
<p>of that moment of genius</p>
<p>of figuring out the intuition of good, essentially.</p>
<p>I mean, we thought that that was&hellip;</p>
<p>Or is it?</p>
<p>Maybe it&rsquo;s not, right?</p>
<p>We thought that about chess, right?</p>
<p>That the ability to see like 10, 15,</p>
<p>sometimes 20 steps ahead was not a calculation</p>
<p>that humans were performing.</p>
<p>It was some kind of weird intuition</p>
<p>about different patterns, about board positions,</p>
<p>about the relative positions,</p>
<p>somehow stitching stuff together.</p>
<p>And a lot of it is just like intuition,</p>
<p>but then you have like alpha,</p>
<p>I guess zero be the first one that did the self play.</p>
<p>It just came up with this.</p>
<p>It was able to learn through self play mechanism,</p>
<p>this kind of intuition.</p>
<p>Exactly.</p>
<p>But just like you said, it&rsquo;s so fascinating to think,</p>
<p>well, they&rsquo;re in the space of totally new ideas.</p>
<p>Can that be done in developing theorems?</p>
<p>We know it can be done by neural networks</p>
<p>because we did it with the neural networks</p>
<p>in the craniums of the great mathematicians of humanity.</p>
<p>And I&rsquo;m so glad you brought up alpha zero</p>
<p>because that&rsquo;s the counter example.</p>
<p>It turned out we were flattering ourselves</p>
<p>when we said intuition is something different.</p>
<p>Only humans can do it.</p>
<p>It&rsquo;s not information processing.</p>
<p>It used to be that way.</p>
<p>Again, it&rsquo;s really instructive, I think,</p>
<p>to compare the chess computer Deep Blue</p>
<p>that beat Kasparov with alpha zero</p>
<p>that beat Lisa Dahl at Go.</p>
<p>Because for Deep Blue, there was no intuition.</p>
<p>There was some, humans had programmed in some intuition.</p>
<p>After humans had played a lot of games,</p>
<p>they told the computer, count the pawn as one point,</p>
<p>the bishop is three points, rook is five points,</p>
<p>and so on, you add it all up,</p>
<p>and then you add some extra points for past pawns</p>
<p>and subtract if the opponent has it and blah, blah, blah.</p>
<p>And then what Deep Blue did was just search.</p>
<p>Just very brute force and tried many, many moves ahead,</p>
<p>all these combinations and a prune tree search.</p>
<p>And it could think much faster than Kasparov, and it won.</p>
<p>And that, I think, inflated our egos</p>
<p>in a way it shouldn&rsquo;t have,</p>
<p>because people started to say, yeah, yeah,</p>
<p>it&rsquo;s just brute force search, but it has no intuition.</p>
<p>Alpha zero really popped our bubble there,</p>
<p>because what alpha zero does,</p>
<p>yes, it does also do some of that tree search,</p>
<p>but it also has this intuition module,</p>
<p>which in geek speak is called a value function,</p>
<p>where it just looks at the board</p>
<p>and comes up with a number for how good is that position.</p>
<p>The difference was no human told it</p>
<p>how good the position is, it just learned it.</p>
<p>And mu zero is the coolest or scariest of all,</p>
<p>depending on your mood,</p>
<p>because the same basic AI system</p>
<p>will learn what the good board position is,</p>
<p>regardless of whether it&rsquo;s chess or Go or Shogi</p>
<p>or Pacman or Lady Pacman or Breakout or Space Invaders</p>
<p>or any number, a bunch of other games.</p>
<p>You don&rsquo;t tell it anything,</p>
<p>and it gets this intuition after a while for what&rsquo;s good.</p>
<p>So this is very hopeful for science, I think,</p>
<p>because if it can get intuition</p>
<p>for what&rsquo;s a good position there,</p>
<p>maybe it can also get intuition</p>
<p>for what are some good directions to go</p>
<p>if you&rsquo;re trying to prove something.</p>
<p>I often, one of the most fun things in my science career</p>
<p>is when I&rsquo;ve been able to prove some theorem about something</p>
<p>and it&rsquo;s very heavily intuition guided, of course.</p>
<p>I don&rsquo;t sit and try all random strings.</p>
<p>I have a hunch that, you know,</p>
<p>this reminds me a little bit of about this other proof</p>
<p>I&rsquo;ve seen for this thing.</p>
<p>So maybe I first, what if I try this?</p>
<p>Nah, that didn&rsquo;t work out.</p>
<p>But this reminds me actually,</p>
<p>the way this failed reminds me of that.</p>
<p>So combining the intuition with all these brute force</p>
<p>capabilities, I think it&rsquo;s gonna be able to help physics too.</p>
<p>Do you think there&rsquo;ll be a day when an AI system</p>
<p>being the primary contributor, let&rsquo;s say 90% plus,</p>
<p>wins the Nobel Prize in physics?</p>
<p>Obviously they&rsquo;ll give it to the humans</p>
<p>because we humans don&rsquo;t like to give prizes to machines.</p>
<p>It&rsquo;ll give it to the humans behind the system.</p>
<p>You could argue that AI has already been involved</p>
<p>in some Nobel Prizes, probably,</p>
<p>maybe something with black holes and stuff like that.</p>
<p>Yeah, we don&rsquo;t like giving prizes to other life forms.</p>
<p>If someone wins a horse racing contest,</p>
<p>they don&rsquo;t give the prize to the horse either.</p>
<p>That&rsquo;s true.</p>
<p>But do you think that we might be able to see</p>
<p>something like that in our lifetimes when AI,</p>
<p>so like the first system I would say</p>
<p>that makes us think about a Nobel Prize seriously</p>
<p>is like Alpha Fold is making us think about</p>
<p>in medicine, physiology, a Nobel Prize,</p>
<p>perhaps discoveries that are direct result</p>
<p>of something that&rsquo;s discovered by Alpha Fold.</p>
<p>Do you think in physics we might be able</p>
<p>to see that in our lifetimes?</p>
<p>I think what&rsquo;s probably gonna happen</p>
<p>is more of a blurring of the distinctions.</p>
<p>So today if somebody uses a computer</p>
<p>to do a computation that gives them the Nobel Prize,</p>
<p>nobody&rsquo;s gonna dream of giving the prize to the computer.</p>
<p>They&rsquo;re gonna be like, that was just a tool.</p>
<p>I think for these things also,</p>
<p>people are just gonna for a long time</p>
<p>view the computer as a tool.</p>
<p>But what&rsquo;s gonna change is the ubiquity of machine learning.</p>
<p>I think at some point in my lifetime,</p>
<p>finding a human physicist who knows nothing</p>
<p>about machine learning is gonna be almost as hard</p>
<p>as it is today finding a human physicist</p>
<p>who doesn&rsquo;t says, oh, I don&rsquo;t know anything about computers</p>
<p>or I don&rsquo;t use math.</p>
<p>That would just be a ridiculous concept.</p>
<p>You see, but the thing is there is a magic moment though,</p>
<p>like with Alpha Zero, when the system surprises us</p>
<p>in a way where the best people in the world</p>
<p>truly learn something from the system</p>
<p>in a way where you feel like it&rsquo;s another entity.</p>
<p>Like the way people, the way Magnus Carlsen,</p>
<p>the way certain people are looking at the work of Alpha Zero,</p>
<p>it&rsquo;s like, it truly is no longer a tool</p>
<p>in the sense that it doesn&rsquo;t feel like a tool.</p>
<p>It feels like some other entity.</p>
<p>So there&rsquo;s a magic difference like where you&rsquo;re like,</p>
<p>if an AI system is able to come up with an insight</p>
<p>that surprises everybody in some like major way</p>
<p>that&rsquo;s a phase shift in our understanding</p>
<p>of some particular science</p>
<p>or some particular aspect of physics,</p>
<p>I feel like that is no longer a tool.</p>
<p>And then you can start to say</p>
<p>that like it perhaps deserves the prize.</p>
<p>So for sure, the more important</p>
<p>and the more fundamental transformation</p>
<p>of the 21st century science is exactly what you&rsquo;re saying,</p>
<p>which is probably everybody will be doing machine learning.</p>
<p>It&rsquo;s to some degree.</p>
<p>Like if you want to be successful</p>
<p>at unlocking the mysteries of science,</p>
<p>you should be doing machine learning.</p>
<p>But it&rsquo;s just exciting to think about like,</p>
<p>whether there&rsquo;ll be one that comes along</p>
<p>that&rsquo;s super surprising and they&rsquo;ll make us question</p>
<p>like who the real inventors are in this world.</p>
<p>Yeah.</p>
<p>Yeah, I think the question of,</p>
<p>isn&rsquo;t if it&rsquo;s gonna happen, but when?</p>
<p>And, but it&rsquo;s important.</p>
<p>Honestly, in my mind, the time when that happens</p>
<p>is also more or less the same time</p>
<p>when we get artificial general intelligence.</p>
<p>And then we have a lot bigger things to worry about</p>
<p>than whether we should get the Nobel prize or not, right?</p>
<p>Yeah.</p>
<p>Because when you have machines</p>
<p>that can outperform our best scientists at science,</p>
<p>they can probably outperform us</p>
<p>at a lot of other stuff as well,</p>
<p>which can at a minimum make them</p>
<p>incredibly powerful agents in the world.</p>
<p>And I think it&rsquo;s a mistake to think</p>
<p>we only have to start worrying about loss of control</p>
<p>when machines get to AGI across the board,</p>
<p>where they can do everything, all our jobs.</p>
<p>Long before that, they&rsquo;ll be hugely influential.</p>
<p>We talked at length about how the hacking of our minds</p>
<p>with algorithms trying to get us glued to our screens,</p>
<p>right, has already had a big impact on society.</p>
<p>That was an incredibly dumb algorithm</p>
<p>in the grand scheme of things, right?</p>
<p>The supervised machine learning,</p>
<p>yet that had huge impact.</p>
<p>So I just don&rsquo;t want us to be lulled</p>
<p>into false sense of security</p>
<p>and think there won&rsquo;t be any societal impact</p>
<p>until things reach human level,</p>
<p>because it&rsquo;s happening already.</p>
<p>And I was just thinking the other week,</p>
<p>when I see some scaremonger going,</p>
<p>oh, the robots are coming,</p>
<p>the implication is always that they&rsquo;re coming to kill us.</p>
<p>Yeah.</p>
<p>And maybe you should have worried about that</p>
<p>if you were in Nagorno Karabakh</p>
<p>during the recent war there.</p>
<p>But more seriously, the robots are coming right now,</p>
<p>but they&rsquo;re mainly not coming to kill us.</p>
<p>They&rsquo;re coming to hack us.</p>
<p>They&rsquo;re coming to hack our minds,</p>
<p>into buying things that maybe we didn&rsquo;t need,</p>
<p>to vote for people who may not have</p>
<p>our best interest in mind.</p>
<p>And it&rsquo;s kind of humbling, I think,</p>
<p>actually, as a human being to admit</p>
<p>that it turns out that our minds are actually</p>
<p>much more hackable than we thought.</p>
<p>And the ultimate insult is that we are actually</p>
<p>getting hacked by the machine learning algorithms</p>
<p>that are, in some objective sense,</p>
<p>much dumber than us, you know?</p>
<p>But maybe we shouldn&rsquo;t be so surprised</p>
<p>because, you know, how do you feel about cute puppies?</p>
<p>Love them.</p>
<p>So, you know, you would probably argue</p>
<p>that in some across the board measure,</p>
<p>you&rsquo;re more intelligent than they are,</p>
<p>but boy, are cute puppies good at hacking us, right?</p>
<p>Yeah.</p>
<p>They move into our house, persuade us to feed them</p>
<p>and do all these things.</p>
<p>And what do they ever do but for us?</p>
<p>Yeah.</p>
<p>Other than being cute and making us feel good, right?</p>
<p>So if puppies can hack us,</p>
<p>maybe we shouldn&rsquo;t be so surprised</p>
<p>if pretty dumb machine learning algorithms can hack us too.</p>
<p>Not to speak of cats, which is another level.</p>
<p>And I think we should,</p>
<p>to counter your previous point about there,</p>
<p>let us not think about evil creatures in this world.</p>
<p>We can all agree that cats are as close</p>
<p>to objective evil as we can get.</p>
<p>But that&rsquo;s just me saying that.</p>
<p>Okay, so you have.</p>
<p>Have you seen the cartoon?</p>
<p>I think it&rsquo;s maybe the onion</p>
<p>with this incredibly cute kitten.</p>
<p>And it just says, it&rsquo;s underneath something</p>
<p>that thinks about murder all day.</p>
<p>Exactly.</p>
<p>That&rsquo;s accurate.</p>
<p>You&rsquo;ve mentioned offline that there might be a link</p>
<p>between post biological AGI and SETI.</p>
<p>So last time we talked,</p>
<p>you&rsquo;ve talked about this intuition</p>
<p>that we humans might be quite unique</p>
<p>in our galactic neighborhood.</p>
<p>Perhaps our galaxy,</p>
<p>perhaps the entirety of the observable universe</p>
<p>who might be the only intelligent civilization here,</p>
<p>which is, and you argue pretty well for that thought.</p>
<p>So I have a few little questions around this.</p>
<p>One, the scientific question,</p>
<p>in which way would you be,</p>
<p>if you were wrong in that intuition,</p>
<p>in which way do you think you would be surprised?</p>
<p>Like why were you wrong?</p>
<p>We find out that you ended up being wrong.</p>
<p>Like in which dimension?</p>
<p>So like, is it because we can&rsquo;t see them?</p>
<p>Is it because the nature of their intelligence</p>
<p>or the nature of their life is totally different</p>
<p>than we can possibly imagine?</p>
<p>Is it because the,</p>
<p>I mean, something about the great filters</p>
<p>and surviving them,</p>
<p>or maybe because we&rsquo;re being protected from signals,</p>
<p>all those explanations for why we haven&rsquo;t heard</p>
<p>a big, loud, like red light that says we&rsquo;re here.</p>
<p>So there are actually two separate things there</p>
<p>that I could be wrong about,</p>
<p>two separate claims that I made, right?</p>
<p>One of them is, I made the claim,</p>
<p>I think most civilizations,</p>
<p>when you&rsquo;re going from simple bacteria like things</p>
<p>to space colonizing civilizations,</p>
<p>they spend only a very, very tiny fraction</p>
<p>of their life being where we are.</p>
<p>That I could be wrong about.</p>
<p>The other one I could be wrong about</p>
<p>is the quite different statement that I think that actually</p>
<p>I&rsquo;m guessing that we are the only civilization</p>
<p>in our observable universe</p>
<p>from which light has reached us so far</p>
<p>that&rsquo;s actually gotten far enough to invent telescopes.</p>
<p>So let&rsquo;s talk about maybe both of them in turn</p>
<p>because they really are different.</p>
<p>The first one, if you look at the N equals one,</p>
<p>the data point we have on this planet, right?</p>
<p>So we spent four and a half billion years</p>
<p>fluxing around on this planet with life, right?</p>
<p>We got, and most of it was pretty lame stuff</p>
<p>from an intelligence perspective,</p>
<p>you know, it was bacteria and then the dinosaurs spent,</p>
<p>then the things gradually accelerated, right?</p>
<p>Then the dinosaurs spent over a hundred million years</p>
<p>stomping around here without even inventing smartphones.</p>
<p>And then very recently, you know,</p>
<p>it&rsquo;s only, we&rsquo;ve only spent 400 years</p>
<p>going from Newton to us, right?</p>
<p>In terms of technology.</p>
<p>And look what we&rsquo;ve done even, you know,</p>
<p>when I was a little kid, there was no internet even.</p>
<p>So it&rsquo;s, I think it&rsquo;s pretty likely for,</p>
<p>in this case of this planet, right?</p>
<p>That we&rsquo;re either gonna really get our act together</p>
<p>and start spreading life into space, the century,</p>
<p>and doing all sorts of great things,</p>
<p>or we&rsquo;re gonna wipe out.</p>
<p>It&rsquo;s a little hard.</p>
<p>If I, I could be wrong in the sense that maybe</p>
<p>what happened on this earth is very atypical.</p>
<p>And for some reason, what&rsquo;s more common on other planets</p>
<p>is that they spend an enormously long time</p>
<p>futzing around with the ham radio and things,</p>
<p>but they just never really take it to the next level</p>
<p>for reasons I don&rsquo;t, I haven&rsquo;t understood.</p>
<p>I&rsquo;m humble and open to that.</p>
<p>But I would bet at least 10 to one</p>
<p>that our situation is more typical</p>
<p>because the whole thing with Moore&rsquo;s law</p>
<p>and accelerating technology,</p>
<p>it&rsquo;s pretty obvious why it&rsquo;s happening.</p>
<p>Everything that grows exponentially,</p>
<p>we call it an explosion,</p>
<p>whether it&rsquo;s a population explosion or a nuclear explosion,</p>
<p>it&rsquo;s always caused by the same thing.</p>
<p>It&rsquo;s that the next step triggers a step after that.</p>
<p>So I, we, tomorrow&rsquo;s technology,</p>
<p>today&rsquo;s technology enables tomorrow&rsquo;s technology</p>
<p>and that enables the next level.</p>
<p>And as I think, because the technology is always better,</p>
<p>of course, the steps can come faster and faster.</p>
<p>On the other question that I might be wrong about,</p>
<p>that&rsquo;s the much more controversial one, I think.</p>
<p>But before we close out on this thing about,</p>
<p>if, the first one, if it&rsquo;s true</p>
<p>that most civilizations spend only a very short amount</p>
<p>of their total time in the stage, say,</p>
<p>between inventing</p>
<p>telescopes or mastering electricity</p>
<p>and leaving there and doing space travel,</p>
<p>if that&rsquo;s actually generally true,</p>
<p>then that should apply also elsewhere out there.</p>
<p>So we should be very, very,</p>
<p>we should be very, very surprised</p>
<p>if we find some random civilization</p>
<p>and we happen to catch them exactly</p>
<p>in that very, very short stage.</p>
<p>It&rsquo;s much more likely</p>
<p>that we find a planet full of bacteria.</p>
<p>Or that we find some civilization</p>
<p>that&rsquo;s already post biological</p>
<p>and has done some really cool galactic construction projects</p>
<p>in their galaxy.</p>
<p>Would we be able to recognize them, do you think?</p>
<p>Is it possible that we just can&rsquo;t,</p>
<p>I mean, this post biological world,</p>
<p>could it be just existing in some other dimension?</p>
<p>It could be just all a virtual reality game</p>
<p>for them or something, I don&rsquo;t know,</p>
<p>that it changes completely</p>
<p>where we won&rsquo;t be able to detect.</p>
<p>We have to be honestly very humble about this.</p>
<p>I think I said earlier the number one principle</p>
<p>of being a scientist is you have to be humble</p>
<p>and willing to acknowledge that everything we think,</p>
<p>guess might be totally wrong.</p>
<p>Of course, you could imagine some civilization</p>
<p>where they all decide to become Buddhists</p>
<p>and very inward looking</p>
<p>and just move into their little virtual reality</p>
<p>and not disturb the flora and fauna around them</p>
<p>and we might not notice them.</p>
<p>But this is a numbers game, right?</p>
<p>If you have millions of civilizations out there</p>
<p>or billions of them,</p>
<p>all it takes is one with a more ambitious mentality</p>
<p>that decides, hey, we are gonna go out</p>
<p>and settle a bunch of other solar systems</p>
<p>and maybe galaxies.</p>
<p>And then it doesn&rsquo;t matter</p>
<p>if they&rsquo;re a bunch of quiet Buddhists,</p>
<p>we&rsquo;re still gonna notice that expansionist one, right?</p>
<p>And it seems like quite the stretch to assume that,</p>
<p>now we know even in our own galaxy</p>
<p>that there are probably a billion or more planets</p>
<p>that are pretty Earth like.</p>
<p>And many of them are formed over a billion years</p>
<p>before ours, so had a big head start.</p>
<p>So if you actually assume also</p>
<p>that life happens kind of automatically</p>
<p>on an Earth like planet,</p>
<p>I think it&rsquo;s quite the stretch to then go and say,</p>
<p>okay, so there are another billion civilizations out there</p>
<p>that also have our level of tech</p>
<p>and they all decided to become Buddhists</p>
<p>and not a single one decided to go Hitler on the galaxy</p>
<p>and say, we need to go out and colonize</p>
<p>or not a single one decided for more benevolent reasons</p>
<p>to go out and get more resources.</p>
<p>That seems like a bit of a stretch, frankly.</p>
<p>And this leads into the second thing</p>
<p>you challenged me that I might be wrong about,</p>
<p>how rare or common is life, you know?</p>
<p>So Francis Drake, when he wrote down the Drake equation,</p>
<p>multiplied together a huge number of factors</p>
<p>and then we don&rsquo;t know any of them.</p>
<p>So we know even less about what you get</p>
<p>when you multiply together the whole product.</p>
<p>Since then, a lot of those factors</p>
<p>have become much better known.</p>
<p>One of his big uncertainties was</p>
<p>how common is it that a solar system even has a planet?</p>
<p>Well, now we know it very common.</p>
<p>Earth like planets, we know we have better.</p>
<p>There are a dime a dozen, there are many, many of them,</p>
<p>even in our galaxy.</p>
<p>At the same time, you know, we have thanks to,</p>
<p>I&rsquo;m a big supporter of the SETI project and its cousins</p>
<p>and I think we should keep doing this</p>
<p>and we&rsquo;ve learned a lot.</p>
<p>We&rsquo;ve learned that so far,</p>
<p>all we have is still unconvincing hints, nothing more, right?</p>
<p>And there are certainly many scenarios</p>
<p>where it would be dead obvious.</p>
<p>If there were a hundred million</p>
<p>other human like civilizations in our galaxy,</p>
<p>it would not be that hard to notice some of them</p>
<p>with today&rsquo;s technology and we haven&rsquo;t, right?</p>
<p>So what we can say is, well, okay,</p>
<p>we can rule out that there is a human level of civilization</p>
<p>on the moon and in fact, the many nearby solar systems</p>
<p>where we cannot rule out, of course,</p>
<p>that there is something like Earth sitting in a galaxy</p>
<p>five billion light years away.</p>
<p>But we&rsquo;ve ruled out a lot</p>
<p>and that&rsquo;s already kind of shocking</p>
<p>given that there are all these planets there, you know?</p>
<p>So like, where are they?</p>
<p>Where are they all?</p>
<p>That&rsquo;s the classic Fermi paradox.</p>
<p>And so my argument, which might very well be wrong,</p>
<p>it&rsquo;s very simple really, it just goes like this.</p>
<p>Okay, we have no clue about this.</p>
<p>It could be the probability of getting life</p>
<p>on a random planet, it could be 10 to the minus one</p>
<p>a priori or 10 to the minus five, 10, 10 to the minus 20,</p>
<p>10 to the minus 30, 10 to the minus 40.</p>
<p>Basically every order of magnitude is about equally likely.</p>
<p>When then do the math and ask the question,</p>
<p>how close is our nearest neighbor?</p>
<p>It&rsquo;s again, equally likely that it&rsquo;s 10 to the 10 meters away,</p>
<p>10 to 20 meters away, 10 to the 30 meters away.</p>
<p>We have some nerdy ways of talking about this</p>
<p>with Bayesian statistics and a uniform log prior,</p>
<p>but that&rsquo;s irrelevant.</p>
<p>This is the simple basic argument.</p>
<p>And now comes the data.</p>
<p>So we can say, okay, there are all these orders</p>
<p>of magnitude, 10 to the 26 meters away,</p>
<p>there&rsquo;s the edge of our observable universe.</p>
<p>If it&rsquo;s farther than that, light hasn&rsquo;t even reached us yet.</p>
<p>If it&rsquo;s less than 10 to the 16 meters away,</p>
<p>well, it&rsquo;s within Earth&rsquo;s,</p>
<p>it&rsquo;s no farther away than the sun.</p>
<p>We can definitely rule that out.</p>
<p>So I think about it like this,</p>
<p>a priori before we looked at the telescopes,</p>
<p>it could be 10 to the 10 meters, 10 to the 20,</p>
<p>10 to the 30, 10 to the 40, 10 to the 50, 10 to blah, blah, blah.</p>
<p>Equally likely anywhere here.</p>
<p>And now we&rsquo;ve ruled out like this chunk.</p>
<p>And here is the edge of our observable universe already.</p>
<p>So I&rsquo;m certainly not saying I don&rsquo;t think</p>
<p>there&rsquo;s any life elsewhere in space.</p>
<p>If space is infinite,</p>
<p>then you&rsquo;re basically a hundred percent guaranteed</p>
<p>that there is, but the probability that there is life,</p>
<p>that the nearest neighbor,</p>
<p>it happens to be in this little region</p>
<p>between where we would have seen it already</p>
<p>and where we will never see it.</p>
<p>There&rsquo;s actually significantly less than one, I think.</p>
<p>And I think there&rsquo;s a moral lesson from this,</p>
<p>which is really important,</p>
<p>which is to be good stewards of this planet</p>
<p>and this shot we&rsquo;ve had.</p>
<p>It can be very dangerous to say,</p>
<p>oh, it&rsquo;s fine if we nuke our planet or ruin the climate</p>
<p>or mess it up with unaligned AI,</p>
<p>because I know there is this nice Star Trek fleet out there.</p>
<p>They&rsquo;re gonna swoop in and take over where we failed.</p>
<p>Just like it wasn&rsquo;t the big deal</p>
<p>that the Easter Island losers wiped themselves out.</p>
<p>That&rsquo;s a dangerous way of lulling yourself</p>
<p>into false sense of security.</p>
<p>If it&rsquo;s actually the case that it might be up to us</p>
<p>and only us, the whole future of intelligent life</p>
<p>in our observable universe,</p>
<p>then I think it really puts a lot of responsibility</p>
<p>on our shoulders.</p>
<p>It&rsquo;s inspiring, it&rsquo;s a little bit terrifying,</p>
<p>but it&rsquo;s also inspiring.</p>
<p>But it&rsquo;s empowering, I think, most of all,</p>
<p>because the biggest problem today is,</p>
<p>I see this even when I teach,</p>
<p>so many people feel that it doesn&rsquo;t matter what they do</p>
<p>or we do, we feel disempowered.</p>
<p>Oh, it makes no difference.</p>
<p>This is about as far from that as you can come.</p>
<p>But we realize that what we do</p>
<p>on our little spinning ball here in our lifetime</p>
<p>could make the difference for the entire future of life</p>
<p>in our universe.</p>
<p>How empowering is that?</p>
<p>Yeah, survival of consciousness.</p>
<p>I mean, a very similar kind of empowering aspect</p>
<p>of the Drake equation is,</p>
<p>say there is a huge number of intelligent civilizations</p>
<p>that spring up everywhere,</p>
<p>but because of the Drake equation,</p>
<p>which is the lifetime of a civilization,</p>
<p>maybe many of them hit a wall.</p>
<p>And just like you said, it&rsquo;s clear that that,</p>
<p>for us, the great filter,</p>
<p>the one possible great filter seems to be coming</p>
<p>in the next 100 years.</p>
<p>So it&rsquo;s also empowering to say,</p>
<p>okay, well, we have a chance to not,</p>
<p>I mean, the way great filters work,</p>
<p>they just get most of them.</p>
<p>Exactly.</p>
<p>Nick Bostrom has articulated this really beautifully too.</p>
<p>Every time yet another search for life on Mars</p>
<p>comes back negative or something,</p>
<p>I&rsquo;m like, yes, yes.</p>
<p>Our odds for us surviving is the best.</p>
<p>You already made the argument in broad brush there, right?</p>
<p>But just to unpack it, right?</p>
<p>The point is we already know</p>
<p>there is a crap ton of planets out there</p>
<p>that are Earth like,</p>
<p>and we also know that most of them do not seem</p>
<p>to have anything like our kind of life on them.</p>
<p>So what went wrong?</p>
<p>There&rsquo;s clearly one step along the evolutionary,</p>
<p>at least one filter or roadblock</p>
<p>in going from no life to spacefaring life.</p>
<p>And where is it?</p>
<p>Is it in front of us or is it behind us, right?</p>
<p>If there&rsquo;s no filter behind us,</p>
<p>and we keep finding all sorts of little mice on Mars</p>
<p>or whatever, right?</p>
<p>That&rsquo;s actually very depressing</p>
<p>because that makes it much more likely</p>
<p>that the filter is in front of us.</p>
<p>And that what actually is going on</p>
<p>is like the ultimate dark joke</p>
<p>that whenever a civilization</p>
<p>invents sufficiently powerful tech,</p>
<p>it&rsquo;s just, you just set your clock.</p>
<p>And then after a little while it goes poof</p>
<p>for one reason or other and wipes itself out.</p>
<p>Now wouldn&rsquo;t that be like utterly depressing</p>
<p>if we&rsquo;re actually doomed?</p>
<p>Whereas if it turns out that there is a really,</p>
<p>there is a great filter early on</p>
<p>that for whatever reason seems to be really hard</p>
<p>to get to the stage of sexually reproducing organisms</p>
<p>or even the first ribosome or whatever, right?</p>
<p>Or maybe you have lots of planets with dinosaurs and cows,</p>
<p>but for some reason they tend to get stuck there</p>
<p>and never invent smartphones.</p>
<p>All of those are huge boosts for our own odds</p>
<p>because been there done that, you know?</p>
<p>It doesn&rsquo;t matter how hard or unlikely it was</p>
<p>that we got past that roadblock</p>
<p>because we already did.</p>
<p>And then that makes it likely</p>
<p>that the future is in our own hands, we&rsquo;re not doomed.</p>
<p>So that&rsquo;s why I think the fact</p>
<p>that life is rare in the universe,</p>
<p>it&rsquo;s not just something that there is some evidence for,</p>
<p>but also something we should actually hope for.</p>
<p>So that&rsquo;s the end, the mortality,</p>
<p>the death of human civilization</p>
<p>that we&rsquo;ve been discussing in life,</p>
<p>maybe prospering beyond any kind of great filter.</p>
<p>Do you think about your own death?</p>
<p>Does it make you sad that you may not witness some of the,</p>
<p>you know, you lead a research group</p>
<p>on working some of the biggest questions</p>
<p>in the universe actually,</p>
<p>both on the physics and the AI side?</p>
<p>Does it make you sad that you may not be able</p>
<p>to see some of these exciting things come to fruition</p>
<p>that we&rsquo;ve been talking about?</p>
<p>Of course, of course it sucks, the fact that I&rsquo;m gonna die.</p>
<p>I remember once when I was much younger,</p>
<p>my dad made this remark that life is fundamentally tragic.</p>
<p>And I&rsquo;m like, what are you talking about, daddy?</p>
<p>And then many years later, I felt,</p>
<p>now I feel I totally understand what he means.</p>
<p>You know, we grow up, we&rsquo;re little kids</p>
<p>and everything is infinite and it&rsquo;s so cool.</p>
<p>And then suddenly we find out that actually, you know,</p>
<p>you got to serve only,</p>
<p>this is the, you&rsquo;re gonna get game over at some point.</p>
<p>So of course it&rsquo;s something that&rsquo;s sad.</p>
<p>Are you afraid?</p>
<p>No, not in the sense that I think anything terrible</p>
<p>is gonna happen after I die or anything like that.</p>
<p>No, I think it&rsquo;s really gonna be a game over,</p>
<p>but it&rsquo;s more that it makes me very acutely aware</p>
<p>of what a wonderful gift this is</p>
<p>that I get to be alive right now.</p>
<p>And is a steady reminder to just live life to the fullest</p>
<p>and really enjoy it because it is finite, you know.</p>
<p>And I think actually, and we know we all get</p>
<p>the regular reminders when someone near and dear to us dies</p>
<p>that one day it&rsquo;s gonna be our turn.</p>
<p>It adds this kind of focus.</p>
<p>I wonder what it would feel like actually</p>
<p>to be an immortal being if they might even enjoy</p>
<p>some of the wonderful things of life a little bit less</p>
<p>just because there isn&rsquo;t that.</p>
<p>Finiteness?</p>
<p>Yeah.</p>
<p>Do you think that could be a feature, not a bug,</p>
<p>the fact that we beings are finite?</p>
<p>Maybe there&rsquo;s lessons for engineering</p>
<p>in artificial intelligence systems as well</p>
<p>that are conscious.</p>
<p>Like do you think it makes, is it possible</p>
<p>that the reason the pistachio ice cream is delicious</p>
<p>is the fact that you&rsquo;re going to die one day</p>
<p>and you will not have all the pistachio ice cream</p>
<p>that you could eat because of that fact?</p>
<p>Well, let me say two things.</p>
<p>First of all, it&rsquo;s actually quite profound</p>
<p>what you&rsquo;re saying.</p>
<p>I do think I appreciate the pistachio ice cream</p>
<p>a lot more knowing that I will,</p>
<p>there&rsquo;s only a finite number of times I get to enjoy that.</p>
<p>And I can only remember a finite number of times</p>
<p>in the past.</p>
<p>And moreover, my life is not so long</p>
<p>that it just starts to feel like things are repeating</p>
<p>themselves in general.</p>
<p>It&rsquo;s so new and fresh.</p>
<p>I also think though that death is a little bit overrated</p>
<p>in the sense that it comes from a sort of outdated view</p>
<p>of physics and what life actually is.</p>
<p>Because if you ask, okay, what is it that&rsquo;s gonna die</p>
<p>exactly, what am I really?</p>
<p>When I say I feel sad about the idea of myself dying,</p>
<p>am I really sad that this skin cell here is gonna die?</p>
<p>Of course not, because it&rsquo;s gonna die next week anyway</p>
<p>and I&rsquo;ll grow a new one, right?</p>
<p>And it&rsquo;s not any of my cells that I&rsquo;m associating really</p>
<p>with who I really am.</p>
<p>Nor is it any of my atoms or quarks or electrons.</p>
<p>In fact, basically all of my atoms get replaced</p>
<p>on a regular basis, right?</p>
<p>So what is it that&rsquo;s really me</p>
<p>from a more modern physics perspective?</p>
<p>It&rsquo;s the information in processing me.</p>
<p>That&rsquo;s where my memory, that&rsquo;s my memories,</p>
<p>that&rsquo;s my values, my dreams, my passion, my love.</p>
<p>That&rsquo;s what&rsquo;s really fundamentally me.</p>
<p>And frankly, not all of that will die when my body dies.</p>
<p>Like Richard Feynman, for example, his body died of cancer,</p>
<p>but many of his ideas that he felt made him very him</p>
<p>actually live on.</p>
<p>This is my own little personal tribute to Richard Feynman.</p>
<p>I try to keep a little bit of him alive in myself.</p>
<p>I&rsquo;ve even quoted him today, right?</p>
<p>Yeah, he almost came alive for a brief moment</p>
<p>in this conversation, yeah.</p>
<p>Yeah, and this honestly gives me some solace.</p>
<p>When I work as a teacher, I feel,</p>
<p>if I can actually share a bit about myself</p>
<p>that my students feel worthy enough to copy and adopt</p>
<p>as some part of things that they know</p>
<p>or they believe or aspire to,</p>
<p>now I live on also a little bit in them, right?</p>
<p>And so being a teacher is a little bit</p>
<p>of what I, that&rsquo;s something also that contributes</p>
<p>to making me a little teeny bit less mortal, right?</p>
<p>Because I&rsquo;m not, at least not all gonna die all at once,</p>
<p>right?</p>
<p>And I find that a beautiful tribute to people</p>
<p>we do not respect.</p>
<p>If we can remember them and carry in us</p>
<p>the things that we felt was the most awesome about them,</p>
<p>right, then they live on.</p>
<p>And I&rsquo;m getting a bit emotional here,</p>
<p>but it&rsquo;s a very beautiful idea you bring up there.</p>
<p>I think we should stop this old fashioned materialism</p>
<p>and just equate who we are with our quirks and electrons.</p>
<p>There&rsquo;s no scientific basis for that really.</p>
<p>And it&rsquo;s also very uninspiring.</p>
<p>Now, if you look a little bit towards the future, right?</p>
<p>One thing which really sucks about humans dying is that even</p>
<p>though some of their teachings and memories and stories</p>
<p>and ethics and so on will be copied by those around them,</p>
<p>hopefully, a lot of it can&rsquo;t be copied</p>
<p>and just dies with them, with their brain.</p>
<p>And that really sucks.</p>
<p>That&rsquo;s the fundamental reason why we find it so tragic</p>
<p>when someone goes from having all this information there</p>
<p>to the more just gone, ruined, right?</p>
<p>With more post biological intelligence,</p>
<p>that&rsquo;s going to shift a lot, right?</p>
<p>The only reason it&rsquo;s so hard to make a backup of your brain</p>
<p>in its entirety is exactly</p>
<p>because it wasn&rsquo;t built for that, right?</p>
<p>If you have a future machine intelligence,</p>
<p>there&rsquo;s no reason for why it has to die at all.</p>
<p>If you want to copy it, whatever it is,</p>
<p>into some other machine intelligence,</p>
<p>whatever it is, into some other quark blob, right?</p>
<p>You can copy not just some of it, but all of it, right?</p>
<p>And so in that sense,</p>
<p>you can get immortality because all the information</p>
<p>can be copied out of any individual entity.</p>
<p>And it&rsquo;s not just mortality that will change</p>
<p>if we get to more post biological life.</p>
<p>It&rsquo;s also with that, very much the whole individualism</p>
<p>we have now, right?</p>
<p>The reason that we make such a big difference</p>
<p>between me and you is exactly because</p>
<p>we&rsquo;re a little bit limited in how much we can copy.</p>
<p>Like I would just love to go like this</p>
<p>and copy your Russian skills, Russian speaking skills.</p>
<p>Wouldn&rsquo;t it be awesome?</p>
<p>But I can&rsquo;t, I have to actually work for years</p>
<p>if I want to get better on it.</p>
<p>But if we were robots.</p>
<p>Just copy and paste freely, then that loses completely.</p>
<p>It washes away the sense of what immortality is.</p>
<p>And also individuality a little bit, right?</p>
<p>We would start feeling much more,</p>
<p>maybe we would feel much more collaborative with each other</p>
<p>if we can just, hey, you know, I&rsquo;ll give you my Russian,</p>
<p>you can give me your Russian</p>
<p>and I&rsquo;ll give you whatever,</p>
<p>and suddenly you can speak Swedish.</p>
<p>Maybe that&rsquo;s less a bad trade for you,</p>
<p>but whatever else you want from my brain, right?</p>
<p>And there&rsquo;ve been a lot of sci fi stories</p>
<p>about hive minds and so on,</p>
<p>where people, where experiences</p>
<p>can be more broadly shared.</p>
<p>And I think one, we don&rsquo;t,</p>
<p>I don&rsquo;t pretend to know what it would feel like</p>
<p>to be a super intelligent machine,</p>
<p>but I&rsquo;m quite confident that however it feels</p>
<p>about mortality and individuality</p>
<p>will be very, very different from how it is for us.</p>
<p>Well, for us, mortality and finiteness</p>
<p>seems to be pretty important at this particular moment.</p>
<p>And so all good things must come to an end.</p>
<p>Just like this conversation, Max.</p>
<p>I saw that coming.</p>
<p>Sorry, this is the world&rsquo;s worst translation.</p>
<p>I could talk to you forever.</p>
<p>It&rsquo;s such a huge honor that you&rsquo;ve spent time with me.</p>
<p>The honor is mine.</p>
<p>Thank you so much for getting me essentially</p>
<p>to start this podcast by doing the first conversation,</p>
<p>making me realize falling in love</p>
<p>with conversation in itself.</p>
<p>And thank you so much for inspiring</p>
<p>so many people in the world with your books,</p>
<p>with your research, with your talking,</p>
<p>and with the other, like this ripple effect of friends,</p>
<p>including Elon and everybody else that you inspire.</p>
<p>So thank you so much for talking today.</p>
<p>Thank you, I feel so fortunate</p>
<p>that you&rsquo;re doing this podcast</p>
<p>and getting so many interesting voices out there</p>
<p>into the ether and not just the five second sound bites,</p>
<p>but so many of the interviews I&rsquo;ve watched you do.</p>
<p>You really let people go in into depth</p>
<p>in a way which we sorely need in this day and age.</p>
<p>That I got to be number one, I feel super honored.</p>
<p>Yeah, you started it.</p>
<p>Thank you so much, Max.</p>
<p>Thanks for listening to this conversation</p>
<p>with Max Tegmark, and thank you to our sponsors,</p>
<p>the Jordan Harbinger Show, For Sigmatic Mushroom Coffee,</p>
<p>BetterHelp Online Therapy, and ExpressVPN.</p>
<p>So the choice is wisdom, caffeine, sanity, or privacy.</p>
<p>Choose wisely, my friends.</p>
<p>And if you wish, click the sponsor links below</p>
<p>to get a discount and to support this podcast.</p>
<p>And now let me leave you with some words from Max Tegmark.</p>
<p>If consciousness is the way that information feels</p>
<p>when it&rsquo;s processed in certain ways,</p>
<p>then it must be substrate independent.</p>
<p>It&rsquo;s only the structure of information processing</p>
<p>that matters, not the structure of the matter</p>
<p>doing the information processing.</p>
<p>Thank you for listening, and hope to see you next time.</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/english/">English</a>
        
            <a href="/tags/podcast/">Podcast</a>
        
            <a href="/tags/lex-fridman-podcast/">Lex Fridman Podcast</a>
        
    </section>


    </footer>


    
</article>

    

    

<div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064"
         crossorigin="anonymous"></script>
    <ins class="adsbygoogle"
         style="display:block; text-align:center;"
         data-ad-layout="in-article"
         data-ad-format="fluid"
         data-ad-client="ca-pub-9206135835124064"
         data-ad-slot="1055602464"></ins>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>
<aside class="related-content--wrapper">
    <h2 class="section-title">Related content</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/en/1310500372/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500372" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #368 - Eliezer Yudkowsky: Dangers of AI and the End of Human Civilization</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500371/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500371" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #367 - Sam Altman: OpenAI CEO on GPT-4, ChatGPT, and the Future of AI</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500370/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500370" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #366 - Shannon Curry: Johnny Depp &amp; Amber Heard Trial, Marriage, Dating &amp; Love</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500369/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500369" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #365 - Sam Harris: Trump, Pandemic, Twitter, Elon, Bret, IDW, Kanye, AI &amp; UFOs</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500368/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500368" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #364 - Chris Voss: FBI Hostage Negotiator</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2021 - 
        
        2023 SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics
    </section>
    
    <section class="powerby">
        

        As an Amazon Associate I earn from qualifying purchases üõí<br/>

        Built with <a href="https://swiest.com/" target="_blank" rel="noopener">(Ôæâ‚óï„ÉÆ‚óï)Ôæâü™Ñüíûüíñü•∞ across the glüåçüåèüåébe</a> <br />
        
        
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>


<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Serif&family=Noto+Serif+Armenian&family=Noto+Serif+Bengali&family=Noto+Serif+Devanagari&family=Noto+Serif+Georgian&family=Noto+Serif+Gujarati&family=Noto+Serif+HK&family=Noto+Serif+Hebrew&family=Noto+Serif+JP&family=Noto+Serif+KR&family=Noto+Serif+Kannada&family=Noto+Serif+Khmer&family=Noto+Serif+Lao&family=Noto+Serif+Makasar&family=Noto+Serif+Malayalam&family=Noto+Serif+Myanmar&family=Noto+Serif+Oriya&family=Noto+Serif+SC&family=Noto+Serif+Sinhala&family=Noto+Serif+TC&family=Noto+Serif+Tamil&family=Noto+Serif+Telugu&family=Noto+Serif+Thai&family=Noto+Serif+Tibetan&display=swap" rel="stylesheet">

    </body>
</html>

<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Video Transcript ﻿hi everyone I&amp;rsquo;m Alexander Botev I&amp;rsquo;m
currently a PhD student at University
College London and I&amp;rsquo;m doing her
internship in the robotics team for
investigating model-based reinforcement
learning for continuous control so I&amp;rsquo;m
gonna chime in some of the things that
Sally earlier mentioned on model-based
control so let&amp;rsquo;s dive in so this is the
outline of the talk and so let&amp;rsquo;s start
first I&amp;rsquo;m going to introduce why we care"><title>Investigating Model Based RL for Continuous Control ｜ Alex Botev ｜ 2018 Summer Intern Open House ｜ OpenAI | SWIEST</title>
<link rel=canonical href=https://swiest.com/en/1_syif82cty/><link rel=stylesheet href=/scss/style.min.9a6fe90535a0e5c60443841f100f7b698092d48dba43fdb6386bb69b6559bc3d.css><script>document.oncontextmenu=function(){return!1},document.onselectstart=function(){return!1},document.oncopy=function(){return!1},document.oncut=function(){return!1}</script><script src=https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js></script><script type=text/javascript>$(document).ready(function(){$("#back-to-top").hide(),$(function(){$(window).scroll(function(){$(window).scrollTop()>600?$("#back-to-top").fadeIn(500):$("#back-to-top").fadeOut(500)}),$("#back-to-top").click(function(){return $("body,html").animate({scrollTop:0},500),!1})})})</script><meta property="og:title" content="Investigating Model Based RL for Continuous Control ｜ Alex Botev ｜ 2018 Summer Intern Open House ｜ OpenAI"><meta property="og:description" content="Video Transcript ﻿hi everyone I&amp;rsquo;m Alexander Botev I&amp;rsquo;m
currently a PhD student at University
College London and I&amp;rsquo;m doing her
internship in the robotics team for
investigating model-based reinforcement
learning for continuous control so I&amp;rsquo;m
gonna chime in some of the things that
Sally earlier mentioned on model-based
control so let&amp;rsquo;s dive in so this is the
outline of the talk and so let&amp;rsquo;s start
first I&amp;rsquo;m going to introduce why we care"><meta property="og:url" content="https://swiest.com/en/1_syif82cty/"><meta property="og:site_name" content="SWIEST - Transcripts · Screenplays · Lyrics"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="English"><meta property="article:tag" content="Video Transcripts"><meta property="article:tag" content="OpenAI"><meta property="article:published_time" content="2023-11-06T04:15:27+00:00"><meta property="article:modified_time" content="2023-11-06T04:15:27+00:00"><meta name=twitter:title content="Investigating Model Based RL for Continuous Control ｜ Alex Botev ｜ 2018 Summer Intern Open House ｜ OpenAI"><meta name=twitter:description content="Video Transcript ﻿hi everyone I&amp;rsquo;m Alexander Botev I&amp;rsquo;m
currently a PhD student at University
College London and I&amp;rsquo;m doing her
internship in the robotics team for
investigating model-based reinforcement
learning for continuous control so I&amp;rsquo;m
gonna chime in some of the things that
Sally earlier mentioned on model-based
control so let&amp;rsquo;s dive in so this is the
outline of the talk and so let&amp;rsquo;s start
first I&amp;rsquo;m going to introduce why we care"><link rel="shortcut icon" href=/favicon.ico><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"dark")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu307e6a33fa6fd661ccda3b77024ef5c2_252345_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>SWIEST - Transcripts · Screenplays · Lyrics</a></h1><h2 class=site-description>🧙🪄🌎</h2></div></header><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><li><a href=/tags/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg><span>Tags</span></a></li><li><a href=/chart/podcastchart.html target=_blank><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-apple-podcast" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M18.364 18.364a9 9 0 10-12.728.0"/><path d="M11.766 22h.468a2 2 0 001.985-1.752l.5-4A2 2 0 0012.734 14h-1.468a2 2 0 00-1.985 2.248l.5 4A2 2 0 0011.766 22z"/><path d="M12 9m-2 0a2 2 0 104 0 2 2 0 10-4 0"/></svg><span>Podcasts</span></a></li><li><a href=/radio.html target=_blank><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-radio" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14 3 4.629 6.749A1 1 0 004 7.677V19a1 1 0 001 1h14a1 1 0 001-1V8a1 1 0 00-1-1H4.5"/><path d="M4 12h16"/><path d="M7 12v-2"/><path d="M17 16v.01"/><path d="M13 16v.01"/></svg><span>Radio</span></a></li><div class=menu-bottom-section><li id=i18n-switch><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg><select name=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://swiest.com/ selected>English</option><option value=https://swiest.com/af/>Afrikaans</option><option value=https://swiest.com/am/>አማርኛ</option><option value=https://swiest.com/ar/>العربية</option><option value=https://swiest.com/az/>Azərbaycan</option><option value=https://swiest.com/be/>беларускі</option><option value=https://swiest.com/bg/>български</option><option value=https://swiest.com/bn/>বাংলা</option><option value=https://swiest.com/bo/>བོད་སྐད་</option><option value=https://swiest.com/bs/>Bosanski</option><option value=https://swiest.com/ca/>Català</option><option value=https://swiest.com/zh-hans/>简体中文</option><option value=https://swiest.com/zh-hant/>繁體中文</option><option value=https://swiest.com/cs/>Čeština</option><option value=https://swiest.com/el/>ελληνικά</option><option value=https://swiest.com/cy/>Cymraeg</option><option value=https://swiest.com/da/>Dansk</option><option value=https://swiest.com/de/>Deutsch</option><option value=https://swiest.com/eo/>Esperanto</option><option value=https://swiest.com/es-es/>Español (España)</option><option value=https://swiest.com/es-419/>Español (Latinoamérica)</option><option value=https://swiest.com/et/>Eesti</option><option value=https://swiest.com/eu/>Euskara</option><option value=https://swiest.com/haw/>ʻŌlelo Hawaiʻi</option><option value=https://swiest.com/fa/>فارسی</option><option value=https://swiest.com/fi/>Suomi</option><option value=https://swiest.com/fo/>Føroyskt</option><option value=https://swiest.com/fr/>Français</option><option value=https://swiest.com/fy/>Frysk</option><option value=https://swiest.com/ga/>Gaeilge</option><option value=https://swiest.com/gl/>Galego</option><option value=https://swiest.com/gu/>ગુજરાતી</option><option value=https://swiest.com/he/>עִברִית</option><option value=https://swiest.com/km/>កម្ពុជា។</option><option value=https://swiest.com/hi/>हिन्दी</option><option value=https://swiest.com/hr/>Hrvatski</option><option value=https://swiest.com/ht/>Kreyòl Ayisyen</option><option value=https://swiest.com/hu/>Magyar</option><option value=https://swiest.com/hy/>Հայերեն</option><option value=https://swiest.com/ig/>Ásụ̀sụ́ Ìgbò</option><option value=https://swiest.com/id/>Bahasa Indonesia</option><option value=https://swiest.com/is/>Íslenska</option><option value=https://swiest.com/it/>Italiano</option><option value=https://swiest.com/ja/>日本語</option><option value=https://swiest.com/jv/>Basa Jawa</option><option value=https://swiest.com/ka/>ქართული</option><option value=https://swiest.com/kk/>Қазақша</option><option value=https://swiest.com/kn/>ಕನ್ನಡ</option><option value=https://swiest.com/ko/>한국어</option><option value=https://swiest.com/or/>ଓଡ଼ିଆ</option><option value=https://swiest.com/ckb/>کوردی</option><option value=https://swiest.com/ky/>Кыргызча</option><option value=https://swiest.com/la/>Latina</option><option value=https://swiest.com/lb/>Lëtzebuergesch</option><option value=https://swiest.com/lo/>ພາສາລາວ</option><option value=https://swiest.com/lt/>Lietuvių</option><option value=https://swiest.com/lv/>Latviešu</option><option value=https://swiest.com/mk/>Македонски</option><option value=https://swiest.com/ml/>മലയാളം</option><option value=https://swiest.com/mn/>Монгол хэл</option><option value=https://swiest.com/mr/>मराठी</option><option value=https://swiest.com/sw/>Kiswahili</option><option value=https://swiest.com/ms/>Bahasa Melayu</option><option value=https://swiest.com/my/>မြန်မာ</option><option value=https://swiest.com/ne/>नेपाली</option><option value=https://swiest.com/nl/>Nederlands</option><option value=https://swiest.com/no/>Norsk</option><option value=https://swiest.com/pa/>ਪੰਜਾਬੀ</option><option value=https://swiest.com/pl/>Polski</option><option value=https://swiest.com/pt-br/>Português Brasil</option><option value=https://swiest.com/pt-pt/>Português Europeu</option><option value=https://swiest.com/ro/>Română</option><option value=https://swiest.com/ru/>Русский</option><option value=https://swiest.com/rw/>Kinyarwanda</option><option value=https://swiest.com/si/>සිංහල</option><option value=https://swiest.com/sk/>Slovenčina</option><option value=https://swiest.com/sl/>Slovenščina</option><option value=https://swiest.com/sq/>Shqip</option><option value=https://swiest.com/sr/>Српски (Srpski)</option><option value=https://swiest.com/su/>Basa Sunda</option><option value=https://swiest.com/sv/>Svenska</option><option value=https://swiest.com/ta/>தமிழ்</option><option value=https://swiest.com/te/>తెలుగు</option><option value=https://swiest.com/tg/>Тоҷикӣ</option><option value=https://swiest.com/th/>ไทย</option><option value=https://swiest.com/tk/>Türkmenler</option><option value=https://swiest.com/tl/>Filipino</option><option value=https://swiest.com/tr/>Türkçe</option><option value=https://swiest.com/uk/>Українська</option><option value=https://swiest.com/ur/>اردو</option><option value=https://swiest.com/uz/>O'zbekcha</option><option value=https://swiest.com/vi/>Tiếng Việt</option><option value=https://swiest.com/yi/>אידיש</option><option value=https://swiest.com/zh-hk/>粵語</option><option value=https://swiest.com/zu/>IsiZulu</option></select></li><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#video>Video</a></li><li><a href=#transcript>Transcript</a></li></ol></nav></div></section><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><ins class=adsbygoogle style=display:block data-ad-client=ca-pub-9206135835124064 data-ad-slot=8754979142 data-ad-format=auto data-full-width-responsive=true></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></aside><a id=back-to-top href=#><img src=/img/top_hu7c2829da96df0e9f8f0191d120020b22_22287_40x0_resize_box_3.png></a><main class="main full-width"><form action=/search/ class="search-form widget"><p><label>Search</label>
<input name=keyword required placeholder="Type something...">
<button title=Search><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg></button></p></form><article class=main-article><header class=article-header><div class=article-details><header class=article-tags><a href=/tags/english/>English
</a><a href=/tags/video-transcripts/>Video Transcripts
</a><a href=/tags/openai/>OpenAI</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/en/1_syif82cty/>Investigating Model Based RL for Continuous Control ｜ Alex Botev ｜ 2018 Summer Intern Open House ｜ OpenAI</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>2023-11-06</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>11 minute read</time></div></footer></div></header><div class=article-content><p style=text-align:center><a href=https://amzn.to/3Nrdcwk target=_blank>🎁Amazon Prime</a>
<a href=https://amzn.to/3RIBkxg target=_blank>📖Kindle Unlimited</a>
<a href=https://amzn.to/3Rqmudl target=_blank>🎧Audible Plus</a>
<a href=https://amzn.to/3TuLbbj target=_blank>🎵Amazon Music Unlimited</a>
<a href="https://www.iherb.com/?rcode=EID1574" target=_blank>🌿iHerb</a>
<a href="https://accounts.binance.com/register?ref=72302422" target=_blank>💰Binance</a></p></div><div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><ins class=adsbygoogle style=display:block data-ad-client=ca-pub-9206135835124064 data-ad-slot=8754979142 data-ad-format=auto data-full-width-responsive=true></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><section class=article-content><h2 id=video>Video</h2><div class=video-wrapper><iframe loading=lazy src=https://www.youtube.com/embed/1_sYif82CtY allowfullscreen title="YouTube Video"></iframe></div><h2 id=transcript>Transcript</h2><p>﻿hi everyone I&rsquo;m Alexander Botev I&rsquo;m</p><p>currently a PhD student at University</p><p>College London and I&rsquo;m doing her</p><p>internship in the robotics team for</p><p>investigating model-based reinforcement</p><p>learning for continuous control so I&rsquo;m</p><p>gonna chime in some of the things that</p><p>Sally earlier mentioned on model-based</p><p>control so let&rsquo;s dive in so this is the</p><p>outline of the talk and so let&rsquo;s start</p><p>first I&rsquo;m going to introduce why we care</p><p>about model-based around so I have a</p><p>small intro I guess most of you are</p><p>going to be familiar but in a formal</p><p>learning we have an agent that interacts</p><p>with an environment which is different</p><p>than standard supervised or unsupervised</p><p>learning settings and the way that the</p><p>agent interacts is true actions and the</p><p>environments gives you back so</p><p>observations in a reward signal the</p><p>agents try to maximize the cumulative</p><p>sum of the rewards over the episodes</p><p>that he experienced now the main</p><p>difference between a model free</p><p>algorithm and a model based algorithm is</p><p>that model free algorithms try to learn</p><p>a policy PI and most often either a</p><p>value function or an action state value</p><p>function which try to estimate how good</p><p>a particular state in the environment</p><p>are for the agent to be in and this is</p><p>done solidly true using the reward</p><p>signal and experience with the</p><p>environment well in model-based RL we</p><p>try to learn additionally an internal</p><p>dynamics model which essentially two</p><p>models how does the environment evolve</p><p>and each tries to match what you</p><p>experience to the environment and in</p><p>general we assume that we don&rsquo;t have</p><p>access to the environment of how it</p><p>works except through sampling so what</p><p>are the potential benefits of actually</p><p>using an internal model at all so to</p><p>some extent the kind of grail that you</p><p>would have if you have a perfect and</p><p>ideal model is that we can solve any</p><p>task without ever interacting with the</p><p>environment so if you have a perfect</p><p>model you can simulate it as much as you</p><p>want without interrupting with the treu</p><p>environment and you can solve any task</p><p>maybe that&rsquo;s solving the task</p><p>might be hard but that&rsquo;s kind of a</p><p>concern that we won&rsquo;t consider too much</p><p>so however in practice we have to learn</p><p>this model so one of the benefits is</p><p>first your task independent so all of</p><p>the policies and value functions that a</p><p>model three algorithm estimates are</p><p>based on specific rewards so if you</p><p>slightly change your task it means that</p><p>usually you have to retrain from scratch</p><p>if you have a model that&rsquo;s very</p><p>accurately learn on one task and you</p><p>change the reward you can retrain your</p><p>model free algorithm or do any kind of</p><p>other search based on top of it without</p><p>interacting with the environment it&rsquo;s</p><p>usually trains in a supervised way</p><p>sometimes in unsupervised but the</p><p>benefit of this is that it&rsquo;s much easier</p><p>to train much more stable as these</p><p>techniques don&rsquo;t rely on things like</p><p>bootstrapping which make training more</p><p>unstable importantly we can do we can do</p><p>planning with much more sophisticated</p><p>algorithms so for instance you can do</p><p>trajectory optimization in robotics if</p><p>you have a good model you can do tree</p><p>search like in going chess and so one of</p><p>the very hardly often mentioned argument</p><p>is that you use better you better use</p><p>your data essentially rather than just</p><p>using a single scale or rewards signal</p><p>by trying to capture all variety of the</p><p>environment you are somehow learning</p><p>faster by using more information from</p><p>the environment so a wise model based</p><p>learning card however so currently most</p><p>model free algorithm are actually more</p><p>sample efficient and model based</p><p>algorithms and have better asymptotic</p><p>behavior so here I&rsquo;m going to show you</p><p>two environment basic it&rsquo;s the same</p><p>environment so it&rsquo;s a very simple you</p><p>have a ball that you control with torque</p><p>we thread you&rsquo;re gonna see how does the</p><p>real environment look like and with</p><p>green it&rsquo;s the prediction of a model so</p><p>on the left hand side we&rsquo;re gonna</p><p>measure just one step prediction so</p><p>every state the model is given what&rsquo;s in</p><p>the environment predicts the next state</p><p>and then project that</p><p>oops can we play the left one so as we</p><p>can see one step prediction model is</p><p>almost indistinguishable from the real</p><p>environment the error is literally very</p><p>tiny so things seems to work now okay so</p><p>maybe then we&rsquo;ve solved model based RL</p><p>but if we try now to start from a state</p><p>and unroll the environment in the model</p><p>totally independently of each other then</p><p>we get this kind of behavior and like</p><p>you can see how far off the model is</p><p>going and essentially this is exactly</p><p>the same model and then if you try to</p><p>train on this kind of model by just</p><p>unrolling your model then essentially</p><p>the policies that you get are very</p><p>suboptimal and don&rsquo;t sometimes even work</p><p>at all in the real environment so some</p><p>of the difficulties of training a</p><p>dynamics model so first not all aspects</p><p>of the environment might be relevant for</p><p>any task that you care about so for</p><p>instance if you&rsquo;re a house robot maybe</p><p>what&rsquo;s playing on the TV will never be</p><p>helpful but that might be very</p><p>complicated to model so you might be</p><p>wasting a lot of capacity of your model</p><p>on that kind of details essentially what</p><p>I showed earlier is probably one of the</p><p>biggest issue is that compounding errors</p><p>lead to very bad predictions in long</p><p>horizons essentially one of the main</p><p>issues currently with model-based RL is</p><p>that if you try to unroll the models for</p><p>longer horizons they start start to</p><p>become so far away from the truth that</p><p>training on them as long as it&rsquo;s</p><p>important for planning that you have</p><p>long horizon goals doesn&rsquo;t work very</p><p>hard to estimate uncertainty for</p><p>flexible models so this is in general</p><p>problem with neural networks in machine</p><p>learning and usually we want to use</p><p>neural networks as the dynamics are</p><p>actually very complicated and finally so</p><p>in terms of the sample efficiency so</p><p>ultimata based RL might sound like it</p><p>should be more sample efficient</p><p>ever in practice to learn a very</p><p>accurate model which you can use to do</p><p>anything useful your model will require</p><p>probably a lots of data so to some</p><p>extent here there&rsquo;s the trade of that if</p><p>to get an accurate dynamics model you</p><p>require more data then your model free</p><p>algorithm requires to learn a policy</p><p>then essentially a model based RL</p><p>approach will never be more sample</p><p>efficient than a model free and there&rsquo;s</p><p>sort of kind of gray area of whether</p><p>it&rsquo;s harder to learn the dynamics or</p><p>it&rsquo;s very easy to learn the policy and</p><p>then you don&rsquo;t need a model so what I&rsquo;m</p><p>investigating during my internship and</p><p>main area of research was to investigate</p><p>an idea called valve expansion and</p><p>that&rsquo;s mainly for actor critic</p><p>architectures so one of the main ideas</p><p>of AL expansion is to kind of try to use</p><p>the model in order to improve a model</p><p>free algorithm in terms of its sample</p><p>efficiency instability so in standard</p><p>actor critic we kind of have a action</p><p>and state coming from the environment</p><p>and then a next state sample from the</p><p>environment and we try to regress our</p><p>action value function to the target</p><p>which is usually bootstrapped</p><p>so what value expansion does is after we</p><p>learn a model denoted in here is Chi</p><p>essentially for every offline data that</p><p>we have collected we can use the model</p><p>to unroll it multiple steps and this way</p><p>we can get on policy targets so we don&rsquo;t</p><p>need any Corrections for instance that</p><p>you&rsquo;d usually need with important</p><p>sampling and we can get multiple step</p><p>horizon targets using unrolling the</p><p>model now this of course realized that</p><p>the model stew is reasonably accurate to</p><p>work but essentially allows us to get a</p><p>much more stable or targets and much</p><p>better potentially by having multiple of</p><p>them and now I&rsquo;m gonna show you some</p><p>results and some conclusions from my</p><p>experience in trying all this in</p><p>robotics so the first environment</p><p>that evaluated on was the standard fetch</p><p>tasks so these are essentially</p><p>environments in a simulator where you</p><p>have a robot with seven degrees of</p><p>freedom and it has a gripper so one of</p><p>the environment is just moving the arm</p><p>to a specific location the pick and</p><p>places you have to place a block to a</p><p>specific location the push environment</p><p>you&rsquo;re pushing a block on a table to</p><p>application the changes on every episode</p><p>and the slide which is usually quite</p><p>difficult if you have a puck that&rsquo;s on a</p><p>sliding table and you have to just push</p><p>it around until it gets to the right</p><p>place but there you must be careful that</p><p>you don&rsquo;t overshoot so here I&rsquo;m just</p><p>putting two plots of some of the work</p><p>that I&rsquo;ve done there&rsquo;s a lot more but I</p><p>don&rsquo;t want to bore you too much so</p><p>essentially the black lines are a</p><p>baseline deterministic policy gradient</p><p>algorithm with hints on experience Lee</p><p>replay and double key learning and it</p><p>specifically optimized to be a sample</p><p>efficient as possible so there was a big</p><p>hyper parameter search that I did and</p><p>essentially the red and the blue curves</p><p>are model-based approaches with value</p><p>expansion and as we can see they</p><p>outperform the baseline in some cases</p><p>that&rsquo;s more significant than the others</p><p>but by choosing so they&rsquo;re a bit fragile</p><p>to some of the hyper parameters but</p><p>essentially all of the final results</p><p>were achieved with the same hyper</p><p>parameters across the task so this</p><p>showed that this algorithm actually</p><p>could help and improve at least so</p><p>sometimes it&rsquo;s up to five times better</p><p>sample efficiency so some take away from</p><p>my experiments which i think are quite</p><p>important so using an samples for the</p><p>dynamics model was always necessary a</p><p>single model never worked or was never</p><p>able to beat the baseline and I think</p><p>this is reassuring team in the community</p><p>recently so training dynamics models</p><p>usually training them on multiple step</p><p>losses essentially trying to make them</p><p>more consistent as you feed their own</p><p>predictions into them also was necessary</p><p>for improving the baseline otherwise</p><p>usually the models converged the same</p><p>value but you kind of don&rsquo;t get an</p><p>improving in sample efficiency also for</p><p>the valley expansion always if you</p><p>expand for more than one or two steps</p><p>was needed in order to get an actual</p><p>benefit of the method and one thing</p><p>which was quite interesting is that</p><p>being pessimistic seems to help so</p><p>essentially when you get the multiple</p><p>horizon targets rather than taking an</p><p>average or exponential average like TG</p><p>lambda taking a minimum over the</p><p>different horizons seems to do better in</p><p>the harder environments and this is kind</p><p>of similar to the double Q learning</p><p>we&rsquo;re essentially trying off to kill</p><p>some of the overestimation bias of your</p><p>q function and I&rsquo;m gonna skip this cuz I</p><p>don&rsquo;t have time so thank you very much</p><p>for your time</p><p>[Applause]</p><p>[Music]</p><p>so the video that I shot was with a</p><p>model the trains on the dynamics and in</p><p>both cases it&rsquo;s like it&rsquo;s trained on the</p><p>same environment the difference is of</p><p>how you generate the video so in one you</p><p>start with a state of the environment</p><p>you enroll it once and then you predict</p><p>the next state and visualize that</p><p>however after that you feed whatever was</p><p>in the real environment to the model in</p><p>the second video that I showed where you</p><p>get this huge divergence essentially you</p><p>start from a state and then you unroll</p><p>the two things total independently so</p><p>then they are the model at every step</p><p>takes its own predictions from the</p><p>previous time step and never kind of</p><p>gets grounded again to the true</p><p>environment does that make sense okay</p><p>yeah like I can explain that later</p><p>[Laughter]</p><p>so do you mean whoever this during</p><p>training or during test time so yeah so</p><p>one of the points for the multiple step</p><p>loss was essentially that however so you</p><p>can do a multiple step loss where you</p><p>feed the prediction of the model into</p><p>itself multiple times and then you</p><p>ground it to the real what you saw in</p><p>the real world you can do that however</p><p>only with deterministic models and in my</p><p>experience actually with that kind of</p><p>loss for longer horizons I usually</p><p>needed to use maybe five to eight steps</p><p>horizon losses the deterministic models</p><p>did much better than stochastic ones</p><p>which however dose you can train only</p><p>with a single step</p><p>so um basically it was 5050 pretty much</p><p>on two of the environments the awesome</p><p>actually on one of the environment the</p><p>septic behavior was even slightly better</p><p>than the baseline which probably</p><p>suggests that the model has learned very</p><p>accurately the environment and you&rsquo;re</p><p>gaining something more by having</p><p>multiple horizon targets in the other</p><p>environments in two of them they kind of</p><p>converge to the same thing in one of</p><p>them the model-based approach actually</p><p>is a bit worse in asymptotic behavior so</p><p>in general you can always do the valve</p><p>expansion and also interpolate between</p><p>the valve expansion target and the</p><p>real-world target which is only one step</p><p>and that for instance can elevate that</p><p>problem okay thanks for the questions</p><p>[Applause]</p></section><footer class=article-footer><section class=article-tags><a href=/tags/english/>English</a>
<a href=/tags/video-transcripts/>Video Transcripts</a>
<a href=/tags/openai/>OpenAI</a></section></footer></article><div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><ins class=adsbygoogle style=display:block;text-align:center data-ad-layout=in-article data-ad-format=fluid data-ad-client=ca-pub-9206135835124064 data-ad-slot=1055602464></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/en/at2xkqjazns/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/AT2XkqJAZns data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>Towards Epileptic Seizure Prediction with Deep Network ｜ Kata Slama ｜ OpenAI Scholars Demo Day 2020 ｜ OpenAI</h2></div></a></article><article><a href=/en/jzohw-eybtq/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/JZOHW-eYBtQ data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>Introductions by Sam Altman & Greg Brockman ｜ OpenAI Scholars Demo Day 2020 ｜ OpenAI</h2></div></a></article><article><a href=/en/-fozam9xqs4/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/-FoZAM9xqS4 data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>OpenAI Five vs. OG, Game 2 ｜ OpenAI Five Finals (4⧸6) ｜ OpenAI</h2></div></a></article><article><a href=/en/u9mjuukhuzk/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/U9mJuUkhUzk data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>OpenAI DevDay, Opening Keynote ｜ OpenAI</h2></div></a></article><article><a href=/en/lpe5gwuqa-k/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/lpe5Gwuqa-k data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>Scaling Laws for Language Transfer Learning ｜ Christina Kim ｜ OpenAI Scholars Demo Day 2021 ｜ OpenAI</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2021 -
2023 SWIEST - Transcripts · Screenplays · Lyrics</section><section class=powerby>As an Amazon Associate I earn from qualifying purchases 🛒<br>Built with <a href=https://swiest.com/ target=_blank rel=noopener>(ﾉ◕ヮ◕)ﾉ🪄💞💖🥰 across the gl🌍🌏🌎be</a><br></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Serif&family=Noto+Serif+Armenian&family=Noto+Serif+Bengali&family=Noto+Serif+Devanagari&family=Noto+Serif+Georgian&family=Noto+Serif+Gujarati&family=Noto+Serif+HK&family=Noto+Serif+Hebrew&family=Noto+Serif+JP&family=Noto+Serif+KR&family=Noto+Serif+Kannada&family=Noto+Serif+Khmer&family=Noto+Serif+Lao&family=Noto+Serif+Makasar&family=Noto+Serif+Malayalam&family=Noto+Serif+Myanmar&family=Noto+Serif+Oriya&family=Noto+Serif+SC&family=Noto+Serif+Sinhala&family=Noto+Serif+TC&family=Noto+Serif+Tamil&family=Noto+Serif+Telugu&family=Noto+Serif+Thai&family=Noto+Serif+Tibetan&display=swap" rel=stylesheet></body></html>
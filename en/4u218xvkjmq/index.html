<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Video Transcript Ôªøyou
hello everybody we&amp;rsquo;re gonna go ahead and
get started
fantastic welcome to open AI scholars
demo day thank you all for being here
this evening we have eight presentations
tonight as a result of our Scholars
program we had 550 people apply and
eight scholars so that makes us a bit
more competitive than Harvard but you
know we don&amp;rsquo;t want it we don&amp;rsquo;t want to
break any who and the scholars have"><title>OpenAI Scholars Demo Day 2019 ÔΩú OpenAI | SWIEST</title>
<link rel=canonical href=https://swiest.com/en/4u218xvkjmq/><link rel=stylesheet href=/scss/style.min.9a6fe90535a0e5c60443841f100f7b698092d48dba43fdb6386bb69b6559bc3d.css><script>document.oncontextmenu=function(){return!1},document.onselectstart=function(){return!1},document.oncopy=function(){return!1},document.oncut=function(){return!1}</script><script src=https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js></script><script type=text/javascript>$(document).ready(function(){$("#back-to-top").hide(),$(function(){$(window).scroll(function(){$(window).scrollTop()>600?$("#back-to-top").fadeIn(500):$("#back-to-top").fadeOut(500)}),$("#back-to-top").click(function(){return $("body,html").animate({scrollTop:0},500),!1})})})</script><meta property="og:title" content="OpenAI Scholars Demo Day 2019 ÔΩú OpenAI"><meta property="og:description" content="Video Transcript Ôªøyou
hello everybody we&amp;rsquo;re gonna go ahead and
get started
fantastic welcome to open AI scholars
demo day thank you all for being here
this evening we have eight presentations
tonight as a result of our Scholars
program we had 550 people apply and
eight scholars so that makes us a bit
more competitive than Harvard but you
know we don&amp;rsquo;t want it we don&amp;rsquo;t want to
break any who and the scholars have"><meta property="og:url" content="https://swiest.com/en/4u218xvkjmq/"><meta property="og:site_name" content="SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="English"><meta property="article:tag" content="Video Transcripts"><meta property="article:tag" content="OpenAI"><meta property="article:published_time" content="2023-11-06T14:35:28+00:00"><meta property="article:modified_time" content="2023-11-06T14:35:28+00:00"><meta name=twitter:title content="OpenAI Scholars Demo Day 2019 ÔΩú OpenAI"><meta name=twitter:description content="Video Transcript Ôªøyou
hello everybody we&amp;rsquo;re gonna go ahead and
get started
fantastic welcome to open AI scholars
demo day thank you all for being here
this evening we have eight presentations
tonight as a result of our Scholars
program we had 550 people apply and
eight scholars so that makes us a bit
more competitive than Harvard but you
know we don&amp;rsquo;t want it we don&amp;rsquo;t want to
break any who and the scholars have"><link rel="shortcut icon" href=/favicon.ico><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"dark")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu307e6a33fa6fd661ccda3b77024ef5c2_252345_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics</a></h1><h2 class=site-description>üßôü™Ñüåé</h2></div></header><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><li><a href=/tags/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg><span>Tags</span></a></li><li><a href=/chart/podcastchart.html target=_blank><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-apple-podcast" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M18.364 18.364a9 9 0 10-12.728.0"/><path d="M11.766 22h.468a2 2 0 001.985-1.752l.5-4A2 2 0 0012.734 14h-1.468a2 2 0 00-1.985 2.248l.5 4A2 2 0 0011.766 22z"/><path d="M12 9m-2 0a2 2 0 104 0 2 2 0 10-4 0"/></svg><span>Podcasts</span></a></li><li><a href=/radio.html target=_blank><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-radio" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14 3 4.629 6.749A1 1 0 004 7.677V19a1 1 0 001 1h14a1 1 0 001-1V8a1 1 0 00-1-1H4.5"/><path d="M4 12h16"/><path d="M7 12v-2"/><path d="M17 16v.01"/><path d="M13 16v.01"/></svg><span>Radio</span></a></li><div class=menu-bottom-section><li id=i18n-switch><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg><select name=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://swiest.com/ selected>English</option><option value=https://swiest.com/af/>Afrikaans</option><option value=https://swiest.com/am/>·ä†·àõ·à≠·äõ</option><option value=https://swiest.com/ar/>ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</option><option value=https://swiest.com/az/>Az…ôrbaycan</option><option value=https://swiest.com/be/>–±–µ–ª–∞—Ä—É—Å–∫—ñ</option><option value=https://swiest.com/bg/>–±—ä–ª–≥–∞—Ä—Å–∫–∏</option><option value=https://swiest.com/bn/>‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ</option><option value=https://swiest.com/bo/>‡Ωñ‡Ωº‡Ωë‡ºã‡Ω¶‡æê‡Ωë‡ºã</option><option value=https://swiest.com/bs/>Bosanski</option><option value=https://swiest.com/ca/>Catal√†</option><option value=https://swiest.com/zh-hans/>ÁÆÄ‰Ωì‰∏≠Êñá</option><option value=https://swiest.com/zh-hant/>ÁπÅÈ´î‰∏≠Êñá</option><option value=https://swiest.com/cs/>ƒåe≈°tina</option><option value=https://swiest.com/el/>ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨</option><option value=https://swiest.com/cy/>Cymraeg</option><option value=https://swiest.com/da/>Dansk</option><option value=https://swiest.com/de/>Deutsch</option><option value=https://swiest.com/eo/>Esperanto</option><option value=https://swiest.com/es-es/>Espa√±ol (Espa√±a)</option><option value=https://swiest.com/es-419/>Espa√±ol (Latinoam√©rica)</option><option value=https://swiest.com/et/>Eesti</option><option value=https://swiest.com/eu/>Euskara</option><option value=https://swiest.com/haw/> ª≈ålelo Hawai ªi</option><option value=https://swiest.com/fa/>ŸÅÿßÿ±ÿ≥€å</option><option value=https://swiest.com/fi/>Suomi</option><option value=https://swiest.com/fo/>F√∏royskt</option><option value=https://swiest.com/fr/>Fran√ßais</option><option value=https://swiest.com/fy/>Frysk</option><option value=https://swiest.com/ga/>Gaeilge</option><option value=https://swiest.com/gl/>Galego</option><option value=https://swiest.com/gu/>‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä</option><option value=https://swiest.com/he/>◊¢÷¥◊ë◊®÷¥◊ô◊™</option><option value=https://swiest.com/km/>·ûÄ·ûò·üí·ûñ·ûª·ûá·û∂·üî</option><option value=https://swiest.com/hi/>‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</option><option value=https://swiest.com/hr/>Hrvatski</option><option value=https://swiest.com/ht/>Krey√≤l Ayisyen</option><option value=https://swiest.com/hu/>Magyar</option><option value=https://swiest.com/hy/>’Ä’°’µ’•÷Ä’•’∂</option><option value=https://swiest.com/ig/>√Ås·ª•ÃÄs·ª•ÃÅ √ågb√≤</option><option value=https://swiest.com/id/>Bahasa Indonesia</option><option value=https://swiest.com/is/>√çslenska</option><option value=https://swiest.com/it/>Italiano</option><option value=https://swiest.com/ja/>Êó•Êú¨Ë™û</option><option value=https://swiest.com/jv/>Basa Jawa</option><option value=https://swiest.com/ka/>·É•·Éê·É†·Éó·É£·Éö·Éò</option><option value=https://swiest.com/kk/>“ö–∞–∑–∞“õ—à–∞</option><option value=https://swiest.com/kn/>‡≤ï‡≤®‡≥ç‡≤®‡≤°</option><option value=https://swiest.com/ko/>ÌïúÍµ≠Ïñ¥</option><option value=https://swiest.com/or/>‡¨ì‡¨°‡¨º‡¨ø‡¨Ü</option><option value=https://swiest.com/ckb/>⁄©Ÿàÿ±ÿØ€å</option><option value=https://swiest.com/ky/>–ö—ã—Ä–≥—ã–∑—á–∞</option><option value=https://swiest.com/la/>Latina</option><option value=https://swiest.com/lb/>L√´tzebuergesch</option><option value=https://swiest.com/lo/>‡∫û‡∫≤‡∫™‡∫≤‡∫•‡∫≤‡∫ß</option><option value=https://swiest.com/lt/>Lietuvi≈≥</option><option value=https://swiest.com/lv/>Latvie≈°u</option><option value=https://swiest.com/mk/>–ú–∞–∫–µ–¥–æ–Ω—Å–∫–∏</option><option value=https://swiest.com/ml/>‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç</option><option value=https://swiest.com/mn/>–ú–æ–Ω–≥–æ–ª —Ö—ç–ª</option><option value=https://swiest.com/mr/>‡§Æ‡§∞‡§æ‡§†‡•Ä</option><option value=https://swiest.com/sw/>Kiswahili</option><option value=https://swiest.com/ms/>Bahasa Melayu</option><option value=https://swiest.com/my/>·Äô·Äº·Äî·Ä∫·Äô·Ä¨</option><option value=https://swiest.com/ne/>‡§®‡•á‡§™‡§æ‡§≤‡•Ä</option><option value=https://swiest.com/nl/>Nederlands</option><option value=https://swiest.com/no/>Norsk</option><option value=https://swiest.com/pa/>‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä</option><option value=https://swiest.com/pl/>Polski</option><option value=https://swiest.com/pt-br/>Portugu√™s Brasil</option><option value=https://swiest.com/pt-pt/>Portugu√™s Europeu</option><option value=https://swiest.com/ro/>Rom√¢nƒÉ</option><option value=https://swiest.com/ru/>–†—É—Å—Å–∫–∏–π</option><option value=https://swiest.com/rw/>Kinyarwanda</option><option value=https://swiest.com/si/>‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω</option><option value=https://swiest.com/sk/>Slovenƒçina</option><option value=https://swiest.com/sl/>Sloven≈°ƒçina</option><option value=https://swiest.com/sq/>Shqip</option><option value=https://swiest.com/sr/>–°—Ä–ø—Å–∫–∏ (Srpski)</option><option value=https://swiest.com/su/>Basa Sunda</option><option value=https://swiest.com/sv/>Svenska</option><option value=https://swiest.com/ta/>‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç</option><option value=https://swiest.com/te/>‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</option><option value=https://swiest.com/tg/>–¢–æ“∑–∏–∫”£</option><option value=https://swiest.com/th/>‡πÑ‡∏ó‡∏¢</option><option value=https://swiest.com/tk/>T√ºrkmenler</option><option value=https://swiest.com/tl/>Filipino</option><option value=https://swiest.com/tr/>T√ºrk√ße</option><option value=https://swiest.com/uk/>–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</option><option value=https://swiest.com/ur/>ÿßÿ±ÿØŸà</option><option value=https://swiest.com/uz/>O'zbekcha</option><option value=https://swiest.com/vi/>Ti·∫øng Vi·ªát</option><option value=https://swiest.com/yi/>◊ê◊ô◊ì◊ô◊©</option><option value=https://swiest.com/zh-hk/>Á≤µË™û</option><option value=https://swiest.com/zu/>IsiZulu</option></select></li><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#video>Video</a></li><li><a href=#transcript>Transcript</a></li></ol></nav></div></section><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><ins class=adsbygoogle style=display:block data-ad-client=ca-pub-9206135835124064 data-ad-slot=8754979142 data-ad-format=auto data-full-width-responsive=true></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></aside><a id=back-to-top href=#><img src=/img/top_hu7c2829da96df0e9f8f0191d120020b22_22287_40x0_resize_box_3.png></a><main class="main full-width"><form action=/search/ class="search-form widget"><p><label>Search</label>
<input name=keyword required placeholder="Type something...">
<button title=Search><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg></button></p></form><article class=main-article><header class=article-header><div class=article-details><header class=article-tags><a href=/tags/english/>English
</a><a href=/tags/video-transcripts/>Video Transcripts
</a><a href=/tags/openai/>OpenAI</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/en/4u218xvkjmq/>OpenAI Scholars Demo Day 2019 ÔΩú OpenAI</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>2023-11-06</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>75 minute read</time></div></footer></div></header><div class=article-content><p style=text-align:center><a href=https://amzn.to/3Nrdcwk target=_blank>üéÅAmazon Prime</a>
<a href=https://amzn.to/3RIBkxg target=_blank>üìñKindle Unlimited</a>
<a href=https://amzn.to/3Rqmudl target=_blank>üéßAudible Plus</a>
<a href=https://amzn.to/3TuLbbj target=_blank>üéµAmazon Music Unlimited</a>
<a href="https://www.iherb.com/?rcode=EID1574" target=_blank>üåøiHerb</a>
<a href="https://accounts.binance.com/register?ref=72302422" target=_blank>üí∞Binance</a></p></div><div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><ins class=adsbygoogle style=display:block data-ad-client=ca-pub-9206135835124064 data-ad-slot=8754979142 data-ad-format=auto data-full-width-responsive=true></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><section class=article-content><h2 id=video>Video</h2><div class=video-wrapper><iframe loading=lazy src=https://www.youtube.com/embed/4u218xVkjmQ allowfullscreen title="YouTube Video"></iframe></div><h2 id=transcript>Transcript</h2><p>Ôªøyou</p><p>hello everybody we&rsquo;re gonna go ahead and</p><p>get started</p><p>fantastic welcome to open AI scholars</p><p>demo day thank you all for being here</p><p>this evening we have eight presentations</p><p>tonight as a result of our Scholars</p><p>program we had 550 people apply and</p><p>eight scholars so that makes us a bit</p><p>more competitive than Harvard but you</p><p>know we don&rsquo;t want it we don&rsquo;t want to</p><p>break any who and the scholars have</p><p>spent the past three months studying</p><p>machine learning full-time two of those</p><p>months have been spent on a</p><p>self-selected curriculum going through</p><p>the different skills that they need to</p><p>be able to complete a final presentation</p><p>which they will be showing to you today</p><p>this is a project that they&rsquo;ve completed</p><p>in one month which I&rsquo;ve been told is a</p><p>very short amount of time to complete a</p><p>experiment in machine learning when you</p><p>are just a beginner however they&rsquo;ve been</p><p>assisted by our really awesome mentors</p><p>who are a combination of opening eye and</p><p>external folks and we&rsquo;re really excited</p><p>to have everyone here so I&rsquo;m gonna go</p><p>ahead and pass it off to Ilya who&rsquo;s</p><p>gonna say a few words before we get</p><p>started</p><p>Thanks hello everyone and welcome to the</p><p>scholars demo day it&rsquo;s really exciting</p><p>to see the projects that the scholars</p><p>were able to accomplish in just one</p><p>month one thing about machine learning</p><p>is that it&rsquo;s not the easiest field to</p><p>enter without mentorship and working</p><p>closely with a good mentor can really</p><p>narrow down the very big surface area of</p><p>machine learning down to manageable</p><p>pieces that make progress far more</p><p>rapidly and I can say for myself I</p><p>definitely wouldn&rsquo;t be where I am</p><p>without the mentorship that I received</p><p>so with this I want to say thank you and</p><p>real gratitude to all the mentors who</p><p>helped bring the scholars to where they</p><p>are right now</p><p>and finally doing a project is just in</p><p>just one months is not an easy feat at</p><p>all so congratulations on the scholars</p><p>who&rsquo;ve done this and let&rsquo;s see let&rsquo;s see</p><p>the projects</p><p>[Applause]</p><p>thank you for coming to our demo day</p><p>today I&rsquo;m going to talk about exploring</p><p>gamma the discount of the future or the</p><p>weight of the past so here&rsquo;s a bit of</p><p>background for those who are unfamiliar</p><p>with reinforcement learning it is the</p><p>framework where an agent takes action in</p><p>the environment to maximize cumulative</p><p>rewards so the measure we care about X</p><p>the expected sum of total rewards and</p><p>the measure we actually optimize is they</p><p>expected sum of total discounted rewards</p><p>so the difference of here is the</p><p>discount and not only it prevents us</p><p>from getting explosive sum to infinity</p><p>but also it injects some preferences</p><p>which I&rsquo;m going to talk about so what is</p><p>discount factor from economics this</p><p>discount factor gamma specifies some</p><p>intertemporal temp preferences and what</p><p>I mean by preferences is to imagine a</p><p>thought experiment where if I hand you a</p><p>full approached moral or some gamma</p><p>fraction of an apple today and then I</p><p>twist the gamma until you find the</p><p>single gamma that you are indifferent</p><p>between the two then this gamma reviews</p><p>your preference so with this preference</p><p>in mind let&rsquo;s look at a toy example in</p><p>some simple grid word experiment here we</p><p>have two agent one has a low discount</p><p>preference so it will have a what prefer</p><p>immediate rewards such as the coins on</p><p>the Left which has lower value or the</p><p>agent on below has a high discount</p><p>meaning it will prefer long-term future</p><p>reward which is attainment so we can see</p><p>different preferences results in</p><p>distinctive behavior that agent could</p><p>express as a beginner 2rl algorithm we</p><p>may just ask which discount factor gamma</p><p>to use well some will tell you 0.99 or</p><p>otherwise say something close to one no</p><p>no not one for boundedness and some ways</p><p>say just try a set of commas and pick</p><p>the best so none of these answers seemed</p><p>entirely satisfactory so</p><p>I found out this about Blackwell</p><p>optimality principle actually says that</p><p>in all environments there exists an</p><p>optimal policy that simultaneously</p><p>optimal with gamma higher than some</p><p>threshold so looking at this principle</p><p>we may find it quite intuitive because</p><p>given some real world environments where</p><p>we always a maximize cumulative reward</p><p>that applies gamma equals 1 however in</p><p>real what we can now use gamma equal to</p><p>1 but having a gamma of 0.99 results in</p><p>a very similar intertemporal preference</p><p>as a gamma equal to 1 having a gamma</p><p>0.99 in the diamond case in the in the</p><p>grillwork case the agent will still pick</p><p>diamond over the coin despite having a</p><p>slightly lower discount factor so the</p><p>question I&rsquo;m interested in is to tip our</p><p>algorithms always find the Blackwell</p><p>optimal policy for gammas above the</p><p>threshold well let&rsquo;s go back to this</p><p>time we&rsquo;ll use a lower exploration</p><p>instead of Hayek&rsquo;s persian in the</p><p>previous case and we see the behavior is</p><p>the same for the low discount agent</p><p>however for the highest own agent it</p><p>cannot obtain any reward so not</p><p>necessarily in this case that Alton</p><p>power algorithm could find this kind of</p><p>optimal policy so in this work we will</p><p>demonstrate this issue and propose</p><p>methods to repair it to do that I set up</p><p>to world war environment one with sparse</p><p>reward the other with dense reward so</p><p>the agent in red is trying to collect</p><p>all the yellow coins which will give</p><p>them positive reward and they want to</p><p>avoid poison we shall give them negative</p><p>reward and lastly they they don&rsquo;t want</p><p>to get trapped otherwise they will die</p><p>so the algorithm I use is from baselines</p><p>overnight spaced lines D queuing and</p><p>then the experiment setup is I only vary</p><p>the discount factor and keeping</p><p>everything else constant and here&rsquo;s the</p><p>initial results on the sparse</p><p>environment so I pick a set of gammas</p><p>from 0.1 0.2 0.5 0.8 and 0.99 as you can</p><p>see on</p><p>right curve the highest gamma 0.99</p><p>yields the best performance and this</p><p>seems like there might be a stretch</p><p>between gamma 0.5 and point 9 0.8 such</p><p>that above that all gammas or high</p><p>gammas yield optimal performance so this</p><p>is quite consistent with the back</p><p>blackwell optimality principle let&rsquo;s</p><p>look like how let&rsquo;s look at how this</p><p>will work in a dense environment so here</p><p>we see that the highest gamma which is</p><p>in pink 0.99 is actually not doing well</p><p>it&rsquo;s not even doing as well as the gamma</p><p>0.5 in this case so we see some</p><p>inconsistency with the Blackwell</p><p>optimality principle let&rsquo;s dig further</p><p>so after thinking about it my hypothesis</p><p>to this behavior is that discount factor</p><p>may play a dual role in DQ and update</p><p>and specifically it not only explicitly</p><p>specifies that intertemporal preferences</p><p>which is to discount the future but</p><p>implicitly it includes some confidence</p><p>on bootstrapping of the function</p><p>approximator</p><p>which is to weighing the past so I</p><p>proposed a time variant discounting</p><p>gamma of T so that when we specify a</p><p>male pick five a fraction that&rsquo;s a some</p><p>fraction of the total time steps and</p><p>during that period we are going to vary</p><p>our gamma from 0.1 to a final gamma that</p><p>we specified so that they have a linear</p><p>schedule then we wait earlier</p><p>experiences less the gamma will keep</p><p>fixed after the myopic fraction fraction</p><p>so using this simple scheme on the dense</p><p>environment</p><p>t queuing algorithm let&rsquo;s compare the</p><p>experiments so the pink curve is still</p><p>the fixed gamma from the in the dense</p><p>environment and although other colorful</p><p>curves are always different myopic</p><p>fraction and to see this more clearly I</p><p>grouped all myopic fraction setups in a</p><p>single block in blue and you can see</p><p>other myopic fraction no matter how</p><p>large it is outperforms the basslines</p><p>algorithm with fixed gamma and because</p><p>0.99 what wasn&rsquo;t doing well and then</p><p>with this initial myopia it could</p><p>actually become optimal so this is a</p><p>really good result and let&rsquo;s see if it</p><p>can work in the path gamma which is 0.8</p><p>in the tense environment so I do the</p><p>same thing where the pink curve is the</p><p>fixed gamma 0.8 which was the best gamma</p><p>in the dense environment and all the</p><p>myopic fraction are in blue on the right</p><p>side and then we see that any male fake</p><p>fraction could eventually achieve the</p><p>same level of performance as the</p><p>original fixed gamma and just take</p><p>longer so what about the sparse</p><p>environment so I tried the same thing</p><p>where on the yeah so we have the fixed</p><p>gamma on the left we have gamma of 0.8</p><p>on the right we have gamma point 99 and</p><p>you can see subsequently as we increase</p><p>the myopic fraction the longer it takes</p><p>to reach optimal but they reach optimal</p><p>nonetheless so fixed gamma yields the</p><p>best performance in this sparse</p><p>environment but all my opaque fraction</p><p>converts to optimal eventually so it</p><p>doesn&rsquo;t really hurt in the long run so</p><p>summing up from all this initial amalia</p><p>results we see that high discount gamma</p><p>could become optimal with initial myopia</p><p>and the benefits of this simple scheme</p><p>is that we don&rsquo;t need to find humour</p><p>because it improves learning intense</p><p>reward environments and doesn&rsquo;t harm</p><p>learning in sparse reward environment</p><p>however you may say that all our</p><p>hypothesis is that this initial myopia</p><p>will mitigate bias thus better</p><p>performance there might be a competing</p><p>hypothesis is by introducing this</p><p>initial myopia you may have more</p><p>exploration which might lead to better</p><p>performance so I&rsquo;m trying to see whether</p><p>this benefits is result of bias</p><p>reduction or exploring</p><p>to do that I set up three experiment</p><p>where are fixed the discount factor and</p><p>for the baseline setup I have zero</p><p>myopia and load low exploration for the</p><p>myopia setup I have low myopia and low</p><p>exploration and for the exploration</p><p>setup I have zero myopia and high</p><p>exploration so to see the results let&rsquo;s</p><p>look at the plot on the left here our</p><p>gamma is fixed to be zero point eight</p><p>across our three setups and for the</p><p>baseline it is in pink and then we see</p><p>that it has similar performance as the</p><p>myopia which is in orange and</p><p>exploration actually did worse than both</p><p>of them as it because it takes longer</p><p>and on the right we have our gamma of</p><p>0.99 which is considered the poor gamma</p><p>in the dense environment and we can see</p><p>with myopia actually outperform post</p><p>baseline and exploration and with the</p><p>exploration although is eventually</p><p>surpasses baseline a bit but still takes</p><p>a long time so we can sort of conclude</p><p>that exploration helps but not</p><p>significantly and it takes longer</p><p>training time but with myopia it</p><p>improves significantly and converges</p><p>faster so as part of the future</p><p>directions we try to formalize this dual</p><p>role intuition and run more experiments</p><p>on standard testing ground and I will</p><p>also want to compare a myopia schedule</p><p>lambda versus gamma in PPO I have some</p><p>initial results and happy to discuss</p><p>offline after the presentation so just a</p><p>recap here some related works and final</p><p>takeaway discount factor matters in deep</p><p>reinforcement learning and it has a dual</p><p>role that specifies intertemporal</p><p>preferences and also includes confidence</p><p>on bootstrapping from function</p><p>approximation and a simple myopic</p><p>schedule is a robust and effective way</p><p>to improve performance and the same</p><p>logic may work beyond EQ</p><p>and discrete action state framework</p><p>thank you I&rsquo;m happy to take questions</p><p>[Applause]</p><p>yeah so in the baselines dqn they have</p><p>the similar linear schedule for</p><p>exploration where it started low I</p><p>started high and become low and stay low</p><p>for the whole time yeah</p><p>hello I was wondering if you can think</p><p>of a way so I really liked this idea of</p><p>seeing Gemma as something that kind of</p><p>enclose your confidence do you think</p><p>there&rsquo;s a way to kind of like take you</p><p>uncertainty into account more directly</p><p>it seems like you&rsquo;re currently doing it</p><p>over time but do you think there&rsquo;s a</p><p>there&rsquo;s a way to like make that more</p><p>explicit by I don&rsquo;t know getting some</p><p>uncertainty estimate for your cue</p><p>function thank you it&rsquo;s a really good</p><p>question I&rsquo;m thinking to first try it on</p><p>the generalized advantages estimate</p><p>because it exhibits basically separates</p><p>out lamda and n gamma in the equation</p><p>where I feel like lamda is precisely the</p><p>role it plays in our confidence in</p><p>bootstrapping so I guess by varying</p><p>lambda perhaps in the algorithm PPO</p><p>could help with this issue yeah</p><p>any other questions</p><p>[Applause]</p><p>okay so I&rsquo;d like to begin by thanking</p><p>everyone at open AI who organized this</p><p>event you can&rsquo;t hear me how about now</p><p>okay and I&rsquo;d also like to thank all of</p><p>you for attending so previously I was a</p><p>PhD student at the University of Chicago</p><p>where I studied cell biology but these</p><p>days wood excites me most is the</p><p>prospect of getting general-purpose</p><p>robots deployed in the real world and</p><p>then getting them to do useful things</p><p>and so one challenge associated with</p><p>this is that robots will have to learn</p><p>how to solve new tasks with little to no</p><p>external feedback so for the past month</p><p>I&rsquo;ve been working on a project that</p><p>leverages a robot&rsquo;s internal motivation</p><p>and over in order to overcome this lack</p><p>of supervision so I&rsquo;ll start by placing</p><p>intrinsic motivation in the context of</p><p>reinforcement learning so in the</p><p>reinforcement learning setting you have</p><p>an agent that interacts with an</p><p>environment via its policy so the policy</p><p>takes in observations and outputs</p><p>actions so at every time step the agent</p><p>takes an action and receives a reward</p><p>from the environment so by learning to</p><p>maximize its total expected reward the</p><p>agent can find a good policy and</p><p>eventually learn how to solve a given</p><p>task but finding a good policy can</p><p>actually be very difficult when the</p><p>rewards from the environment are sparse</p><p>so what we really need our reward</p><p>functions that are intrinsic to the</p><p>agent instead of relying solely on</p><p>sparse extrinsic rewards okay so how do</p><p>you get how do you get dense intrinsic</p><p>rewards so there&rsquo;s actually been a lot</p><p>of work in this area but most approach</p><p>that are a lot of approaches Center on</p><p>some notion of novelty so by seeking</p><p>novelty the agent is driven to explore</p><p>and in doing so learns new skills that</p><p>might help it solve a given task</p><p>and so this approach is actually had a</p><p>lot had a lot of success recently with</p><p>solving challenging Atari games like</p><p>Montezuma&rsquo;s Revenge</p><p>however these ideas haven&rsquo;t really been</p><p>applied to robotics so in this project</p><p>I&rsquo;ll show you how using a simple</p><p>formulation for intrinsic rewards will</p><p>lead to nice solutions for solving</p><p>challenging robotics problems so before</p><p>I discuss the method that I use I&rsquo;ll</p><p>talk about the fet robotics environments</p><p>which were developed in-house at open AI</p><p>by matthias sitting in the back and so</p><p>the observations include the position</p><p>and velocity of the gripper and it also</p><p>includes the pose the linear velocity</p><p>and angular velocity of any object that</p><p>might be in the same the action space is</p><p>continuous and it&rsquo;s four dimensional and</p><p>it includes the first three dimensions</p><p>correspond to change in position along x</p><p>y&amp;z and the fourth dimension corresponds</p><p>to opening and closing the gripper and</p><p>so again we care about the sparse reward</p><p>setting so here the agent receives at</p><p>every time step award of 0 if it solves</p><p>a task and negative one otherwise and I</p><p>just want to point out that for all of</p><p>the environments that I&rsquo;m going to talk</p><p>about which include reaching pushing</p><p>sliding and picking place the agent has</p><p>at most 50 time steps to actually solve</p><p>the task ok so here&rsquo;s an outline of the</p><p>method that I use so the diagram on the</p><p>left you can see that it&rsquo;s very similar</p><p>to the reinforcement learning setting</p><p>that I talked about a few slides ago but</p><p>here in addition to the agent having a</p><p>policy it also has a dynamics model so</p><p>the dynamics model takes as input the</p><p>agent&rsquo;s current state and its action and</p><p>it makes a prediction for the next state</p><p>and I also want to note that we we could</p><p>set up the dynamics model to make</p><p>predictions for the robots change in</p><p>state</p><p>and so here&rsquo;s the intrinsic reward his</p><p>intrinsic reward that I&rsquo;m using and what</p><p>you&rsquo;ll notice is that it&rsquo;s simply the</p><p>prediction error of the dynamics model</p><p>and so larger prediction errors should</p><p>lead to larger intrinsic rewards and so</p><p>the idea is that the the idea is that</p><p>the agent shouldn&rsquo;t get stuck in regions</p><p>that it&rsquo;s already explored and instead</p><p>will be encouraged to explore elsewhere</p><p>okay so for training I use simple fully</p><p>connected networks for both the policy</p><p>and the dynamics model I train the</p><p>policy using PPO which is an actor</p><p>critic architecture that was developed</p><p>here by John Shulman and briefly since</p><p>PPO is on policy I use a number of</p><p>actors in parallel to collect a large</p><p>and diverse set of data that can</p><p>actually be used to update the</p><p>parameters of both the policy and the</p><p>dynamics model okay so here&rsquo;s my first</p><p>set of results so I&rsquo;m applying this</p><p>method to first to the simplest of the</p><p>tasks which is reaching and here I&rsquo;m</p><p>comparing a baseline PPO implementation</p><p>with no intrinsic rewards to intrinsic</p><p>rewards that were that were generated by</p><p>predicting the full neck state or by</p><p>predicting the change in state and so</p><p>for a simple task like reaching it turns</p><p>out that you don&rsquo;t actually need the</p><p>intrinsic rewards but adding them you</p><p>you can see that the agents learns to</p><p>solve the task much more quickly and you</p><p>can see in the bottom panel that as the</p><p>agent learns the intrinsic reward</p><p>actually goes down okay so I next looked</p><p>at the pushing task which is actually</p><p>quite a bit more complicated than the</p><p>reaching task so unlike reaching the</p><p>baseline policy is not actually able to</p><p>solve this task but when you add in the</p><p>dense intrinsic rewards you can see that</p><p>the agent quickly learn</p><p>how to solve solve the task and near the</p><p>end of training its solving nearly 100</p><p>percent of the episodes and so when I</p><p>approach the pushing task I actually</p><p>looked at a number of different hyper</p><p>parameters and so I&rsquo;ll talk about three</p><p>here briefly so I looked at the size of</p><p>the individual layers within the network</p><p>and as you might expect larger networks</p><p>tend to perform better than smaller</p><p>networks I also looked at the learning</p><p>rate for the dynamics model and so this</p><p>actually turned out to be very important</p><p>so as you can see here smaller learning</p><p>rates tend to outperform larger learning</p><p>rates and the last thing I looked at was</p><p>saying whether or not resetting the</p><p>environment early after the agent has</p><p>already solved the task will lead to</p><p>better performance so that&rsquo;s where you</p><p>can see here so after the agent has</p><p>solved the task by doing an early reset</p><p>that leads to a significant boost in</p><p>performance ok so I next looked at the</p><p>pick-and-place task and this s is</p><p>actually step up in difficulty from the</p><p>pushing task but here the story is more</p><p>or less the same the baseline policy</p><p>cannot solve the task but adding in</p><p>either intrinsic reward can actually</p><p>lead to the agent learning how to solve</p><p>the task and for reasons I don&rsquo;t quite</p><p>understand yet predicting the full next</p><p>state tends to outperform predicting the</p><p>change in state so that&rsquo;s something I</p><p>want to look at a little bit more in the</p><p>future ok so the last thing I looked at</p><p>was a sliding task it turns out that by</p><p>comparison the sliding task is probably</p><p>more difficult than the others and so</p><p>here I actually needed a larger Network</p><p>than what was required for the others</p><p>for the other tasks and I also needed</p><p>twice as many environment interactions</p><p>for the agent to learn how to solve this</p><p>okay so to summarize consistent with</p><p>previous works my results demonstrate</p><p>that adding intrinsic rewards can</p><p>actually be useful in solving</p><p>challenging tasks with sparse rewards</p><p>although unnecessary</p><p>for solving the reaching task adding</p><p>intrinsic rewards as an exploration</p><p>bonus actually leads to improve</p><p>performance and intrinsic rewards were</p><p>actually necessary for solving a more</p><p>difficult task so in the future I plan</p><p>to look at slightly more complicated</p><p>tasks such as tool use and block</p><p>stacking and I also plan to look at</p><p>different kinds of inputs both both for</p><p>the policy and for the dynamics model</p><p>and in particular I&rsquo;m really interested</p><p>in looking at combining different sensor</p><p>sensor modalities so looking at</p><p>combining images with depth maps with</p><p>contact information okay so with that</p><p>I&rsquo;d like to thank Maddie for organizing</p><p>the Scholars Program I&rsquo;d like to show my</p><p>appreciation to scholars past and</p><p>present I&rsquo;d like to give a special thank</p><p>you to Matthias Alex and Lillian they</p><p>gave me really good feedback that I was</p><p>able to use to make modifications to</p><p>fetch environments I&rsquo;d like to thank</p><p>Yura Harry Igor and Wojcik with all of</p><p>whom I&rsquo;ve had really enlightening</p><p>conversations and I&rsquo;d like to give a</p><p>special thank you to Yura and Harry</p><p>whose previous work was actually the</p><p>inspiration for this project and last</p><p>but not least I&rsquo;d like to thank thank my</p><p>mentor for y&rsquo;all who provided me with a</p><p>lot of encouragement and support over</p><p>the last couple of months so with that</p><p>I&rsquo;d be happy to take any questions</p><p>[Applause]</p><p>have you tried Alice tiems is that</p><p>awfully that&rsquo;s a good question probably</p><p>when I start to incorporate different</p><p>sensory modalities I&rsquo;ll have to move to</p><p>using Elysee Ames also may help with the</p><p>Delta version of the inputs yeah I could</p><p>so I wanted to try it originally but the</p><p>the simple method that I used already</p><p>had such good performance that I&rsquo;d sort</p><p>of put that off any other questions</p><p>was was there any stochastic</p><p>stochasticity in the environment yes so</p><p>every time the environment was reset the</p><p>target location would change and if</p><p>there were a block in the scene the</p><p>starting location of the block would</p><p>also change oh I see was there any</p><p>stochasticity in the transitions like</p><p>from one point in time to the next point</p><p>so like this kind of movement there was</p><p>no stochasticity the movements were</p><p>completely deterministic I see</p><p>how important was it to trade off the</p><p>goal-directed reward versus the</p><p>intrinsic reward like did that require</p><p>some tuning or not so much</p><p>I think most of that tuning came from</p><p>tuning the learning rate for the</p><p>dynamics model because at the if the</p><p>Dyna if the learning rate for the</p><p>dynamics model was too big then the</p><p>agent would very quickly learn how the</p><p>environment worked and so the intrinsic</p><p>rewards I would get would be really</p><p>small whereas if you delayed that it</p><p>would get larger rewards early</p><p>encouraging exploration but if you&rsquo;re if</p><p>you&rsquo;re referring to like the size of the</p><p>rewards the relative to rewards say</p><p>negative one versus with the intrinsic</p><p>reward was I did tune that quite a bit</p><p>turns out one works really nicely which</p><p>offsets the reward of negative one from</p><p>the environment yeah what larger rewards</p><p>don&rsquo;t work and smaller intrinsic rewards</p><p>don&rsquo;t work any other questions great</p><p>thank you</p><p>[Applause]</p><p>you</p><p>hi everyone thank you for being here</p><p>tonight and louder okay is it any better</p><p>okay thank you so um welcome so I will</p><p>be talking about my experiment with fine</p><p>tuning GPT two small model for question</p><p>answering tonight before I start to talk</p><p>about giving you two details about my</p><p>experiment I would like to mention the</p><p>question that I was interested in which</p><p>guided me in my decision to work on this</p><p>problem one of the biggest challenges</p><p>that we have in natural language</p><p>understanding today is the ability to</p><p>create systems that have common sense</p><p>neat reasoning which is the ability of</p><p>an intelligent system to come up with</p><p>common-sense knowledge and reason about</p><p>a given text so this is still a</p><p>notoriously difficult task and although</p><p>we have very high-performing language</p><p>models and systems today we are still</p><p>struggling to perform better in this one</p><p>so in this chart you will see you notice</p><p>that there&rsquo;s an increase in the</p><p>benchmark models and data sets that have</p><p>come out in the recent years that are</p><p>targeted exclusively for common sense</p><p>reasoning especially in 2018 alone we</p><p>have a number of data sets that were</p><p>specifically designed for common sense</p><p>reasoning and you will realize that most</p><p>of them are targeting the task of</p><p>question answering so reasoning is</p><p>necessary in in performing better better</p><p>especially in most of the NLU tasks</p><p>that&rsquo;s because most of the time we will</p><p>not have the solution given in a</p><p>linguistic context and there will be a</p><p>lot of ambiguities in the language that</p><p>we need to our systems are intelligent</p><p>systems needs to be able to figure out</p><p>so this is this is one of the reasons</p><p>why I</p><p>I chose the task of question answering</p><p>for my fine tuning first of all QA is</p><p>one of the most important natural</p><p>language understanding tasks that will</p><p>allow us to measure how a system is</p><p>doing in terms of common-sense reasoning</p><p>in addition to that QA requires a mix of</p><p>language processing and reasoning skills</p><p>within a single task and that&rsquo;s why it&rsquo;s</p><p>more practical than probably dealing</p><p>with some other more complicated than</p><p>defined tasks in addition to that better</p><p>reasoning achieved to create systems</p><p>could be applied to a variety of systems</p><p>that is not limited to natural language</p><p>processing or understanding such as</p><p>vision and robotics the examples of</p><p>which we have started to see lately so</p><p>the approach that I take in my project</p><p>was to analyze the patterns that a</p><p>fine-tuned GPT small performance on QA</p><p>tasks could reveal about how a language</p><p>mount model attains and performs</p><p>reasoning so for this I have</p><p>experimented with this small model and</p><p>finding that on the Stanford</p><p>question-answering dataset for those of</p><p>you who are not familiar with the data</p><p>set I will I will be giving some</p><p>informations about that oh sorry I meant</p><p>to go back the statue of the models</p><p>amount of architectures that I have</p><p>worked on the first one you see on the</p><p>on the left is a linear classifier that</p><p>I put on top of GPT to a small model the</p><p>first public release one and the one on</p><p>the on the right will be a bio SCM in a</p><p>naive attempt to circumvent and</p><p>bi-directional uni directionality of GPT</p><p>to most of my results will be based on</p><p>the linear model because after going</p><p>through a lot of hyper parameter tuning</p><p>and</p><p>I noticed that actually new model</p><p>perform better for those of you who are</p><p>not familiar with the squat data side</p><p>squat dataset contains over a hundred</p><p>thousand question-and-answer pairs and</p><p>which which have answers and it also has</p><p>the point tube model also has over fifty</p><p>thousand unanswerable questions with</p><p>some plausible answers added and if your</p><p>model comes up with those plausible</p><p>answer is still very informative about</p><p>how your model is doing in these two</p><p>examples that are taken from the data</p><p>sets you will see that the question with</p><p>the answer is a very factor it&rsquo;s styled</p><p>simple question and the one on the other</p><p>side which has a plausible answer but</p><p>you cannot actually retrieve the answer</p><p>from the from the passage that is given</p><p>for that question so my model was these</p><p>are representative of some of the</p><p>questions that my model was able to</p><p>answer correctly or abstain from</p><p>answering because it did not have an</p><p>answer in the data set squall has in the</p><p>development Setsuko has over 11,000</p><p>questions and 5000 of which is</p><p>unanswerable and because I thought</p><p>focusing on the unanswerable questions</p><p>and the performance of the model on this</p><p>kind of question or informative about</p><p>how the reasoning and common-sense</p><p>working for the model most of my the</p><p>most of the numbers and reports that I</p><p>will be presenting today based on those</p><p>type of questions although there are</p><p>other data sets that are specifically</p><p>designed for common-sense reasoning I</p><p>use squaws because it will be it will</p><p>give me a good start to understand how</p><p>such a powerful language model although</p><p>I&rsquo;m using the smallest version will</p><p>perform in common sense and what are the</p><p>mistakes what are the successes and</p><p>failures that the model will make and it</p><p>will guide me to work on more complex</p><p>sophisticated data sets going forward so</p><p>the linear model that I implemented</p><p>worked relatively better in the</p><p>unanswerable questions where it was able</p><p>to pick up that the answered the</p><p>question did not have an answer and</p><p>abstain from answering to that in the</p><p>plausible answer section the model was</p><p>able to pick first I&rsquo;ve seen from out a</p><p>spring like okay this question does not</p><p>have an answer but also able to come up</p><p>with the wasa it got wrong was able to</p><p>come up with the plausible answers which</p><p>was not extremely unreasonable in this</p><p>example this is a repeat this is</p><p>representative of the strategy that the</p><p>model learned from the training the</p><p>model takes attends to the first few</p><p>tokens in the question text when it&rsquo;s</p><p>trying to answer a question this</p><p>repeated so many times that I was like</p><p>okay this is this is definitely taking</p><p>some of the attention to the first few</p><p>question few tokens in the question text</p><p>and then trying to pull out the answer</p><p>from the from the passage as you can see</p><p>in this example the model predicted that</p><p>the answer contained the little phrase</p><p>that I correctly at the bottom but the</p><p>cone but the real answer the correct</p><p>answer was the the word concrete if if</p><p>we go back to the original paper of GPT</p><p>to I notice that some of the heuristics</p><p>mentioned in the paper such as how the</p><p>unsupervised 0 shall in zero shot</p><p>setting the create ask was taking up on</p><p>the questions like who what where and</p><p>when the same pattern kind of appeared</p><p>in the model that I also trained in here</p><p>some of my observations in include that</p><p>the model performs comparatively better</p><p>on questions that are unanswerable and</p><p>partial matches mainly consist of</p><p>initial tokens from the question and</p><p>when an answer is expressed in words</p><p>that are different from the question in</p><p>a way you can think about it as a</p><p>paraphrasing the model often fails to</p><p>recognize it if the order of the words</p><p>are different or used synonyms or</p><p>antonyms for expressing the same idea</p><p>the model was not performing very well</p><p>so for future directions I would like to</p><p>experiment more with the bigger model</p><p>which has been very recently released</p><p>and work more on the common-sense</p><p>reasoning datasets that are specifically</p><p>designed for this task in addition to</p><p>that I would like to work more on</p><p>natural language understanding two</p><p>unsupervised learning because the main</p><p>idea behind GPT 2 was also to eliminate</p><p>the need to fine tune models and create</p><p>task specific architectures so through</p><p>common sense reasoning I feel like</p><p>there&rsquo;s a lot that can be done in that</p><p>department to see if there if we can</p><p>actually do that and as a long-term goal</p><p>I would like to explore the interactions</p><p>between natural language understanding</p><p>and other deep learning research thank</p><p>you</p><p>[Applause]</p><p>I&rsquo;m happy to answer questions</p><p>the questions okay thank you</p><p>[Applause]</p><p>no sorry</p><p>hello okay so good evening everyone</p><p>today I&rsquo;m going to talk about my project</p><p>during the three three months Scholar</p><p>Program and the project is about</p><p>sentiment analysis using reinforcement</p><p>learning and since before I go into</p><p>details of my project I want to first</p><p>thank my mentor Azalea who gave me who</p><p>guided me in the process three months</p><p>with great passion and also thanks to</p><p>open a I provide me such good</p><p>opportunity and the rich resource to</p><p>learn and to develop my project okay let</p><p>me introduce more details about my</p><p>project first I want you to talk about</p><p>the motivations of my project with the</p><p>development of the neural network</p><p>there&rsquo;s an LP becomes a very hot topic</p><p>and because NLP can build the</p><p>computational algorithms and also let</p><p>the computer to learn and analyzed and</p><p>represent human language and there&rsquo;s a</p><p>lot of NLP tasks and among all the NLP</p><p>tasks sentiment analysis has achieved</p><p>very good performance and there&rsquo;s a lot</p><p>of well-tuned measured online so for</p><p>this project we propose some novel</p><p>models that can combine reinforcement</p><p>learning and a supervisor and the appeal</p><p>method to predict incentive sentiment</p><p>and send him sentiment of a sentence</p><p>and the currently as mentioned before</p><p>there&rsquo;s a lot of well chewed and also</p><p>help a lot of supervised learning method</p><p>about sentiment analysis online so I</p><p>think we consider this project and we</p><p>consider our L might self learn and</p><p>capture some missing informations based</p><p>on the current models so let me talk</p><p>about details about</p><p>as mentioned before we proposed to naga</p><p>models that combine reinforcement</p><p>learning and the sentiment analysis</p><p>supervised learning method and the first</p><p>model I will say or I would call it the</p><p>sentence structure in the simple words</p><p>we just like simple crucial words in a</p><p>sentence that are useful to predict the</p><p>sentence sentiment and we can see the</p><p>models here first when the model</p><p>consists two parts the first is the</p><p>policy network and the second is the</p><p>classification network we can see the</p><p>graph examples here for the sentence</p><p>restructure the first the top rectangle</p><p>represents the policy Network and that</p><p>means for each sentence for each word in</p><p>a sentence we sample based on the policy</p><p>whether to keep or repair until 8 this</p><p>word in a sentence to predict the</p><p>sentence sentiment so after the policy</p><p>network we actually pass the selected</p><p>sub sentence to our classification</p><p>network for example we have a sentence</p><p>and there&rsquo;s a lot of such words so maybe</p><p>as or as we expected we may remove those</p><p>words to predict the sentiment and also</p><p>the prediction will not be worse so</p><p>based on the selected sub sentence we</p><p>pass the sub sentence into the</p><p>classification that work and for this</p><p>classification that work it actually we</p><p>use the traditional supervisor learning</p><p>NLP method like a wave child long</p><p>short-term memory</p><p>transformer and also protein birth so so</p><p>some details on the right part we can</p><p>consider the sentiment analysis task as</p><p>a sequential decision and current</p><p>decision in a sentence always say</p><p>whether to keep or remove this word in a</p><p>sentence will affect the following</p><p>decisions and also affect the following</p><p>predictions</p><p>and which all this sequential process</p><p>can be naturally adjusted by the policy</p><p>weighed and measured and here we use the</p><p>delay and delay the reward that means we</p><p>cannot have the reward until we reach</p><p>the very end of our sentence once we</p><p>have the production of the sentence send</p><p>her a statement we can have a reward</p><p>with value and we take value 1 if we</p><p>predict correctly and with value</p><p>negative 1 if we predict well and for</p><p>the action as mentioned before it&rsquo;s just</p><p>for each word we&rsquo;ll decide whether to</p><p>keep it in the sentence or just delete</p><p>this sentence just delete this word in</p><p>the sentence and use the sub sentence to</p><p>predict the sentiment so this is our</p><p>first model and our second model I would</p><p>call we use the word probabilities in</p><p>the sentence to predict this sentence</p><p>sentiment so instead of considering two</p><p>networks we are only using one network</p><p>and will predict the probability of each</p><p>word and use the sum of the word log</p><p>probabilities to change the language</p><p>model and using PPO loss function</p><p>actually we&rsquo;ve also tried other policy</p><p>gradient loss functions so we can also</p><p>see the model structure in detail here</p><p>so it represents our what probability</p><p>Network so for each word we have the</p><p>output we can have the output tensor of</p><p>the word probabilities after using the</p><p>protein pert Network and then we still</p><p>use the previous reward function with</p><p>value 1 if we predict correctly and the</p><p>value 0 if we predict wrong</p><p>so one thing we tried different with</p><p>previous is here we are not only using</p><p>the forward sentence order we&rsquo;re also</p><p>using the backward sentence order so in</p><p>this case we can have two probabilities</p><p>of the word sent word probability and we</p><p>can define the law</p><p>functions with only the forward sentence</p><p>other probabilities oh we can combine</p><p>using the both forward and a backward</p><p>sentence other probabilities so let&rsquo;s</p><p>see our dataset and the experiment here</p><p>so basically we evaluate our models</p><p>unsent and stamp resentment chip Bank</p><p>which is a public dataset originally</p><p>with five classes but for our experiment</p><p>we adopt binary labels so we use one for</p><p>positively positive sentence and zero</p><p>for negative sentence and here is the</p><p>detailed layers we tried for two models</p><p>first afford our first model the</p><p>sentence restructure for the policy</p><p>Network we&rsquo;ll use our print long short</p><p>term memory that output the action</p><p>sequence tensor and also state value</p><p>tensor and once we pass the selected sub</p><p>sentence to the classification Network</p><p>we tried transformer and the pre-trained</p><p>Burt has supervised the method here for</p><p>the frigate bird we&rsquo;ll just use the</p><p>simplest virgin birth the the name</p><p>should call based on case the bird model</p><p>and then we to chain the policy network</p><p>we&rsquo;ve tried vanilla policy gradient</p><p>actor critique and proximal policy</p><p>optimization and wizards and for our</p><p>second model the word probabilities we</p><p>adopt protein birth again the simplest</p><p>the birth model to output the</p><p>probabilities of each word and then</p><p>define the PPO loss to chain the model</p><p>so let&rsquo;s see the result we&rsquo;ve tried to</p><p>in valuation metrics one is the accuracy</p><p>accuracy metric that is we can consider</p><p>as the binary or discrete evaluation</p><p>metric because we have to set a</p><p>threshold based on the probabilities</p><p>from the softmax layer and another in</p><p>valuation metric is called the AUC</p><p>metric and this we can consider</p><p>as a continuous evaluation metric so</p><p>first we can see the left stop figure</p><p>and based on the based on the</p><p>transformer when we add reinforcement</p><p>learning algorithms with the PPO has</p><p>better performance compared to the</p><p>transformer only and for the bird we can</p><p>produce compare comparable result and</p><p>for the right sub figure we can see the</p><p>word probability using PPO here we are</p><p>using the post forward and a backward</p><p>word probabilities so one expect one</p><p>leap reason for the lower AUC evaluation</p><p>metric can be because per for the</p><p>embedding of the bird we are actually</p><p>considered about the position or</p><p>embedded so that means when we take the</p><p>backward order sometimes we might have</p><p>the poor performance but that&rsquo;s our</p><p>guest and we haven&rsquo;t figured out why it</p><p>has some low performance now let me talk</p><p>about the conclusion and the takeaways</p><p>for this project so first we can see for</p><p>the sentence restructure model will find</p><p>adding reinforcement learning method can</p><p>improve the performance based on the</p><p>transformer model and produced</p><p>comparable result on protein pert and</p><p>also I&rsquo;ve I&rsquo;ve grabbed the the little</p><p>words from the testing set based our our</p><p>sentence restructure model and most</p><p>words are like the such word as we</p><p>expected so this words are not very</p><p>important to predict sentiment set as</p><p>sentence segment in common sense and for</p><p>the AUC sometimes it should an</p><p>improvement with our models so this may</p><p>be a direction for our future like</p><p>stress howatuney</p><p>for the probabilities if we want to use</p><p>the accuracy magic</p><p>and another takeaway for this whole</p><p>project I think there&rsquo;s a lot of</p><p>combination with reinforcement learning</p><p>and a LLP tasks but for sentiment</p><p>analysis we should admit this task is</p><p>not a complex task and also this task is</p><p>very concrete task we just need to</p><p>predict which class the tension and the</p><p>sentence becomes or belongs to so in</p><p>this case actually with very concrete</p><p>language tasks maybe using reinforcement</p><p>learning cannot have better performance</p><p>compared to with only a supervised</p><p>learning method but maybe for more</p><p>complex language tasks like text</p><p>generation and a summarization using</p><p>reinforcement learning can have better</p><p>performance and also I&rsquo;ve read some</p><p>paper during my project and find there&rsquo;s</p><p>a lot of implementation in text</p><p>generation and a summarization using</p><p>reinforcement learning and the last</p><p>thing or last the benefit for me is</p><p>during this project I got the chance to</p><p>handle all the supervised learning</p><p>algorithms for sentiment analysis like</p><p>long short-term memory attention</p><p>transformer and protein part and this</p><p>gave me a good opportunity to build a</p><p>pipeline with all the others to provide</p><p>the learning for sentiment analysis</p><p>thank you</p><p>[Applause]</p><p>do you find any specific reasons as to</p><p>why RL could be a better you know method</p><p>for the text summarization I think once</p><p>I&rsquo;m not quite sure but my understanding</p><p>that we cannot have good performance for</p><p>sentiment analysis because we are</p><p>actually have very concrete actions we</p><p>just want we just have to predict</p><p>whether it&rsquo;s positive or negative in</p><p>this case for very concrete language</p><p>tasks it&rsquo;s very hard to define a good</p><p>reward function but for text translation</p><p>and a text summarization actually we can</p><p>have a very good definition or very good</p><p>define for the reward function and with</p><p>a good reward it might be trainable for</p><p>using reinforcement learning algorithms</p><p>yeah that&rsquo;s my understanding</p><p>maybe more complex language tasks can</p><p>have better performance using</p><p>reinforcement learning thank you</p><p>thank you so good news we&rsquo;re running</p><p>we&rsquo;re running right on schedule we&rsquo;re</p><p>gonna go ahead and take a quick</p><p>15-minute break we&rsquo;re gonna start the</p><p>next presentation at 6:45 so planned to</p><p>be back in your seats a minute or two</p><p>before then and thank you all so much</p><p>you</p><p>besides therapeutical treatment for</p><p>disease for example in this situation</p><p>the doctor at each time doctor leads to</p><p>decide whether to do mechanic validation</p><p>or vasopressor and the reward is whether</p><p>the patient will like discharge the from</p><p>hospital helped to fool house</p><p>just doesn&rsquo;t work</p><p>in drug discovery researchers needs to</p><p>develop some new drug structure in order</p><p>to cure some disease at each state they</p><p>start with the simplest structure and at</p><p>each state you need to decide how to</p><p>grow this structure to to some desired</p><p>properties of these components so in</p><p>this situation exploration and exploit</p><p>exploitation is really important and in</p><p>some less risky environments doctors</p><p>need to decide one and what kind of lab</p><p>tests need to be scheduled in some cases</p><p>the tests are very costly and also it</p><p>may have some side effect on the patient</p><p>so it&rsquo;s also important to decide some</p><p>optimal sequential actions on these</p><p>recently there are some literature&rsquo;s</p><p>working in this direction most of them</p><p>are model-based they are they have</p><p>simulated environments so they can get</p><p>many samples and relatively fewer paper</p><p>our focus our model 3 we work with</p><p>purely observational data in this case</p><p>they have we have limited sample size</p><p>and the current status of this kind of</p><p>research is really preliminary and there</p><p>are a lot of challenges associated with</p><p>our observational data I will talk about</p><p>this at the end of the talk so my</p><p>project focuses on sepsis treatment in</p><p>the intensive care unit the main</p><p>reference is this recent lateral paper</p><p>by Komarovsky and I used the same mimic</p><p>3 dataset which is a very large</p><p>electronic health record data set and I</p><p>used the same definition of subsist</p><p>cohort and the same state and action</p><p>space different from their paper</p><p>I use I use a different reward design so</p><p>in the original paper they assign</p><p>positive 100 reward to the treat the</p><p>patient who successfully discharged and</p><p>assigned a minus 100 reward to those who</p><p>deceased in the hospital or within 90</p><p>days after discharge and for the RL</p><p>algorithm they use the policy iteration</p><p>and I use policy situation as well as</p><p>q-learning and they also use E I see you</p><p>Dennis L which is a much larger data set</p><p>of electronic health record but I didn&rsquo;t</p><p>use that and we have a little bit</p><p>different in sample size so this is the</p><p>structure of my project I call it AI</p><p>physician so it basically the input is</p><p>the electronic health record and the</p><p>output is the optimal policy suggested</p><p>by our our algorithms so in the first</p><p>test</p><p>so the first staff works with the raw</p><p>data set and extracted to be regular</p><p>process the time series then with this</p><p>regular time series I extract it to</p><p>Markov decision tuples and based on it I</p><p>apply several different or our</p><p>algorithms because observational data</p><p>are always measured irregularly and</p><p>always some missing data in it to deal</p><p>with it I used to master first days</p><p>heuristic master for example the body</p><p>temperature are measured in saucers or</p><p>fara heads so there are some basic trend</p><p>transformation between them this can</p><p>help with us with to deal with some of</p><p>the missing data and I also use k-means</p><p>clustering so basically the states</p><p>within the same class they should have</p><p>similar</p><p>measurements so I feel some of the</p><p>missing data are using one using the</p><p>values from the observation the same</p><p>cluster and for interpret interpolation</p><p>I use sample and a whole master this is</p><p>basically approximate the time series</p><p>using the step function and we can use</p><p>more sophisticated Gaussian process so</p><p>at the end I&rsquo;m interested in 24 hours</p><p>before and 48 hours after the onset of</p><p>sepsis and I extracted the sample every</p><p>four hours so for each patient we have</p><p>at most 18 steps for each patient so</p><p>once we have the regular time series we</p><p>can use the data to fill in the</p><p>environment and the replay pathogen is</p><p>container of Markov decision process</p><p>tuples that we can feed into the</p><p>algorithm this one example is the tag</p><p>tabular q the environment is used as a</p><p>folder that policy iteration algorithm</p><p>so we need to estimate from the samples</p><p>the transition matrix and the reward</p><p>matrix and will be playback buffer</p><p>actually did something like transform</p><p>the regular time series into M if it</p><p>happens and this structure follows the</p><p>open area team designed so it&rsquo;s very</p><p>week later if we want to try most more</p><p>sophisticated algorithm we can just</p><p>replace this tabular here with some</p><p>other algorithm so once we have the</p><p>regular time series we want to build our</p><p>MVP tapas the variable that</p><p>characterized the state are of 47</p><p>dimensions it basically includes the</p><p>vital measurements of</p><p>patient and also the lab measures these</p><p>are continuous measurement but I use a</p><p>clustering master to cluster it into 750</p><p>discrete state and also for the action</p><p>we care about two actions the IV fluid</p><p>and vasopressin toesik administrated in</p><p>four hours so this two are also</p><p>continuous but I put them into for each</p><p>of them up put them into like five</p><p>discreet pins</p><p>so there are 25 actions in total and and</p><p>for the reward design I care about three</p><p>things the first things is that whether</p><p>the vitals or the lab measurements stay</p><p>within the desired range so look at</p><p>looking at the first graph if the values</p><p>stay in that is a desired orange we</p><p>assign like zero reward but if it&rsquo;s like</p><p>outside of this normal range we we give</p><p>some negative reward that&rsquo;s basically a</p><p>penalty and the second thing we care</p><p>about if is that if if there is a chart</p><p>or sharp change in consecutive</p><p>measurements so if the change is smaller</p><p>than twenty percent then the reward is</p><p>zero but if it&rsquo;s larger than 20 percent</p><p>then it&rsquo;s going down and it&rsquo;s a negative</p><p>reward and also we consider whether this</p><p>patient successfully survived substance</p><p>were it&rsquo;s this or he or she deceased off</p><p>within 80 days of discharge so it&rsquo;s a</p><p>positive 50 and the legacy of 50 so in</p><p>the end we have two hundred seventy</p><p>eight thousand plus tuples and with this</p><p>whole dataset will run 400 times each</p><p>time we cut the</p><p>into training data set and testing data</p><p>set and we learned a policy on the</p><p>training data set and we evaluate the</p><p>policy in the testing data set because</p><p>that this is observational data so we</p><p>need to do off policy evaluation I use</p><p>weighted importance sampling with</p><p>bootstrapping and at the end we have a</p><p>hundred policies and associated miri</p><p>words so we choose the best policy with</p><p>the past memory world</p><p>the algorithm we use is the first one is</p><p>the physicians optimal policy the</p><p>dynamic programming policy with</p><p>estimated probability transition matrix</p><p>and reward matrix and the standard IQ</p><p>policy the weighted importance sampling</p><p>is for the off policy evaluation so we</p><p>have the behavior policy which generates</p><p>the sample we have and we have with our</p><p>learned policy PI one and we want to</p><p>estimate the value of Taiwan from the</p><p>data trajectories generated by PI zero</p><p>for each trajectory in the sample data</p><p>we define this likelihood ratio as the</p><p>ratio between how likely this sample can</p><p>appear in the four by following the</p><p>learned policy and the likelihood of</p><p>this sample so if showing up using the</p><p>behavior policy and then we</p><p>and then we define the weighted</p><p>importance sampling estimator as this</p><p>this part is the true reward of this</p><p>trajectory and this is basically the</p><p>likelihood ratio between how lightly it</p><p>is appeared in the learn the policy</p><p>world behavior policy and finally this</p><p>one is the estimator for one trajectory</p><p>so there are n trajectories in the</p><p>entire data set so we take the average</p><p>and then it&rsquo;s the value of this learn</p><p>the policy now this is the convergence</p><p>of Q learning we can see that after few</p><p>steps the the difference between</p><p>sequential a Q value and also the</p><p>variance decrease and this is the Q</p><p>table we learned for different policies</p><p>we can see that the physician policy</p><p>concentrates on five actions</p><p>so this the x-axis represents the</p><p>actions taken and the y axis corresponds</p><p>to the different states and you can see</p><p>that for action 0 5 10 15 this is the</p><p>action code it corresponds to very</p><p>oppressive zero that is the low low low</p><p>low as the dosage of vasopressor</p><p>it shows that the part of physicians</p><p>policy prefer low value of vasopressor</p><p>but the DP policy and Q learn is more</p><p>diffused over these different actions</p><p>this plot shows the distribution of</p><p>different actions over 750 States again</p><p>we can see that this is the vasopressor</p><p>dosage</p><p>the physicians policy many focus the</p><p>know vessel pressure dosage but the DP</p><p>policy and the q-learning also suggests</p><p>higher vessel pressure dosage and the</p><p>lower dosage of IV fluids we also</p><p>compared the value of different policies</p><p>trajectories so this the mean reward for</p><p>the DP policy is this value and for the</p><p>Cure policy&rsquo;s despair value we can see</p><p>that the Q policy gives higher reward</p><p>average on average and also this the x</p><p>axis corresponds to single trajectories</p><p>we</p><p>so the x-axis corresponds to 50</p><p>different trajectories so we can see</p><p>that for each trajectory</p><p>so basically it shows that pure learning</p><p>gives much better result and so the</p><p>fundamental challenges arise from the</p><p>fact that we work we are working with</p><p>observational data so we have limited</p><p>sample size and also we have to work</p><p>with off policy algorithm and how do we</p><p>estimate the policy from samples</p><p>generated by our different policy and in</p><p>the medical application</p><p>there&rsquo;s also partial observation there</p><p>may exist confounding factors and also</p><p>how to design reward functions so that</p><p>we can encode a domain knowledge to help</p><p>guide a eye physician to like a better</p><p>decision and future works is also</p><p>related to these problems</p><p>so I immediate thing we can do is we do</p><p>like more sophisticated data</p><p>interpolation for example with Gaussian</p><p>process so in that case we can sample</p><p>like more frequently in this case we can</p><p>get more data points then we can use</p><p>some more sophisticated method and also</p><p>there&rsquo;s like model-based LR if we can</p><p>estimate the environments then we can</p><p>simulate this more samples then we can</p><p>also again use that some deep R for the</p><p>algorithm and another 2 is related to</p><p>off policy evaluation and the reward</p><p>design so that&rsquo;s all thank you</p><p>any questions</p><p>hi thank you for your talk this is very</p><p>interesting can you go back to the</p><p>example you gave with regards to</p><p>vasopressors yeah okay great um do you</p><p>know if there&rsquo;s anything that you&rsquo;ve</p><p>seen in the mimicry literature about</p><p>this behavior by the doctors I&rsquo;m just</p><p>curious</p><p>it&rsquo;s like if this has been mentioned if</p><p>you men saw this mentioned either in in</p><p>like the original paper that you were</p><p>citing from yeah nature digital medicine</p><p>or like you know something with about</p><p>this kind of behavior I mean because I&rsquo;m</p><p>assuming that if you don&rsquo;t have even if</p><p>you have a different reward like</p><p>something like this when you&rsquo;re taking</p><p>what the action you might take this</p><p>might have something you might see</p><p>something related to this I&rsquo;m just kind</p><p>of curious about how you would interpret</p><p>this sort of behavior right so in the</p><p>lateral paper they actually mentioned</p><p>that according to some like medical</p><p>research they also found that the</p><p>doctors tends to use nest vasopressor</p><p>dosage and this case is also like active</p><p>research area in the medical field</p><p>how do you come up with rewards you know</p><p>you had chosen positive and negative 50</p><p>if you choose other things will to</p><p>behave will the policy change yes I</p><p>think so because this we encode like</p><p>three different things if we increase</p><p>like the magnitude of the final four the</p><p>outcome at the final step then the wait</p><p>for the is to like the vital normal</p><p>range of vitals and sharp changes will</p><p>like discrete the weight of the mood is</p><p>great so it&rsquo;s a very well it&rsquo;s a good</p><p>reward or not right yeah that&rsquo;s a very</p><p>good question I think for now mmm I</p><p>think for now it many depends on the</p><p>doctors domain knowledge right for war</p><p>we can experiment with the observational</p><p>data to see like to do some back testing</p><p>on the observation data to choose the</p><p>like the pasture design so it&rsquo;s a</p><p>combination of domain knowledge and some</p><p>testing very cross validation awesome</p><p>I went to a admin conference at Stanford</p><p>some time ago and there was a researcher</p><p>who&rsquo;s arguing for doctors against using</p><p>black box models in medicine I saw some</p><p>visualization last black box or because</p><p>you&rsquo;re definitely able to see some</p><p>values inside of the model so it&rsquo;s not</p><p>black box right mm-hmm right so for the</p><p>reward design you can actually put some</p><p>domain knowledge in there so it&rsquo;s not</p><p>that black box and then because this</p><p>works with tabular learning so it&rsquo;s like</p><p>the easier I think it&rsquo;s the easiest</p><p>algorithm on the RL so it can offer some</p><p>insights into the data in the situation</p><p>but I think if we want to use like more</p><p>like policy gradient or DDP tea then it</p><p>may not be able to offer some intuitive</p><p>interpretations but this is also an</p><p>active research area</p><p>[Applause]</p><p>[Music]</p><p>[Music]</p><p>can everyone hear me mmm</p><p>great well welcome everyone thank you</p><p>all for coming this evening my name is</p><p>Edgar and I&rsquo;m going to be talking about</p><p>knowledge distillation for transformer</p><p>language models just to give you an</p><p>overview what I won&rsquo;t be talking about</p><p>I&rsquo;ll introduce the transformer it</p><p>successes and limitations give you some</p><p>background about knowledge distillation</p><p>give you an interpretation of what</p><p>knowledge distillation can mean in this</p><p>context</p><p>I&rsquo;ll give you guys the approach that I</p><p>took I line some future work that can be</p><p>done and I&rsquo;ll have some time for</p><p>questions so the transformer model is</p><p>the latest and greatest one of the</p><p>latest and greatest in neural networks</p><p>that I&rsquo;ve just come out it&rsquo;s very</p><p>powerful it can generate language like</p><p>human being it can answer questions he</p><p>can summarize texts and so much more</p><p>this was very powerful and very exciting</p><p>to me because I wanted to make resources</p><p>for people who might not necessarily</p><p>have resources and one of the ways to</p><p>provide resources people as is through</p><p>human language and so when I was</p><p>starting to play with these models I</p><p>went into some trouble and that&rsquo;s that</p><p>they&rsquo;re huge</p><p>my little MacBook was huffing and</p><p>puffing and crashing with memory and so</p><p>I couldn&rsquo;t imagine what would happened</p><p>if I try to run one of these on my phone</p><p>and so I was hoping to try to compress</p><p>these models and make them smaller while</p><p>still having similar levels of</p><p>performance and it&rsquo;s going to give you</p><p>give you guys a look like introduction</p><p>to the transformer but my peers have</p><p>done such a good job I&rsquo;ll skip past that</p><p>and tell you guys about what knowledge</p><p>distillation is so knowledge</p><p>distillation is getting a larger well</p><p>trained teacher neural network to teach</p><p>smaller and untrained student neural</p><p>network by getting the student to mimic</p><p>the outputs of the teacher if we say</p><p>that we give them both models the same</p><p>input and they give about the same</p><p>output you say that</p><p>student model is performing about just</p><p>as good as the teacher even though it&rsquo;s</p><p>smaller and in this specific context</p><p>something that what happens when we do</p><p>knowledge distillation is we give the</p><p>neural network a sequence of words and</p><p>we mask some of them and usually we have</p><p>a loss function because and when the</p><p>transformer spits out its output it</p><p>gives a probability distribution over</p><p>words and the goal is to try to guess</p><p>the word that is being masked and you</p><p>want to minimize this so you want to get</p><p>the word right but in knowledge</p><p>distillation we&rsquo;re able to provide the</p><p>student Network so much more information</p><p>by giving it the output of the teacher</p><p>so in this case we the output of the</p><p>teacher is a distribution over words</p><p>probably the ability distribution as</p><p>opposed to just a label which is one for</p><p>the correct word and zero for the others</p><p>and so what this distribution does is</p><p>give a probability over reasonable words</p><p>which will distill some knowledge into</p><p>the smaller Network so the man went to</p><p>the blank the right word is store but if</p><p>the larger neural network is able to</p><p>give the student the information that</p><p>another possible reasonable word is</p><p>groceries or bakery or and any other</p><p>synonym the student will be able to</p><p>learn a lot more and hopefully be able</p><p>to spit out the similar same</p><p>distribution for other words and some</p><p>more interpretation as to what this</p><p>means we have this idea that the neural</p><p>networks have a like certain solution</p><p>space and if the if we believe that we</p><p>can distill knowledge to the small</p><p>network we believe that the smaller</p><p>still</p><p>Network can inhabit a similar solution</p><p>space as a as a teacher and we see that</p><p>happening and sometimes that happens and</p><p>sometimes that doesn&rsquo;t so to give you an</p><p>example of when the case where and like</p><p>a function can inhabit similar solution</p><p>space to another function but it doesn&rsquo;t</p><p>necessarily do that as in this simple</p><p>case of curve fitting the green curve</p><p>can encompass every single which is a</p><p>much higher order polynomial can</p><p>encompass every single solution and</p><p>represent this function and so much more</p><p>because it has all these higher-order</p><p>terms but it usually doesn&rsquo;t even know</p><p>it&rsquo;s solution space is much larger and</p><p>that&rsquo;s because it doesn&rsquo;t have the right</p><p>inductive biases to find that solution</p><p>that we&rsquo;re looking for in the current</p><p>way that we train that model and so the</p><p>hope for the student and the teacher</p><p>transformer models the idea is that the</p><p>larger transformer can represent a</p><p>larger solution space than the smaller</p><p>one and the larger transformer converges</p><p>to a particular solution space and the</p><p>smaller transformer converges to another</p><p>different solution space and by training</p><p>these the smaller network on the larger</p><p>network we get the like region of</p><p>convergence in solution space to get</p><p>more closely to where the larger model</p><p>is so that they can represent the symbol</p><p>or if not same transformer by spinning</p><p>out the same outputs despite being a</p><p>different model with different weights</p><p>and different sizes and so my approach</p><p>to this was taking births the</p><p>bi-directional transform</p><p>and instead of having a either 24 layers</p><p>or 12 layers with 9 layers and I wanted</p><p>to reduce the dimension of the vectors</p><p>that it works with to 576 as opposed to</p><p>768 or 1024 and this initial approach</p><p>true proved to be awful actually</p><p>initially I had a accuracy of about 5%</p><p>even though I trained for 30 to GPU days</p><p>and upon closer inspection it kind of</p><p>makes sense because I ended up with</p><p>about 72 terabytes of data Wikipedia was</p><p>initially 12 gigabytes of text but by</p><p>saving the outputs of the student or of</p><p>the teacher their own network I ended up</p><p>with a ridiculous amount of data and too</p><p>much data for the GPU it was not able to</p><p>handle long sequences and that&rsquo;s because</p><p>for as opposed to each word having a</p><p>label I ended up with you have this huge</p><p>vector giving a probability distribution</p><p>for over words which is a 3522 size</p><p>vector and so I had to think of</p><p>something different because it wasn&rsquo;t it</p><p>wasn&rsquo;t going too well and so I thought</p><p>about the outputs of that teacher neural</p><p>network and realized that most of those</p><p>most of the information in that</p><p>distribution is not very useful if we</p><p>think about a masked word and consider a</p><p>30,000 552 vocabulary most of those</p><p>words are not going to be sending them</p><p>as most of them are not going to be</p><p>important and upon looking at them I</p><p>most of them had a dish probability of</p><p>like 2 to the negative 8 or something</p><p>very small not useful so I decided to</p><p>instead truncate the output of that</p><p>teacher neural network and only consider</p><p>10 words which left me with 384</p><p>gigabytes of data instead</p><p>and with the time that I had which was</p><p>about eight GPU days I was I was able to</p><p>get an increase to about a 7% and so</p><p>this leaves an exciting work of scaling</p><p>this the the big dogs</p><p>some of them sitting right here train</p><p>their neural network for 2048 TPU days</p><p>or 240 GPU days which is which is a lot</p><p>and so it leaves a very exciting</p><p>opportunity to scale up this task and</p><p>see if we can have a similar performance</p><p>so I like to thank my mentor</p><p>Susan Zhang wonderful guidance the rest</p><p>of the open area scholars or provided me</p><p>with a lot of support and Maddy Hall who</p><p>was able to organize all of us despite</p><p>juggling like 20 other things at the</p><p>same time thank you</p><p>[Applause]</p><p>no questions no questions okay</p><p>you</p><p>thank you so much for the support</p><p>everyone here listening to us okay and</p><p>thank you so much for a I for this</p><p>propionate for this opportunity</p><p>especially for my mentor okay Akuma Ram</p><p>and so before I start I&rsquo;m going to talk</p><p>about myself so I was a support engineer</p><p>in my previous life I was doing network</p><p>security and after that I decided to</p><p>move to education and I have been doing</p><p>tons of things for education right now</p><p>actually I started a program as you as</p><p>soon as I shown open air I to try to</p><p>convince 60 plus year old people that AI</p><p>is important is just for Spanish</p><p>speakers and that&rsquo;s a picture of the</p><p>group right now one of the guys Rodrigo</p><p>is taking the lead</p><p>teaching them tensorflow to Udacity so</p><p>I&rsquo;m very proud i&rsquo;ve also do other things</p><p>i do kids and teachers that&rsquo;s me at</p><p>columbia showing the IGBT to unicorn</p><p>example to some very interested teachers</p><p>and and so my motivation behind this</p><p>project is that I think education is</p><p>completely broken I think it was badly</p><p>designed right now schools and and the</p><p>best way for us to learn and this is</p><p>what science is telling us is to do</p><p>projects right the way that we&rsquo;re doing</p><p>this program is to go there dive deep</p><p>into a subject and do something that is</p><p>meaningful for us and this apply</p><p>knowledge but the schools cannot use it</p><p>why I actually am on the founding team</p><p>of a school in New York because there is</p><p>no there&rsquo;s no curriculum for</p><p>project-based learning there is no time</p><p>for the teachers to learn how to do</p><p>themselves projects and if you have many</p><p>projects in a school in a classroom many</p><p>kids one project is a mess in the</p><p>classroom and the poor teachers are</p><p>actually implementing these things in</p><p>the classroom I spend tons of times</p><p>looking for resources on the web</p><p>adapting them and trying to like you</p><p>know customize into a classroom so right</p><p>now AI in education is just tracking is</p><p>just trying to see how we can</p><p>personalize learning but what if we can</p><p>actually unlock the educational content</p><p>in the web so this is a proof of concept</p><p>for that so this is my model it&rsquo;s richer</p><p>so creature what it does is that it was</p><p>trained on data from the Exploratorium</p><p>here in San Francisco actually they have</p><p>a great data set of activities there are</p><p>kind of projects that are used just for</p><p>learning and inquiry and we have 3700 of</p><p>these activities so I trained Bert</p><p>oh I&rsquo;m not going to explain there</p><p>because I think so when we form a little</p><p>tree that already did that so I trained</p><p>bird to actually use another data set</p><p>that has 280,000 projects DIY projects</p><p>meaning like how to build a bicycle how</p><p>to cook and chill a bus or how to create</p><p>a robot that will tell me if I need to</p><p>water my plants to tell you what</p><p>products you need to do to learn</p><p>something in particular like in this</p><p>case biology so what did I chose</p><p>Instructables so you know I have to</p><p>collect my own data which Jesus got but</p><p>the thing is that instructable is a</p><p>marvelous website with very well</p><p>documented project but the goal of this</p><p>is not to learn is to make something</p><p>that is functional so if you learn if</p><p>you put in the search like I want to do</p><p>biology</p><p>injures return to 12 projects and three</p><p>of them are actually not related to</p><p>biology but they have great great</p><p>projects so with creature we can</p><p>actually get 200 projects and the</p><p>weather right because of the weather I</p><p>trained it we can actually get project I</p><p>have explanations on them so I&rsquo;m just</p><p>going to read the first paragraph of</p><p>this project it says what does just your</p><p>heart sounds like how fast is it beating</p><p>what&rsquo;s the pattern a few students may be</p><p>able to detect movement will whomp part</p><p>of the beat which I got probably heard</p><p>in movies too this has to do with the</p><p>fact that I were hard has four chambers</p><p>which we will get to soon this comes</p><p>from two pairs of bolts in the chambers</p><p>of the heart</p><p>so this instructor wall was actually</p><p>meant to be done for like my students</p><p>and we have like many of these in the in</p><p>the search so right now the model is</p><p>doing 30 to 50 percent depend on the</p><p>labeled I have explanations from those</p><p>two to hundreds so how did I do this so</p><p>the way that I did it is I took all the</p><p>data from the text of all the activities</p><p>from Instructables and I divided between</p><p>four things descriptions instructions</p><p>explanations and tools and materials and</p><p>with that I trained bird to actually</p><p>predict what will be the learning topic</p><p>so each of these inspiratory on projects</p><p>they have some labels of what would you</p><p>learn and I end up having 50,000</p><p>examples I trained bird for like Tupac 9</p><p>to the minus 5 learning rate and I got I</p><p>took</p><p>20% of that for like validation I got</p><p>very good accuracy so I decided to try</p><p>it on destructible data so in this truck</p><p>table data again I took the task I chunk</p><p>it into a 512 tokens and then I predict</p><p>with that</p><p>what would you learn to do it so it was</p><p>good the result was kind of good but</p><p>then I you like develop this interface</p><p>to do active learning so what I do with</p><p>this is that I get the chunk of the text</p><p>that I actually match and then I can say</p><p>if it&rsquo;s like positive or negative and</p><p>with this I improve it a lot and you</p><p>know together a bit more validation I</p><p>use the own app library that is a</p><p>reduction the dimensions reduction</p><p>library to try to see what&rsquo;s happening</p><p>under the hood so this is a</p><p>visualization of the topic physics we</p><p>can see on the purple we can see</p><p>something here so on the purple one</p><p>that&rsquo;s mechanics on the blue one that&rsquo;s</p><p>electricity and magnetism and between</p><p>them kind of sound is bridging those two</p><p>clusters the orange is light and there</p><p>is like a tiny little bit green there</p><p>that is intersecting with with soundest</p><p>waves so actually wave somehow are very</p><p>far from sound but it&rsquo;s very interesting</p><p>this is like a lien under interesting -</p><p>like exploring what&rsquo;s happening with</p><p>these embeddings so I hope that by now I</p><p>convince you that AI can be used for</p><p>something else besides tracking in</p><p>education I would be very interested in</p><p>trying to do a web app what you can also</p><p>filter these projects by tools and time</p><p>that you have this was my eighth</p><p>experiment on the rest didn&rsquo;t work very</p><p>well but one of them that was promising</p><p>is trying to see what is the expertise</p><p>on these projects and I use another data</p><p>set from like hackster so I&rsquo;m very</p><p>interested in like developing levels of</p><p>expertise on these kinds of things and</p><p>so I&rsquo;m there I now look into something</p><p>to keep feeling my creature so if you</p><p>know about penny grant or something like</p><p>us to do with AI own education I would</p><p>be very interesting to hear it that&rsquo;s it</p><p>thank you</p><p>[Applause]</p><p>did you ever think about using video</p><p>yeah yeah so I actually that was one of</p><p>my first options trying to use video</p><p>especially from like TEDx a they have</p><p>like very cool content but I started</p><p>using pictures actually and it wasn&rsquo;t</p><p>very promising so then I got a little</p><p>bit like scared about trying video but I</p><p>think that&rsquo;s one of the things that I</p><p>would like to try</p><p>you</p><p>from a paper by a Borgia that looks at</p><p>all of the different gang metrics that</p><p>we have he evaluates over 24</p><p>quantitative measures and five</p><p>qualitative measures all of which are at</p><p>times used to evaluate ganz none of</p><p>which are optimal and none of which are</p><p>the tolkien&rsquo;s metric that can tell us</p><p>everything we need to know so the two</p><p>most quoted scores you&rsquo;ll find around</p><p>ganz are the inception score and the</p><p>fresh iron scepter distance both of them</p><p>are based on state-of-the-art</p><p>classifiers inception by Google and they</p><p>look out in the inception scores case</p><p>they look at marginal entropy of or</p><p>marginal distribution of labels in the</p><p>fresh eye inception distance case they</p><p>look at taking real and fake samples</p><p>putting through inception layers and at</p><p>the max pool three of v3 and look at the</p><p>difference between men and co-variants</p><p>it gives us an idea but you come out</p><p>with a scalar value so if I said to you</p><p>I have a Gann that&rsquo;s got a scalar value</p><p>of eight and one with 40 I&rsquo;m not sure</p><p>you could tell me kind of what that</p><p>means other than a lower scores better</p><p>in third similarly in March</p><p>researchers from here at open AI and</p><p>Google released the activation Atlas</p><p>which gives us an unprecedented ability</p><p>to look inside neural networks and</p><p>visualize what&rsquo;s going on inside the</p><p>back black box so for my project I</p><p>wanted to look at what would happen if</p><p>we tried to bring these two things</p><p>together and evaluate ganz using the</p><p>activation Atlas so in terms of the</p><p>project I needed a whole ton of samples</p><p>thankfully big and the mammoth and</p><p>current state of the art Gann released</p><p>their fully trained image net generators</p><p>and so I was able to use began as one</p><p>set of samples and then SN Gann which is</p><p>the spectral normalization gang created</p><p>by me otto and others was state of the</p><p>art a few years ago and you know brought</p><p>a lot to the field especially around</p><p>normalization and it had a fydd score of</p><p>around 27 and they also open sourced</p><p>their fully trained image net generator</p><p>and so I was able to do it so we&rsquo;re</p><p>considering here real images from image</p><p>net and Gann samples from began with a</p><p>fydd score around eight point seven and</p><p>scan samples from SN gam with a fydd</p><p>score around 27 to see what we can find</p><p>I then took the inception v1 Network I</p><p>used v1 to be consistent with the work</p><p>of the original activation Atlas paper</p><p>for clarity I should note that the</p><p>fresher inception distance is most often</p><p>cow</p><p>headed on the v3 Network however began</p><p>actually used v2 for their calculations</p><p>and SN GaN used only 5000 samples for</p><p>their so it gives you a sense of how</p><p>difficult metrics are in this space</p><p>having done that</p><p>passing all of these samples through the</p><p>inception v1 network across nine layers</p><p>and capturing activations your hundreds</p><p>of thousands of amped activations right</p><p>massive amounts of dimensions to bring</p><p>that down to something that we can kind</p><p>of comprehend in at least I can used you</p><p>map to dimensionality reduce to 2d and</p><p>having done that we can then apply a</p><p>grid over our dense scatter plot to find</p><p>meaningful kind of grid clusters having</p><p>done that we then need to generate icons</p><p>for each of these grid cells so a grid</p><p>cell will contain a number of related</p><p>activations and the part that really</p><p>brings the activation to life activation</p><p>Atlas is creating these feature</p><p>visualizations which give you a visual</p><p>view of a direction in activation space</p><p>having done that we can then also</p><p>calculate the KL divergence between</p><p>distributions right because these grid</p><p>cells give us a sense of density between</p><p>the different distributions and then</p><p>lastly we can create some atlases and in</p><p>this case in contrast to the original</p><p>activation Atlas each grid cell in the</p><p>Gantt &rsquo;less will not only have the</p><p>feature visualization of the grid cell</p><p>but also a bounded colored order which</p><p>gives you an indication of the</p><p>log-likelihood between two distributions</p><p>so this particular example is comparing</p><p>began to SN gown and the blue dark blue</p><p>around the border means that in this</p><p>square began has a much higher density</p><p>it also has at the top the count of the</p><p>samples in the square so you can see how</p><p>many instances of samples from began</p><p>versus SN gamma in that Square and on</p><p>the bottom we have an indication of the</p><p>maximum logit which can give you an</p><p>indication of what that grid cell is</p><p>doing in regard to softmax at an indica</p><p>t&rsquo;v level lastly you&rsquo;ve also got a grid</p><p>spell reference which I find really</p><p>helpful when we&rsquo;re trying to talk about</p><p>these R squares and identify them so</p><p>just very briefly I&rsquo;ve implemented</p><p>visualization of these icons very much</p><p>in keeping with the original page</p><p>and found like them that in regularizing</p><p>these icons it was really whitening as</p><p>you can see on the far right of the</p><p>slide here that did the most to ensure</p><p>that we had as much detail and kind of</p><p>visual fidelity as possible so what did</p><p>I find in terms of the difference</p><p>between all of these distributions and</p><p>again don&rsquo;t worry about getting the</p><p>numbers on here but broadly what we find</p><p>was really interesting is if you</p><p>compared began to image net layer by</p><p>layer the biggest divergence between</p><p>those two distributions actually occurs</p><p>right at the beginning of the network in</p><p>mixed 3a and 3b which is kind of</p><p>surprising in a way to me because early</p><p>on in an inception network the network</p><p>is looking at very low level detail</p><p>right it&rsquo;s looking for shapes texture</p><p>kind of brightness and so forth and one</p><p>would presume that that might be very</p><p>very similar no matter which</p><p>distribution you&rsquo;re putting through and</p><p>I&rsquo;ll talk a little bit about what that</p><p>may indicate then if you wanted to look</p><p>at where do we see the biggest</p><p>difference between big an and SN Y and</p><p>if we&rsquo;re trying to compare how good</p><p>those two distributions are where do we</p><p>see that well actually if we change the</p><p>heat map to remove those first layers we</p><p>can better see the differentiation</p><p>that&rsquo;s not being kind of watered down by</p><p>the first layers and actually it&rsquo;s right</p><p>there at mixed 5b at the end and the</p><p>bright red SN GaN really in the later</p><p>layers doesn&rsquo;t match the image net</p><p>distribution nearly as well as began and</p><p>that&rsquo;s where we can see some of the</p><p>differentiation so what I&rsquo;d like to do</p><p>now is just give you a few examples from</p><p>mixed 3a and mixed 5b to bring it to</p><p>life so this is the activation Atlas 40</p><p>by 40 began versus imagenet for D first</p><p>layer in the network and what you can</p><p>see is that each of the icons is not an</p><p>image that you might recognize right</p><p>we&rsquo;re not seeing dogs or cats at this</p><p>level of the layer this is where we see</p><p>texture but what you saw is on one side</p><p>of the map we have that blood-red area</p><p>right a huge divergence and so if I look</p><p>at cell say 1511 in mix 3a what we saw</p><p>there is if you can see there&rsquo;s 1300 and</p><p>two samples from imagenet that fit into</p><p>that grid square but zero samples from</p><p>big Dan actually go into the grid</p><p>squares we&rsquo;re seeing already a huge</p><p>divergence</p><p>and when you look at the data set</p><p>examples of what the network is picking</p><p>up in that grid square it&rsquo;s looking for</p><p>highly detailed textural areas you can</p><p>see it&rsquo;s not just grass up the top with</p><p>a very cute puppy that wasn&rsquo;t actually</p><p>the spatial activation the grass is like</p><p>long fronds of grass right at the base</p><p>you can see on the far right a placemat</p><p>and a manhole cover with very detailed</p><p>intricate texture and actually the two</p><p>labels most represented in this grid</p><p>cell were hay and manhole covers manhole</p><p>covers surprised me at first but when</p><p>you look at them there&rsquo;s actually it&rsquo;s</p><p>over there right columns second from the</p><p>bottom</p><p>manhole color has actually have a lot of</p><p>texture in the ash felt around the</p><p>manhole cover itself and on the metal of</p><p>the manhole cover so we&rsquo;re seeing a lot</p><p>of texture so the question for me is</p><p>we&rsquo;ll then where did all the big and hay</p><p>and manhole covers go right where would</p><p>they be and so it actually led me to go</p><p>all the way to the other side of the map</p><p>to sell 20:32 and that&rsquo;s where you start</p><p>to see data set examples from big gun</p><p>and a divergence between began and image</p><p>nets so you&rsquo;ve got way more big gun</p><p>examples in this cell than image net and</p><p>what we can see are a lot of the same</p><p>themes so we&rsquo;ve got hey we have grass in</p><p>the bottom you even see a gray sweater</p><p>which is kind of evoking that same idea</p><p>of really detailed gray texture but the</p><p>fidelity of the texture here and the</p><p>level of detail is much less and so</p><p>already the activation atlas has shown</p><p>how the network is massively kind of</p><p>separating these and I think at 3a you</p><p>see both began and SN GaN exhibit this</p><p>same kind of quality and I think it&rsquo;s</p><p>telling us more about the difference</p><p>between the fidelity of images</p><p>especially Gann samples I was putting</p><p>through a 128 resolution and if you took</p><p>say the 512 began I&rsquo;d be interested to</p><p>see how this may change so then the</p><p>question I think was really interesting</p><p>is can the activation atlas tell us</p><p>something more about how big Dan differs</p><p>from SN Gunn where where do we see that</p><p>and for that we need to look at layer 5b</p><p>which is right at the end of the network</p><p>before you hit a soft max before it&rsquo;s</p><p>finally classifying and so for this one</p><p>I&rsquo;m showing the activation Atlas at</p><p>mixed 5b 40 by 40 grid the coloring is</p><p>showing you in</p><p>red cells where SN Gann has a much</p><p>higher density in blue cells where big</p><p>down has a much higher density right so</p><p>what&rsquo;s this going to tell us well the</p><p>first thing that sent me to was cell 223</p><p>which has a very big density divergence</p><p>and what&rsquo;s interesting about this cell</p><p>is it&rsquo;s looking for the Intel Busha dog</p><p>which I didn&rsquo;t know but is like a swiss</p><p>mountain dog very brown white and black</p><p>and it has a very specific look and in</p><p>this cell there&rsquo;s only about 20 to 24</p><p>examples of both imagenet and began but</p><p>they&rsquo;re very well-structured well-formed</p><p>shapes with very clear details and</p><p>snouts an interesting in this cell SN</p><p>Gynt Dan didn&rsquo;t manage to have any of</p><p>its samples recognized and I think what</p><p>we&rsquo;ll see as we go through some of these</p><p>examples is the same thing playing out</p><p>began has both a self attention</p><p>mechanism which allows it to maintain a</p><p>much better sense of global structure as</p><p>well as the mass scale of the batch</p><p>sizes that put through allows it to</p><p>maintain not just good texture in its</p><p>samples but also global structure so if</p><p>you go to cell 35 20 and we look at</p><p>cocktail shakers something that I don&rsquo;t</p><p>think we talked about nearly enough when</p><p>we look at Gans um but this is a really</p><p>interesting cell for me because this 328</p><p>began our samples in here in this cell</p><p>image met hits a similar kind of density</p><p>around 269 but SN gain only has 61</p><p>samples picked up and if you look to the</p><p>far right of the slide which are the SN</p><p>gain samples you can see that it&rsquo;s</p><p>struggling to maintain the outline shape</p><p>that structure and only kind of picking</p><p>up on some of the shape cues and if you</p><p>go to cell 21 22 now I&rsquo;m not here to</p><p>tell you that began are solved the</p><p>uncanny valley but this cell is looking</p><p>at melo or one piece bathing suits and</p><p>what i find fascinating about how this</p><p>has been picked up by the atlas is that</p><p>both image netting began in this cell</p><p>have around about 150 samples which all</p><p>represent the bikini or one-piece</p><p>bathing suit right and if you think</p><p>about an image and it turns what it&rsquo;s</p><p>looking for its looking for some sans</p><p>water a lot of human skin in the image</p><p>right and maybe a piece of material SN</p><p>gain manages to hit those themes but</p><p>they&rsquo;re very textural with very little</p><p>shape and distinction in contrast began</p><p>and</p><p>and I use this yes you can actually see</p><p>it&rsquo;s actually hit the shape of a</p><p>triangle bikini and a one-piece male Oh</p><p>which in comparison to early Ganz which</p><p>could only really do skies and broad</p><p>textural things the ability to pick out</p><p>individual shapes in such a complex area</p><p>is pretty phenomenal and you&rsquo;re seeing</p><p>that play out so one of the questions I</p><p>then had the ever ending one is well</p><p>where on earth did all the SN Ganz</p><p>apples go right if we&rsquo;ve got all of</p><p>these cells where began is that the same</p><p>sort of density as imagenet and doing</p><p>well in a distribution term where did SN</p><p>going go and that sent me to the bottom</p><p>of the map to cell 316 weasel cell so</p><p>the weasel cell here is looking for</p><p>things that are kind of white or caramel</p><p>and fluffy and don&rsquo;t have sharp edges or</p><p>very defined kind of structure you see</p><p>things like polar bears turning up the</p><p>second image there on image net might be</p><p>a little confusing because it&rsquo;s a</p><p>scooter but the part of the image that</p><p>was spatially activated was actually a</p><p>fluffy and sheepskin rug at the back of</p><p>the scooter and that&rsquo;s what it picked up</p><p>on so that&rsquo;s the sort of thing you see</p><p>in this cell and both began an image net</p><p>have you know densities there SN gaining</p><p>contrast has two x they&rsquo;re not the</p><p>density of image net in this cell and</p><p>while at first you see some of the</p><p>similar sorts of themes polar bears and</p><p>fluffiness if you actually look through</p><p>the rest of the data set examples you</p><p>start to see and so it&rsquo;s quite small</p><p>here but what you actually see are a</p><p>whole lot of examples from non angora</p><p>fluffy classes whether it&rsquo;s dogs or even</p><p>inanimate objects like buildings that</p><p>have so much artifact and kind of</p><p>overexposure in the image that they&rsquo;re</p><p>lumped in here and picked up by the</p><p>classifier in this area and so for me</p><p>what&rsquo;s what&rsquo;s really interesting here is</p><p>that it helped for me characterize a</p><p>couple of things one what on earth does</p><p>a fydd score of eight versus a fydd</p><p>score of 27 mean right at one level I</p><p>know that the mean and the covariance</p><p>are the distributions getting closer at</p><p>seven but well it&rsquo;s also kind of helping</p><p>me see here is okay if I look across the</p><p>activation Atlas well began and its</p><p>shape capacity and fidelity is allowing</p><p>it to hit many of the same grid squares</p><p>in a similar issue density to image net</p><p>SN gown is hitting some</p><p>and then having these clumps of clusters</p><p>in kind of amorphous areas and that&rsquo;s</p><p>helped helping to do it the other thing</p><p>I found really fascinating about using</p><p>the Atlas to understand it is when I</p><p>first created all the samples for SN</p><p>Gann honestly I freaked out a bit</p><p>because I look to them he&rsquo;s not good</p><p>enough this doesn&rsquo;t look like SN Gann</p><p>and what I realized is I was used to</p><p>looking at SN Gann samples in contact</p><p>sheets that showed me ten images all</p><p>from one class at a time and my human</p><p>eye when seeing ten images all from the</p><p>one class at once would infer structure</p><p>and infer a kind of meaning in those</p><p>images that the classifier may not well</p><p>pick up and so that&rsquo;s something I</p><p>thought was really interesting about</p><p>using this as a way to guide which</p><p>samples to investigate and so forth</p><p>because there&rsquo;s always an element of</p><p>going back to the samples in Ganz and so</p><p>this can help accelerate that so lastly</p><p>future directions</p><p>obviously there&rsquo;s about 81 activation</p><p>atlases if you think of a number of grid</p><p>cells number of sorry grid size by date</p><p>of distributions by options so working</p><p>on an interface that would allow us to</p><p>do that we can extend samples some non</p><p>Gans I&rsquo;d love to put the spaz</p><p>transformer through this there&rsquo;d also be</p><p>an opportunity I think to keep samples</p><p>the same but actually referred</p><p>discriminator out of again and actually</p><p>use that and sort of track how different</p><p>samples are being evaluated by a</p><p>discriminator through and similar to the</p><p>MIT sale work on a gander section it</p><p>would be great to take a generator and</p><p>look layer by layer how is that</p><p>generator doing what it does so lastly I</p><p>do want to thank everybody here thank</p><p>you first angel the end of my talk but</p><p>thank you to open AI Christy and Maddy</p><p>amazing and helpful and to Chris Ola and</p><p>Ludvig Schubert from the clarity team</p><p>have been amazingly open with not just</p><p>their time but their knowledge and</p><p>wisdom which has been phenomenal and</p><p>it&rsquo;s been a blast my fellow scholars so</p><p>thank you</p><p>[Applause]</p><p>how do you how do you pick cell 223 yeah</p><p>so I tried as much as possible not to</p><p>cherry-pick and just look the thing so</p><p>the the way I looked at it was to take</p><p>the log likelihood of each cell and</p><p>literally rank it and go for the ones</p><p>with the greatest divergence as the</p><p>greatest difference and interrogate</p><p>those first from both ends both from</p><p>where began was really overweight and</p><p>then where SN gown was really overweight</p><p>and compared and come at it from those</p><p>angles having done that once you&rsquo;re</p><p>inside a cell and you start seeing</p><p>something like you know all the hey</p><p>labels seem to be here then I would sort</p><p>of say okay where are all the hey labels</p><p>for the alternate distribution because</p><p>that can give us an idea of almost for</p><p>mix 3a at least I always think about it</p><p>as twin grid cells right like how are</p><p>these related and trying to look into it</p><p>that way does that help</p><p>so with big yeah I know that in the</p><p>paper they have like a truncated yeah</p><p>the truncation trick if non-tariff is</p><p>very cool with began it&rsquo;s you can</p><p>truncate the values of the normal that&rsquo;s</p><p>used for the noise vector before they</p><p>develop the samples and if in effect if</p><p>it&rsquo;s very very load you get a small</p><p>variety of samples coming out but</p><p>amazing fidelity it&rsquo;s almost like a and</p><p>and these the guys themselves say it&rsquo;s</p><p>almost like cherry picking your very</p><p>best options or if the truncation is</p><p>very high you end up with large variety</p><p>not so great samples these were all done</p><p>at point five truncation and a midpoint</p><p>I did some early work on looking at the</p><p>truncation and you kind of saw</p><p>gratifyingly what you might expect which</p><p>is as the truncation blew out it just</p><p>kind of and decrease you everything just</p><p>kind of exacerbated right things that</p><p>were not going so well got worse and so</p><p>on and so forth but it&rsquo;s a in that</p><p>sample extension bucket and I want to do</p><p>some more that&rsquo;s fine any other</p><p>questions</p><p>where you once got a question sorry yeah</p><p>I&rsquo;m wondering independent of the</p><p>comparisons between the two ganz mhm I&rsquo;m</p><p>wondering just what were your overall</p><p>thoughts on how well began compared to</p><p>just the image net distribution yeah so</p><p>well right let me can I not make people</p><p>sick so it if you look at apologies guys</p><p>if you look at the KL divergence by</p><p>layer if we can get there so on this</p><p>slide what you can see is big and versus</p><p>image net is our first one right so if</p><p>you discount layer 3a and 3b for a</p><p>moment actually all the way through</p><p>you&rsquo;ve got pretty good kind of tracking</p><p>between image net and began and you only</p><p>see a lot of the divergence at 5b so you</p><p>I really want to rerun this with a much</p><p>higher fidelity big gun sample because a</p><p>lot of the best big and stuff that you</p><p>see those you know quintessential</p><p>examples are coming out of there 512</p><p>resolution and this is only done at 128</p><p>so I think that you would see an</p><p>improvement there and you know it&rsquo;s only</p><p>in those later layers and it&rsquo;s very</p><p>specific</p><p>unlike SN GaN where you&rsquo;re seeing kind</p><p>of broad issues with things being lumped</p><p>into the grid cells with git begin it</p><p>seems to be specific to categories those</p><p>categories that require more structure</p><p>and so forth does that help</p><p>so if everything were blue it would mean</p><p>that your kale divergence is close to</p><p>zero right which means that the</p><p>distributions are quite close the issue</p><p>with using the inception classifier</p><p>network is that saying the way in which</p><p>the classifier network understands your</p><p>images with the purpose of classifying</p><p>them they are very very close so you</p><p>have then tipped over the valance that</p><p>your swimming suit is recognizable as a</p><p>swimming suit but having a classifier</p><p>give you a complete accurate score on</p><p>being a swimming suit does not mean that</p><p>it is photorealistic to human eyes so</p><p>it&rsquo;s getting you a part of the way there</p><p>but if you want to do a comparison for</p><p>like photorealistic images classifier</p><p>alone doesn&rsquo;t do that any other</p><p>questions</p><p>[Applause]</p><p>so for people who might have thought of</p><p>questions during the break or just want</p><p>to chat a little bit more informally</p><p>with some of the scholars as well as the</p><p>mentors we will be here till around 8:30</p><p>and we can just mix and mingle over in</p><p>the living room as well as here but one</p><p>more round of applause for all the</p><p>scholars and thank you all for coming</p><p>and watching the presentations with us</p><p>you</p><p>you</p></section><footer class=article-footer><section class=article-tags><a href=/tags/english/>English</a>
<a href=/tags/video-transcripts/>Video Transcripts</a>
<a href=/tags/openai/>OpenAI</a></section></footer></article><div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><ins class=adsbygoogle style=display:block;text-align:center data-ad-layout=in-article data-ad-format=fluid data-ad-client=ca-pub-9206135835124064 data-ad-slot=1055602464></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/en/at2xkqjazns/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/AT2XkqJAZns data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>Towards Epileptic Seizure Prediction with Deep Network ÔΩú Kata Slama ÔΩú OpenAI Scholars Demo Day 2020 ÔΩú OpenAI</h2></div></a></article><article><a href=/en/jzohw-eybtq/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/JZOHW-eYBtQ data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>Introductions by Sam Altman & Greg Brockman ÔΩú OpenAI Scholars Demo Day 2020 ÔΩú OpenAI</h2></div></a></article><article><a href=/en/-fozam9xqs4/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/-FoZAM9xqS4 data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>OpenAI Five vs. OG, Game 2 ÔΩú OpenAI Five Finals (4‚ß∏6) ÔΩú OpenAI</h2></div></a></article><article><a href=/en/u9mjuukhuzk/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/U9mJuUkhUzk data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>OpenAI DevDay, Opening Keynote ÔΩú OpenAI</h2></div></a></article><article><a href=/en/lpe5gwuqa-k/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/lpe5Gwuqa-k data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>Scaling Laws for Language Transfer Learning ÔΩú Christina Kim ÔΩú OpenAI Scholars Demo Day 2021 ÔΩú OpenAI</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2021 -
2023 SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics</section><section class=powerby>As an Amazon Associate I earn from qualifying purchases üõí<br>Built with <a href=https://swiest.com/ target=_blank rel=noopener>(Ôæâ‚óï„ÉÆ‚óï)Ôæâü™Ñüíûüíñü•∞ across the glüåçüåèüåébe</a><br></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Serif&family=Noto+Serif+Armenian&family=Noto+Serif+Bengali&family=Noto+Serif+Devanagari&family=Noto+Serif+Georgian&family=Noto+Serif+Gujarati&family=Noto+Serif+HK&family=Noto+Serif+Hebrew&family=Noto+Serif+JP&family=Noto+Serif+KR&family=Noto+Serif+Kannada&family=Noto+Serif+Khmer&family=Noto+Serif+Lao&family=Noto+Serif+Makasar&family=Noto+Serif+Malayalam&family=Noto+Serif+Myanmar&family=Noto+Serif+Oriya&family=Noto+Serif+SC&family=Noto+Serif+Sinhala&family=Noto+Serif+TC&family=Noto+Serif+Tamil&family=Noto+Serif+Telugu&family=Noto+Serif+Thai&family=Noto+Serif+Tibetan&display=swap" rel=stylesheet></body></html>
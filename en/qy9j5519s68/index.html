<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Video Transcript Ôªøhello everyone I am excited to be
presenting my scholars projects I can
focus on social learning and independent
multi-agent reinforcement learning so
groups my interest in social learning
came through reflection on how it is
that I as a human have the capacities
that I do so if I had happens to have
been born in the woods away from all
other humans
I would probably just have like quickly"><title>Social learning in independent multi-agent reinfor‚Ä¶ ÔΩú Kamal N‚Äôdousse ÔΩú OpenAI Scholars Demo Day 2020 ÔΩú OpenAI | SWIEST</title>
<link rel=canonical href=https://swiest.com/en/qy9j5519s68/><link rel=stylesheet href=/scss/style.min.9a6fe90535a0e5c60443841f100f7b698092d48dba43fdb6386bb69b6559bc3d.css><script>document.oncontextmenu=function(){return!1},document.onselectstart=function(){return!1},document.oncopy=function(){return!1},document.oncut=function(){return!1}</script><script src=https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js></script><script type=text/javascript>$(document).ready(function(){$("#back-to-top").hide(),$(function(){$(window).scroll(function(){$(window).scrollTop()>600?$("#back-to-top").fadeIn(500):$("#back-to-top").fadeOut(500)}),$("#back-to-top").click(function(){return $("body,html").animate({scrollTop:0},500),!1})})})</script><meta property="og:title" content="Social learning in independent multi-agent reinfor‚Ä¶ ÔΩú Kamal N‚Äôdousse ÔΩú OpenAI Scholars Demo Day 2020 ÔΩú OpenAI"><meta property="og:description" content="Video Transcript Ôªøhello everyone I am excited to be
presenting my scholars projects I can
focus on social learning and independent
multi-agent reinforcement learning so
groups my interest in social learning
came through reflection on how it is
that I as a human have the capacities
that I do so if I had happens to have
been born in the woods away from all
other humans
I would probably just have like quickly"><meta property="og:url" content="https://swiest.com/en/qy9j5519s68/"><meta property="og:site_name" content="SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="English"><meta property="article:tag" content="Video Transcripts"><meta property="article:tag" content="OpenAI"><meta property="article:published_time" content="2023-11-06T03:40:54+00:00"><meta property="article:modified_time" content="2023-11-06T03:40:54+00:00"><meta name=twitter:title content="Social learning in independent multi-agent reinfor‚Ä¶ ÔΩú Kamal N‚Äôdousse ÔΩú OpenAI Scholars Demo Day 2020 ÔΩú OpenAI"><meta name=twitter:description content="Video Transcript Ôªøhello everyone I am excited to be
presenting my scholars projects I can
focus on social learning and independent
multi-agent reinforcement learning so
groups my interest in social learning
came through reflection on how it is
that I as a human have the capacities
that I do so if I had happens to have
been born in the woods away from all
other humans
I would probably just have like quickly"><link rel="shortcut icon" href=/favicon.ico><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"dark")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu307e6a33fa6fd661ccda3b77024ef5c2_252345_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics</a></h1><h2 class=site-description>üßôü™Ñüåé</h2></div></header><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><li><a href=/tags/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg><span>Tags</span></a></li><li><a href=/chart/podcastchart.html target=_blank><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-apple-podcast" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M18.364 18.364a9 9 0 10-12.728.0"/><path d="M11.766 22h.468a2 2 0 001.985-1.752l.5-4A2 2 0 0012.734 14h-1.468a2 2 0 00-1.985 2.248l.5 4A2 2 0 0011.766 22z"/><path d="M12 9m-2 0a2 2 0 104 0 2 2 0 10-4 0"/></svg><span>Podcasts</span></a></li><li><a href=/radio.html target=_blank><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-radio" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14 3 4.629 6.749A1 1 0 004 7.677V19a1 1 0 001 1h14a1 1 0 001-1V8a1 1 0 00-1-1H4.5"/><path d="M4 12h16"/><path d="M7 12v-2"/><path d="M17 16v.01"/><path d="M13 16v.01"/></svg><span>Radio</span></a></li><div class=menu-bottom-section><li id=i18n-switch><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg><select name=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://swiest.com/ selected>English</option><option value=https://swiest.com/af/>Afrikaans</option><option value=https://swiest.com/am/>·ä†·àõ·à≠·äõ</option><option value=https://swiest.com/ar/>ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</option><option value=https://swiest.com/az/>Az…ôrbaycan</option><option value=https://swiest.com/be/>–±–µ–ª–∞—Ä—É—Å–∫—ñ</option><option value=https://swiest.com/bg/>–±—ä–ª–≥–∞—Ä—Å–∫–∏</option><option value=https://swiest.com/bn/>‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ</option><option value=https://swiest.com/bo/>‡Ωñ‡Ωº‡Ωë‡ºã‡Ω¶‡æê‡Ωë‡ºã</option><option value=https://swiest.com/bs/>Bosanski</option><option value=https://swiest.com/ca/>Catal√†</option><option value=https://swiest.com/zh-hans/>ÁÆÄ‰Ωì‰∏≠Êñá</option><option value=https://swiest.com/zh-hant/>ÁπÅÈ´î‰∏≠Êñá</option><option value=https://swiest.com/cs/>ƒåe≈°tina</option><option value=https://swiest.com/el/>ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨</option><option value=https://swiest.com/cy/>Cymraeg</option><option value=https://swiest.com/da/>Dansk</option><option value=https://swiest.com/de/>Deutsch</option><option value=https://swiest.com/eo/>Esperanto</option><option value=https://swiest.com/es-es/>Espa√±ol (Espa√±a)</option><option value=https://swiest.com/es-419/>Espa√±ol (Latinoam√©rica)</option><option value=https://swiest.com/et/>Eesti</option><option value=https://swiest.com/eu/>Euskara</option><option value=https://swiest.com/haw/> ª≈ålelo Hawai ªi</option><option value=https://swiest.com/fa/>ŸÅÿßÿ±ÿ≥€å</option><option value=https://swiest.com/fi/>Suomi</option><option value=https://swiest.com/fo/>F√∏royskt</option><option value=https://swiest.com/fr/>Fran√ßais</option><option value=https://swiest.com/fy/>Frysk</option><option value=https://swiest.com/ga/>Gaeilge</option><option value=https://swiest.com/gl/>Galego</option><option value=https://swiest.com/gu/>‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä</option><option value=https://swiest.com/he/>◊¢÷¥◊ë◊®÷¥◊ô◊™</option><option value=https://swiest.com/km/>·ûÄ·ûò·üí·ûñ·ûª·ûá·û∂·üî</option><option value=https://swiest.com/hi/>‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</option><option value=https://swiest.com/hr/>Hrvatski</option><option value=https://swiest.com/ht/>Krey√≤l Ayisyen</option><option value=https://swiest.com/hu/>Magyar</option><option value=https://swiest.com/hy/>’Ä’°’µ’•÷Ä’•’∂</option><option value=https://swiest.com/ig/>√Ås·ª•ÃÄs·ª•ÃÅ √ågb√≤</option><option value=https://swiest.com/id/>Bahasa Indonesia</option><option value=https://swiest.com/is/>√çslenska</option><option value=https://swiest.com/it/>Italiano</option><option value=https://swiest.com/ja/>Êó•Êú¨Ë™û</option><option value=https://swiest.com/jv/>Basa Jawa</option><option value=https://swiest.com/ka/>·É•·Éê·É†·Éó·É£·Éö·Éò</option><option value=https://swiest.com/kk/>“ö–∞–∑–∞“õ—à–∞</option><option value=https://swiest.com/kn/>‡≤ï‡≤®‡≥ç‡≤®‡≤°</option><option value=https://swiest.com/ko/>ÌïúÍµ≠Ïñ¥</option><option value=https://swiest.com/or/>‡¨ì‡¨°‡¨º‡¨ø‡¨Ü</option><option value=https://swiest.com/ckb/>⁄©Ÿàÿ±ÿØ€å</option><option value=https://swiest.com/ky/>–ö—ã—Ä–≥—ã–∑—á–∞</option><option value=https://swiest.com/la/>Latina</option><option value=https://swiest.com/lb/>L√´tzebuergesch</option><option value=https://swiest.com/lo/>‡∫û‡∫≤‡∫™‡∫≤‡∫•‡∫≤‡∫ß</option><option value=https://swiest.com/lt/>Lietuvi≈≥</option><option value=https://swiest.com/lv/>Latvie≈°u</option><option value=https://swiest.com/mk/>–ú–∞–∫–µ–¥–æ–Ω—Å–∫–∏</option><option value=https://swiest.com/ml/>‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç</option><option value=https://swiest.com/mn/>–ú–æ–Ω–≥–æ–ª —Ö—ç–ª</option><option value=https://swiest.com/mr/>‡§Æ‡§∞‡§æ‡§†‡•Ä</option><option value=https://swiest.com/sw/>Kiswahili</option><option value=https://swiest.com/ms/>Bahasa Melayu</option><option value=https://swiest.com/my/>·Äô·Äº·Äî·Ä∫·Äô·Ä¨</option><option value=https://swiest.com/ne/>‡§®‡•á‡§™‡§æ‡§≤‡•Ä</option><option value=https://swiest.com/nl/>Nederlands</option><option value=https://swiest.com/no/>Norsk</option><option value=https://swiest.com/pa/>‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä</option><option value=https://swiest.com/pl/>Polski</option><option value=https://swiest.com/pt-br/>Portugu√™s Brasil</option><option value=https://swiest.com/pt-pt/>Portugu√™s Europeu</option><option value=https://swiest.com/ro/>Rom√¢nƒÉ</option><option value=https://swiest.com/ru/>–†—É—Å—Å–∫–∏–π</option><option value=https://swiest.com/rw/>Kinyarwanda</option><option value=https://swiest.com/si/>‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω</option><option value=https://swiest.com/sk/>Slovenƒçina</option><option value=https://swiest.com/sl/>Sloven≈°ƒçina</option><option value=https://swiest.com/sq/>Shqip</option><option value=https://swiest.com/sr/>–°—Ä–ø—Å–∫–∏ (Srpski)</option><option value=https://swiest.com/su/>Basa Sunda</option><option value=https://swiest.com/sv/>Svenska</option><option value=https://swiest.com/ta/>‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç</option><option value=https://swiest.com/te/>‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</option><option value=https://swiest.com/tg/>–¢–æ“∑–∏–∫”£</option><option value=https://swiest.com/th/>‡πÑ‡∏ó‡∏¢</option><option value=https://swiest.com/tk/>T√ºrkmenler</option><option value=https://swiest.com/tl/>Filipino</option><option value=https://swiest.com/tr/>T√ºrk√ße</option><option value=https://swiest.com/uk/>–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</option><option value=https://swiest.com/ur/>ÿßÿ±ÿØŸà</option><option value=https://swiest.com/uz/>O'zbekcha</option><option value=https://swiest.com/vi/>Ti·∫øng Vi·ªát</option><option value=https://swiest.com/yi/>◊ê◊ô◊ì◊ô◊©</option><option value=https://swiest.com/zh-hk/>Á≤µË™û</option><option value=https://swiest.com/zu/>IsiZulu</option></select></li><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#video>Video</a></li><li><a href=#transcript>Transcript</a></li></ol></nav></div></section><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><ins class=adsbygoogle style=display:block data-ad-client=ca-pub-9206135835124064 data-ad-slot=8754979142 data-ad-format=auto data-full-width-responsive=true></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></aside><a id=back-to-top href=#><img src=/img/top_hu7c2829da96df0e9f8f0191d120020b22_22287_40x0_resize_box_3.png></a><main class="main full-width"><form action=/search/ class="search-form widget"><p><label>Search</label>
<input name=keyword required placeholder="Type something...">
<button title=Search><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg></button></p></form><article class=main-article><header class=article-header><div class=article-details><header class=article-tags><a href=/tags/english/>English
</a><a href=/tags/video-transcripts/>Video Transcripts
</a><a href=/tags/openai/>OpenAI</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/en/qy9j5519s68/>Social learning in independent multi-agent reinfor‚Ä¶ ÔΩú Kamal N‚Äôdousse ÔΩú OpenAI Scholars Demo Day 2020 ÔΩú OpenAI</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>2023-11-06</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>15 minute read</time></div></footer></div></header><div class=article-content><p style=text-align:center><a href=https://amzn.to/3Nrdcwk target=_blank>üéÅAmazon Prime</a>
<a href=https://amzn.to/3RIBkxg target=_blank>üìñKindle Unlimited</a>
<a href=https://amzn.to/3Rqmudl target=_blank>üéßAudible Plus</a>
<a href=https://amzn.to/3TuLbbj target=_blank>üéµAmazon Music Unlimited</a>
<a href="https://www.iherb.com/?rcode=EID1574" target=_blank>üåøiHerb</a>
<a href="https://accounts.binance.com/register?ref=72302422" target=_blank>üí∞Binance</a></p></div><div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><ins class=adsbygoogle style=display:block data-ad-client=ca-pub-9206135835124064 data-ad-slot=8754979142 data-ad-format=auto data-full-width-responsive=true></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><section class=article-content><h2 id=video>Video</h2><div class=video-wrapper><iframe loading=lazy src=https://www.youtube.com/embed/Qy9J5519s68 allowfullscreen title="YouTube Video"></iframe></div><h2 id=transcript>Transcript</h2><p>Ôªøhello everyone I am excited to be</p><p>presenting my scholars projects I can</p><p>focus on social learning and independent</p><p>multi-agent reinforcement learning so</p><p>groups my interest in social learning</p><p>came through reflection on how it is</p><p>that I as a human have the capacities</p><p>that I do so if I had happens to have</p><p>been born in the woods away from all</p><p>other humans</p><p>I would probably just have like quickly</p><p>starve to death but thanks to my ability</p><p>to tap into cultural knowledge I have</p><p>the potential to do all sorts of awesome</p><p>things like participate in a space</p><p>program or lie in bed all day and browse</p><p>Twitter and I think if one were to if</p><p>one were an alien who just appeared on</p><p>earth and saw an example of a human in</p><p>isolation I think it would be very</p><p>surprising to see the broad variety that</p><p>of behaviors that groups of humans are</p><p>able to exhibit or that individual</p><p>humans can do if they can tap into that</p><p>cultural knowledge so yeah because of</p><p>the centrality of social learning to the</p><p>human intelligence I think it&rsquo;s</p><p>important to understand the</p><p>circumstances in which social learning</p><p>can take place and so in order to sort</p><p>of experiment with this there&rsquo;s a cool</p><p>anecdote from experimental sociology so</p><p>basically a group of monkeys were put in</p><p>a room along with a ladder and some</p><p>bananas were suspended from the ceiling</p><p>such that they could be accessed by a</p><p>monkey who climbed the ladder but were</p><p>otherwise inaccessible and really</p><p>quickly do you won&rsquo;t be cheering your</p><p>slides so that was the image you&rsquo;re</p><p>talking about yes</p><p>you</p><p>you</p><p>apologies is that working better that&rsquo;s</p><p>cool yeah so um yeah there&rsquo;s a group of</p><p>monkeys in a room they can only reach</p><p>the bananas using a ladder and anytime a</p><p>monkey climbed the ladder to access the</p><p>bananas experimenters would spray the</p><p>rest of the monkeys with cold water so</p><p>the other monkey&rsquo;s learned that they</p><p>should you know beat up the monkey did</p><p>that prior to climb the ladder in order</p><p>to prevent themselves from getting</p><p>sprayed and so this behavior persisted</p><p>even after the monkeys stopped being</p><p>sprayed with water and even more</p><p>interestingly when new monkeys were</p><p>introduced into the group after the</p><p>water spraying it ceased the new monkeys</p><p>of course would try to get to the</p><p>bananas and then the other monkeys would</p><p>beat them up so they would learn not to</p><p>access the bananas not to get the</p><p>bananas but they would also learn to</p><p>punish other monkeys that tried to get</p><p>the bananas this became like a cultural</p><p>phenomenon among the monkeys so as it</p><p>happens this experiment is apocryphal</p><p>and did not happen but I think it&rsquo;s</p><p>still serves as an interesting template</p><p>for how we can try to understand social</p><p>learning so the question I&rsquo;m interested</p><p>in answering is that of whether</p><p>independent reinforcement learning</p><p>agents can learn from each other just by</p><p>virtue of the fact that they exist in</p><p>the same environment and can maybe</p><p>observe one another and I think this is</p><p>an important question because in</p><p>reinforcement learning it becomes more</p><p>capable it seems likely that there will</p><p>be many environments in which many</p><p>reinforcement learning agents might</p><p>interact so for instance stock trading</p><p>autonomous and adaptive robot X trading</p><p>stocks in a market and so it&rsquo;s clearly</p><p>important to understand the</p><p>circumstances in which they might learn</p><p>from one another and exhibit behavior</p><p>that we might not expect if we were only</p><p>looking at one of them in isolation</p><p>so I will break my talk down into two</p><p>parts first I&rsquo;ll discuss the tools that</p><p>I use to approach this question</p><p>and in particular the environments and</p><p>reinforcement learning algorithms that I</p><p>used and then I&rsquo;ll talk about some</p><p>actual experiments about learning from</p><p>experts so I developed a an open-source</p><p>grid world implementation called marl</p><p>grid which is fits the standard open a</p><p>IgM API it&rsquo;s easy to extend so it&rsquo;s easy</p><p>to put a bunch of a large number of HS</p><p>in the environment and it&rsquo;s very</p><p>configurable and there are also some</p><p>registered environments so that for</p><p>reproducibility and given how obscure</p><p>this domain is I&rsquo;m surprised that it&rsquo;s</p><p>already got a little bit of traction on</p><p>github and this is an example of</p><p>visualizations that I&rsquo;ve built these</p><p>agents are effectively untrained but</p><p>it&rsquo;s easy to include a lot of them in</p><p>the environment and visualize what each</p><p>of them is doing</p><p>and the particular scenario that I spent</p><p>a lot of time working with I call goal</p><p>cycle so in this environment there are a</p><p>number of gold tiles and agents in the</p><p>environment are rewarded for traversing</p><p>them in a certain order and they&rsquo;re</p><p>penalized any time they mess up that</p><p>order and it&rsquo;s one can experiment with</p><p>this particular environment the one that</p><p>I&rsquo;m trying here by installing it with a</p><p>Python package from github</p><p>so the that this environment is kind of</p><p>like an analogue to the room with the</p><p>monkeys so the reinforcement learning</p><p>agents that exist in this environment</p><p>can observe one another and in principle</p><p>interact with one another there are a</p><p>couple interesting things about this</p><p>environment the penalty is configurable</p><p>and changing the value of the penalty</p><p>changes the difficulty of learning to</p><p>explore the environment effectively when</p><p>the penalty is low the agents kind of</p><p>ignore the penalty incurred by stepping</p><p>on the tiles out of order so on the</p><p>video on the left the agent is not</p><p>cycling through them in order and</p><p>anytime the agent steps on a tile out of</p><p>order it&rsquo;s color resets to red when the</p><p>penalty is very high exploration is</p><p>costly because the occurring the</p><p>penalties is aversive and the agents</p><p>learn to step on the first tile where</p><p>they get a reward and then they just</p><p>avoid all of them so by controlling</p><p>again by controlling the value of this</p><p>penalty we can change the difficulty of</p><p>exploration and in the context of social</p><p>learning we change the difficulty of</p><p>learning the effective strategy directly</p><p>from the environment as opposed to</p><p>learning it by observing other agents</p><p>and then the other big tool was</p><p>reinforcement learning algorithms that I</p><p>used</p><p>so I started by implementing dqn which</p><p>like pretty standard for this sort of</p><p>simple environment but I needed to add</p><p>memory in order for the agents to be</p><p>able to learn strategies that unfold</p><p>over the more than one time step this</p><p>didn&rsquo;t work super well and I spent a lot</p><p>of effort trying to improve it notably a</p><p>limited prioritize experience replay</p><p>which is kind of tricky with the</p><p>addition of the LS TM that it still</p><p>didn&rsquo;t work very well and i sorry</p><p>implemented PPO and immediately found a</p><p>pretty big improvement but further I</p><p>found that carrying carrying over the</p><p>some of the tricks from the architect to</p><p>implementation and notably refreshing</p><p>the hidden states that are collected</p><p>during the environment over the course</p><p>of update steps significantly improve</p><p>the agents capacity to use their</p><p>memories to accomplish tasks and these</p><p>diagrams show or these plots show the</p><p>difference that it made for a simple</p><p>goal cycle environment where the agent</p><p>is learning to traverse the goals</p><p>so basically when this trick is applied</p><p>the agents are able to achieve much</p><p>higher rewards and their training is</p><p>much more stable so you have to recap a</p><p>large part of the effort of the project</p><p>went into developing the reinforcement</p><p>learning algorithms and environments</p><p>that allow agents to effectively learn</p><p>tasks that are amenable to the kind of</p><p>experiments that I will discuss so um</p><p>revisiting the original question I&rsquo;m</p><p>interested in knowing when independent</p><p>agents can learn to can learn from</p><p>experts to accomplish tasks or can</p><p>acquire skills from experts so what this</p><p>might look like is we might have a bunch</p><p>of experts who have a high level of</p><p>skill and a novice who&rsquo;s introduced to</p><p>the environment</p><p>initially is there an unskilful but then</p><p>is able to get to the point of expertise</p><p>just by observing the experts and we&rsquo;d</p><p>also want it to be the case that the if</p><p>the novice was alone they would be</p><p>unable to learn and their skill would</p><p>remain low so there is a paper that</p><p>addresses a question like this it called</p><p>observational learning by reinforcement</p><p>learning by divorce&rsquo; at all from deep</p><p>mind and in their paper the experts are</p><p>hard coded and novices use RL to</p><p>accomplish a task in a simple grid world</p><p>so the diagram on the top shows like a</p><p>bird&rsquo;s eye view of the map the expert in</p><p>blue optimally travels to a goal which</p><p>is which at each episode is placed</p><p>randomly at one of these sixteen</p><p>positions and the novice needs to learn</p><p>to get to the goal as well here&rsquo;s an</p><p>image of the video of that they found</p><p>that the experts help the novices learn</p><p>more quickly but the presence of the</p><p>experts even in the presence of the</p><p>experts the novices say the experts</p><p>don&rsquo;t cause the novices to do any better</p><p>ultimately than they would if they were</p><p>learning alone so I</p><p>started by trying to replicate the first</p><p>finding in a simple cluttered grid world</p><p>which is like the goal cycle grid</p><p>world&rsquo;s I showed earlier but where</p><p>there&rsquo;s only one goal and found that</p><p>found very convincingly that the</p><p>presence of experts didn&rsquo;t help the</p><p>novice agents learn to accomplish their</p><p>task any more quickly and the takeaway</p><p>here is kind of that it&rsquo;s like hard to</p><p>learn from social cues in these</p><p>environments but that doesn&rsquo;t prove that</p><p>it&rsquo;s impossible and in order to look in</p><p>a more targeted way for the</p><p>circumstances in which this might happen</p><p>my effort shifted to different</p><p>environments and in particular the goal</p><p>goal cycle environment so the goal of my</p><p>experiments has been to construct a</p><p>scenario where in contrast to the oyster</p><p>results novices and experts are the same</p><p>sort of agent so they&rsquo;re both trained by</p><p>the reinforcement learning where</p><p>solitary novices struggle to learn and</p><p>where the presence of experts helps and</p><p>ideally we&rsquo;d want the novices to be able</p><p>to themselves become experts so that we</p><p>can see that they like have mastered the</p><p>skill and as a bonus ideally the whereas</p><p>in the borsa case the the information</p><p>that the novices get from the experts is</p><p>or there&rsquo;s not all that much information</p><p>that the novices can get from the</p><p>experts because the goal is unlike one</p><p>of sixteen places and the novices could</p><p>just like memorize the potential places</p><p>we want something that looks a bit more</p><p>like a skill and so in the we get this</p><p>in the goal cycle environment because</p><p>the process of spawning in a new</p><p>environment and trying out the different</p><p>possible cycles until identifying the</p><p>correct one is more is a closer analogue</p><p>to skilled and just like queuing as to</p><p>which quadrant their goal is in or</p><p>something like that</p><p>so I found that when the golf cycles are</p><p>masked from the view of novice agents</p><p>novices do in fact learn to follow</p><p>experts and this is consistent with the</p><p>results from Porsche so in both of these</p><p>videos both of these videos exhibit this</p><p>behavior the novices are shown in the</p><p>bottom of the columns on the right and</p><p>yeah in both cases the novices are doing</p><p>like a really robust kind of like the</p><p>following behavior yeah here the one of</p><p>the experts happens to have spawned in a</p><p>trap</p><p>basically and in these cases because the</p><p>novices is the novices are just</p><p>following the experts they end up</p><p>converging to slightly lower performance</p><p>than the experts as you can see in this</p><p>graph so the so far the conclusion that</p><p>are drawn is that it&rsquo;s like very hard to</p><p>learn from whether it to learn from</p><p>experts and when it&rsquo;s possible to</p><p>acquire a skill directly from the</p><p>environment it is likely that agents</p><p>will do that so in order to the next</p><p>steps for this project which I&rsquo;ll</p><p>continue working on focus on trying to</p><p>create environments where the social</p><p>that were the information available from</p><p>the experts is more valuable cue as to</p><p>how to obtain a high reward than the</p><p>information available directly from the</p><p>environment and so I plan to increase</p><p>the number of goals and experiment with</p><p>different penalty values and so on also</p><p>the in the example that I showed the</p><p>following behavior while it does help</p><p>the agent a crew more rewards isn&rsquo;t</p><p>quite the same skill that the experts</p><p>are showing going back to the monkey</p><p>analogy we want the novice agents to be</p><p>doing the same thing that the experts</p><p>are doing exhibiting the same skillful</p><p>behavior</p><p>and so a better way to measure that</p><p>would be by looking at the performance</p><p>of the agents when they&rsquo;re moved to a</p><p>new environment without agent without</p><p>experts and another approach is to add</p><p>mechanisms to encourage agents to learn</p><p>socially it&rsquo;s not clear for instance to</p><p>what degree humans are social learners</p><p>because of like biological because</p><p>they&rsquo;re biologically predisposed to do</p><p>so as opposed to because of the</p><p>environments that they&rsquo;re in obviously</p><p>by comparing to animals we might expect</p><p>the former but yes so we can introduce</p><p>we can similarly introduce like these</p><p>priors into agents and then we can</p><p>characterize the emergence of the social</p><p>behavior by varying or turning down that</p><p>fryer so yeah I&rsquo;d like to thank my</p><p>mentor Natasha</p><p>who&rsquo;s been incredibly supportive and</p><p>incredibly helpful in both helping me</p><p>like make the best use of learning</p><p>resources and helping me engage with the</p><p>broader research community I&rsquo;d like to</p><p>thank the program coordinators Mariah</p><p>and Kristina for helping the program</p><p>grow smoothly even in light of the</p><p>pandemic I&rsquo;d like to thank my fellow</p><p>scholars for a lot of incredibly</p><p>informative discussions and yeah just</p><p>generally being extremely supportive</p><p>special shout outs to wince and biases</p><p>for helping me keep track of my</p><p>experiments and also to Alethea power</p><p>for lending me a graphics card that I&rsquo;ve</p><p>been using for some of these experiments</p><p>yeah so I have time for some questions</p><p>so the first question is kind of novice</p><p>become more expert than an expert such</p><p>that other experts learn from it</p><p>that&rsquo;s a great question in the</p><p>experiments I&rsquo;ve been doing the experts</p><p>continue to learn alongside the novices</p><p>so here for instance in this plot the</p><p>experts are still learning but because</p><p>in this environment they happen to be</p><p>close to optimal so we don&rsquo;t see much</p><p>change as they continue to adapt but in</p><p>principle yes this could happen</p><p>I think another interesting direction is</p><p>for understanding like social behavior</p><p>and independent multi-agent</p><p>reinforcement learning is to carefully</p><p>study the impact of just like learning</p><p>in a group which is kind of similar to</p><p>that</p><p>cool so another question is could you</p><p>elaborate on hidden state refreshing in</p><p>your agent when do you refresh the</p><p>hidden state and how old does it differ</p><p>from the r2d2 approach so the agents</p><p>that I had trained so I trained a lot</p><p>the agents of PPO</p><p>with PPO agents alternate between</p><p>collecting experiments collecting</p><p>experience in an environment and</p><p>updating based on that experience so</p><p>during the update phase the agents</p><p>sample their experience and perform a</p><p>bunch of small updates based on that</p><p>batch of experience before discarding it</p><p>at the end of each update so the hidden</p><p>States so the typical way that the</p><p>agents are in typical PPR lsdm</p><p>implementations the agents will save</p><p>their hidden states as they interact</p><p>with the environment so this is like</p><p>remembering what was in their mind</p><p>alongside the experiences and then they</p><p>will sample those as they are doing each</p><p>of these like little updates but the</p><p>nature of the experience that they</p><p>collected depends on the values in the</p><p>hidden state and the values in the</p><p>hidden state depend on their parameters</p><p>so as they update the hidden states the</p><p>the behavior in their the behavior that</p><p>they&rsquo;re learning firm becomes less and</p><p>less representative of the earth is the</p><p>big divergence between the behavior and</p><p>there between the data and their</p><p>experience and the parameters of the</p><p>current values of the parameters so I</p><p>found that it wasn&rsquo;t too costly to do</p><p>this and I have some tweaks to my like</p><p>lsdm implementation that facilitate this</p><p>and in the end I end up refreshing them</p><p>basically between each iteration each</p><p>gradient step and the r2d2 approach</p><p>differs in a few ways the reason for</p><p>those differences I think is mainly that</p><p>the r2d2 approach is off policy</p><p>and so each gradient step has the</p><p>potential to or that the volume of</p><p>experience that can go into each update</p><p>is much larger and so because of this</p><p>they need to employ some tricks to make</p><p>sure that the that the hidden states</p><p>don&rsquo;t get too stale without refreshing</p><p>it between each iteration because that&rsquo;d</p><p>be very costly but for PPO and on policy</p><p>reinforcement learning it like didn&rsquo;t</p><p>matter too much</p><p>another question is why do you think</p><p>that proximal policy optimization worked</p><p>so well that&rsquo;s a good question</p><p>I think let&rsquo;s see so I have been</p><p>thinking a bunch about this and I think</p><p>that a a lot of it in practice comes</p><p>from the fact that I my implementation</p><p>of PPO is based on the spinning up</p><p>implementation and I guess spinning up</p><p>also deserves a shout out and so it</p><p>inherited a lot of tweaks that helped</p><p>help the agent learn stabili and perform</p><p>well and it is possible that if I yeah</p><p>so I hesitate to say that PPO is better</p><p>than the arc um that&rsquo;s certainly my</p><p>experience but I think I inherited a lot</p><p>of a lot of improvements from the</p><p>implementation that I based it off of</p><p>and then yeah the hidden state</p><p>refreshing I think is interesting it is</p><p>yeah it helped immensely with robustness</p><p>and yeah I think the reason is that it</p><p>prevents the policy for making big</p><p>changes over the course of each update</p><p>and this yeah helps it helps ensure that</p><p>the policy is consistent with the data</p><p>that it&rsquo;s learning from I guess I would</p><p>be interested to for some clarification</p><p>on that question but</p></section><footer class=article-footer><section class=article-tags><a href=/tags/english/>English</a>
<a href=/tags/video-transcripts/>Video Transcripts</a>
<a href=/tags/openai/>OpenAI</a></section></footer></article><div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><ins class=adsbygoogle style=display:block;text-align:center data-ad-layout=in-article data-ad-format=fluid data-ad-client=ca-pub-9206135835124064 data-ad-slot=1055602464></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/en/at2xkqjazns/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/AT2XkqJAZns data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>Towards Epileptic Seizure Prediction with Deep Network ÔΩú Kata Slama ÔΩú OpenAI Scholars Demo Day 2020 ÔΩú OpenAI</h2></div></a></article><article><a href=/en/jzohw-eybtq/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/JZOHW-eYBtQ data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>Introductions by Sam Altman & Greg Brockman ÔΩú OpenAI Scholars Demo Day 2020 ÔΩú OpenAI</h2></div></a></article><article><a href=/en/-fozam9xqs4/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/-FoZAM9xqS4 data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>OpenAI Five vs. OG, Game 2 ÔΩú OpenAI Five Finals (4‚ß∏6) ÔΩú OpenAI</h2></div></a></article><article><a href=/en/u9mjuukhuzk/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/U9mJuUkhUzk data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>OpenAI DevDay, Opening Keynote ÔΩú OpenAI</h2></div></a></article><article><a href=/en/lpe5gwuqa-k/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/lpe5Gwuqa-k data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>Scaling Laws for Language Transfer Learning ÔΩú Christina Kim ÔΩú OpenAI Scholars Demo Day 2021 ÔΩú OpenAI</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2021 -
2023 SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics</section><section class=powerby>As an Amazon Associate I earn from qualifying purchases üõí<br>Built with <a href=https://swiest.com/ target=_blank rel=noopener>(Ôæâ‚óï„ÉÆ‚óï)Ôæâü™Ñüíûüíñü•∞ across the glüåçüåèüåébe</a><br></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Serif&family=Noto+Serif+Armenian&family=Noto+Serif+Bengali&family=Noto+Serif+Devanagari&family=Noto+Serif+Georgian&family=Noto+Serif+Gujarati&family=Noto+Serif+HK&family=Noto+Serif+Hebrew&family=Noto+Serif+JP&family=Noto+Serif+KR&family=Noto+Serif+Kannada&family=Noto+Serif+Khmer&family=Noto+Serif+Lao&family=Noto+Serif+Makasar&family=Noto+Serif+Malayalam&family=Noto+Serif+Myanmar&family=Noto+Serif+Oriya&family=Noto+Serif+SC&family=Noto+Serif+Sinhala&family=Noto+Serif+TC&family=Noto+Serif+Tamil&family=Noto+Serif+Telugu&family=Noto+Serif+Thai&family=Noto+Serif+Tibetan&display=swap" rel=stylesheet></body></html>
<!DOCTYPE html>
<html lang="en" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='Video Transcript ÔªøHello, and good morning everyone!
Hi! I&amp;rsquo;m Josh Achiam, I&amp;rsquo;m a Safety researcher
here at OpenAI and I&amp;rsquo;m the main author of Spinning Up in Deep RL up and thank you
all so much for being here today at OpenAI&amp;rsquo;s 1st Spinning Up Workshop.
oFor people who are tuning in on the livestream I&amp;rsquo;d like to let you know that
there is a minor technical difficulty and so we will not be able to broadcast'>
<title>OpenAI Spinning Up in Deep RL Workshop ÔΩú OpenAI | SWIEST</title>

<link rel='canonical' href='https://swiest.com/en/fdy7dt3ijgy/'>

<link rel="stylesheet" href="/scss/style.min.9a6fe90535a0e5c60443841f100f7b698092d48dba43fdb6386bb69b6559bc3d.css"><script>
    document.oncontextmenu = function(){ return false; };
    document.onselectstart = function(){ return false; };
    document.oncopy = function(){ return false; };
    document.oncut = function(){ return false; };
</script>

<script src="https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>


<script type="text/javascript">
    $(document).ready(function(){
     
     $("#back-to-top").hide();
     
     $(function () {
      $(window).scroll(function(){
       if ($(window).scrollTop()>600){
        $("#back-to-top").fadeIn(500);
       }else{
        $("#back-to-top").fadeOut(500);
       }
     });
     
     $("#back-to-top").click(function(){
      $('body,html').animate({scrollTop:0},500);
       return false;
      });
     });
    });
    </script><meta property='og:title' content='OpenAI Spinning Up in Deep RL Workshop ÔΩú OpenAI'>
<meta property='og:description' content='Video Transcript ÔªøHello, and good morning everyone!
Hi! I&amp;rsquo;m Josh Achiam, I&amp;rsquo;m a Safety researcher
here at OpenAI and I&amp;rsquo;m the main author of Spinning Up in Deep RL up and thank you
all so much for being here today at OpenAI&amp;rsquo;s 1st Spinning Up Workshop.
oFor people who are tuning in on the livestream I&amp;rsquo;d like to let you know that
there is a minor technical difficulty and so we will not be able to broadcast'>
<meta property='og:url' content='https://swiest.com/en/fdy7dt3ijgy/'>
<meta property='og:site_name' content='SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='English' /><meta property='article:tag' content='Video Transcripts' /><meta property='article:tag' content='OpenAI' /><meta property='article:published_time' content='2023-11-06T03:59:54&#43;00:00'/><meta property='article:modified_time' content='2023-11-06T03:59:54&#43;00:00'/>
<meta name="twitter:title" content="OpenAI Spinning Up in Deep RL Workshop ÔΩú OpenAI">
<meta name="twitter:description" content="Video Transcript ÔªøHello, and good morning everyone!
Hi! I&amp;rsquo;m Josh Achiam, I&amp;rsquo;m a Safety researcher
here at OpenAI and I&amp;rsquo;m the main author of Spinning Up in Deep RL up and thank you
all so much for being here today at OpenAI&amp;rsquo;s 1st Spinning Up Workshop.
oFor people who are tuning in on the livestream I&amp;rsquo;d like to let you know that
there is a minor technical difficulty and so we will not be able to broadcast">
    <link rel="shortcut icon" href="/favicon.ico" />
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin="anonymous"></script>
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "dark");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu307e6a33fa6fd661ccda3b77024ef5c2_252345_300x0_resize_box_3.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">‚ú®</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics</a></h1>
            <h2 class="site-description">üåçüåèüåé</h2>
        </div>
    </header><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        
        
        <li >
            <a href='/tags/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11 3L20 12a1.5 1.5 0 0 1 0 2L14 20a1.5 1.5 0 0 1 -2 0L3 11v-4a4 4 0 0 1 4 -4h4" />
  <circle cx="9" cy="9" r="2" />
</svg>



                
                <span>Tags</span>
            </a>
        </li>
        
        
        <li >
            <a href='/chart/podcastchart.html' target="_blank">
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-apple-podcast" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M18.364 18.364a9 9 0 1 0 -12.728 0" />
  <path d="M11.766 22h.468a2 2 0 0 0 1.985 -1.752l.5 -4a2 2 0 0 0 -1.985 -2.248h-1.468a2 2 0 0 0 -1.985 2.248l.5 4a2 2 0 0 0 1.985 1.752z" />
  <path d="M12 9m-2 0a2 2 0 1 0 4 0a2 2 0 1 0 -4 0" />
</svg>
                
                <span>Podcasts</span>
            </a>
        </li>
        


        <div class="menu-bottom-section">
                <li id="i18n-switch">  
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M4 5h7" />
  <path d="M9 3v2c0 4.418 -2.239 8 -5 8" />
  <path d="M5 9c-.003 2.144 2.952 3.908 6.7 4" />
  <path d="M12 20l4 -9l4 9" />
  <path d="M19.1 18h-6.2" />
</svg>



                    <select name="language" onchange="window.location.href = this.selectedOptions[0].value">
                        
                            <option value="https://swiest.com/" selected>English</option>
                        
                            <option value="https://swiest.com/af/" >Afrikaans</option>
                        
                            <option value="https://swiest.com/am/" >·ä†·àõ·à≠·äõ</option>
                        
                            <option value="https://swiest.com/ar/" >ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</option>
                        
                            <option value="https://swiest.com/az/" >Az…ôrbaycan</option>
                        
                            <option value="https://swiest.com/be/" >–±–µ–ª–∞—Ä—É—Å–∫—ñ</option>
                        
                            <option value="https://swiest.com/bg/" >–±—ä–ª–≥–∞—Ä—Å–∫–∏</option>
                        
                            <option value="https://swiest.com/bn/" >‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ</option>
                        
                            <option value="https://swiest.com/bo/" >‡Ωñ‡Ωº‡Ωë‡ºã‡Ω¶‡æê‡Ωë‡ºã</option>
                        
                            <option value="https://swiest.com/bs/" >Bosanski</option>
                        
                            <option value="https://swiest.com/ca/" >Catal√†</option>
                        
                            <option value="https://swiest.com/zh-hans/" >ÁÆÄ‰Ωì‰∏≠Êñá</option>
                        
                            <option value="https://swiest.com/zh-hant/" >ÁπÅÈ´î‰∏≠Êñá</option>
                        
                            <option value="https://swiest.com/cs/" >ƒåe≈°tina</option>
                        
                            <option value="https://swiest.com/el/" >ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨</option>
                        
                            <option value="https://swiest.com/cy/" >Cymraeg</option>
                        
                            <option value="https://swiest.com/da/" >Dansk</option>
                        
                            <option value="https://swiest.com/de/" >Deutsch</option>
                        
                            <option value="https://swiest.com/eo/" >Esperanto</option>
                        
                            <option value="https://swiest.com/es-es/" >Espa√±ol (Espa√±a)</option>
                        
                            <option value="https://swiest.com/es-419/" >Espa√±ol (Latinoam√©rica)</option>
                        
                            <option value="https://swiest.com/et/" >Eesti</option>
                        
                            <option value="https://swiest.com/eu/" >Euskara</option>
                        
                            <option value="https://swiest.com/haw/" > ª≈ålelo Hawai ªi</option>
                        
                            <option value="https://swiest.com/fa/" >ŸÅÿßÿ±ÿ≥€å</option>
                        
                            <option value="https://swiest.com/fi/" >Suomi</option>
                        
                            <option value="https://swiest.com/fo/" >F√∏royskt</option>
                        
                            <option value="https://swiest.com/fr/" >Fran√ßais</option>
                        
                            <option value="https://swiest.com/fy/" >Frysk</option>
                        
                            <option value="https://swiest.com/ga/" >Gaeilge</option>
                        
                            <option value="https://swiest.com/gl/" >Galego</option>
                        
                            <option value="https://swiest.com/gu/" >‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä</option>
                        
                            <option value="https://swiest.com/he/" >◊¢÷¥◊ë◊®÷¥◊ô◊™</option>
                        
                            <option value="https://swiest.com/km/" >·ûÄ·ûò·üí·ûñ·ûª·ûá·û∂·üî</option>
                        
                            <option value="https://swiest.com/hi/" >‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</option>
                        
                            <option value="https://swiest.com/hr/" >Hrvatski</option>
                        
                            <option value="https://swiest.com/ht/" >Krey√≤l Ayisyen</option>
                        
                            <option value="https://swiest.com/hu/" >Magyar</option>
                        
                            <option value="https://swiest.com/hy/" >’Ä’°’µ’•÷Ä’•’∂</option>
                        
                            <option value="https://swiest.com/ig/" >√Ås·ª•ÃÄs·ª•ÃÅ √ågb√≤</option>
                        
                            <option value="https://swiest.com/id/" >Bahasa Indonesia</option>
                        
                            <option value="https://swiest.com/is/" >√çslenska</option>
                        
                            <option value="https://swiest.com/it/" >Italiano</option>
                        
                            <option value="https://swiest.com/ja/" >Êó•Êú¨Ë™û</option>
                        
                            <option value="https://swiest.com/jv/" >Basa Jawa</option>
                        
                            <option value="https://swiest.com/ka/" >·É•·Éê·É†·Éó·É£·Éö·Éò</option>
                        
                            <option value="https://swiest.com/kk/" >“ö–∞–∑–∞“õ—à–∞</option>
                        
                            <option value="https://swiest.com/kn/" >‡≤ï‡≤®‡≥ç‡≤®‡≤°</option>
                        
                            <option value="https://swiest.com/ko/" >ÌïúÍµ≠Ïñ¥</option>
                        
                            <option value="https://swiest.com/or/" >‡¨ì‡¨°‡¨º‡¨ø‡¨Ü</option>
                        
                            <option value="https://swiest.com/ckb/" >⁄©Ÿàÿ±ÿØ€å</option>
                        
                            <option value="https://swiest.com/ky/" >–ö—ã—Ä–≥—ã–∑—á–∞</option>
                        
                            <option value="https://swiest.com/la/" >Latina</option>
                        
                            <option value="https://swiest.com/lb/" >L√´tzebuergesch</option>
                        
                            <option value="https://swiest.com/lo/" >‡∫û‡∫≤‡∫™‡∫≤‡∫•‡∫≤‡∫ß</option>
                        
                            <option value="https://swiest.com/lt/" >Lietuvi≈≥</option>
                        
                            <option value="https://swiest.com/lv/" >Latvie≈°u</option>
                        
                            <option value="https://swiest.com/mk/" >–ú–∞–∫–µ–¥–æ–Ω—Å–∫–∏</option>
                        
                            <option value="https://swiest.com/ml/" >‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç</option>
                        
                            <option value="https://swiest.com/mn/" >–ú–æ–Ω–≥–æ–ª —Ö—ç–ª</option>
                        
                            <option value="https://swiest.com/mr/" >‡§Æ‡§∞‡§æ‡§†‡•Ä</option>
                        
                            <option value="https://swiest.com/sw/" >Kiswahili</option>
                        
                            <option value="https://swiest.com/ms/" >Bahasa Melayu</option>
                        
                            <option value="https://swiest.com/my/" >·Äô·Äº·Äî·Ä∫·Äô·Ä¨</option>
                        
                            <option value="https://swiest.com/ne/" >‡§®‡•á‡§™‡§æ‡§≤‡•Ä</option>
                        
                            <option value="https://swiest.com/nl/" >Nederlands</option>
                        
                            <option value="https://swiest.com/no/" >Norsk</option>
                        
                            <option value="https://swiest.com/pa/" >‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä</option>
                        
                            <option value="https://swiest.com/pl/" >Polski</option>
                        
                            <option value="https://swiest.com/pt-br/" >Portugu√™s Brasil</option>
                        
                            <option value="https://swiest.com/pt-pt/" >Portugu√™s Europeu</option>
                        
                            <option value="https://swiest.com/ro/" >Rom√¢nƒÉ</option>
                        
                            <option value="https://swiest.com/ru/" >–†—É—Å—Å–∫–∏–π</option>
                        
                            <option value="https://swiest.com/rw/" >Kinyarwanda</option>
                        
                            <option value="https://swiest.com/si/" >‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω</option>
                        
                            <option value="https://swiest.com/sk/" >Slovenƒçina</option>
                        
                            <option value="https://swiest.com/sl/" >Sloven≈°ƒçina</option>
                        
                            <option value="https://swiest.com/sq/" >Shqip</option>
                        
                            <option value="https://swiest.com/sr/" >–°—Ä–ø—Å–∫–∏ (Srpski)</option>
                        
                            <option value="https://swiest.com/su/" >Basa Sunda</option>
                        
                            <option value="https://swiest.com/sv/" >Svenska</option>
                        
                            <option value="https://swiest.com/ta/" >‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç</option>
                        
                            <option value="https://swiest.com/te/" >‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</option>
                        
                            <option value="https://swiest.com/tg/" >–¢–æ“∑–∏–∫”£</option>
                        
                            <option value="https://swiest.com/th/" >‡πÑ‡∏ó‡∏¢</option>
                        
                            <option value="https://swiest.com/tk/" >T√ºrkmenler</option>
                        
                            <option value="https://swiest.com/tl/" >Filipino</option>
                        
                            <option value="https://swiest.com/tr/" >T√ºrk√ße</option>
                        
                            <option value="https://swiest.com/uk/" >–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</option>
                        
                            <option value="https://swiest.com/ur/" >ÿßÿ±ÿØŸà</option>
                        
                            <option value="https://swiest.com/uz/" >O&#39;zbekcha</option>
                        
                            <option value="https://swiest.com/vi/" >Ti·∫øng Vi·ªát</option>
                        
                            <option value="https://swiest.com/yi/" >◊ê◊ô◊ì◊ô◊©</option>
                        
                            <option value="https://swiest.com/zh-hk/" >Á≤µË™û</option>
                        
                            <option value="https://swiest.com/zu/" >IsiZulu</option>
                        
                    </select>
                </li>
            
            
            
                <li id="dark-mode-toggle">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <span>Dark Mode</span>
                </li>
            
        </div>
    </ol>
</aside>

    
    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">Table of contents</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#video">Video</a></li>
    <li><a href="#transcript">Transcript</a></li>
  </ol>
</nav>
        </div>
    </section>

            
        
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064"
            crossorigin="anonymous"></script>
        
        <ins class="adsbygoogle"
            style="display:block"
             data-ad-client="ca-pub-9206135835124064"
             data-ad-slot="8754979142"
             data-ad-format="auto"
             data-full-width-responsive="true"></ins>
        <script>
             (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
    </aside>

    
    
    <a id="back-to-top" href="#">
        <img src="/img/top_hu7c2829da96df0e9f8f0191d120020b22_22287_40x0_resize_box_3.png" />
    </a>
    

            <main class="main full-width"><form action="/search/" class="search-form widget">
            <p>
                <label>Search</label>
                <input name="keyword" required placeholder="Type something..." />
        
                <button title="Search">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                </button>
            </p>
        </form><article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-tags">
        
            <a href="/tags/english/" >
                English
            </a>
        
            <a href="/tags/video-transcripts/" >
                Video Transcripts
            </a>
        
            <a href="/tags/openai/" >
                OpenAI
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/en/fdy7dt3ijgy/">OpenAI Spinning Up in Deep RL Workshop ÔΩú OpenAI</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2023-11-06</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    129 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>
<div class="article-content">
    <p style="text-align:center">
       <a href="https://amzn.to/471i0jl" target="_blank">üéÅAmazon Prime</a>
       <a href="https://amzn.to/3Fulwaf" target="_blank">üíóThe Drop</a>
       <a href="https://amzn.to/3QDVlVf" target="_blank">üìñKindle Unlimited</a>
       <a href="https://amzn.to/3FqzNoB" target="_blank">üéßAudible Plus</a>
       <a href="https://amzn.to/3tMT3dm" target="_blank">üéµAmazon Music Unlimited</a>
       <a href="https://www.iherb.com/?rcode=EID1574" target="_blank">üåøiHerb</a>
       <a href="https://accounts.binance.com/register?ref=72302422" target="_blank">üí∞Binance</a>
    </p>
</div>
<div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064"
     crossorigin="anonymous"></script>
    
    <ins class="adsbygoogle"
         style="display:block"
         data-ad-client="ca-pub-9206135835124064"
         data-ad-slot="8754979142"
         data-ad-format="auto"
         data-full-width-responsive="true"></ins>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>


    <section class="article-content">
    
    
    <h2 id="video">Video</h2>
<div class="video-wrapper">
    <iframe loading="lazy" 
            src="https://www.youtube.com/embed/fdY7dt3ijgY" 
            allowfullscreen 
            title="YouTube Video"
    >
    </iframe>
</div>

<h2 id="transcript">Transcript</h2>
<p>ÔªøHello, and good morning everyone!</p>
<p>Hi! I&rsquo;m Josh Achiam, I&rsquo;m a Safety researcher</p>
<p>here at OpenAI and I&rsquo;m the main author
of Spinning Up in Deep RL up and thank you</p>
<p>all so much for being here today at OpenAI&rsquo;s 1st Spinning Up Workshop.</p>
<p>oFor people who are tuning in on the
livestream I&rsquo;d like to let you know that</p>
<p>there is a minor technical difficulty
and so we will not be able to broadcast</p>
<p>the slides directly from my computer
into the livestream video so you&rsquo;ll be</p>
<p>seeing the screen through the camera. In
the event that that&rsquo;s not enough for you</p>
<p>to see it clearly I just open sourced
the repo that has the PDFs for these</p>
<p>slides so please go to github.com slash
open AI slash spinning up - workshop and</p>
<p>you&rsquo;ll find in the RL intro folder RL
intro PDF which will be the presentation</p>
<p>that I&rsquo;m about to give so hopefully that
makes it easier for you to follow along</p>
<p>so since this is kind of a new thing
that we&rsquo;re doing I&rsquo;d like to start today</p>
<p>by talking about what it is and why
we&rsquo;re doing it and what we hope you get</p>
<p>out of it from being here education at
open AI is this concept that as part of</p>
<p>our mission we want to make sure that we
provide for the public good and that we</p>
<p>help foster a global community around
AGI which is the thing that at opening I</p>
<p>we care the most about and are trying to
figure out how to make sure happens in a</p>
<p>way that&rsquo;s safe and beneficial for all
of humanity so for those of you who</p>
<p>aren&rsquo;t already familiar AGI is
artificial general intelligence the idea</p>
<p>is that this is going to be some very
powerful AI technology that&rsquo;ll have the</p>
<p>ability to change pretty much everything
about how we do anything something that</p>
<p>could potentially do most economically
valuable work something that could solve</p>
<p>tasks that currently only human
intelligence is capable of solving and</p>
<p>so we think it&rsquo;s really important that
we help people become aware of what AGI</p>
<p>is and what the technology that&rsquo;ll
likely underlie it is so that you can</p>
<p>think critically about issues that might
come up in the future and also if you&rsquo;re</p>
<p>interested
participate because we really need for</p>
<p>people to step up and help make sure
that this technology is safe and does</p>
<p>what we wanted to do and doesn&rsquo;t cause
anything harmful or detrimental to the</p>
<p>world so spinning up is the first thing
that we&rsquo;re launching under this</p>
<p>education at open AI initiative and the
goal is to help people acquire technical</p>
<p>skills in the research topics that we
care about so spinning up in deep RL is</p>
<p>a resource that hopefully all of you
have seen by now it contains a number of</p>
<p>different pieces including a short intro
to reinforcement learning so what is</p>
<p>this thing that we&rsquo;re doing so much
research about at open AI an essay about</p>
<p>how you would go about becoming a
researcher if you&rsquo;re interested in</p>
<p>joining a curated list of important
papers in the field so this is</p>
<p>particularly important because since
this is an emerging field there isn&rsquo;t</p>
<p>really a clear consensus on the best way
to learn it or a textbook that</p>
<p>completely illuminates the way from
start to finish and a lot of the</p>
<p>important knowledge right now is still
in research papers so if you want to</p>
<p>find out the most stuff about this you
have to go digging and hopefully this</p>
<p>helps you figure out where to look also
a code repo of key algorithms because</p>
<p>for any of you who have tried hacking in
this field before I&rsquo;m sure you found</p>
<p>that there were a lot of very confusing
resources out there really excellent</p>
<p>ones but nonetheless ones that made
non-obvious choices and didn&rsquo;t clearly</p>
<p>connect what they were doing to why they
were doing it and so we hope that the</p>
<p>repo that we provide and spinning up in
deep RL is part of something to bridge</p>
<p>the gap there and of course some
exercises so if you want to actually try</p>
<p>coding something up there are a few
ideas there for for what to do to get</p>
<p>you familiar with some of the key pieces
of math or algorithms or what kind of</p>
<p>bugs you might expect and so why are we
having workshops so in addition to</p>
<p>putting these resources online we think
it&rsquo;s gonna really help people if we work</p>
<p>with you one-on-one if we can see you
face-to-face and talk with you and have</p>
<p>the kind of conversations and share the
ideas that just you know don&rsquo;t come up</p>
<p>in the sort of open loop control thing
that happens when we put information on</p>
<p>the Internet
today we&rsquo;d like to have you come away</p>
<p>from this with a better sense of what
the current capabilities and limitations</p>
<p>are in deeper I&rsquo;ll tell you a little bit
about what kind of research is out there</p>
<p>so if you want to go and follow some
line of thinking you know what&rsquo;s been</p>
<p>done and what hasn&rsquo;t and we&rsquo;d like you
to actually try building and running</p>
<p>algorithms for deep reinforcement
learning for possibly for the first time</p>
<p>and show you how to be confident in
doing that so that if you want to keep</p>
<p>doing it afterwards you&rsquo;re able to all
right so then what is deep reinforcement</p>
<p>learning why do we need it why do we
care about it</p>
<p>deep reinforcement learning is the
combination of reinforcement learning</p>
<p>with deep learning RL reinforcement
learning is about solving problems by</p>
<p>trial and error and deep learning is
about using these very powerful function</p>
<p>approximate errs called deep neural
networks to solve problems and deep</p>
<p>reinforcement learning is just
straightforwardly the combination where</p>
<p>we&rsquo;re gonna have something that&rsquo;s
learning by trial and error and the</p>
<p>thing that&rsquo;s getting learned is a deep
neural network that&rsquo;s going to make some</p>
<p>kind of decision or evaluate some
situation and use that ultimately to in</p>
<p>some environment make decisions that
lead to rewards where reward is just</p>
<p>some measure of how good or bad an
outcome was so when would you want to</p>
<p>use RL RL is useful when for one there&rsquo;s
a sequential decision making problem -</p>
<p>you don&rsquo;t know what the right thing to
do in that situation is already if you</p>
<p>have the optimal behavior say from
having watched human experts enough and</p>
<p>you have just a ton of data on exactly
what to do in every situation then you</p>
<p>can use the standard tools of say
supervised learning to exactly get some</p>
<p>machine learning system to duplicate
that behavior but when you don&rsquo;t have</p>
<p>access to that or when you suspect that
what appears to be expert human behavior</p>
<p>is in fact suboptimal
in that situation you may want to try</p>
<p>reinforcement learning instead because
it could discover</p>
<p>things that wouldn&rsquo;t have otherwise been
known and you also have to be able to</p>
<p>evaluate whether or not a behavior or an
outcome was good or bad this is pretty</p>
<p>critical
so RL is good when it&rsquo;s easier to</p>
<p>evaluate behaviors than to generate them
or to exactly solve for them and when</p>
<p>would you use deep learning
so the typical paradigm for deep</p>
<p>learning is that you want to approximate
some very complicated function a</p>
<p>function that usually requires some
amount of intelligence so for instance</p>
<p>if a human looks at a picture of a bird
and then knows what species of bird that</p>
<p>is that&rsquo;s a thing that you can&rsquo;t really
write down a simple mathematical rule to</p>
<p>do if you want to get a machine to do
that you have to teach it from data and</p>
<p>other problems that you know you would
want to do this for typically have</p>
<p>inputs or outputs that are very high
dimensional because it&rsquo;s just quite hard</p>
<p>to from an image or from a video stream
or from an audio stream go to a decision</p>
<p>rule without doing some sort of learning
in the middle and also you typically</p>
<p>want to have lots and lots of data
because getting machine learning systems</p>
<p>to behave in any reasonable way requires
that you give them sufficient examples</p>
<p>and there are tons of problems where
this is exactly what you have and in</p>
<p>those domains deep learning has been
very successful at exceeding whatever</p>
<p>was previously the state of the art from
any other methods that existed before</p>
<p>and creating things that are now
standard consumer products things that</p>
<p>were magic 10 years ago are like
completely normal now the idea that we</p>
<p>have super excellent image recognition
facial classification that you can talk</p>
<p>to your phone and it&rsquo;s going to know
what you said and it&rsquo;s not just going to</p>
<p>come up with some completely random
gobbledygook</p>
<p>this is getting better because we&rsquo;re
able to leverage this very powerful</p>
<p>technology that is deep learning for
these problems and so deep RL is when</p>
<p>you have some very hard high dimensional
problem where you can evaluate behaviors</p>
<p>and you want to get a machine to learn
how to do it because you can&rsquo;t write</p>
<p>down how it should in fact behave and
some very simple examples of this are</p>
<p>say video games we
you want to go from a computer looking</p>
<p>at an image of the screen so just raw
pixels to a decision rule that scores</p>
<p>the most possible points in the game or
behaves in a way which is cool or</p>
<p>interesting or exciting or perhaps a
really sophisticated strategy game like</p>
<p>go we&rsquo;re really deep thinking and
intuition and creativity is necessary to</p>
<p>make progress you can&rsquo;t write down a
simple rule for that but you can learn</p>
<p>it with reinforcement learning or
perhaps you want to control some complex</p>
<p>humanoid some some robot to run around
and do stuff</p>
<p>or maybe something which is a little
less silly maybe a little more real</p>
<p>maybe you want to get robots in a
factory to quickly learn a new task when</p>
<p>the robot uprising happens it&rsquo;s because
of this we&rsquo;re very sorry for this</p>
<p>research this was trained by the way
with an algorithm that was developed</p>
<p>here at open a I called proximal policy
optimization it&rsquo;s one of the algorithms</p>
<p>and spinning up and if you haven&rsquo;t had
any experience with it then we won&rsquo;t get</p>
<p>into it in this lecture today but any
other point in the afternoon during the</p>
<p>hackathon happy to go into detail so
before we proceed into the are L</p>
<p>specific stuff this is a crowd with a
pretty wide range of backgrounds and so</p>
<p>I just want to do a very brief recap of
some of the patterns from deep learning</p>
<p>what do you expect when you set up a
deep learning problem what does that</p>
<p>look like what do you have to think
about so we typically talk about it in</p>
<p>terms of the language of finding a model
that is able to give the right outputs</p>
<p>for certain inputs so in this case the
model is going to be some function of</p>
<p>the inputs and parameters and the
parameters are adjustable we control</p>
<p>them we change them and we want to
change them in a way that&rsquo;s going to</p>
<p>make the model behave according to some
design specification the way that we</p>
<p>provide the design specification and get
the parameters to satisfy it is by</p>
<p>setting up some kind of loss function
this tells you in a nutshell how good</p>
<p>the model is at doing the thing that you
want it to do usually some measure of</p>
<p>just how close the output from the model
is to the</p>
<p>I earn output and the critical thing
about this loss function is that it has</p>
<p>to be differentiable with respect to the
parameters in the model and when you</p>
<p>have that set up oh and of course
there&rsquo;s data as well so you have a bunch</p>
<p>of different examples of inputs and
outputs and your loss function reflects</p>
<p>how well your model performs across all
of them typically as just some average</p>
<p>overpour data point losses so with this
set up you can then proceed to find the</p>
<p>optimal model through gradient descent
the idea is that the gradient is a</p>
<p>mathematical object that tells you how
much the loss changes in response to a</p>
<p>change in the parameters and then you
want to knowing that change the</p>
<p>parameters in a way which is fruitful
that is it reduces the loss it reduces</p>
<p>the measure of error so what makes deep
learning deep what is the deep part it&rsquo;s</p>
<p>this idea that function composition is
at the core of the models that we make</p>
<p>and that we consider so function
composition just means that you have a</p>
<p>bunch of different parametrized
functions and the outputs of one are the</p>
<p>inputs to the next one and you can
arrange these in many different</p>
<p>topologies we&rsquo;ll call these
architectures for neural networks the</p>
<p>very simplest kind is just one where you
have an input layer and then there is a</p>
<p>matrix that multiplies that and then you
maybe add some bias to that vector and</p>
<p>then you pass that through a nonlinear
activation function typically this is</p>
<p>going to squash the outputs from that
first linear transformation into</p>
<p>something which maybe is in the range
from 0 to 1 or 0 to infinity something</p>
<p>relatively simple but that non-linearity
happens to do a lot of work and then</p>
<p>when you have successive layers what it
allows the model to do ultimately is</p>
<p>represent successively more complex
features internally so you might think</p>
<p>of the output of each layer as being a
new representation of the original input</p>
<p>which has maybe rearranged the
information in a way which is easier for</p>
<p>some kind of final decision making
procedure at the end of the network to</p>
<p>make the right decision based on aside
from that very simple</p>
<p>there are also substantially more
complex ones so the other two diagrams</p>
<p>on this slide are for lsdm networks so
that&rsquo;s in the lower left and the</p>
<p>transformer network that&rsquo;s on the right
an LS TM network is a recurrent neural</p>
<p>network the idea is that this is the
kind of network that can accept a time</p>
<p>series of inputs and produce a time
series of outputs and internally it has</p>
<p>some very complicated mechanisms for
making sure that information gets</p>
<p>propagated effectively across time steps
in a hidden state so that when you make</p>
<p>a decision somewhere in the future you
can remember something that you saw in</p>
<p>the past and then you can update the
network in a way which is stable and</p>
<p>reasonable the transformer network is
substantially more complicated and it</p>
<p>allows networks to do something called
attending over their various inputs so</p>
<p>at attention it&rsquo;s something which is a
concept that we can all kind of relate</p>
<p>to when we look at the world we don&rsquo;t
actually process literally every piece</p>
<p>of data that we take in concurrently we
particularly attend to whatever happens</p>
<p>to be say in the center of our field of
view or whatever we&rsquo;re thinking about at</p>
<p>the moment whatever is most urgent and
attention neural networks are able to</p>
<p>basically do that when they make some
decision on the basis of a lot of data</p>
<p>they can select out the most important
pieces of the data for making particular</p>
<p>kinds of decisions and that turns out to
be very helpful in practice a few other</p>
<p>things about deep learning and this is
mostly just I&rsquo;m checking off some boxes</p>
<p>if you want depth on this I strongly
recommend that you go see the spinning</p>
<p>up essay where there are a bunch of
links to papers and other resources that</p>
<p>will give you detailed information about
this but to check off the boxes we might</p>
<p>talk about regularizer x&rsquo; so the idea is
that sometimes optimizing your loss</p>
<p>function picking the model that actually
gives the lowest value of your loss</p>
<p>function may not be the best thing to do
you may wind up with a phenomenon called</p>
<p>overfitting where you&rsquo;ve made your model
behave perfectly with respect to the</p>
<p>data that you showed it but then it does
a terrible job when it&rsquo;s given any other</p>
<p>data
because it learned a decision rule which</p>
<p>was entirely too specific but with
regularization you trade off the loss</p>
<p>against something which has nothing to
do with performance on the particular</p>
<p>task but just kind of says hey cool your
jets a little bit</p>
<p>don&rsquo;t be so avid about satisfying that
objective and then it turns out that</p>
<p>regularization actually leads to models
that do a better job of generalizing to</p>
<p>unseen data then there are also a couple
of things that make the optimization</p>
<p>process smoother and easier so you might
do some kind of normalization technique</p>
<p>where internally there&rsquo;s some output in
the middle of the network where it&rsquo;s</p>
<p>good to adjustably rescale that and
shift it around and that&rsquo;s better than</p>
<p>just letting the network do whatever it
would have done if you didn&rsquo;t do this</p>
<p>kind of normalization it&rsquo;s sort of
spooky and there are some legitimate</p>
<p>complaints inside the community about
whether or not we really understand why</p>
<p>this helps but it seems to so it&rsquo;s worth
knowing about also you might use a more</p>
<p>powerful optimizer than standard
gradient descent this comes up also in</p>
<p>reinforcement learning actually many of
the things that we&rsquo;ve been talking about</p>
<p>in these past few slides show up in deep
reinforcement learning which is why I&rsquo;m</p>
<p>bringing them up
adaptive optimizers do something special</p>
<p>in figuring out how to tune the learning
rate the amount by which you change each</p>
<p>parameter at each step of updating in a
way which leads to typically faster</p>
<p>convergence so you get to the to the
optimum point a little bit sooner or a</p>
<p>little bit easier there&rsquo;s also the
Reaper amortization trick but that&rsquo;s</p>
<p>quite complicated and so we won&rsquo;t
actually talk about it it&rsquo;s on the slide</p>
<p>so that you know where to look all right
that&rsquo;s all this stuff from deep learning</p>
<p>that I wanted to talk about
now onto reinforcement learning so first</p>
<p>and foremost we have to talk about how
do you formulate a reinforcement</p>
<p>learning problem what does that mean
what does that do what are the pieces of</p>
<p>it how do they fit together we typically
use the language of saying that there&rsquo;s</p>
<p>an agent that interacts with an
environment so the agent is whatever</p>
<p>thing is making some kind of decision
the environment is wherever those</p>
<p>decisions are happening and the thing
that creates the consequences of those</p>
<p>decisions
and there&rsquo;s this loop where the</p>
<p>environment has some state and has some
measure of how good it is to be in that</p>
<p>state that&rsquo;s a reward and the agent gets
to observe the state and possibly the</p>
<p>reward it uses the reward for learning
whether or not it observes it as a</p>
<p>subtle technical detail but anyway
okay the agent gets a state observation</p>
<p>and a reward and then the agent makes
some kind of decision about what action</p>
<p>to take it picks the action and it
executes sit in the environment and then</p>
<p>the state of the environment changes
there&rsquo;s a new state of the environment</p>
<p>the agent perceives it the agent acts
etc the goal of the agent is to figure</p>
<p>out what decisions will maximize the sum
total of rewards that it&rsquo;ll ever get</p>
<p>actually it&rsquo;s slightly more specific
than this and there are a couple of</p>
<p>different formulations that we can
choose and we&rsquo;ll talk about them</p>
<p>momentarily but that&rsquo;s basically it in a
nutshell we want to maximize this sum of</p>
<p>rewards that we get and the agent is
going to figure out how to attain that</p>
<p>goal through trial and error so you just
don&rsquo;t know in advance what the right</p>
<p>thing to do is so you have to just try
things see what happens see how much</p>
<p>reward you get and then adjust your
decision on the basis of that so</p>
<p>reinforcement learning is about
algorithms for doing precisely that but</p>
<p>before we can talk about the algorithms
we have to introduce a bunch of</p>
<p>terminology for those of you who have
done the work of going through the</p>
<p>spinning up material online this will
probably be quite familiar and I&rsquo;m</p>
<p>mostly going through it for the benefit
of the audience that I expect might</p>
<p>watch this in the future as a starting
point for this so bear with me I&rsquo;ll try</p>
<p>to go through this reasonably quickly
but we have to talk about observations</p>
<p>and actions policies trajectories
rewards and returns what the RL</p>
<p>optimization problem actually is how we
formalize it and then value in action</p>
<p>value functions and also advantage
functions so there&rsquo;s a whole lot of</p>
<p>stuff that you kind of have to know and
unpack in order to really fruitfully</p>
<p>progress and reinforcement learning and
and these are just those central pieces</p>
<p>so observations and actions a state is
something which tells you absolutely</p>
<p>everything about the environment
the agent usually doesn&rsquo;t get access to</p>
<p>the state there is usually some stuff
that&rsquo;s just hidden from the agent so</p>
<p>what the agent perceives is called an
observation if the observation contains</p>
<p>all the information in a state we called
this environment fully observed if it</p>
<p>doesn&rsquo;t we call it partially observed
and states observations and actions can</p>
<p>be continuous or discrete for all of the
problems that we care about in deep RL</p>
<p>the observations are continuous and the
actions might be discrete or continuous</p>
<p>a policy is a rule for selecting actions
there are a couple of different ways</p>
<p>that you can get to this kind of rule
we typically classify them as one of two</p>
<p>kinds stochastic or deterministic a
stochastic policy is a rule for randomly</p>
<p>selecting an action on the basis of the
most recent observation or possibly</p>
<p>preceding observations as well a
deterministic policy is just a map</p>
<p>directly from observation to action and
no randomness involved at all you may be</p>
<p>wondering why it would be useful to have
a random policy at all because it might</p>
<p>seem like randomness is just sort of
dangerous but actually it can be quite</p>
<p>helpful and there are some very
principled ways of optimizing stochastic</p>
<p>policies and it&rsquo;s a little bit harder to
optimize completely deterministic</p>
<p>policies there may also be a matter of
robustness in that having a little bit</p>
<p>of randomness can make you more robust
sometimes to perturbation then having</p>
<p>learned a brittle specific deterministic
policy so now just to give some sort of</p>
<p>concrete examples in tensorflow
because I assume that most of you will</p>
<p>probably have met tensorflow as your
first deep learning library and if not</p>
<p>pi torch and for those of you who are
stuck with tensorflow I&rsquo;m so sorry you</p>
<p>probably should have picked pi torch I
know I should have but but here we are</p>
<p>so in in tensorflow for a stochastic
policy over discrete actions we might</p>
<p>first set up a placeholder for loading
in observations and then we might set up</p>
<p>a multi-layer perceptron network and MLP
network so this is just the most basic</p>
<p>kind of feed-forward neural network the
thing that I talked about earlier which</p>
<p>is a succession of linear transforms of
inputs followed by nonlinear transforms</p>
<p>of inputs
in this case the linear transforms take</p>
<p>you to something of size 64 and there
are two of them and then the activation</p>
<p>is at an H activation so this gets you
to a range of minus one to one in a nice</p>
<p>smooth way and and then we produce
logits based on the output from that</p>
<p>piece of the network so logits are
basically something that proceeds having</p>
<p>probabilities for particular actions if
you take the softmax of the logit that&rsquo;s</p>
<p>not a function you&rsquo;re familiar with I
recommend looking it up it&rsquo;s just</p>
<p>something that exponentiate Sall the
logits and then divides by the sum of of</p>
<p>those exponentiated logins so so it
normalizes the distribution to to being</p>
<p>a probability distribution all the
entries have to be greater than zero and</p>
<p>sum up to 1 so we get logits and then we
get actions by using TF multinomial to</p>
<p>sample something stochastically assuming
that the probabilities are based on</p>
<p>taking the softmax of those logits you
can ignore the squeeze that&rsquo;s just there</p>
<p>for making sure that certain things
actually work and then in the</p>
<p>deterministic policy let&rsquo;s say we have a
continuous action case so we want to</p>
<p>output a vector of actions where each
entry can be any real value number we</p>
<p>will just go from observation to network
to a final layer which is just going to</p>
<p>be the actions all right so that&rsquo;s
policies let&rsquo;s talk about a trajectory a</p>
<p>trajectory is a complete sequence of
states and actions through the history</p>
<p>of an environment the agent starts in a
state takes an action then there&rsquo;s a</p>
<p>next state next action etc the first
state in the environment is sampled from</p>
<p>some previous distribution over starting
States and then afterwards state</p>
<p>transitions are going to be either
deterministic or stochastic but there&rsquo;s</p>
<p>just some rule in the environment that
given the current state and the current</p>
<p>action whatever action the agent took
picks what the next state is a</p>
<p>trajectory is also sometimes called an
episode or a rollout you&rsquo;ll see this</p>
<p>terminology used completely
interchangeably so just be aware that&rsquo;s</p>
<p>out there there&rsquo;s I&rsquo;m so sorry in every
new ish field a lot of terminology</p>
<p>confusion we&rsquo;re different people in
different</p>
<p>areas of academia worked on it for a
while and use different terms and then</p>
<p>in the end we&rsquo;re left with just a weird
mishmash notation - you&rsquo;re gonna see</p>
<p>some notation where states and actions
are notated by s and a and then in code</p>
<p>you&rsquo;ll see some places where it&rsquo;s X and
u and this is because of the ancient</p>
<p>eternal conflict between the control
theorists and the reinforcement learning</p>
<p>theorists and we&rsquo;re just stuck with it
now so that aside let&rsquo;s talk about</p>
<p>rewards and returns so a reward function
is going to map from the states and</p>
<p>actions or states and actions and
possibly next States on to just some</p>
<p>number that tells you good or bad
positive is good negative is bad the</p>
<p>more positive the better and you have to
if you&rsquo;re a designer setting up a</p>
<p>reinforcement learning problem you have
to pick with that reward function is</p>
<p>going to be so you want to make sure
that you incentivize the stuff that you</p>
<p>want to have happen and disincentivize
stuff that you don&rsquo;t want to have happen</p>
<p>so as a very simple example suppose that
you want a robot to run forward but you</p>
<p>don&rsquo;t want it to waste a ton of energy
so maybe you will give it a reward</p>
<p>proportional to its forward velocity but
you&rsquo;ll penalize it proportionally to the</p>
<p>some of the action magnet or to the
action magnitude so you&rsquo;ll discourage</p>
<p>superfluous actions the return of a
trajectory is going to be some</p>
<p>cumulative reward along it we have two
ways of formulating this and what you&rsquo;re</p>
<p>going to find in deep reinforcement
learning implementations is that we&rsquo;re</p>
<p>going to completely conflate which
problem we&rsquo;re trying to solve with the</p>
<p>other but the finite horizon
undiscounted sum of rewards works when</p>
<p>you have a finite horizon it doesn&rsquo;t
work when you have an infinite horizon</p>
<p>because if you have an infinite sum of
things it might diverge unless you do</p>
<p>some kind of discounting so in this
other case infinite horizon discounted</p>
<p>some of returns you have a discount
factor gamma between 0 &amp; 1 and that&rsquo;s</p>
<p>how you down weight things that happen
in the future this makes sure that this</p>
<p>is a reasonably well-defined quantity
but why would it make sense to discount</p>
<p>things you probably would rather someone
tell you that they&rsquo;re gonna give you</p>
<p>$100 today than $100 in a hundred years
right like it&rsquo;s just good to get</p>
<p>upfront then there&rsquo;s the reward to go
this is closely related it&rsquo;s basically</p>
<p>just a measure of return starting from a
particular time step or state so the</p>
<p>reward to go from some point in time is
just the sum of rewards that&rsquo;ll happen</p>
<p>after that point in time and now we can
talk about the reinforcement learning</p>
<p>problem just formally we&rsquo;re going to set
up a performance measure for a</p>
<p>particular policy PI J of Pi which is
the expected value of return for</p>
<p>whichever formulation we&rsquo;ve picked
according to a distribution over</p>
<p>trajectories in the environment based on
the choice of policy so what that means</p>
<p>is that again start states come from a
starting distribution transitions in the</p>
<p>environment are based on something in
the environment that transition</p>
<p>distribution P and actions will come
from the policy conditioned on the the</p>
<p>observations of the states and we want
to find the optimal policy PI star which</p>
<p>maximizes this now we have to talk about
value functions so value functions are</p>
<p>measures of how much reward you expect
to get from a particular State or state</p>
<p>action pair assuming that you&rsquo;re going
to behave a certain way so we have the</p>
<p>on policy value function and action
value function V PI and Q PI which</p>
<p>respectively tell you how good it is to
be in a particular state and how good it</p>
<p>is to be in a particular state action
pair assuming that forever after being</p>
<p>in those places you act according to the
policy PI and then there&rsquo;s also V star</p>
<p>and Q star same thing except if you were
to act according to the optimal policy</p>
<p>it&rsquo;s great to know Q star as we&rsquo;ll talk
about momentarily value and action value</p>
<p>functions are connected the value is
just the expected action value expecting</p>
<p>over what option you might take
according to the current policy and the</p>
<p>advantage function tells you how much
better a given action is than average</p>
<p>and it&rsquo;s just the difference between Q
and V these value functions satisfy</p>
<p>recursive bellman equations these are
super important and they&rsquo;re the</p>
<p>foundation of a bunch of algorithms so
they&rsquo;re really worth knowing and kind of</p>
<p>just worth grappling with I think that
these can be particularly tricky at</p>
<p>first I remember the first time that I
met reinforcement learning I was just so</p>
<p>turned around and lost by these the
notion that there was going to be this</p>
<p>recursive equation where the definition
of a thing depended on itself was quite</p>
<p>confusing but it&rsquo;s it&rsquo;s it&rsquo;s worth just
hitting your head on for a while until</p>
<p>it makes sense but what it&rsquo;s saying is
that the value of being in a particular</p>
<p>place is going to be as good as whatever
reward you get for being in that place</p>
<p>plus all the rewards that you&rsquo;ll ever
get for all the places you&rsquo;ll go</p>
<p>afterwards now why is it great to know Q
star Q star tells you if you&rsquo;re gonna</p>
<p>act according to the optimal policy
forever after you started in this state</p>
<p>and took this action and we don&rsquo;t care
what policy this action came from how</p>
<p>well will you do so that means that if
you want to do the best you possibly can</p>
<p>do all you need to know is what action
maximizes Q star in a particular state</p>
<p>and then take that action because that&rsquo;s
gonna be the best action in that state</p>
<p>and then afterwards you&rsquo;ve assumed that
you&rsquo;re gonna do the best that you can</p>
<p>ever possibly do so if you have Q star
you basically have the optimal policy so</p>
<p>this is going to lead us ultimately to
the two different kinds of algorithms</p>
<p>and reinforcement learning for control
where in one case we&rsquo;ll try to directly</p>
<p>optimize a policy and in the other case
we&rsquo;ll try to find Q star now if we want</p>
<p>to find Q star we have to set up a
function approximator for it q theta</p>
<p>which will represent by some kind of
deep neural network and we&rsquo;re gonna want</p>
<p>to measure how good is it at
approximating Q star and this is what</p>
<p>that recursive bellman equation is gonna
be really helpful for because the</p>
<p>beautiful thing is we don&rsquo;t need to have
acted according to the optimal policy to</p>
<p>check how well Q theta fits that bellman
equation we just need a bunch of</p>
<p>examples of state action next state and
reward tuples and if we have enough of</p>
<p>those over enough of the environment
then we can probably do a pretty good</p>
<p>job of fitting q theta based on that
bellman equation based on maybe this</p>
<p>means squared bellman error and then use
that afterwards for control which is</p>
<p>having a decision-making rule by the way
I apologize if anything has been</p>
<p>confusing about my using sort of the
terminology of control interchangeably</p>
<p>with the terminology of reinforcement
learning when I say control I mean</p>
<p>having the best
policy so now what kinds of RL</p>
<p>algorithms are out there behold a
taxonomy which is much more restrictive</p>
<p>than it looks it looks very pretty and
it looks very definitive but it&rsquo;s</p>
<p>actually masking a lot of subtlety and
you know detailed choices and the fact</p>
<p>that there&rsquo;s actually a lot more bleed
over between these things than you might</p>
<p>expect
but at a very high level this is a</p>
<p>useful picture to start with that we
have two different kinds of RL</p>
<p>algorithms ones where we have access to
the model of the environment and ones</p>
<p>where we don&rsquo;t so what that means a
model of the environment is something</p>
<p>which tells us if we&rsquo;re in a given state
and we take a particular action what&rsquo;s</p>
<p>gonna happen next the model would
predict what the state of the</p>
<p>environment will be after that and
that&rsquo;s really useful because if we can</p>
<p>forward simulate the environment then
that&rsquo;s extremely helpful for evaluating</p>
<p>our current policy it&rsquo;s extremely
helpful for figuring out what a better</p>
<p>action would be than the one that we
might want to take so if you don&rsquo;t have</p>
<p>a model you&rsquo;re quite limited you just
have to figure out how to do well based</p>
<p>on experiences that you&rsquo;ve seen your
direct interactions of the environment</p>
<p>you don&rsquo;t get any other information but
if you do have a model it&rsquo;s quite</p>
<p>potentially powerful although as we&rsquo;ll
discuss the methods for model-based</p>
<p>reinforcement learning are not quite as
mature so far as the methods for model</p>
<p>free reinforcement learning so now okay
that last slide was just a ton of</p>
<p>acronyms maybe not that insightful let&rsquo;s
talk about what these algorithms are</p>
<p>doing there are three key pieces in any
reinforcement learning algorithm for one</p>
<p>you&rsquo;re going to run the policy in the
environment you&rsquo;re going to actually try</p>
<p>things and get to some signal error or
otherwise and then you&rsquo;re going to have</p>
<p>to reflect and evaluate whether or not
those decisions were good ones whether</p>
<p>or not those actions were the right ones
you have to figure out how good your</p>
<p>current policy is so that you can use
that information to improve it so you</p>
<p>run the policy you evaluate the policy
you improve the policy and there are a</p>
<p>bunch of different ways of doing that
and we&rsquo;ll go into some depth about how</p>
<p>different algorithms go about doing that</p>
<p>so let&rsquo;s start with policy optimization</p>
<p>minor interlude in the chat last night I
surveyed people to see what they were</p>
<p>interested in
I asked if people were interested in</p>
<p>math there&rsquo;s gonna be some math so first
at a very high level zooming out ten</p>
<p>thousand foot view in policy
optimization we&rsquo;re going to run the</p>
<p>policy by collecting complete
trajectories or snippets of trajectories</p>
<p>based on our current stochastic policy
and we&rsquo;re going to explicitly represent</p>
<p>that stochastic policy with a neural
network that perhaps gives these</p>
<p>sufficient statistics of the action
distribution or something else that we</p>
<p>can use to derive that and sample from
it and then we&rsquo;re going to evaluate the</p>
<p>policy by figuring out the on policy
value function and advantage function</p>
<p>and we&rsquo;re going to evaluate those things
for all the states and actions in the</p>
<p>trajectories that we sampled and then
we&rsquo;re going to improve the policy by</p>
<p>making it more likely that we take the
actions that led to higher advantage and</p>
<p>making it less likely that we take the
actions that led to lower advantage less</p>
<p>likely that we take the bad actions how
do we do that we&rsquo;re going to have to</p>
<p>talk about some math now I realize
there&rsquo;s a chance that most of you maybe</p>
<p>weren&rsquo;t expecting that we would be doing
any kind of deep mathematical excursion</p>
<p>but if there&rsquo;s one thing that I want you
to take away from today aside from just</p>
<p>being excited about deep RL it&rsquo;s a
realization that there are some</p>
<p>limitations to what deep RL can
currently do and that this is not really</p>
<p>a hundred percent done as a technology
where you can just apply it to a problem</p>
<p>without really thinking about what it&rsquo;s
doing under the hood and get a good</p>
<p>solution it&rsquo;s not a black box technology
yet so if you want to try deep RL on a</p>
<p>problem and grapple with getting it to
work you do have to kind of understand</p>
<p>what&rsquo;s going on under the hood and that
means taking a look at some of the gory</p>
<p>mathematical details understanding how
they connect and forming an intuition</p>
<p>for how those details will shape the
failure</p>
<p>most of your algorithm so what we&rsquo;ll
talk about we&rsquo;re just gonna talk about</p>
<p>vanilla policy gradient we&rsquo;re gonna talk
about how you derive the policy gradient</p>
<p>and a bunch of different equivalent
expressions for it and then we&rsquo;ll get to</p>
<p>the pseudocode for the sort of standard
version of vanilla policy gradient which</p>
<p>includes maybe a few more tricks and
details than the very most basic vanilla</p>
<p>version apologies for the choice of
words there but all of this stuff is</p>
<p>critical to understanding more advanced
policy optimization algorithms like Terp</p>
<p>oh and PPO we won&rsquo;t be covering them in
these slides but again happy to talk</p>
<p>about them offline during the hackathon
so in policy grading algorithms what we</p>
<p>want to do is we want to find some kind
of expression for the gradient of the</p>
<p>policy performance with respect to the
parameters of the policy and we want to</p>
<p>just directly gradient to send on those
parameters so we&rsquo;re going to move the</p>
<p>parameters in the direction that
increases performance and is this gonna</p>
<p>be easy or hard well if we just try
putting the gradient onto the policy</p>
<p>performance we run into a problem all
the parameters are down here in the</p>
<p>distribution they&rsquo;re not inside here
where we would like them if we want to</p>
<p>get something that we can actually use
we&rsquo;ll have to do some messy work to</p>
<p>bring the gradient inside of an
expectation which we could then form a</p>
<p>sample estimate of so step one to
getting the gradient symbol somewhere</p>
<p>helpful we&rsquo;re going to recognize that
this expectation can be rewritten as an</p>
<p>integral going through all of the events
in trajectory space every possible</p>
<p>trajectory of the density the
probability mass or density for that</p>
<p>trajectory based on that policy times
the return that you would get for being</p>
<p>on that trajectory and now we can bring
the gradient in because the limits of</p>
<p>this integral don&rsquo;t have anything to do
with the parameters and then we apply</p>
<p>the log derivative trick so this is a
really helpful mathematical trick comes</p>
<p>up all over the place and deep
reinforcement learning it&rsquo;s basically</p>
<p>just this notion that the derivative of
log of sum</p>
<p>thing is one over that something times
the derivative of that something and we</p>
<p>rearrange it slightly but it lets us go
from the gradient with respect to theta</p>
<p>of P to P times gradient log P this is
great because now we have an expectation</p>
<p>again we have an expectation based on
trajectories sampled according to the</p>
<p>current policy so if we have that data
we can make a sample estimate certainly</p>
<p>so the very nice thing here is that what
we did after bringing the gradient</p>
<p>inside the integral and doing this log
derivative trick is that we now have</p>
<p>something which is an expectation again
because we&rsquo;re integrating through all</p>
<p>possible trajectories of the probability
density associated to that trajectory</p>
<p>times something which is a function of
that trajectory so this is an</p>
<p>expectation and we can form a sample
estimate of it that we can use in a</p>
<p>practical algorithm but we&rsquo;re not
completely finished yet because we still</p>
<p>have to talk about what&rsquo;s the gradient
of that log probability for a trajectory</p>
<p>how does that depend on the parameters
of the policy so let&rsquo;s go back to the</p>
<p>picture that we had in the beginning
there&rsquo;s a starting state which is drawn</p>
<p>from some distribution based on the
environment and then after that you pick</p>
<p>it the agent picks in action based on PI
theta and it has probability PI theta a</p>
<p>given s for time step 0 then the
environment picks the next state</p>
<p>according to whatever distribution it
has over next States given your most</p>
<p>recent action in the most recent state
by the way this is something that I lost</p>
<p>over earlier slightly more formalism
details that you don&rsquo;t quite need to</p>
<p>know but this is called the Markov
property this notion that picking the</p>
<p>next state only depends on the most
recent thing that happened and doesn&rsquo;t</p>
<p>depend on the past before it that&rsquo;s the
the Markov property and you&rsquo;ll find a</p>
<p>whole bunch of math if you go digging
for it but you don&rsquo;t have to for for</p>
<p>this at the very least so then what we
have is that the probability of the</p>
<p>trajectory is going to be just the
probability of that first state x the</p>
<p>probabilities of each transition and
action selection that happens afterwards</p>
<p>so we get that expression up there at
the top and now if we want to take its</p>
<p>gradient of its log we just
pretty straightforwardly compute first</p>
<p>the log of that thing turns that product
into a bunch of sums the gradient goes</p>
<p>through the sums and now all the terms
that are based on distributions from the</p>
<p>environment have no dependence on the
parameters of the policy the environment</p>
<p>doesn&rsquo;t care what the policy is it&rsquo;s
just going to behave in whatever way it</p>
<p>does so those have no dependence on the
parameters those derivatives are zero</p>
<p>and what we&rsquo;re left with is just
something which is as some overtime</p>
<p>steps of gradients of the policy and the
beautiful thing is because we control</p>
<p>the policy and we have explicitly
represented it as a neural network and</p>
<p>we can compute all of its gradients this
is a thing that we can calculate so now</p>
<p>we&rsquo;re at something where we can in fact
calculate a sample estimate of this</p>
<p>gradient of policy performance and use
that as the basis for a gradient ascent</p>
<p>algorithm for improving performance but
it&rsquo;s not good enough we&rsquo;re not done yet</p>
<p>yes the function capital e so so this
this capital e is an expectation and if</p>
<p>we want to form an estimate for the
expectation so we&rsquo;re not going to</p>
<p>compute the expectation exactly what
we&rsquo;re going to do is we&rsquo;re going to see</p>
<p>what happens for a bunch of different
trajectories that are sampled according</p>
<p>to the distribution specified in that
expectation and then we&rsquo;re just going to</p>
<p>average them and in the limit as we have
an infinite amount of data that sample</p>
<p>average becomes exactly equal to the
expectation yes</p>
<p>absolutely absolutely you can so it is a
bunch of derivatives of the final output</p>
<p>with respect to each one of the
parameters right because there are many</p>
<p>inputs to this function and we&rsquo;re going
to have a derivative with respect to all</p>
<p>of them yes I&rsquo;m sorry can you repeat the
question yes can we tie this explicitly</p>
<p>to reward so inside the expectation here
we have R of tau so that&rsquo;s the return</p>
<p>measure that we&rsquo;ve chosen whichever one
we picked either the infinite horizon</p>
<p>discounted sum of rewards along the
trajectory tau or just the finite</p>
<p>horizon undiscounted sum of rewards so
that R of tau is the sum of all the</p>
<p>rewards in a particular trajectory and
that&rsquo;s actually why the the variance of</p>
<p>this is going to be so unnecessarily
high they&rsquo;re going to be a bunch of</p>
<p>terms in this sample expression actually
just in that expectation which which</p>
<p>have expectation zero on average they&rsquo;re
zero they don&rsquo;t contribute anything but</p>
<p>we sample them anyway and the samples
will have noise on them and so we&rsquo;ll</p>
<p>just wind up getting the noise we won&rsquo;t
get much signal from them so can we</p>
<p>eliminate a whole bunch of terms yes we
absolutely can the intuition here is</p>
<p>that if I give you a reward in the past
and you want to update the action that</p>
<p>you just took really what you care about
for figuring out whether or not the</p>
<p>action that you just took was good or
bad are the consequences of that action</p>
<p>you don&rsquo;t care about what preceded it
that action and what preceded it are</p>
<p>almost completely uncorrelated there
you&rsquo;re not going to to get anything by</p>
<p>by updating the likelihood of that
action based on an old reward so that in</p>
<p>expectation is going to be zero and
knowing that we can now expand out this</p>
<p>return measure and we&rsquo;re going to
get this in the finite horizon case just</p>
<p>for simplicity but this analysis also
extends to the infinite horizon case so</p>
<p>we now have a sum of grad log probs of
the policy times the sum of rewards</p>
<p>we&rsquo;re gonna pull the sums out of this
expression so that we can just look at a</p>
<p>policy update at a particular time step
times a reward from a different time</p>
<p>step and then based on that thing that
we asserted above we&rsquo;re gonna drop all</p>
<p>the terms that are inconsequential all
of those are zero and so what we&rsquo;re left</p>
<p>with after we take away all the ones
where T greater than T Prime we&rsquo;re left</p>
<p>with this sum sum over the time steps
for the policy times a sum over time</p>
<p>steps for rewards that goes for all of
the time steps after the corresponding</p>
<p>policy time step and then if we bring
that back in what we&rsquo;re seeing now is</p>
<p>that we want to for each time step
adjust the probability of the action</p>
<p>from that time step in proportion to the
sum of rewards that came afterwards only</p>
<p>the consequences of an action will
affect its update yes so it&rsquo;s not that</p>
<p>you don&rsquo;t consider past actions the sum
over here in the beginning runs over all</p>
<p>time steps so every action is going to
get to some update it&rsquo;s just a matter of</p>
<p>which rewards are used in figuring out
the update for that action and it should</p>
<p>only be the ones that were consequences
of it yes yes</p>
<p>um well we do care about the future
right because here we have a sum of</p>
<p>rewards after a particular time step all
the rewards in the future from that time</p>
<p>step so so that expectation that&rsquo;s just
saying that an action that happens later</p>
<p>shouldn&rsquo;t be affected by a reward that
happened before it it should only only</p>
<p>be affected by the rewards that happen
afterwards so in the in the next slide</p>
<p>actually we&rsquo;ll see how this expression
that we have down here at the bottom</p>
<p>connects to the value functions so what
we currently have is what I&rsquo;ll call the</p>
<p>reward to go policy gradient because
what we&rsquo;re doing is we&rsquo;re adjusting the</p>
<p>probabilities of action proportionally
to the reward to go what we&rsquo;re going to</p>
<p>do now is go from that into an
expression that has q pi the action</p>
<p>value on policy for a state action pair
instead of that reward to go and this</p>
<p>works because you can break up the
expectation so first we&rsquo;re gonna pull</p>
<p>the sum over time steps out of this and
then this expectation over trajectories</p>
<p>this is sort of subtle and and maybe a
little math here then we can go into</p>
<p>detail on here but I recommend that you
go look on the spinning up website in RL</p>
<p>intro part 3 there&rsquo;s a link separately
to a proof about this but if we think</p>
<p>about the average thing that&rsquo;s going to
happen over all trajectories that&rsquo;s</p>
<p>going to be equivalent to the average
thing that happens over all of the cases</p>
<p>of something with the first T time steps
of the trajectory we&rsquo;re inside of the</p>
<p>expectation we&rsquo;ve moved all the stuff
that happens in the future</p>
<p>and we were able to move it inside past
this one because this only depends on</p>
<p>time step T this doesn&rsquo;t depend on stuff
after T so only this</p>
<p>it&rsquo;s gonna be affected by averaging over
the future and then it turns out that</p>
<p>that expression the average sum of
rewards that you get starting from a</p>
<p>time step assuming that the state and
action for that time step were fixed</p>
<p>that&rsquo;s exactly equal to the action value
that&rsquo;s exactly saying how good is it to</p>
<p>be in a particular state take a
particular action and then forever after</p>
<p>act according to a particular policy and
now we have this expression for the</p>
<p>policy gradient at the bottom we&rsquo;re most
of the way through the math okay but</p>
<p>what is a baseline a baseline is a
really important thing because it&rsquo;s</p>
<p>another tool in our Arsenal for taking a
policy gradient expression and turning</p>
<p>it into something which is lower
variance more likely to be useful for</p>
<p>producing a good update to the policy
and it&rsquo;s also the namesake for opening</p>
<p>eye baselines
well let&rsquo;s save one of them it&rsquo;s a</p>
<p>couple of things but we have a
expression here at the top which I claim</p>
<p>is basically true which is that the
gradient policy gradient is the thing</p>
<p>that we had before but instead of Q we
subtract out some function of state some</p>
<p>function b of st and i claim that in
expectation it works out exactly the</p>
<p>same and so there&rsquo;s a short proof here
for that which is that if we look at the</p>
<p>expectation for that part of it what
happens if you take the expected</p>
<p>gradient of the log probability of an
action in a state times some function b</p>
<p>of st the b doesn&rsquo;t have anything to do
with the action so it&rsquo;s a constant with</p>
<p>respect to this expectation so we pull
it out and then what we&rsquo;re left with is</p>
<p>an expectation over actions which will
rewrite and now we have it in</p>
<p>probability times grad log prop we&rsquo;re
going to reverse the log derivative</p>
<p>trick from earlier so this is now an
integral over actions of the gradient of</p>
<p>the probability of that action and we
can pull out the gradients</p>
<p>we&rsquo;re just sort of reversing the
procedure from earlier this thing this</p>
<p>integral over all possible actions of
the probabilities of those actions</p>
<p>that&rsquo;s just going to sum up to one
that&rsquo;s just saying probability</p>
<p>distribution is normalized all of the
chances together have to come out to</p>
<p>equaling 100% of sum them up and the
derivative of a constant since that&rsquo;s a</p>
<p>constant is nothing constant has no rate
of change</p>
<p>so we get zero so all of the terms of
grad log prob times the baseline in</p>
<p>expectation or zero so we&rsquo;re free to add
this baseline without changing what the</p>
<p>policy gradient is in expectation but we
can pick it in ways that are fruitful</p>
<p>and make the estimate better so the
typical thing to do is to pick the</p>
<p>baseline to be the value function and
this leads us to kind of our our final</p>
<p>sort of ultimate form of the policy
gradient the form with advantage</p>
<p>functions and why is this good why is
this good the advantage function says</p>
<p>how much better in action is than
average why would you prefer that over</p>
<p>just how good the action is well let&rsquo;s
say you have two actions one gets you a</p>
<p>hundred dollars one gets you one hundred
and one dollars you only sample the one</p>
<p>that gets you one hundred now when
you&rsquo;re trying to update your policy you</p>
<p>can feel really great about that oh man
100 is a big number I feel great</p>
<p>I&rsquo;m gonna double down on that action
you&rsquo;re acting sub-optimally if you had</p>
<p>been picking 5050 on average you would
have gotten a hundred dollars and fifty</p>
<p>cents and you would have realized that
the advantage of taking the action that</p>
<p>you picked one hundred dollars and fifty
minus a hundred dollars and fifty cents</p>
<p>you lost fifty cents should pick the
other action so you prefer to use</p>
<p>advantages to figure out which actions
to increase the likelihood of as opposed</p>
<p>to just Q values all right summing it up
we have these four different forms of</p>
<p>the policy gradient they&rsquo;re all tightly
connected we care about the last one but</p>
<p>to get to the last one we had to go
through the pain but now that we&rsquo;ve all</p>
<p>gone through that pain together you&rsquo;re
stronger you can go and you can</p>
<p>implement this and it&rsquo;ll work and you&rsquo;ll
know why it works and you&rsquo;ll feel good</p>
<p>about that and if it breaks you can fix
it</p>
<p>all right so then just to sum it up this
key concept we want to push up the</p>
<p>probabilities of good actions push down
the probabilities of bad ones and also</p>
<p>importantly that expectation requires
trajectories sampled from the current</p>
<p>policy so this is the concept of being
on policy and reinforcement learning</p>
<p>that if you want to update your policy
you have to use data from that policy</p>
<p>you can&rsquo;t use data from some other
policy unless you appropriately</p>
<p>reweighed it but relating data is
complicated and really tricky so it&rsquo;s</p>
<p>sort of preferred to not do it unless
you are trying to build something new</p>
<p>and cool and super sample efficient and
you&rsquo;re willing to spend a lot of time</p>
<p>and effort doing research on making sure
that it actually works but ok so the</p>
<p>policy gradient expression gives us the
policy improvement step coming back</p>
<p>coming back a bit oh yeah sure the
question was how do we know what the</p>
<p>average reward would have been so that
we could figure out how to make the</p>
<p>advantage function in the first place do
we compute it as we go and and actually</p>
<p>that&rsquo;s exactly what the next slide is
about which is how do we do that</p>
<p>business of policy evaluation how do we
find an estimate of the advantage</p>
<p>function which is actually good and
reasonable if we just have a bunch of</p>
<p>data where do we get the value function
that we might use to subtract out as a</p>
<p>baseline and the idea here is that we&rsquo;re
going to learn it from data and</p>
<p>typically it&rsquo;s going to be by regression
so this will be a subroutine that you&rsquo;ll</p>
<p>find in most policy optimization
algorithms where you&rsquo;re going to have a</p>
<p>value function approximator another
neural network and you&rsquo;re going to at</p>
<p>each epoch of the policy optimization
algorithm update the value network to</p>
<p>try to match the empirical returns that
you saw so for a particular state the</p>
<p>value should be more or less the sum of
discounted rewards that you saw off to</p>
<p>then
and then when you have the value</p>
<p>function approximator you can use that
to estimate advantages and we&rsquo;ll talk a</p>
<p>bit about estimating advantages from
value function approximate us on the</p>
<p>next slide but first you may have
noticed that I pulled a fast one on you</p>
<p>which is that we went from in all the
preceding slides dealing with the finite</p>
<p>horizon undiscounted case and then here
in our optimization problem for learning</p>
<p>the value function I&rsquo;ve dropped in
discount factors why is that the answer</p>
<p>is because everyone does it this is
where there&rsquo;s not a particularly good</p>
<p>reason in my opinion that this happens
but pretty much every policy</p>
<p>optimization algorithm that I&rsquo;m aware of
every every single implementation uses</p>
<p>discounted value functions and advantage
functions but then treats the policy</p>
<p>optimization part as undiscounted it
creates some bias it seems to work</p>
<p>shrug it&rsquo;s perfectly reasonable to do
that so it sometimes seems to be helpful</p>
<p>to set the discount factor to something
a little smaller than one so keeping it</p>
<p>completely undiscounted would be gamma
equals 1 for whatever reason with some</p>
<p>optimization problems there&rsquo;s some some
RL problems it&rsquo;s a little bit harder if</p>
<p>you pick gamma equals 1 than gamma 0.95
and i can&rsquo;t say that there&rsquo;s a</p>
<p>particularly good reason for this I
would speculate that like in the</p>
<p>beginning of training if you pick a very
high discount factor those empirical</p>
<p>returns will be very noisy and if you
choose a discount factor less than 1</p>
<p>what you&rsquo;re going to do is you&rsquo;re going
to attenuate some of the noise you&rsquo;ll</p>
<p>bias that sum of rewards so that
whatever happens soonest matters most</p>
<p>and if you happen to see a few positive
rewards in a row then you&rsquo;ll latch on to</p>
<p>that whereas maybe because of noise if
you had really paid attention to</p>
<p>everything out to infinity you&rsquo;d have
just gotten a bunch of positives and</p>
<p>negatives and positives and negative and
they would have cancelled out uh I think</p>
<p>it&rsquo;s it&rsquo;s ok to think about it like that
yeah</p>
<p>yes yes that after a certain point the
trajectory just ends you get it to time</p>
<p>step T and then it&rsquo;s over
that&rsquo;s finite horizon infinite horizon</p>
<p>you go out to infinity alright so then
how do we calculate the advantage</p>
<p>function given data from trajectories
and a value function approximator so a</p>
<p>thing that I want to introduce here is
this notion of n step advantage</p>
<p>estimates so what you&rsquo;re going to do is
you&rsquo;re going to have a thing over on the</p>
<p>left side that approximates Q pi and a
thing over on the right side that</p>
<p>approximates V PI so this thing for Q pi
remember that that&rsquo;s supposed to be an</p>
<p>estimate for how well you&rsquo;ll ever do if
you start in a state take an action and</p>
<p>then act according to the policy forever
after you can just use the empirical</p>
<p>return the reward to go from that state
as a sample estimate of the expected</p>
<p>value which is the Q value but in an N
step advantage estimate what we&rsquo;re going</p>
<p>to do is we&rsquo;re not going to go all the
way out to the end of the trajectory in</p>
<p>that sample estimate for Q we&rsquo;re going
to go n steps in and then use the value</p>
<p>function approximator to assume what&rsquo;s
going to happen for the rest of it and</p>
<p>this corresponds to a decision about how
much bias or variance we find acceptable</p>
<p>in this advantage estimator so if you
pick n equals 0 then your advantage</p>
<p>estimator in that case would be just the
reward plus gamma times the value</p>
<p>function approximator for the next time
step minus the value function</p>
<p>approximator for the current time step
and that&rsquo;s gonna be very high bias</p>
<p>because whatever is wrong with your
value function is not going to be wrong</p>
<p>with your advantage function but it&rsquo;ll
be really low variance because the only</p>
<p>thing that&rsquo;s going to have variance to
it is the reward and the stochasticity</p>
<p>in the next state transition but if on
the other hand you pick n equals</p>
<p>infinity so for the q approximator you
just take the exact sum of rewards that</p>
<p>you got in the real trajectory and then
at the end you subtract out the value</p>
<p>function at st you&rsquo;re going to accept
all of the variance that&rsquo;s in the</p>
<p>environment
but the nice thing is you don&rsquo;t have</p>
<p>bias in forming your policy gradient
estimator with this because in</p>
<p>expectation the Q part is going to be
exactly Q in expectation and the B part</p>
<p>recall that that was a baseline that we
added with a guarantee of no bias in the</p>
<p>policy gradient so on expectation that
part falls out and it&rsquo;s fine</p>
<p>so the bias-variance tradeoff is
typically mitigated through what we call</p>
<p>generalized advantage estimation so this
is a way of interpolating between all of</p>
<p>those different possible choices of n
step advantage estimate where we use a</p>
<p>factor called lambda so this is sort of
like another discount factor as the</p>
<p>interpolation variable and it&rsquo;s a hyper
parameter and you choose it in each</p>
<p>implementation that you make and it&rsquo;s
generally good to set it somewhere</p>
<p>between like 0.9 and 97 usually it&rsquo;s a
set it and forget it in my experience I</p>
<p>can I can&rsquo;t think of very many cases
when I saw a substantial difference in</p>
<p>algorithm performance from adjusting it
beyond that kind of narrow range if you</p>
<p>set it equal to one then you&rsquo;ll get
exactly the case of the N equals</p>
<p>infinity and if you set it to zero then
you&rsquo;ll get exactly the N equals zero</p>
<p>case so it&rsquo;s good to kind of leave it in
the range where it&rsquo;s putting a little</p>
<p>bit more weight on the real empirical
returns than the biased value estimator</p>
<p>but not all the way to the extreme okay
at long last I give you the pseudocode</p>
<p>for the full vanilla policy gradient
algorithm that incorporates everything</p>
<p>that we&rsquo;ve talked about so far what
we&rsquo;re going to do is collect a set of</p>
<p>trajectories by running the current
policy in the environment and then we&rsquo;ll</p>
<p>compute the rewards to go so that we can
use them as targets for the value</p>
<p>function approximator will compute the
advantage function estimates with any</p>
<p>method of advantage estimation but
typically generalized advantage</p>
<p>estimation and then we&rsquo;re gonna use
those to estimate the policy gradient</p>
<p>with that we take a step of gradient
this gradient descent we might use an</p>
<p>adaptive optimizer like Adam to
accelerate the rate at which we learn</p>
<p>and then we&rsquo;re going to do the
supervised learning problem of trying to</p>
<p>get the value function approximator to
match the empirical returns and that&rsquo;s</p>
<p>how we learn our value function and then
we loop that&rsquo;s vanilla policy gradient</p>
<p>yeah absolutely
so yes usually you will pick networks</p>
<p>have the same size for policy and value
function in cases where the environment</p>
<p>is partially observed you may want to
have a single core recurrent neural</p>
<p>network that&rsquo;s going to be able to
remember past information and then give</p>
<p>that corner all Network separate outputs
for policy and value function and then</p>
<p>you&rsquo;ll train that jointly and it gets a
little bit complicated because I can&rsquo;t</p>
<p>say that there&rsquo;s any good work in RL
theory that I&rsquo;m aware of that reasons</p>
<p>about how it alters performance for the
final policy to be simultaneously</p>
<p>optimizing with respect to both
objectives on the same model but that&rsquo;s</p>
<p>what you would do in that situation so
so yes typically they&rsquo;ll be about the</p>
<p>same size unless they&rsquo;re actually
sharing parameters and then they&rsquo;re sort</p>
<p>of the same model yes</p>
<p>does the choice of initial policy affect
convergence wonderful question and sadly</p>
<p>in a lot of cases yeah so this is part
of what goes into my saying that deep</p>
<p>reinforcement learning is not a
technology that&rsquo;s ready to be used as a</p>
<p>black box yet so when we do experiments
in deep reinforcement learning we</p>
<p>typically run the same exact experiment
with different choices of new of seed</p>
<p>for the random number generators and
what we find is that the seed which in</p>
<p>the beginning of the algorithm only
changes the initialization of the</p>
<p>policies and value functions happens to
matter quite significantly some seeds</p>
<p>learn some seeds don&rsquo;t some seeds learn
much slowly much more slowly than others</p>
<p>and there&rsquo;s no particularly good reason
for it</p>
<p>we are generally quite heartened when we
find an algorithm that appears to be</p>
<p>robust to initial conditions and where
the</p>
<p>average of the learning curves is quite
narrow we think that&rsquo;s great and it</p>
<p>doesn&rsquo;t quite happen as often as we
would hope all right do we have any</p>
<p>other questions about policy gradients
so in the bottom right hand corner there</p>
<p>that says 47 out of 63 I may have
slightly miscalibrated</p>
<p>how long parts one and Part two were
relative to the initial time slots of 45</p>
<p>minutes and 1 hour respectively this is
by far the longer one but since we&rsquo;ve</p>
<p>been at it for an hour I think this is a
good point to take a 15-minute break and</p>
<p>we&rsquo;ll pick back up to discuss q-learning
after coffee thank you so much</p>
<p>we will</p>
<p>we will be resuming with Joshua Humes
introduction to RL in two minutes</p>
<p>hello</p>
<p>hi everyone we&rsquo;re about to get started
for the second part of intro to RL and</p>
<p>just as a heads up
I prepared entirely too many slides for</p>
<p>the hour and 45 minutes that I was
scheduled to speak please bear with that</p>
<p>because you know this is the first time
we&rsquo;re doing this and so I&rsquo;m still</p>
<p>getting calibrated on what we can get
through in that amount of time but</p>
<p>everything that I don&rsquo;t cover by 11 a.m.
when I hand over the mic to the next</p>
<p>speaker I&rsquo;m more than happy to share
with you later today during the</p>
<p>hackathon so in particular the material
that I expect that we won&rsquo;t quite get to</p>
<p>will involve an overview of what&rsquo;s been
accomplished recently in deep</p>
<p>reinforcement learning and where the
challenges and limitations are and what</p>
<p>the research horizons look like on those
limitations but before we do any of that</p>
<p>let&rsquo;s continue our discussion from
earlier and talk about the next major</p>
<p>family for algorithms for deep RL for
control which is to say cue learning so</p>
<p>there are a lot of algorithms that fall
under this umbrella deep Q learning was</p>
<p>one of the first algorithms that really
made deep reinforcement learning viable</p>
<p>and popular speaking from personal
experience I just started my graduate</p>
<p>student career in 2014 when I heard
about the playing Atari with deep</p>
<p>reinforcement learning paper I was just
becoming aware of topics in AI and AI</p>
<p>research and that completely and totally
blew my mind it was the most exciting</p>
<p>thing that I had ever seen that a
computer could just figure out from</p>
<p>looking at what was happening on a
screen how to behave how to play a game</p>
<p>how to do something that I thought
required some human spark of</p>
<p>understanding and capability for joy and
the in the computer had it it was</p>
<p>beautiful and amazing and it made me
want to study this and participate in</p>
<p>taking this technology all the way from
where it was at that point to what it</p>
<p>could be in the future
anyway q-learning</p>
<p>so back to this RL loop that we have run
policy evaluate policy improve policy in</p>
<p>q-learning you run the policy by taking
a step in the environment either</p>
<p>randomly so there&rsquo;s going to be some
stochasticity in what you do or you&rsquo;re</p>
<p>going to act in a way which is called
greedy with respect to your current Q</p>
<p>function approximator so remember what
you&rsquo;re trying to learn is Q star the</p>
<p>optimal action value function and if you
happen to have Q star then whatever</p>
<p>action is the maximum or maximizes q
theta in a particular state is the best</p>
<p>action to take um but when you don&rsquo;t in
fact have q theta equals Q star then the</p>
<p>the maximizing action probably isn&rsquo;t
great so exploring a little bit by</p>
<p>acting randomly is going to help you and
then once you&rsquo;ve taken that step in the</p>
<p>environment so you send an action to it
and you get back a reward in the next</p>
<p>state you store that transition state
action reward next state in a replay</p>
<p>buffer you save it for later because
you&rsquo;re going to use it for learning how</p>
<p>to evaluate the policy which is to say
updating q theta to try to have it fit</p>
<p>that bellman equation and once you have
that the policy improvement step is just</p>
<p>looking into q theta and saying what&rsquo;s
the action that maximizes this policy</p>
<p>improvement is basically implicit in Q
learning and we&rsquo;re gonna structure our</p>
<p>discussion about Q learning around the
original deep Q networks algorithm but</p>
<p>pretty much everything in this
discussion is quite general for Q</p>
<p>learning methods because they all kind
of share this common DNA of you take a</p>
<p>step in the environment you take some
gradient descent steps on your Q</p>
<p>function to minimize a mean squared
bellman error and you use the techniques</p>
<p>that will describe in a minute
experience replay on target networks to</p>
<p>stabilize the learning procedure so Q
learning updates by bootstrapping so</p>
<p>what is what is that it&rsquo;s this notion of
how</p>
<p>are we actually going to fit q2 that
bellman equation so we talked about</p>
<p>minimizing mean squared bellman error
and it&rsquo;s a useful picture to start with</p>
<p>and so I&rsquo;m gonna keep using that
terminology although in a few slides I&rsquo;m</p>
<p>going to tell you something completely
different and ask you to ignore this and</p>
<p>pretend you never heard it but this is
where all the papers start and this is</p>
<p>where all the tutorials starts so it&rsquo;s
good to familiarize you what you&rsquo;re</p>
<p>going to do to update Q is set up this
loss function where you&rsquo;re going to</p>
<p>average or sum over data from your
replay buffer D and you&rsquo;re gonna have</p>
<p>these transitions state action next
state reward and you&rsquo;re going to regress</p>
<p>Q theta against targets Y where those
Y&rsquo;s are obtained basically from that</p>
<p>bellman back up from that bellman
equation as the reward plus the Q value</p>
<p>in the next time step and this is based
on the bellman equation for the optimal</p>
<p>action value function so it&rsquo;s gonna have
that Max over next actions which is to</p>
<p>say that it&rsquo;s going to assume that you
know if Q theta was optimal if it was Q</p>
<p>star then whichever action maximized it
in that state would be the best one to</p>
<p>take and that would be the best value
there so interestingly you don&rsquo;t</p>
<p>propagate gradients through why even
though why has the dependence on the</p>
<p>parameters of Q theta and the reasons
for this are kind of mathy so we&rsquo;ll get</p>
<p>to them in a bit okay
getting this to work so there are two</p>
<p>main techniques that I mentioned there&rsquo;s
experienced replaying there&rsquo;s target</p>
<p>networks
the idea behind experience replay is</p>
<p>just that you want to use a really wide
distribution of data for training your Q</p>
<p>function you don&rsquo;t want to fit it really
well to a very narrow region of</p>
<p>transition space because if you do it&rsquo;s
not gonna be good anywhere else and if</p>
<p>it&rsquo;s not good anywhere else you&rsquo;re not
going to be able to bootstrap it to the</p>
<p>correct values even in the places where
you&rsquo;ve been trying to fit it</p>
<p>you&rsquo;ll get nothing which is actually
useful for control so experience replay</p>
<p>helps you broaden that data distribution
fit q well everywhere gets something</p>
<p>which is good for control target
networks</p>
<p>so bootstrapping with function
approximator is super super super</p>
<p>unstable that thing that we said on the
previous slide where the Y&rsquo;s depend</p>
<p>exactly on the current Thetas actually
throw that out can&rsquo;t do that that won&rsquo;t</p>
<p>work
if you try to do it what&rsquo;s gonna happen</p>
<p>is typically the keys will explode
they&rsquo;ll go to something really large or</p>
<p>really negative and that&rsquo;ll happen
really fast you won&rsquo;t be able to control</p>
<p>it even with reasonably well tuned
learning rates you probably won&rsquo;t be</p>
<p>able to stop it so instead what we&rsquo;re
gonna do is we&rsquo;re gonna have target</p>
<p>network Q theta Targ and we&rsquo;re gonna
make sure that that network tracks</p>
<p>reasonably closely to Q theta but
there&rsquo;s going to be a lag so that it</p>
<p>updates more slowly so that if you make
an update to Q theta which pushes a Q</p>
<p>value too high or a little too low then
that doesn&rsquo;t immediately propagate into</p>
<p>Q theta Targ and therefore does not
propagate into the bootstrap so this is</p>
<p>this wide thing we&rsquo;re gonna call this
the bootstrap and then this tamps down</p>
<p>on instability grants it why if Q
learning is so horrific ly unstable</p>
<p>would we want to do it like this in the
first place why wouldn&rsquo;t we just</p>
<p>differentiate through with respect to
that bootstrap and the answer is it if</p>
<p>you differentiate all the way through it
tends to not work that well and the</p>
<p>reason that this thing does the reason
that it works well if you do this kind</p>
<p>of bootstrapping approach as long as you
take some appropriate precautions has</p>
<p>something to do with the theory
underlying Q learning and we&rsquo;ll talk</p>
<p>about that in a few slides but not quite
yet you&rsquo;re spared for now so also</p>
<p>another note in DQ networks the
particular algorithm that we&rsquo;re talking</p>
<p>about right now
action space matters a lot so what we</p>
<p>did in describing that bootstrap we had
a maximization over actions of the q</p>
<p>function if you have a q function that
accepts as input a continuous state and</p>
<p>a continuous action and feeds that into
a deep neural network trying to figure</p>
<p>out the action that maximizes the Q
function output is really hard that</p>
<p>would be a non-trivial optimization
problem an expensive subroutine so if we</p>
<p>want to be able to
get that max over actions that&rsquo;s a case</p>
<p>where we won&rsquo;t really be able to do it
so dq1 will apply specifically to the</p>
<p>discrete action case where we&rsquo;re able to
use a network architecture that instead</p>
<p>of taking a continuous action as an
input at the bottom of the network emits</p>
<p>action values for each possible output
for each possible action at the end of</p>
<p>the network so a single observation goes
in and then K action values come out</p>
<p>where K is the number of actions one for
each action and then because there&rsquo;s</p>
<p>just a finite number of them it&rsquo;s very
easy to figure out which action maximize</p>
<p>the Q value we can compare all of them
directly so now but we can talk about</p>
<p>the pseudocode for deep Q learning this
is relatively straightforward based on</p>
<p>the stuff that we just described there&rsquo;s
one thing which is a little more</p>
<p>specific than what I mentioned which is
this business of Epsilon greedy</p>
<p>exploration so I mentioned before that
you&rsquo;re going to explore by sometimes</p>
<p>taking a completely random action and
sometimes taking the action which is</p>
<p>greedy which maximizes your current Q
function approximator so epsilon greedy</p>
<p>is a strategy for doing that where with
probability epsilon where epsilon is</p>
<p>going to be something small you&rsquo;ll pick
a completely random action so uniform</p>
<p>random over the K different choices and
with probability 1 minus Epsilon most of</p>
<p>the time you&rsquo;ll pick the action that&rsquo;s
greedy with respect to your current Q</p>
<p>function so that&rsquo;s the run policy step
and then after you store that transition</p>
<p>into the replay buffer and anneal
Epsilon because over time you want to</p>
<p>explore less and exploit more you want
to rely on the policy as it gets better</p>
<p>after doing that you&rsquo;re now going to
evaluate the policy by learning Q star</p>
<p>from the data by improving q theta to be
a better reflector of Q star so that&rsquo;s</p>
<p>exactly the step of gradient descent
that we described which is that you</p>
<p>sample some transitions from your replay
buffer from your from your experience</p>
<p>replay memory and you compute the
bootstraps for those transitions and</p>
<p>there&rsquo;s a special case for if a
transition ended in a terminal state</p>
<p>which
that we don&rsquo;t give it a value after that</p>
<p>particular time step and then we use
those Y values in our bootstraps Q value</p>
<p>regression update the parameters and
then every once in a while with some</p>
<p>frequency will copy over the parameters
of the main q network onto the target</p>
<p>network so that&rsquo;s the target network
lagging the q network ensuring stability</p>
<p>and that&rsquo;s deep Q learning in a nutshell
this algorithm kicked off everything I</p>
<p>mean a whole bunch of stuff that
preceded it you can&rsquo;t really point to</p>
<p>any one moment in the history of a field
that you know had no precedent before</p>
<p>this there was neural fitted Q before
that there was Q learning with linear</p>
<p>function approximation and there were
all kinds of algorithms for trying to</p>
<p>get things to work with nonlinear
function approximation like deep neural</p>
<p>networks but but but this was the one
that got a lot of people really really</p>
<p>excited so anyhow caveat emptor buyer
beware this can break this will not work</p>
<p>on every problem out of the box you&rsquo;ll
try it in some places and it just won&rsquo;t</p>
<p>work you&rsquo;ll fiddle with hyper parameters
and it still won&rsquo;t work you&rsquo;ll try some</p>
<p>tricks to stabilize it because there are
pretty much infinity tricks to make deep</p>
<p>Q learning better at this point and some
of the time that still won&rsquo;t work so</p>
<p>this picture here is from a recent paper
which I really love and which I strongly</p>
<p>recommend that you take a look at if you
get interested in seeing some analysis</p>
<p>of failure modes for algorithms in deep
RL it&rsquo;s called deep reinforcement</p>
<p>learning and the deadly triad the deadly
triad is a set of traits that deep</p>
<p>reinforcement learning algorithms might
have which are known to occasionally</p>
<p>cause divergence and to create
substantial obstacles to theoreticians</p>
<p>who would like to come up with
algorithms that have provable</p>
<p>convergence guarantees so the deadly
triad consists of function approximation</p>
<p>off policy learning and bootstrapping
which are exactly the three things the</p>
<p>deep Q learning relies on we have
function approximation in the form of</p>
<p>neural networks we have auth policy
learning in the form of</p>
<p>spirits replay and we have bootstrapping
in the form of using the target network</p>
<p>with a one-step backup as the regression
target for q and so deep Q learning</p>
<p>works a whole lot of the time and then
some of the time it just doesn&rsquo;t so in</p>
<p>this set of experiments what the
researchers did was they examined deep Q</p>
<p>learning and a few variants of it a
bleeding on whether they would include a</p>
<p>target network so here this Q does not
have a target network the regression</p>
<p>target that it uses is exactly based on
Q theta naught Q theta tark and tried it</p>
<p>with a target network and then tried a
couple of other tricks that relate to</p>
<p>how you use the target network to
possibly either estimate the value in</p>
<p>the bootstrap or select the action in
the bootstrap and those are tricks that</p>
<p>are known to potentially help they
looked at at all these different cases</p>
<p>for many different Atari games as the
experimental test bed and they clipped</p>
<p>the rewards in the environments into a
certain range so that they knew exactly</p>
<p>mathematically what the ceiling for
possible real Q value would be they</p>
<p>chose it to be a hundred and they looked
and saw over all the experiments that</p>
<p>they ran how often did the maximum
absolute learned Q value in an</p>
<p>experiment exceed the threshold which
they knew was the real true maximum</p>
<p>possible Q value and the answer was a
lot so this shows that Q learning</p>
<p>without target networks is very unstable
in that a lot of the time you will get</p>
<p>this this divergence phenomenon and even
as you include tricks that make it</p>
<p>progressively more stable you&rsquo;ll still
expect to see divergence every now and</p>
<p>then so we&rsquo;re gonna dive into a little
bit of math now to kind of get maybe</p>
<p>some intuition for why this is the case
and what deep Q learning algorithms are</p>
<p>really trying to do and how that
translates into the algorithm or doesn&rsquo;t</p>
<p>so we&rsquo;re going to start by taking the
operator view of the bellman equation so</p>
<p>the optimal bellman operator t&rsquo;east
is a map from cue functions on to other</p>
<p>cue functions and the value of T star
for a particular state action pair is</p>
<p>given by the the cue right by the
bellman equation that we saw before the</p>
<p>optimal cue function is the fixed point
of T star so Q star equals T star Q star</p>
<p>that&rsquo;s great and T star has this special
thing about it which is that it&rsquo;s a</p>
<p>contraction map on the space of Q
functions contraction maps have some</p>
<p>very special properties that we&rsquo;re gonna
talk about now yay</p>
<p>so the main thing about a contraction
map is this idea that if you have two</p>
<p>points and you apply the contraction map
to both of them they&rsquo;ll basically be</p>
<p>closer with respect to some distance
function after you&rsquo;ve applied that map</p>
<p>to both of them than they were before
so expressed mathematically we have some</p>
<p>some norm some distance of the norm of a
thing minus the other thing and the norm</p>
<p>of f of X minus f of Y is going to be
less than or equal to some constant</p>
<p>factor times the norm of the difference
between x and y that distance between x</p>
<p>and y and when that beta is less than
one then we have a contraction that&rsquo;s</p>
<p>saying it&rsquo;s getting closer together it&rsquo;s
shrinking why do we care about</p>
<p>contractions because they have unique
fixed points and you can get to them by</p>
<p>just repeatedly applying the operator to
any initial point this is something</p>
<p>called the binocs 20 room if you&rsquo;re
interested in going on Wikipedia and</p>
<p>finding something which is going to be
more precise than however I&rsquo;ve typed</p>
<p>this up but in a nutshell to show you
that they have unique let&rsquo;s forget about</p>
<p>uniqueness for a moment but at the very
least that repeatedly applying this</p>
<p>operator will get you to a fixed point
if we look at a sequence of points X and</p>
<p>we have a contraction map F with modulus
beta and each point in the sequence is</p>
<p>just yet generated by F of the previous
point and we look at the distance</p>
<p>between successive iterates what we see
is that it&rsquo;s shrinking as a function of</p>
<p>the iteration number so in the limit as
the iteration number goes to infinity</p>
<p>that distance will shrink to zero it
will converge repeatedly applying it</p>
<p>will get you to the fixed point t star
is a contraction on Q functions so if</p>
<p>you could represent the entirety of the
Q function that is to say the Q values</p>
<p>for every state action pair in the
entirety of the environment which for</p>
<p>all the environments that we care about
in deep reinforcement learning you</p>
<p>cannot easily do you can only do this
with function approximation which is to</p>
<p>say you&rsquo;re going to generalize whatever
you choose for the value in one state</p>
<p>action pair will have some influence on
another you can&rsquo;t completely separate</p>
<p>them when you do function approximation
but putting that aside so we could</p>
<p>represent all the action values for
every state action pair and we applied T</p>
<p>star the operator to that function we
would get a new function Q which is</p>
<p>closer to optimal than the one that went
in and if we applied it over and over</p>
<p>and over again we would eventually get
to Q star the fixed point of T star this</p>
<p>is value iteration
it&rsquo;s a classic algorithm and</p>
<p>reinforcement learning so before
function approximation before deep when</p>
<p>you had environments where there were a
discrete number of states and a discrete</p>
<p>number of actions and you could
represent the Q values in a table of</p>
<p>elements one for each state action pair
you could compute this exactly and use</p>
<p>this as a way to get to Q star now when
you live in the problems that we do when</p>
<p>you&rsquo;re trying to solve high dimensional
complex video games high dimensional</p>
<p>complex strategy games you can&rsquo;t use the
table yet use a function approximator</p>
<p>and now your problem is that you can&rsquo;t
compute all of T star Qi and even if you</p>
<p>could you probably couldn&rsquo;t find a
choice of parameters that would allow</p>
<p>you to exactly represent it so if you
want to do this kind of value iteration</p>
<p>you have to do it approximately and this
is roughly what Q learning algorithms</p>
<p>with function approximation try to do
which is that they push the parameters</p>
<p>of the network in the direction such
that you move Q theta towards T</p>
<p>star q theta and sometimes this works
and sometimes it doesn&rsquo;t because when</p>
<p>you go to this function approximation
setting this operation is not</p>
<p>necessarily going to be a contraction on
the space of Q functions you might have</p>
<p>lost that property if you did expect
divergence in fact I expect things to</p>
<p>blow up horribly if you preserved it or
if you&rsquo;ve done enough tricks to</p>
<p>stabilize it things will work pretty
well in my experience Q learning</p>
<p>algorithms and their variants tend to be
extremely sample efficient when they</p>
<p>work which is quite desirable and it&rsquo;s
very nice if they can recycle off policy</p>
<p>data because on policy methods sadly
have to throw away tons of it but last</p>
<p>point on Q learning what you normally
see in deep learning algorithms and deep</p>
<p>RL algorithms is that paradigm of
there&rsquo;s an objective function and you</p>
<p>optimize it and you find the model that
optimizes the objective in Q learning</p>
<p>don&rsquo;t be misled into believing however
many times you see it that the mean</p>
<p>squared bellman error it&rsquo;s really the
thing that you&rsquo;re optimizing you change</p>
<p>that function every time you change the
target the thing that you&rsquo;re really</p>
<p>doing is this sort of approximate value
iteration you&rsquo;re trying to apply an</p>
<p>approximate operator which is going to
get you to something better you&rsquo;re not</p>
<p>trying to minimize a loss that&rsquo;s not to
say that there aren&rsquo;t variants of these</p>
<p>kinds of algorithms that do involve
well-defined loss functions there&rsquo;s a</p>
<p>whole family of algorithms called
gradient temporal difference methods</p>
<p>which if you are theoretically inclined
and willing to go down a deep deep deep</p>
<p>rabbit hole I recommend you check out
talk to me if you want references also</p>
<p>in the spinning up key papers doc I
believe there&rsquo;s a book in the bonus</p>
<p>section for classic RL papers and review
papers choppa Sabbath Baris book on RL</p>
<p>algorithms from 2010 which recaps a lot
of this really great old stuff including</p>
<p>gradient temporal difference algorithms
so I recommend you check that out if</p>
<p>you&rsquo;re interested yes I&rsquo;m actually
working on some research on that right</p>
<p>now
like I</p>
<p>talk to me offline yes yes yes so so
this thing yes it&rsquo;s called a temporal</p>
<p>difference error because it is the
difference in the Q value based on the</p>
<p>next time step versus the current time
step yeah yes absolutely what is the</p>
<p>difference between off policy and on
policy the on policy algorithms have</p>
<p>updates which are based on the expected
values of things where the distribution</p>
<p>and that expectation depends on the
current policy so if you want to form a</p>
<p>sample estimate of the thing in the
update equation then you first have to</p>
<p>run the current policy collect
interactions with the environment on the</p>
<p>current policy and use those samples for
forming that sample estimate that&rsquo;s on</p>
<p>policy because all the data that you use
has to be generated by the policy that</p>
<p>you&rsquo;re using at the time in off policy
methods like q-learning what you do when</p>
<p>you make an update is you use experience
which might have been generated by older</p>
<p>policies not the current one so the
current policy you could think of as</p>
<p>being implicitly expressed in the in the
Q function approximator is current value</p>
<p>but many steps ago it was different and
you got whatever data you got from</p>
<p>interacting with the environment you put
that in your replay buffer and then many</p>
<p>steps later you still sample those
states and actions from that replay</p>
<p>buffer to help you form your your new
update to the current q function so when</p>
<p>the data was generated by a different
policy that&rsquo;s off policy yes</p>
<p>in what sort of gaming situation would
we maybe use deep q-learning or like</p>
<p>what&rsquo;s a use case for it so there&rsquo;s a
fabulous use case actually Facebook</p>
<p>recently released a paper on their
machine learning and RL learning there</p>
<p>RL platform called horizon which they
used to train with deep Q learning</p>
<p>neural networks for making decisions
about when to send you push</p>
<p>notifications so actually DQ n is in
your phones right now okay then let&rsquo;s</p>
<p>proceed to the next part which is model
based stuff so I&rsquo;m going to be pretty</p>
<p>brief about model based stuff there&rsquo;s a
very wide variety of different model</p>
<p>based algorithms and we&rsquo;re not going to
drill down into them the way that we</p>
<p>drill down into policy learning and Q
learning but we will give a relatively</p>
<p>brief overview of some of the more
salient points and a few algorithms that</p>
<p>I think are particularly interesting so
back to the loop run policy evaluate</p>
<p>policy improve policy where do models
fit in so recall that a model of the</p>
<p>environment lets you predict what&rsquo;s
gonna happen next you can use that for</p>
<p>pretty much any of these while you&rsquo;re
running your policy before you take an</p>
<p>action you can stop and imagine what&rsquo;s
gonna happen if you try many different</p>
<p>things you can create partial rollouts
that you can use to evaluate your</p>
<p>different choices and then you might
pick something different than you would</p>
<p>have otherwise so that&rsquo;s maybe where it
can appear in running ball and running</p>
<p>the policy in evaluating the policy you
can use that same kind of approach of</p>
<p>just simulating look-ahead data to help
you get a maybe a more stable backup for</p>
<p>your q function or just use some kind of
Monte Carlo tree search style algorithm</p>
<p>where you&rsquo;re going to propagate Q values
back and figure out like an average case</p>
<p>Q value and then for improving the
policy you can regress your policy</p>
<p>network if you have explicitly
represented one towards whatever the</p>
<p>outputs were from that look-ahead
planning process so if you have a model</p>
<p>it&rsquo;s very powerful you can use the
a lot of different ways you can embed it</p>
<p>pretty deeply in into RL the problem is
that models are very hard to learn and</p>
<p>you usually don&rsquo;t have them so let&rsquo;s say
you have just made a wonderful brand new</p>
<p>complex physical robot unless you have a
lot of hours to spare and control theory</p>
<p>expertise you probably do not know how
to fully characterize that and have a</p>
<p>simulator model which is going to be
accurate in any reasonable way certainly</p>
<p>not accurate enough for training it in
simulation and then directly applying</p>
<p>that simulation trained policy into the
real world you may want to try learning</p>
<p>a policy from data but this can be quite
tricky although there are some really</p>
<p>exceptional success cases but because
yes uh yes you could make that argument</p>
<p>so I let&rsquo;s say hardness to learn is not
a fuck oh I suppose sorry the question</p>
<p>was can you make the same argument for
value functions and I would say that</p>
<p>hardness to learn in this case should be
interpreted more as has the research</p>
<p>community figured out really robust
reliable standard methods for doing it</p>
<p>yet but not necessarily whether there&rsquo;s
some intrinsic quality of hardness</p>
<p>finding the correct model is a
supervised learning problem if you have</p>
<p>enough data part of the problem in RL is
that you usually don&rsquo;t have enough data</p>
<p>and you would have to get it by
interacting with the environment and</p>
<p>there may be areas in the environment
very critical to decision making which</p>
<p>you&rsquo;ve just never observed yet so
imagine that you are in a giant maze and</p>
<p>you can try to learn a model of the maze
as you go but until you&rsquo;ve seen the exit</p>
<p>your model does not going to be very
helpful for you and navigating except to</p>
<p>help you perhaps avoid repeating places
that you&rsquo;ve been to already but but yeah</p>
<p>in practice models tend to be so far
hard to learn so let&rsquo;s look at maybe one</p>
<p>case study in ways that you can use
models so this is the case of planning</p>
<p>and/or expert iteration the basic idea
is that you&rsquo;re going to use your model</p>
<p>from a current state to look ahead into
the future and help guide your decision</p>
<p>about what action to take
so in planning you might explicitly just</p>
<p>base your decision about what action to
take on whatever the output from that</p>
<p>look-ahead process is and your current
value function in expert iteration</p>
<p>you&rsquo;re not only going to do that but
then you&rsquo;re also going to have a</p>
<p>explicit representation of a policy
which you&rsquo;ll try to improve by</p>
<p>regressing it towards the output from
the look-ahead process so as a case</p>
<p>study consider alpha 0 alpha 0 is an
algorithm which has succeeded at</p>
<p>achieving superhuman performance in a
wide variety of complex 2-player fully</p>
<p>observed strategy games particularly
chess go and shogi so this was a</p>
<p>successor to alphago the algorithm that
beat human grandmasters and go and alpha</p>
<p>0 at the algorithm level is sort of
beautifully simple you have a neural</p>
<p>network that emits two things a
probability distribution over moves to</p>
<p>play P and a value network that says
basically whether or not you&rsquo;re gonna</p>
<p>win or lose B and you learn this with
this very simple regression approach</p>
<p>where you&rsquo;re gonna move the value
function to be more like whatever the</p>
<p>true outcomes from games work and you&rsquo;re
going to update the policy by using a</p>
<p>model-based look-ahead operator to
figure out what a better policy would</p>
<p>have been based on your current policy
and value function and you&rsquo;re just going</p>
<p>to move your current policy towards that
and then there&rsquo;s also some</p>
<p>regularization very straightforward and
the look ahead is done with Monte Carlo</p>
<p>tree search so that&rsquo;s just stochastic
lis considering different possible</p>
<p>outcomes and then aggregating data after
having done partial rollouts down the</p>
<p>game tree to figure out what would have
been the best thing to do so this is one</p>
<p>model-based approach now this required
having a perfect model of the</p>
<p>environment and in games like chess ergo
this is feasible because you could fully</p>
<p>Express the rules in a way which is easy
to compute and forward simulate</p>
<p>and you don&rsquo;t have to learn anything
from data and you also don&rsquo;t have</p>
<p>anything which is partially observed so
your model doesn&rsquo;t have to do anything</p>
<p>fancy to keep track of what&rsquo;s going on
in the background very straightforward</p>
<p>and this kind of approach can be very
very powerful but the problem is that</p>
<p>most conditions are not quite as ideal
as this so another family of approaches</p>
<p>is where you&rsquo;re going to use the model
for policy evaluation so let&rsquo;s say that</p>
<p>you have learned a model or perhaps
you&rsquo;re given one but more often than not</p>
<p>for these algorithms you&rsquo;re trying to
learn it concurrently with experience</p>
<p>you learn some models and then you&rsquo;re
going to have the agent quote dream in</p>
<p>them the agent will sample a bunch of
fictitious trajectories inside of the</p>
<p>simulator and use those as the basis for
a policy improvement step and algorithms</p>
<p>that are like this
there&rsquo;s model ensemble TRP oh and I want</p>
<p>to say Mehta policy optimization or
model-based Mehta policy optimization</p>
<p>then you could also instead of using
this for computing advantages and and a</p>
<p>policy optimization style improvement
you could use this for Q learning as</p>
<p>well where perhaps instead of forming
the target based on the bootstrap which</p>
<p>might be inaccurate on particular
regions of state action space that you</p>
<p>haven&rsquo;t visited you could use the model
to simulate what the bootstrap might be</p>
<p>in those cases and use that as your
backup for Q learning so that&rsquo;s an</p>
<p>approach called model-based value
expansion and these algorithms the gain</p>
<p>that you get from doing this is
ultimately in-sample efficiency so what</p>
<p>happens in normal deep RL is that you
use tons and tons of data from</p>
<p>interacting with the environment to try
to improve your policy or your q</p>
<p>function and you make progress at
whatever pace when you use the model and</p>
<p>you offload a whole lot of the
improvement steps on to experience</p>
<p>collected in the model that frees you up
from having to have collected that</p>
<p>amount of experience in the real world
as long as your model is good enough if</p>
<p>your models not good this won&rsquo;t be very
helpful but if it is good and if you</p>
<p>only needed a little bit of data to
train your model then you can get a lot</p>
<p>of mileage out of it and your overall RL
algorithm will have used less</p>
<p>interactions with the real environment
and otherwise this is great for cases</p>
<p>where interacting with the real
environment is very expensive so for</p>
<p>instance if you want to train something
on a physical robot that can be an</p>
<p>expensive process the robot might be
slow the robot might break the robot</p>
<p>might have all kinds of things where
it&rsquo;s difficult to get it to do that or</p>
<p>it&rsquo;s difficult to reset it you probably
don&rsquo;t want to have to spend that many</p>
<p>man-hours waiting around for the robot
to finish its learning procedure so if</p>
<p>you can offload some of that time into
simulation then it makes life better</p>
<p>yes is that what you would apply for
self-driving cars that&rsquo;s a good question</p>
<p>so I&rsquo;m not actually all that familiar
with cases where self-driving cars have</p>
<p>fruitfully made use of deep RL that&rsquo;s
not to say that they don&rsquo;t I just don&rsquo;t</p>
<p>know I would imagine that in
self-driving cars it&rsquo;s probably more a</p>
<p>matter of collecting data from
experienced human experts and then using</p>
<p>that data as the basis for learning a
behavioral policy but I&rsquo;m also happy to</p>
<p>you know go through this later and see
what we can find in the literature yes</p>
<p>what would model-based RL be more geared
towards transfer learning I think it</p>
<p>could potentially be quite helpful so
certainly when we think about trying to</p>
<p>get robotics to transfer from say
simulation to reality you know we want</p>
<p>to make sure that the model used in
simulation is high fidelity with respect</p>
<p>to reality and if that&rsquo;s the case then
this model you can think about sim to</p>
<p>real as sort of a model-based approach
and perhaps it&rsquo;s gonna be very helpful</p>
<p>all right and then there&rsquo;s this other
completely orthogonal way of using</p>
<p>models which I&rsquo;m really fond of because
it&rsquo;s just sort of weird which is that</p>
<p>you actually take the model and embed it
inside of a model free agent where the</p>
<p>model is going to receive inputs from
the from the environment and use that</p>
<p>with some internal process of perhaps
imagining some futures and then</p>
<p>transforming whatever representation and
has of those futures into something</p>
<p>which then becomes side information to
the model free agent so you train the</p>
<p>model separately from the agent the
module that provides some information</p>
<p>based on the model to the agent is sort
of decoupled from it except that however</p>
<p>it&rsquo;s going to process however the model
free agent will process that information</p>
<p>is based purely on the model free
learning so this is an approach called</p>
<p>imagination Augmented agents I think
this is really interesting and really</p>
<p>neat I&rsquo;m not aware of a whole lot of
follow-up work from when this came out I</p>
<p>want to say last year or the year before
but I just think that because it is so</p>
<p>different from the other model-based
approaches that&rsquo;s cool whenever there&rsquo;s</p>
<p>something different it&rsquo;s cool all right
that takes me to what was originally</p>
<p>intended to be the end of part one but
it&rsquo;s now the end of both parts thank you</p>
<p>so much</p>
<p>at this point I would like to turn over
the mic and the stage to Matthias</p>
<p>Clapperton who is a researcher on the
robotics team at open AI and he&rsquo;ll be</p>
<p>presenting on the work on the robotics
team for learning how to do complex</p>
<p>manipulation with deep reinforcement
learning on a real physical robot great</p>
<p>thank you
we have a computer suite</p>
<p>yay I think it works okay thank you
cool so hey everybody my name is Matias</p>
<p>as Josh mentioned I&rsquo;m super excited to
be here and talk a little bit about what</p>
<p>robotics that openly is doing and to
talk that I&rsquo;m going to present this call</p>
<p>it&rsquo;s called learning dexterity as I
mentioned this is basically the effort</p>
<p>of the entire robotics teams for many
months so everything I&rsquo;m kind of talking</p>
<p>about is not just my work but these are
robotics teams okay cool</p>
<p>so let&rsquo;s maybe start with talking a
little bit about what robotics at open</p>
<p>era is actually trying to do and the
ultimate goal I guess robotics at open</p>
<p>eye has is suppose some form of general
purpose robot so I think this kind of</p>
<p>picture illustrates as well very well we
have human-like robots today and we know</p>
<p>that humans can do a very very large
amount of different jobs and skills so</p>
<p>that can include things like cooking it
can include things like actual labor in</p>
<p>some form of agricultural thing
maybe it&rsquo;s very precise kind of things</p>
<p>like surgery or building things and
putting things together in this kind of</p>
<p>stuff and ideally we would like to have
a robot that has a similar similar level</p>
<p>of dexterity and a similar level of well
general purpose Ness if you will the way</p>
<p>robotics looks right now it&rsquo;s very
different from that so we have these</p>
<p>kind of very specialized robots so an
example I think that is good it&rsquo;s the</p>
<p>Roomba which is on the lower in the
upper left corner here that can clean</p>
<p>your house but it can only clean your
house it can only vacuum your house and</p>
<p>similarly your things like self-driving
cars which to some extent also robots</p>
<p>that are very good at one thing which is
driving themselves but they cannot do</p>
<p>anything else and the robots there are
more kind of versatile and more</p>
<p>complicated they are either very often
controlled by humans so an example for</p>
<p>that would be doing surgery so we have
robots that can assist humans in that</p>
<p>but they&rsquo;re always controlled by human
operator which is a surgeon or we have</p>
<p>more complicated robots in factories but
those are typically just programmed to</p>
<p>basically blindly execute a given show
secretary so someone sits with the robot</p>
<p>and figures out how to do a certain
process in a factory and the robot is</p>
<p>very very stupid and has no idea what&rsquo;s
going on so the question of course is</p>
<p>how can we kind of step away from that
paradigm and how can we have robots that</p>
<p>work in an actual physical world and
aware of their surroundings and given</p>
<p>that this is the spinning out workshop
that&rsquo;s concerned with ll it&rsquo;s not so</p>
<p>surprising that we think RL may be a
good approach to that and we know that</p>
<p>RL works really well in certain domains
so I&rsquo;ve picked out two examples here</p>
<p>that probably most people have seen on
the left side we have alphago zero</p>
<p>playing against Lisa at all and a game
of Go and as you know alphago zero won</p>
<p>this game
in fact I think one almost all games</p>
<p>that it has ever played and the
follow-up versions of alphago zero</p>
<p>beyond beyond human capabilities when it
comes to playing go similarly we have</p>
<p>dota 2 so this is some of the work that
the dota team at opening AI has been</p>
<p>doing for a while we have this door
abort called opening at five that is</p>
<p>very very good at playing the game dota
2 which is a 5v5 multiplayer game and it</p>
<p>is approaching like professional levels
so it&rsquo;s it&rsquo;s consistently winning and</p>
<p>can semi-pros and we are already playing
against some pros in fact we&rsquo;ve done</p>
<p>that last summer at the International
unfortunately we have not yet won</p>
<p>against those pros so the question is
how does this work in robotics and of</p>
<p>course yes like a lot of work in this in
robotics it&rsquo;s not like we we are the</p>
<p>only ones doing this and I just like to
give a bunch of examples that I think</p>
<p>are kind of illustrating what people are
typically doing today</p>
<p>the first approach here is somewhat
reason it&rsquo;s from 2017 and I think it</p>
<p>looks really cool so you can see the
agent is even able to use certain tools</p>
<p>so in this case a hammer it can open
doors it can do all sorts of things</p>
<p>the unfortunate thing here is that all
of this looks really cool about it&rsquo;s</p>
<p>only in simulation and ultimately in
robotics it doesn&rsquo;t really count if it&rsquo;s</p>
<p>only in simulation because you want the
physical robot to do something otherwise</p>
<p>it&rsquo;s not very useful
so the other approach that people have</p>
<p>been taking is to train on the actual
robot itself so this is a some work from</p>
<p>2016 where people have been doing
dextrose in hand manipulation so the</p>
<p>goal of the robot here is to kind of
manipulate this this tube filled with</p>
<p>coffee beans for some reason into a
target orientation and they do all the</p>
<p>learning on the on the actual robot and
that of course has the advantage of not</p>
<p>having to do any form of transfer
because you learn on the robot you</p>
<p>exactly know how the robot is going to
work and once you have a good policy</p>
<p>you&rsquo;re done the downside of that of
course is that well you have to run on</p>
<p>the actual robot so it kind of breaks a
lot on you it&rsquo;s very slow to do you</p>
<p>can&rsquo;t really scale this up unless you
get a lot of robots which is actually</p>
<p>something that people are doing so this
is the approach thing by Google and</p>
<p>typical Google fashion scale it up so
just get a lot of robots and let them do</p>
<p>it for two months in parallel and then
you can suddenly train on the robot</p>
<p>because well you have 20 of those doing
it in parallel and it can do very</p>
<p>meaningful stuff so in this case they
have learned to grasp arbitrary objects</p>
<p>out of this kind of box that I have
sitting here and this is actually very</p>
<p>impressive demo like this kind of been
picking stuff is actually very hard the</p>
<p>thing is still that obviously this does
not really scale all that well because</p>
<p>this is a relatively simple task yet you
need 20 robots going for two months and</p>
<p>you will also just have to babysit the
robot all the time right like you&rsquo;ll</p>
<p>have to repair it when it breaks you&rsquo;ll
have to kind of reset the environment</p>
<p>when certain objects fall out of the bin
and all of this kind of stuff so it&rsquo;s</p>
<p>just a lot of work so what we&rsquo;re trying
to do is to kind of combine the benefits</p>
<p>of those two approaches so training in
simulation and then transferring to the</p>
<p>physical world which is called sim to
real and I&rsquo;ll be talking a lot more</p>
<p>about this but before I do that I&rsquo;d like
to introduce you to the test that we</p>
<p>actually have in mind when we when we do
our research so we decided to do</p>
<p>dextrose in hand manipulation and the
reason for that is that it is first of</p>
<p>all very hard to do and then second of
all</p>
<p>it is something that we&rsquo;re interested in
because we know that our hands these</p>
<p>universal end effectors right so human
hands are very versatile in what they</p>
<p>can do it they can be very dexterous you
can do an cooking thing or you can</p>
<p>operate on a human if you&rsquo;re searching
at least but you can also do very heavy</p>
<p>lifting with it and you can use tools
made for you nuts hands and these kind</p>
<p>of things so so this is basically the
motivation for why we choose this kind</p>
<p>of hand and just kind of tasks because
it&rsquo;s hard and because it&rsquo;s also</p>
<p>ultimately useful for the channel
purpose robot we would like to build and</p>
<p>the reason why it&rsquo;s hard I think is
summarized relatively well in this this</p>
<p>kind of slide so we use a hand called
the shadow Dexter&rsquo;s hand which is</p>
<p>depicted in this picture it has 24
joints and it has 20 actuators so what</p>
<p>this means is that your policy it and
every time set has to produce an action</p>
<p>for 20 individual actuaries and it
actually has to coordinate right like</p>
<p>you&rsquo;ll have to have different joints
work together to do certain things so</p>
<p>it&rsquo;s a really high dimensional kind of
control problem that&rsquo;s typically well</p>
<p>out of reach of what traditional control
problems can solve as I mentioned</p>
<p>ultimately we wanna run this on real
hardware and so we have to work with the</p>
<p>real hard way and all its flaws and
issues so this includes things like</p>
<p>noisy and delayed sensing so that&rsquo;s just
a fact of physical hardware systems</p>
<p>right like they will not have perfect
information and they will have delays</p>
<p>and certain certain quirks that you kind
of have to deal with the other issue</p>
<p>that comes out of this sensing is that
you actually have to handle partial</p>
<p>observability so in simulation you have
perfect knowledge of everything that&rsquo;s</p>
<p>going on because well it&rsquo;s your
simulation and you can just read out</p>
<p>from your simulation what the current
state is but on the physical system you</p>
<p>can only use what you can actually sense
so obviously certain things like the</p>
<p>friction for instance of the system
cannot directly be observed and then</p>
<p>last of all this is actually super hard
to simulate as it turns out the reason</p>
<p>for that is that you have a lot of
contacts going on so if you have</p>
<p>something in your hand like you kind of
constantly touch it and contexts are</p>
<p>notoriously hard to model accurately
first of all and then the hand itself is</p>
<p>also incredibly complicated so it&rsquo;s 10
actuated which means that you kind of</p>
<p>have tendons pulling and just causes a
lot of unmodeled kind of things in you</p>
<p>and your hardware that you have not
modeled in simulation cool so as I</p>
<p>mentioned we set out to solve this
problem with our seem to real approach</p>
<p>so we trained in simulation and then we
transfer to the physical hardware and</p>
<p>while this sounds very easy it is not
very easy because the transfer problem</p>
<p>as you&rsquo;ll see is actually not very easy
to overcome but before we talk about</p>
<p>that let&rsquo;s have a look at what what we
can do in simulation and what the policy</p>
<p>that we train looks like in simulation I
think this also illustrates the task at</p>
<p>hand so that you can actually understand
later what what the robot is trying to</p>
<p>do so as you can see you kind of have
this block with colored faces and the</p>
<p>task is to rotate this block into the
desired target orientation that you have</p>
<p>and the target is depicted as this kind
of like semi-transparent additional</p>
<p>block on the right hand side so now it&rsquo;s
trying to bring up the blue face a yeah</p>
<p>it got it and then kind of moves on to
the next goal and as you can see it just</p>
<p>kind of involved like it coordinating
its fingers it has to kind of use its</p>
<p>permit it&rsquo;s kind of using gravity to let
it roll and it&rsquo;s like even in simulation</p>
<p>this is not super easy to learn the hard
way itself looks like this so this is</p>
<p>the cage we call it it houses all sorts
of things in the middle of course you</p>
<p>have the shadow Dexter&rsquo;s hand which is
the robot itself and then you have it</p>
<p>surrounded by quite a lot of these face
based tracking cameras so we have 12 of</p>
<p>those in total and what they do is they
provide you with relatively accurate</p>
<p>sensing in in Cartesian space so we have
LED markers on the hand itself so we</p>
<p>know where the hand is and we also have
LED markers on the object so we know</p>
<p>where the object is and those guys
basically they sense the slide of the</p>
<p>LED and since multiple cameras can kind
of see the same LED marker that it can</p>
<p>do triangulation and you can recover the
position in in space from that</p>
<p>information
we also have an alternative setup</p>
<p>because as I mentioned ultimately we&rsquo;d
like to have something that&rsquo;s more</p>
<p>general and having a motion capture
system is not very kind of real-world</p>
<p>like so we also have RGB camera so those
are regular RGB cameras we have three of</p>
<p>them surrounding the scene and they can
also be used for sensing in fact they</p>
<p>can be used for post estimation of the
object so you don&rsquo;t even have to have</p>
<p>any any special kind of sensing on the
object itself the cameras can do it for</p>
<p>you and the reason why we have three is
just just so they can first kind of</p>
<p>recover depth information and then
second they can also kind of work around</p>
<p>occlusions because it&rsquo;s in the hand from
certain angles you cannot sometimes see</p>
<p>the object because it&rsquo;s kind of covered
by the hand so this is how it looks up</p>
<p>close when we run things so as you can
see we have the we have the hand with</p>
<p>the block in it palm and in this case
it&rsquo;s the block that we use for face</p>
<p>based tracking so you can kind of also
see the LEDs on it that we use this is</p>
<p>simply much easier to do when when kind
of testing these algorithms so we have</p>
<p>these kind of world setups all right so
the big question of course is how do we</p>
<p>do the transfer so I showed you a video
of the policy doing its thing in</p>
<p>simulation and I showed you the physical
Hardware so we can have all the building</p>
<p>blocks but how can we actually transfer
it to the physical robot and if you just</p>
<p>train it in simulation it will not work
at all it&rsquo;s the short version so I&rsquo;ll be</p>
<p>showing some kind of numbers for that as
well but there you can believe me if I</p>
<p>say the transfer problem is really the
core issue that we&rsquo;re dealing with here</p>
<p>and the approach that we&rsquo;re taking is
relatively straightforward actually so</p>
<p>what we do is we use two main techniques
the first one of course being</p>
<p>reinforcement learning to learn the
actual control policy and then the</p>
<p>second technique being the main
randomization to make sure that the</p>
<p>learn control policy actually transfers
to the physical system and I&rsquo;ll be</p>
<p>speaking about both of those in a little
bit more detail so let&rsquo;s get started</p>
<p>with the main randomization so this is a
technique that has been used for a</p>
<p>little while
pretty popular paper when it comes to</p>
<p>this is from 2016 in this paper what
they did is they learn to fly a drone</p>
<p>and the way they approach this is they
trained in only in simulation using</p>
<p>these kind of randomized buildings so
you can kind of see it has a lot of</p>
<p>different rooms in it the textures are
very different so the walls look</p>
<p>different of ceilings of floors and they
train a drone to fly in all of those</p>
<p>rooms and what they then do is they take
this drone that was only ever flying</p>
<p>inside a simulation and show that they
can actually fly another completely</p>
<p>different actual room simply because it
kind of has seen all of this variant</p>
<p>doing during its training it kind of
like from its perspective what happens</p>
<p>is that the policy think so justice is
another like randomization it&rsquo;s kind of</p>
<p>weird but oh well I know how to handle
it so it flies in the actual room and</p>
<p>people that open either has been using
similar approaches as well so this is</p>
<p>some work from my colleague Josh Tobin
what he has been doing is he has been</p>
<p>using domain randomization for grasping
so this is using a robot called the</p>
<p>fetch
so it&rsquo;s you&rsquo;ll see a better picture in a</p>
<p>moment but it&rsquo;s basically a a simple
robot armed with a parallel group at the</p>
<p>end and what he would like to do is pick
up these objects that you kind of see in</p>
<p>these randomized scenes and by basically
using the same approach so he&rsquo;s</p>
<p>randomizing all sorts of things like the
looks of the objects of shape of the</p>
<p>objects the background the color of the
table as you can see he can then use</p>
<p>this information or this training to
transfer to the physical robot even</p>
<p>though it has never seen the actual
physical table and what was pretty</p>
<p>surprising in this research is that it
turns out you don&rsquo;t even need</p>
<p>photorealistic rendering so as you can
see like this it looks not realistic at</p>
<p>all it&rsquo;s like pretty computer graphics
and and still it transfers to the</p>
<p>physical to the physical world so the
important thing here is that you have</p>
<p>this variety and not necessarily
realistic environments yeah yes so all</p>
<p>of the the two approaches that I showed
are using</p>
<p>using vision to learn a policy yes in
this case I think it&rsquo;s actually not</p>
<p>using the vision to learn a policy
directly I think it&rsquo;s instead just</p>
<p>predicting the location of the object
and then there&rsquo;s a policy that the</p>
<p>Kinect can grasp it from that so some
some other work in this domain which i</p>
<p>think is equally important is physics
randomization and this has been done by</p>
<p>Jason pang who used to be an intern at
open air in 2017 and he&rsquo;s basically</p>
<p>using the same idea of randomizing but
now for physics instead of visual</p>
<p>appearances so it&rsquo;s kind of hard to like
visualize what&rsquo;s going on but what the</p>
<p>policy in training sees a certain worlds
that are just different so maybe they</p>
<p>have different masses maybe they have
different frictions of the table maybe</p>
<p>the robot itself behaves differently and
so on and so forth and what he was able</p>
<p>to show is that this again is sufficient
to train strictly in simulation and then</p>
<p>transfer to the physical robot so the
test at hand here is again with the</p>
<p>fetch robot and it&rsquo;s trying to move this
this park to the goal location which is</p>
<p>marked in in red and on the left hand
side you see a policy that has been</p>
<p>trained with those physics
randomizations and on the right hand</p>
<p>side it has been trained without and as
you can see obviously the one on the</p>
<p>left hand side does a pretty decent job
it&rsquo;s like relatively precise it can push</p>
<p>the park where it wants to go and the
one on the right kind of freaks out so</p>
<p>it shakes very violently in fact the
building was shaking when he was</p>
<p>deploying this and it cannot really do
with the job and the reason is that it</p>
<p>well has kind of over fit to the
simulation which simply is not fully</p>
<p>accurate even though it&rsquo;s calibrated to
be close to the robot and then it</p>
<p>doesn&rsquo;t generalize to the actual
physical world where&rsquo;s the one with</p>
<p>physics randomization stars okay of
course so that&rsquo;s the main randomization</p>
<p>in a nutshell so both the visual
randomization and the physics</p>
<p>randomization yeah</p>
<p>yeah it&rsquo;s it&rsquo;s not very realistic
honestly I mean it&rsquo;s realistic in the</p>
<p>sense that it&rsquo;s the physical so if you
randomize too much your simulation will</p>
<p>become unstable because you&rsquo;ve set in
certain parameters such that they cannot</p>
<p>make sense anymore
but it&rsquo;s not very realistic like the</p>
<p>masses will be very high sometimes it&rsquo;s
like smart to move the puck and it&rsquo;s</p>
<p>more about diversity again yeah okay
cool</p>
<p>so I&rsquo;ll now speak about our approach so
what I previously talked about was</p>
<p>mostly other people&rsquo;s work even though
they&rsquo;re also in the robotics team but</p>
<p>this is the the learning dexterity
approach that we took so again remember</p>
<p>the goal is to have the shadow hand
rotate an object in hand and to kind of</p>
<p>start it off I think it makes sense to
just give you the the overview of the</p>
<p>entire system and then we&rsquo;ll kind of
dive into some of them details after</p>
<p>that so again as I mentioned everything
we do is only in simulation so we never</p>
<p>see the actual physical robot until we
run on it like we&rsquo;ve never seen it so so</p>
<p>the way it works is that we collect a
lot of data in simulations so we have</p>
<p>many many simulations running in
parallel which is kind of depicted here</p>
<p>in box a and all of those are randomized
which is kind of visualized by them</p>
<p>having different visual appearances but
also think physics randomizations so the</p>
<p>friction and the masses will also be
randomized and using this collected data</p>
<p>we basically end up training two
different networks so one of them is a</p>
<p>policy and the other one is job is a
vision network because we&rsquo;d ultimately</p>
<p>like to run this from vision alone
without the face base the policy network</p>
<p>is what is depicted in Box B here and
the way it works is that it takes the</p>
<p>observed robot state which is the
position of the five fingertips so you</p>
<p>have doting coaches in a space of 15
dimensions in total so it knows where</p>
<p>its fingertips are and then also the
pose of the object so that means just at</p>
<p>the orientation and the rotation in
space sorry the position and the</p>
<p>rotation in space and this information
is then fed into an LSD and policy so</p>
<p>it&rsquo;s a recurrent policy and it produces
the next action and we train this in</p>
<p>simulation using reinforcement learning
the second network that we have which is</p>
<p>actually distinct they are not
end-to-end</p>
<p>this is two networks that we train
separately it&rsquo;s a vision Network and the</p>
<p>rate and vision Network uses works is
that it takes three different images so</p>
<p>remember we had these three RGB cameras
surrounding so images rendered from the</p>
<p>perspective of those but again only in
simulation and then using a</p>
<p>convolutional neural network predicts
the pose of the object from that</p>
<p>information from those images and again
this is only trained in simulation when</p>
<p>it comes to actually deploying this to
transfer as you can maybe kind of guess</p>
<p>is that we can combine those two systems
to get us what we ultimately would like</p>
<p>so you use the actual cameras to sense
the position or the pose of the object</p>
<p>using the vision network so you feed it
into that and then by having the object</p>
<p>pose and the fingertip locations you use
your L SCM policy to produce actions and</p>
<p>that allows the robot to basically see
what is going on and react accordingly</p>
<p>and of all only being trained in
simulation yeah potentially honestly we</p>
<p>have mostly used this approach because
we knew it worked from previous research</p>
<p>it is almost as accurate as face base
and face base is very very accurate I</p>
<p>think if you spend a lot of time you
could probably develop something with</p>
<p>more traditional methods I don&rsquo;t
question it but like we would like to</p>
<p>have something that&rsquo;s more general again
and having a convolution that conversion</p>
<p>neural network - it seemed like the most
general approach we could have yeah yeah</p>
<p>it&rsquo;s kind of interesting so ideally you
would just use whatever the robot has as</p>
<p>joint sensing so it knows it should know
what its own joints are as it turns out</p>
<p>the sensor in the shadow hand uses
hall-effect sensing which is a magnetic</p>
<p>kind of sensor and they interfere quite
a lot so if you think as a close</p>
<p>together you will actually not know
where your fingers are so that&rsquo;s the</p>
<p>reason why we don&rsquo;t use it we would like
to use it but it turned out to be not</p>
<p>precise enough for what we ultimately
wanted to do so we couldn&rsquo;t actually</p>
<p>rely
but yeah you&rsquo;re right like like this is</p>
<p>more for more for work around like
ideally the robot should just tell us</p>
<p>what the joint positions are and then we
wouldn&rsquo;t need the fingertip positions no</p>
<p>it actually has very limited information
it&rsquo;s very surprising that it works like</p>
<p>that yeah yes yeah yeah very good
question this is there&rsquo;s a lot of debate</p>
<p>about this I don&rsquo;t think it does we have
some indication that it doesn&rsquo;t in fact</p>
<p>it seems to help like the performance
seems to improve over the board like we</p>
<p>have certain ways of measuring symptom
transfer and when we randomize more we</p>
<p>tend to get better performance on all
the environments so I don&rsquo;t think it&rsquo;s</p>
<p>it&rsquo;s compromising actually I think it&rsquo;s
more of an adaptive policy but then</p>
<p>there&rsquo;s people who disagree so it&rsquo;s
currently a little bit unclear okay cool</p>
<p>so as I mentioned we need to randomize
and of course we use appearance</p>
<p>randomization so this is only for the
vision Network so this is basically what</p>
<p>I&rsquo;ve described before just for our setup
so you can kind of see we have three</p>
<p>different cameras showing the same scene
and we randomize this scene quite quite</p>
<p>heavily so the robot changes its color
the background changes its color</p>
<p>importantly the block itself stays
mostly the same because it actually has</p>
<p>that color like you cannot randomize the
dye but rarely but we changed the</p>
<p>material of the of the block as well so
it looks slightly different and then we</p>
<p>of course have that vision network which
again is relatively straightforward so</p>
<p>the way it works is it takes those three
camera images then uses convolutions and</p>
<p>the rest net architecture and spatial
softmax to kind of process them and then</p>
<p>simply calculates all the things and
produces the final object position on</p>
<p>object rotation so the pose of the
object and this is simply trained with</p>
<p>supervised learning because in
simulation you actually have perfect</p>
<p>ground truth which is another very
convenient thing you actually perfectly</p>
<p>precisely know where your your object is
you have not to actually sense it at all</p>
<p>and this is what the model actually sees
so it&rsquo;s actually I think very</p>
<p>interesting because it looks very very
different from</p>
<p>randomization and yet it generalizes to
that simply because it has seen enough</p>
<p>variety that it&rsquo;s kind of okay with with
yet another variety that&rsquo;s kind of weird</p>
<p>but still within distribution in that
sense so when it comes to the physics</p>
<p>randomizations that we use we randomized
quite a lot of things as well so we have</p>
<p>things like object dimensions for
instance we have things like masses</p>
<p>obviously and then mostly things about
the robot itself so things like the way</p>
<p>we actuate the robot things like damping
within its joints and all of this stuff</p>
<p>and the reason for that is that it&rsquo;s
actually very hard to measure this so</p>
<p>another neat thing is that you can in
this physics randomization actually</p>
<p>account for your uncertainty so for the
object dimensions we know those with</p>
<p>relatively little uncertainty because we
can just measure the dimension of the</p>
<p>block but things like the actuation we
learn much less about and so we kind of</p>
<p>widen the randomizations for those and
another kind of cool thing is that we</p>
<p>randomized the gravity vector which may
seem a little bit weird but it basically</p>
<p>amounts to like when you when you mount
the hand it&rsquo;s not perfectly parallel to</p>
<p>the to the floor like it will be
slightly angled because of imperfections</p>
<p>and by randomizing the gravity vector
you kind of get this effect as well like</p>
<p>it&rsquo;s sometimes slightly angled and it
turned out to be actually very useful</p>
<p>and then we of course also have noisy
observations and noisy actions simply</p>
<p>because it&rsquo;s a rare reality of the of
the physical system the policy is very</p>
<p>very simple so what it gets is the noisy
observations so that&rsquo;s five fingertip</p>
<p>positions and the poles of the object
and the goal so it&rsquo;s knows what it wants</p>
<p>to do and then we normalize a little bit
so this is just making sure that things</p>
<p>have a zero mean and unit variance and
then use one fully connected value layer</p>
<p>and one lsdm to produce the actual
distribution and from that we sample and</p>
<p>then perform perform that on the robot
so it&rsquo;s a relatively shallow and</p>
<p>relatively small network over all the
more so yeah</p>
<p>they only come in through the simulation
they cannot be observed directly so</p>
<p>sorry</p>
<p>they are simply set in the simulation so
the environment has been changed but the</p>
<p>policy cannot sense this directly it has
to infer this basically because on the</p>
<p>physical robot it also cannot sense it
like we don&rsquo;t know what it is on the</p>
<p>physical system so it basically what
what we think it ultimately ends up</p>
<p>doing is some form of system
identification so when it&rsquo;s running it&rsquo;s</p>
<p>implicitly inferring certain information
about the environment and then using</p>
<p>this information to kind of adapt itself
accordingly yeah sorry I couldn&rsquo;t hear</p>
<p>yeah so so we add Gaussian noise to the
observations and to the actions yeah all</p>
<p>right so I think I&rsquo;m running a little
bit late actually how bad is this huh</p>
<p>okay then we have to hurry a little bit
cooler so disappeared of training let me</p>
<p>speak about this and then I&rsquo;ll show a
video so disappeared training I think is</p>
<p>very interesting because we use
basically the same system that the dota</p>
<p>team uses as well so we have a very
large-scale kind of system and the way</p>
<p>it works is that we have role of workers
who generate a lot of experience and</p>
<p>then we have an optimizing machine
that&rsquo;s kind of using this information to</p>
<p>update its policy and we use approximate
policy optimization for that so a non</p>
<p>policy algorithm as I think josh has
explained earlier today and I think it&rsquo;s</p>
<p>kind of cool that we use the same system
estera let me skip over some things but</p>
<p>I think I&rsquo;ve want to show this so this
is when it&rsquo;s running on the physical</p>
<p>robot as you can see it&rsquo;s using vision
so there are no markers on the actual</p>
<p>object the robot hand is doing all of
this this is not cut in any way it is</p>
<p>not sped up again the goal is depicted
in the right corner here so it will try</p>
<p>to get the e face front and the end face
up top and it will get 250 successful</p>
<p>rotations in this case so it can do
quite a lot of those and it can run on</p>
<p>the on the physical system and if I have
enough time one one kind of final thing</p>
<p>that I think is actually very
interesting is that it actually learns</p>
<p>certain strategies that happen to have
names so we have thing a pivoting where</p>
<p>you kind of like use two fingers to
create a rotational axis and then you</p>
<p>rotate around that and things like
finger gating and the reason why they</p>
<p>have names is because they are used by
humans as well and they have been kind</p>
<p>of studied very well they emerge
automatically in our case so we have</p>
<p>never shown the robot what a human would
do it has kind of discovered that itself</p>
<p>and the reason why they come up is
simply because it has a human-like</p>
<p>morphology right like it has a
human-like hand and it just turns out</p>
<p>that these strategies are equally useful
for humans and robots but they have kind</p>
<p>of been rediscovered quote-unquote which
i think is a really</p>
<p>thing so I wanted to mention that and
yet we have some qualitative results</p>
<p>that show that randomizations are very
important so if you don&rsquo;t randomized you</p>
<p>get no successes if you randomized you
do it turns out memory is very important</p>
<p>so you need an LLC M you cannot simply
have a feed for policy and you need a</p>
<p>lot of experience so for the final
policy we use a hundred years worth of</p>
<p>data so imagine doing that on the
physical robot like probably not such a</p>
<p>good idea so but we can get away with it
because we use simulation so we do all</p>
<p>of this in 50 hours and I think with
that I have to close all right thank you</p>
<p>great thank you so much Matthias we&rsquo;re
gonna switch out the slides and then</p>
<p>please welcome to the stage the leader
of the safety team at open AI dario</p>
<p>Amadei all right just a minute to get
the slides</p>
<p>right</p>
<p>very good thing that you&rsquo;re ensuring
that computers in the future will not be</p>
<p>as malicious so I work on a team at open
AI that thinks about making AI systems</p>
<p>do what humans want them to do
which is you know kind of very central</p>
<p>to open the eyes mission and you know
which which we think of as you know</p>
<p>something that our focus on
distinguishes us from from other</p>
<p>organizations we think it&rsquo;s very
important particularly as systems get</p>
<p>more capable to ensure that they you
know both in a narrow and broad sense</p>
<p>benefit society
so this workshops called spinning up in</p>
<p>in deep RL so it&rsquo;s useful to step back
and you know think about what is what is</p>
<p>RL accomplished in the last couple years
and where is it going so you know this</p>
<p>is actually out of date we should add
add a couple things to it but you know</p>
<p>if we look at playing games like go if
we look at for about a year ago multi</p>
<p>agent behaviors where you can use RL and
self play to train agents to sumo</p>
<p>wrestle each other off a pad we are able
to play competitively against</p>
<p>professional professional players in
dota 2 the robot results which you just</p>
<p>saw and you know we should probably add
just in the last week or two you know</p>
<p>the results we&rsquo;ve seen on StarCraft
which is you know in some ways similar</p>
<p>to dota but just a different kind of
game with the different kind of</p>
<p>properties and yet you know that shows
that these techniques are really are</p>
<p>really pretty general and are are
advancing pretty quickly so you know if</p>
<p>we step back and reflect on you know
kind of where are things going you know</p>
<p>some properties that we could point out
of these RL agents that are becoming</p>
<p>more and more true right that we&rsquo;re not
true five years ago but are becoming</p>
<p>more and
are true we have systems that have an</p>
<p>extended interaction with complex
real-time environment they have a very</p>
<p>high level of autonomy and speed you can
imagine systems like this in the real</p>
<p>world being used to make decisions
faster than humans can intervene or in</p>
<p>more complex ways than humans could you
know could hope to understand so</p>
<p>regulating the economy or financial
system managing large networks of</p>
<p>computers these are the kinds of things
that as RL technology matures it will be</p>
<p>better and better better and better able
to do and you know the these systems</p>
<p>unlike supervised learning systems and
unlike in any interesting way you know</p>
<p>the simple RL systems were a few years
ago these systems are able to teach</p>
<p>themselves and discover their own
strategies and in many cases they</p>
<p>discover non-trivial strategies you know
just like we saw with the robot it kind</p>
<p>of recapitulating a lot of strategies
that humans use you know we see in go</p>
<p>and dota and Starcraft a lot of human
strategies that have names you know the</p>
<p>RL system discovers and recapitulates
but it also sometimes discover</p>
<p>strategies that a human would never
would never have thought of so if we</p>
<p>look at what these properties mean
together one thing it means is that the</p>
<p>connection between us as designers
specifying what we want the system to do</p>
<p>and what the system actually does in
theory the system does in theory if</p>
<p>everything is done right the system does
what we want but that that rope it&rsquo;s</p>
<p>longer it&rsquo;s more afraid it&rsquo;s more
tenuous</p>
<p>than for just kind of less less
autonomous systems that we&rsquo;ve we&rsquo;ve</p>
<p>designed in the past
and there are many ways relative to you</p>
<p>know simple computer systems or machine
learning systems like supervised</p>
<p>learning for for these systems to go
wrong and so a couple years ago several</p>
<p>people on most most of whom are now now
now constitute the the open a nice</p>
<p>safety team started started thinking
about this you know we&rsquo;re worried about</p>
<p>current systems worried about tomorrow
systems eventually we&rsquo;re worried about</p>
<p>you
about about building general</p>
<p>intelligence and what that what that
will mean for the world and making sure</p>
<p>that those systems are safe so you know
we wrote kind of a position paper and</p>
<p>this kind of started us thinking about
you know the directions and how to even</p>
<p>think about this problem of you know do
systems reliably do do what we want them</p>
<p>to do and the the kind of general
framework and division we came up with</p>
<p>was okay so you know let&rsquo;s let&rsquo;s let&rsquo;s
narrowly scope the problem we&rsquo;re not</p>
<p>we&rsquo;re speaking not about kind of wider
or societal impacts although those are</p>
<p>also important but you know just
narrowly the designer had a clear thing</p>
<p>they wanted the system to do and then
you know the system gets trained it gets</p>
<p>deployed it goes through some long
process actual system fails at this</p>
<p>catastrophic ly and we kind of divide it
up into into a couple things one is you</p>
<p>know you&rsquo;re you&rsquo;re giving the system
some Direction some objective function</p>
<p>that it learns from like the reward in
RL there are ways for that to be subtly</p>
<p>wrong and you can get spectacularly
wrong behavior if that happens you might</p>
<p>have the right objective function but
your system has problems with robustness</p>
<p>doesn&rsquo;t generalize well it you know
exhibits on exhibits unpredictable</p>
<p>behavior as its learning it does
dangerous things even if the final</p>
<p>policy it&rsquo;s gonna learn makes sense and
then as a reminder that you know there</p>
<p>like this
all exists on top of kind of software</p>
<p>implementation that has bugs in and of
itself and so you know these the a and B</p>
<p>are new but they&rsquo;re layered on top of
the general just the general</p>
<p>unreliability of software so kind of a
useful way to think about let&rsquo;s put CSI</p>
<p>because it&rsquo;s not really a machine
learning problem or just you know a</p>
<p>reminder that this is layered on top of
existing problems but a crude analogy we</p>
<p>can make is you know it&rsquo;s a bit like the
simple statistical concepts of bias and</p>
<p>variance right better a better objective
function you know that&rsquo;s that&rsquo;s about</p>
<p>reducing bias and making sure your aim
for the right target robustness is is</p>
<p>about making sure that you&rsquo;re narrowly
cluster around the target and that you</p>
<p>always get what you&rsquo;re intending to get
so we&rsquo;re interested in in both problem</p>
<p>because I have limited time I&rsquo;m going to
talk about our work on the getting the</p>
<p>objective function side right I think
you know open AI does more the opening I</p>
<p>safety team does more of that relative
to other you know other teams say at</p>
<p>Google brain or deep mind that that
think about these problems and so I&rsquo;ll</p>
<p>mostly talk about that but increasingly
and maybe I&rsquo;ll have a little bit of time</p>
<p>to talk about it at the end we&rsquo;re also
thinking about the robustness direction</p>
<p>and how these two things interact so
just to be clear about what we mean I</p>
<p>think this this this this video has been
widely circulated so I apologize if for</p>
<p>people who are already familiar with it
but you know about about a year and a</p>
<p>half ago we you know we were we were
training lots of flash games using RL</p>
<p>and you know there there happens to be
this boat race game so you know I I just</p>
<p>set lots of lots of games running with
with a reward function so the way this</p>
<p>boat race works is supposed to go along
the course and you&rsquo;re supposed to that</p>
<p>you&rsquo;re supposed to finish the course but
the way the reward function works and</p>
<p>it&rsquo;s hard to reach in and write a
different reward function is you get you</p>
<p>get points for you know these markers
along the way that are mostly along the</p>
<p>course but it turns out there&rsquo;s this
this little Lagoon in the corner of the</p>
<p>course where you can go around in
circles and get more and more power-ups</p>
<p>and that turns out to get you a faster
rate of power up to naturally finishing</p>
<p>the course there&rsquo;s nothing wrong with RL
here the system did what it was supposed</p>
<p>to do but it identifies the weakness of
the connection between a reward function</p>
<p>in the final behavior the reward
function that you specify that you may</p>
<p>think corresponds to some behavior that
you want may in fact correspond to very</p>
<p>different behaviors and you get no
feedback on that other than just finding</p>
<p>out what the system does right when I
first trained this I trained along with</p>
<p>a bunch of other games
two days later I looked at this I&rsquo;m like</p>
<p>what what in the world has what in the
world is this doing it doesn&rsquo;t make any</p>
<p>sense and then I thought about it alone
I&rsquo;m like oh of course that makes sense</p>
<p>and you know so the more powerful the
system is the more autonomous it is the</p>
<p>less of human is paying attention to it
the more potential there is this is like</p>
<p>you know
generate dozens of these examples but</p>
<p>you know robotic system where we forgot
to make the table totally fixed it has a</p>
<p>high mass but it&rsquo;s not fixed turns out
to be easy it&rsquo;s hard to send the send</p>
<p>the puck exactly to the point you want
it to be it&rsquo;s easier to send the puck</p>
<p>observe if it&rsquo;s gonna be a little to the
right or a little to the left and then</p>
<p>nudge the table so that it hits it
exactly it&rsquo;s very it&rsquo;s very clever it&rsquo;s</p>
<p>a correct solution to the problem but
the problem was not the right problem so</p>
<p>the general approach that we&rsquo;ve kind of
hit on and we&rsquo;ve been pursuing the</p>
<p>strategy for about a year a year a year
a year and a half is that the this</p>
<p>training loop is too long right the
human at the beginning says here&rsquo;s a</p>
<p>mathematical roared function like go go
optimize this then you look back at the</p>
<p>end of training you might get the right
thing you might not if you don&rsquo;t you</p>
<p>have to go back to the beginning or you
know maybe the system is already doing</p>
<p>something dangerous so maybe we should
have humans be involved interactively in</p>
<p>the training process right when we train
humans to do things it&rsquo;s not just like</p>
<p>here&rsquo;s your goal go off tell me what you
did you know two weeks later so if we if</p>
<p>we do this is there a way that we can
use a human to decide what the reward</p>
<p>function is in a continuous way that&rsquo;s
more reliable that&rsquo;s more naturalistic</p>
<p>so that the system ends up imbued with
human goals and values but it&rsquo;s able to</p>
<p>act faster and bigger than human scale
once it&rsquo;s trained it knows what the</p>
<p>human wants and it does it example of
this is like instead of RL we can learn</p>
<p>from demonstrations but that kind of has
the same problem a human demonstrates at</p>
<p>AI system copies it and there&rsquo;s kind of
it&rsquo;s hard it&rsquo;s hard to do better than</p>
<p>the human it&rsquo;s hard to course-correct
it&rsquo;s hard for the human to say you</p>
<p>should be doing this instead of this and
traditional RL has has a loop that&rsquo;s too</p>
<p>long so the kind of first effort we did
in this direction was we called it deep</p>
<p>RL from human preferences so the idea is
you know I want this thing to do a back</p>
<p>flip and I you know it&rsquo;s hard to
mathematically specify the reward</p>
<p>function for a back flip
we tried by looking at all the</p>
<p>individual joint angles and you know it
turns out it just gives you some</p>
<p>think very very like you know very
awkward looking but what we do instead</p>
<p>is and you know this is now running for
the second time but a human looks at the</p>
<p>behavior of the system and says which of
these is more like a backflip than the</p>
<p>other the system just starts by acting
randomly it has it has just like a</p>
<p>random reward function and human gives
it feedback on what what is more like</p>
<p>what the human wants and then the AI
system you know like the the RL system</p>
<p>has a reward predictor and it tries to
fit a reward predictor consistent with</p>
<p>what the human says the human prefers
and then in the background it&rsquo;s running</p>
<p>a whole bunch of copies of of the RL
environment and those copies optimize</p>
<p>the reward function that it learns from
the human the human only ever has to</p>
<p>give feedback on a very small fraction
of the AI systems behavior doesn&rsquo;t have</p>
<p>to see everything it does just has to
get enough samples to give the to give</p>
<p>the policy an idea of what the reward
function should be so another way to put</p>
<p>it is the human trains the reward
function and the reward function trains</p>
<p>the RL system so what I just said can be
kind of pictured in this the grey part</p>
<p>is the standard set up for for for
reinforcement learning where you have an</p>
<p>RL Aughra than the environment they
exchange observations and actions and</p>
<p>there&rsquo;s a reward that kind of that kind
of you know comes from the ether that</p>
<p>was ultimately specified by a designer
but that isn&rsquo;t thought about as being</p>
<p>part of the problem here what we have is
that reward starts out being completely</p>
<p>random and the human sees examples of
the agent&rsquo;s behavior and feeds them to a</p>
<p>reward predictor so the reward predictor
is changing and improving and adapting</p>
<p>over time and the RL system is both
learning from the existing reward</p>
<p>function and adapting to changes in the
reward function</p>
<p>so we did several versions of it in our
paper and we found that a simple active</p>
<p>learning technique helped relative to
random it didn&rsquo;t help by that much but</p>
<p>but it helped the idea is you train an
ensemble of reward predictors that are</p>
<p>trained on subsets of the data and that
that allows you to have kind of like</p>
<p>semi independent predictors and you can
pick examples where the predictors are</p>
<p>uncertain meaning that those are parts
of the space or situations where there&rsquo;s</p>
<p>just the reward predictor has more
uncertainty and so would like more</p>
<p>feedback from the human that helps you
can go much more sophisticated in that</p>
<p>direction right the system could like
ask the human like what you know like</p>
<p>you know what what am i doing that&rsquo;s
wrong what am i doing that&rsquo;s not clear</p>
<p>the human could say to the system like
you know I&rsquo;d like you to produce some</p>
<p>examples of this right and then it
becomes much more like a like teacher to</p>
<p>human teacher to human pupil teaching
process and a lot of what we&rsquo;re doing is</p>
<p>kind of going in that direction but we
kind of have to start so imitation</p>
<p>learning has the following limitations
when you do imitation learning you</p>
<p>except for noise reduction which is
usually a small effect you can&rsquo;t perform</p>
<p>better than the human does so as we&rsquo;ll
see in some future tasks here there are</p>
<p>cases where learning from preferences
allows you to perform better than how</p>
<p>the human does the reason for that is
with imitation learning you just do what</p>
<p>the human does here you learn what the
human wants and once you learn the</p>
<p>reward function you could do it better
than the human right so consider</p>
<p>something like you know if I didn&rsquo;t know
how to play go I can teach you the rules</p>
<p>of go and then you can do RL on the
rules of go and get much better than me</p>
<p>or you can just copy my moves if you&rsquo;re
just copying my moves you can never do</p>
<p>better than me if I teach you the rules
and then you use RL to learn how to</p>
<p>learn how to play you can you can then</p>
<p>you can then in principle do better</p>
<p>another another difference is you tend
to get kind of like</p>
<p>better sample sample you tend to get
like better sample efficiency you can</p>
<p>come up with strategies that a human
wouldn&rsquo;t would you can come up with</p>
<p>strategies that human wouldn&rsquo;t have
thought of and many tasks a human just</p>
<p>can&rsquo;t do so actually this backflip task
it&rsquo;s actually very hard for a human to</p>
<p>demonstrate that task like you&rsquo;d have to
get a VR setup and if we look at like</p>
<p>the tasks of the future right where you
know like let&rsquo;s say I want to defend you</p>
<p>know a large corporate IT network or
something and I want to respond to</p>
<p>threats in real time that&rsquo;s just
something where I I can&rsquo;t get training</p>
<p>data from a human I&rsquo;m asking the machine
to do things that a human can&rsquo;t can&rsquo;t do</p>
<p>which is what we ultimately want AI
systems to be able to do does that kind</p>
<p>of answer the question yeah so we have
an option in this paper for basically I</p>
<p>don&rsquo;t know or I think we had separate
options for I don&rsquo;t know or it just</p>
<p>throws out the data or these two look
about the same in which case it like</p>
<p>waits them equally in the predictor and
in yeah so that&rsquo;s that&rsquo;s easy to</p>
<p>incorporate I think ultimately the
communication needs to be in terms of</p>
<p>language and not in terms of clicking
left or right and then that will kind of</p>
<p>like make a richer space for doing
things and saying I don&rsquo;t know or like</p>
<p>show me some other examples these things
aren&rsquo;t comparable at all become much</p>
<p>more common so the nice thing about this
is given an environment without changing</p>
<p>the code at all only changing what the
human provides as feedback you can get</p>
<p>totally different behaviors so in about
half an hour a human can train this this</p>
<p>are all system this is like simple
simple atari enduro game i can train it</p>
<p>to do the usual thing which is to to
race ahead of all the other cars but i</p>
<p>can also train it to go exactly at the
same speed as other cars and when it</p>
<p>does that you know it&rsquo;s able to actually
get there very very effectively like you</p>
<p>know stay exactly even with other cars
which isn&rsquo;t it isn&rsquo;t easy you have to go</p>
<p>at kind of exactly the same speed and
match their speed and so</p>
<p>exact same code just the human provided
different different feedback one thing</p>
<p>we show is if we don&rsquo;t give you the
rewards for Atari games we just hide</p>
<p>this hide them from you humans giving
feedback on basically you know trying to</p>
<p>get the system to get the highest score
that it can works really well on the</p>
<p>kind of right of each panel those like
colored bars that are moving that</p>
<p>represents how much reward the system is
thinks that it&rsquo;s getting or just how</p>
<p>much how much it thinks to give an
action is good so if you look at the</p>
<p>breakout case when the ball hits the
paddle instead of so on the Left when</p>
<p>the ball hits the paddle instead of you
know instant instead of instead of the</p>
<p>ball going to the bottom it says yeah I
got a lot of reward from that same with</p>
<p>pong the when it surfaces to get oxygen
and Seaquest it&rsquo;s very very very high</p>
<p>very high reward level so the predictors
seem to correspond to what you know to</p>
<p>what human would say is good behavior
which is not surprising because of human</p>
<p>training them so we did did a bunch of
we did a bunch of experiments and you</p>
<p>know with fixed reward Atari games your
goal is just to do as good as you would</p>
<p>if you knew the reward right so you&rsquo;re
like hiding the reward from yourself and</p>
<p>you&rsquo;re trying to learn the reward from a
human so most of the time it does it</p>
<p>does almost as good but actually there
are cases where it can do better</p>
<p>we&rsquo;re in enduro that the algorithm we
used a3c has trouble learning enduro</p>
<p>because of sparsity of the reward but a
human actually helps to shape the reward</p>
<p>right in enduro you have to like kind of
like read the control stick to go at a</p>
<p>certain speed in order to get in order
to get any reward at all so you can</p>
<p>start you can start to move and the RL
system doesn&rsquo;t give you any reward and</p>
<p>then you have to keep moving faster and
faster to get reward and some some</p>
<p>algorithms never figure that out but the
human will basically say okay yeah you</p>
<p>went ahead you made progress that&rsquo;s
better than when you&rsquo;re not moving and</p>
<p>so little by little with just with a few
feedback points it can lead the system</p>
<p>and so the human can shape the reward
and they&rsquo;re actually cases like the the</p>
<p>curve for enduro in the bottom right
where you can actually do better than</p>
<p>the human did or you can actually do
better than that</p>
<p>standard then a standard oral algorithm
did even though you had less information</p>
<p>instead of knowing the right reward
function you just had a human indicate</p>
<p>the reward function also works for a
bunch of kind of like simulated robotics</p>
<p>tasks we haven&rsquo;t really tried it in the
real world relevant to the question</p>
<p>about demonstrations we&rsquo;ve we actually
followed this up with an effort</p>
<p>combining human feedback with
demonstrations so what that did is you</p>
<p>know there&rsquo;s some tasks
human can do it but we liken it you we&rsquo;d</p>
<p>like the RL system to do it better
however we can initialize from human</p>
<p>human demonstrations the AI system
copies that but then on top of that on</p>
<p>top of that initialization we run RL RL
with human preferences so there&rsquo;s no</p>
<p>reward function there&rsquo;s no like
programmatic reward function anywhere</p>
<p>it&rsquo;s entirely learning from humans but
the first step the human demonstrates</p>
<p>and then the second step the AI system
copies in the human says it would be</p>
<p>better if you could do it this way and
again the second step allows you to</p>
<p>exceed human performance or do tasks
that humans can&rsquo;t do right the humans</p>
<p>like this is as well as I know how to do
it the AI system copies that the the</p>
<p>human says you know ok well I wouldn&rsquo;t
be able to do this myself but if you</p>
<p>move back and forth really quickly and
shot those two ships that will be better</p>
<p>than if you didn&rsquo;t do that the AI system
is capable of that and so it can</p>
<p>bootstrap itself to kind of beyond
beyond human capabilities more recently</p>
<p>and we don&rsquo;t have any work out on this
but I think we will soon we&rsquo;ve started</p>
<p>applying this to natural language so in
the last year or so there have been kind</p>
<p>of big a lot of progress on large
language models like open the eyes GPT</p>
<p>and google&rsquo;s burt where you just take a
big corpus of text you train just just a</p>
<p>big transformer model to to predict the
next word or the next token and that</p>
<p>allows you to generate very coherent
text and can also be fine-tuned to solve</p>
<p>a lot of linguistic tasks so one one
idea there is can we find two NAT via RL</p>
<p>from human preferences right I have a
language model it&rsquo;s</p>
<p>a lot of text some of its happy some of
its sad five five minutes yeah you know</p>
<p>some of its formal statements or
informal statements some of its jokes</p>
<p>the language model maybe has some idea
in its internal representation of the</p>
<p>difference between those things but you
know if I just sample from the language</p>
<p>model it just kind of gives me random
samples of stuff so can i push this</p>
<p>language model in directions and to
produce behaviors that only a human can</p>
<p>specify that can&rsquo;t be specified
programmatically so things like</p>
<p>statements that rhyme or our-our
statements that are in iambic pentameter</p>
<p>could you make a system that is you know
from the logic of learning from human</p>
<p>preferences is a better poet than any
human could be or something like this or</p>
<p>you know makes makes like very positive
sentiment statements that are you know</p>
<p>that it&rsquo;s hard to find enough enough
positive sentiment statements to to copy</p>
<p>from so that&rsquo;s the direction we&rsquo;re going
in and then I think you know like a long</p>
<p>term vision for it would be you know we
would you know we want a system that</p>
<p>basically has an ongoing dialogue with
with a human the human asset to do</p>
<p>something really complicated like
planning and executing the mission to</p>
<p>Mars you know the system kind of kind of
clarifies ss4 instructions while it&rsquo;s</p>
<p>learning and while it&rsquo;s doing the task
and we make sure that that things like</p>
<p>pathological solutions the problem don&rsquo;t
happen one way to get to Mars really</p>
<p>quickly is you know to escape from Earth
and propel yourself by dropping a bunch</p>
<p>of like nuclear explosions back at earth
that would work that would get you to</p>
<p>Mars this project called the Orion
project in the 1950s although plan was</p>
<p>to detonate the nuclear weapons when
they were like far away from Earth but</p>
<p>this is not a solution we would favor
how do we make sure that that that AI</p>
<p>systems don&rsquo;t don&rsquo;t do things like that
cool so I&rsquo;ve only talked about a subset</p>
<p>of what the safety team is working on
but you know we have around 15 members</p>
<p>here some of some of these efforts were
done in collaboration with with with</p>
<p>deep mind and various various academic
and</p>
<p>we have a number of kind of interns and
faculty affiliates but you know we&rsquo;re a</p>
<p>safety team is is is continuing to hire
and we&rsquo;re we&rsquo;re interested in you know</p>
<p>further advancing these and these in
other areas thank you so much sorry oh</p>
<p>hello everyone we are now at the
conclusion of today&rsquo;s morning talks but</p>
<p>before we break for lunch I would like
to invite all of the volunteers who are</p>
<p>joining us today from open AI and
Berkeley and New Haven school to please</p>
<p>come up to the front so as we proceed
into the afternoon hackathon and</p>
<p>breakout sessions these will be the
faces that will be around to help you</p>
<p>that you should ask questions to these
people are all talented researchers or</p>
<p>contributors or engineers in this space
many of these people are employees of</p>
<p>open AI ever and we also have I think
the only person here who&rsquo;s not currently</p>
<p>employed by opening I was previously
employed by open AI so if you want to</p>
<p>pick our brains about what it&rsquo;s like
here what we do why it matters please</p>
<p>feel free can we just have everyone get
maybe a sense to introduce themselves</p>
<p>sure I am Daniel I work on the safety
team as a male engineer working on the</p>
<p>language fine-tuning project from a
human feedback</p>
<p>yeah Matthias I&rsquo;ve owned robotics I&rsquo;m
Ethan I&rsquo;m on the safety team working on</p>
<p>model-based or LM safe exploration with
Josh I&rsquo;m Carl I&rsquo;m on the games team</p>
<p>primarily studying transfer learning and
procedurally generated environments my</p>
<p>name is Dylan I&rsquo;m a PhD student at UC
Berkeley and I mainly work on preference</p>
<p>learning I&rsquo;m Amanda and I&rsquo;m on the
policy team here opening I I marry I</p>
<p>work on the safety team on safe
exploration alright and another thing</p>
<p>that I want to say thank you all so much
for being here today something that I</p>
<p>hope we can do is really make this a
useful experience</p>
<p>for all of you and I hope that over the
course of the day that you know you give</p>
<p>us feedback about what you find helpful
and not helpful and what it is that</p>
<p>you&rsquo;re hoping to get out of this
experience so that we can figure out you</p>
<p>know how to help you get to that and and
thank you so much please enjoy lunch</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/english/">English</a>
        
            <a href="/tags/video-transcripts/">Video Transcripts</a>
        
            <a href="/tags/openai/">OpenAI</a>
        
    </section>


    </footer>


    
</article>

    

    

<div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064"
         crossorigin="anonymous"></script>
    <ins class="adsbygoogle"
         style="display:block; text-align:center;"
         data-ad-layout="in-article"
         data-ad-format="fluid"
         data-ad-client="ca-pub-9206135835124064"
         data-ad-slot="1055602464"></ins>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>
<aside class="related-content--wrapper">
    <h2 class="section-title">Related content</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/en/at2xkqjazns/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/AT2XkqJAZns" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Towards Epileptic Seizure Prediction with Deep Network ÔΩú Kata Slama ÔΩú OpenAI Scholars Demo Day 2020 ÔΩú OpenAI</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/jzohw-eybtq/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/JZOHW-eYBtQ" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Introductions by Sam Altman &amp; Greg Brockman ÔΩú OpenAI Scholars Demo Day 2020 ÔΩú OpenAI</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/-fozam9xqs4/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/-FoZAM9xqS4" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">OpenAI Five vs. OG, Game 2 ÔΩú OpenAI Five Finals (4‚ß∏6) ÔΩú OpenAI</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/u9mjuukhuzk/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/U9mJuUkhUzk" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">OpenAI DevDay, Opening Keynote ÔΩú OpenAI</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/lpe5gwuqa-k/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/lpe5Gwuqa-k" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Scaling Laws for Language Transfer Learning ÔΩú Christina Kim ÔΩú OpenAI Scholars Demo Day 2021 ÔΩú OpenAI</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2021 - 
        
        2023 SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics
    </section>
    
    <section class="powerby">
        

        As an Amazon Associate I earn from qualifying purchases üõí<br/>

        Built with <a href="https://swiest.com/" target="_blank" rel="noopener">(Ôæâ‚óï„ÉÆ‚óï)Ôæâü™Ñüíûüíñü•∞ across the glüåçüåèüåébe</a> <br />
        
        
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>


<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Serif&family=Noto+Serif+Armenian&family=Noto+Serif+Bengali&family=Noto+Serif+Devanagari&family=Noto+Serif+Georgian&family=Noto+Serif+Gujarati&family=Noto+Serif+HK&family=Noto+Serif+Hebrew&family=Noto+Serif+JP&family=Noto+Serif+KR&family=Noto+Serif+Kannada&family=Noto+Serif+Khmer&family=Noto+Serif+Lao&family=Noto+Serif+Makasar&family=Noto+Serif+Malayalam&family=Noto+Serif+Myanmar&family=Noto+Serif+Oriya&family=Noto+Serif+SC&family=Noto+Serif+Sinhala&family=Noto+Serif+TC&family=Noto+Serif+Tamil&family=Noto+Serif+Telugu&family=Noto+Serif+Thai&family=Noto+Serif+Tibetan&display=swap" rel="stylesheet">

    </body>
</html>

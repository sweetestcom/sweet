<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Video Transcript ÔªøHello, and good morning everyone!
Hi! I&amp;rsquo;m Josh Achiam, I&amp;rsquo;m a Safety researcher
here at OpenAI and I&amp;rsquo;m the main author of Spinning Up in Deep RL up and thank you
all so much for being here today at OpenAI&amp;rsquo;s 1st Spinning Up Workshop.
oFor people who are tuning in on the livestream I&amp;rsquo;d like to let you know that
there is a minor technical difficulty and so we will not be able to broadcast"><title>OpenAI Spinning Up in Deep RL Workshop ÔΩú OpenAI | SWIEST</title>
<link rel=canonical href=https://swiest.com/en/fdy7dt3ijgy/><link rel=stylesheet href=/scss/style.min.9a6fe90535a0e5c60443841f100f7b698092d48dba43fdb6386bb69b6559bc3d.css><script>document.oncontextmenu=function(){return!1},document.onselectstart=function(){return!1},document.oncopy=function(){return!1},document.oncut=function(){return!1}</script><script src=https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js></script><script type=text/javascript>$(document).ready(function(){$("#back-to-top").hide(),$(function(){$(window).scroll(function(){$(window).scrollTop()>600?$("#back-to-top").fadeIn(500):$("#back-to-top").fadeOut(500)}),$("#back-to-top").click(function(){return $("body,html").animate({scrollTop:0},500),!1})})})</script><meta property="og:title" content="OpenAI Spinning Up in Deep RL Workshop ÔΩú OpenAI"><meta property="og:description" content="Video Transcript ÔªøHello, and good morning everyone!
Hi! I&amp;rsquo;m Josh Achiam, I&amp;rsquo;m a Safety researcher
here at OpenAI and I&amp;rsquo;m the main author of Spinning Up in Deep RL up and thank you
all so much for being here today at OpenAI&amp;rsquo;s 1st Spinning Up Workshop.
oFor people who are tuning in on the livestream I&amp;rsquo;d like to let you know that
there is a minor technical difficulty and so we will not be able to broadcast"><meta property="og:url" content="https://swiest.com/en/fdy7dt3ijgy/"><meta property="og:site_name" content="SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="English"><meta property="article:tag" content="Video Transcripts"><meta property="article:tag" content="OpenAI"><meta property="article:published_time" content="2023-11-06T03:59:54+00:00"><meta property="article:modified_time" content="2023-11-06T03:59:54+00:00"><meta name=twitter:title content="OpenAI Spinning Up in Deep RL Workshop ÔΩú OpenAI"><meta name=twitter:description content="Video Transcript ÔªøHello, and good morning everyone!
Hi! I&amp;rsquo;m Josh Achiam, I&amp;rsquo;m a Safety researcher
here at OpenAI and I&amp;rsquo;m the main author of Spinning Up in Deep RL up and thank you
all so much for being here today at OpenAI&amp;rsquo;s 1st Spinning Up Workshop.
oFor people who are tuning in on the livestream I&amp;rsquo;d like to let you know that
there is a minor technical difficulty and so we will not be able to broadcast"><link rel="shortcut icon" href=/favicon.ico><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"dark")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu307e6a33fa6fd661ccda3b77024ef5c2_252345_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics</a></h1><h2 class=site-description>üßôü™Ñüåé</h2></div></header><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg><span>Search</span></a></li><li><a href=/tags/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg><span>Tags</span></a></li><li><a href=/chart/podcastchart.html target=_blank><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-apple-podcast" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M18.364 18.364a9 9 0 10-12.728.0"/><path d="M11.766 22h.468a2 2 0 001.985-1.752l.5-4A2 2 0 0012.734 14h-1.468a2 2 0 00-1.985 2.248l.5 4A2 2 0 0011.766 22z"/><path d="M12 9m-2 0a2 2 0 104 0 2 2 0 10-4 0"/></svg><span>Podcasts</span></a></li><li><a href=/radio.html target=_blank><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-radio" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M14 3 4.629 6.749A1 1 0 004 7.677V19a1 1 0 001 1h14a1 1 0 001-1V8a1 1 0 00-1-1H4.5"/><path d="M4 12h16"/><path d="M7 12v-2"/><path d="M17 16v.01"/><path d="M13 16v.01"/></svg><span>Radio</span></a></li><div class=menu-bottom-section><li id=i18n-switch><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg><select name=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://swiest.com/ selected>English</option><option value=https://swiest.com/af/>Afrikaans</option><option value=https://swiest.com/am/>·ä†·àõ·à≠·äõ</option><option value=https://swiest.com/ar/>ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</option><option value=https://swiest.com/az/>Az…ôrbaycan</option><option value=https://swiest.com/be/>–±–µ–ª–∞—Ä—É—Å–∫—ñ</option><option value=https://swiest.com/bg/>–±—ä–ª–≥–∞—Ä—Å–∫–∏</option><option value=https://swiest.com/bn/>‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ</option><option value=https://swiest.com/bo/>‡Ωñ‡Ωº‡Ωë‡ºã‡Ω¶‡æê‡Ωë‡ºã</option><option value=https://swiest.com/bs/>Bosanski</option><option value=https://swiest.com/ca/>Catal√†</option><option value=https://swiest.com/zh-hans/>ÁÆÄ‰Ωì‰∏≠Êñá</option><option value=https://swiest.com/zh-hant/>ÁπÅÈ´î‰∏≠Êñá</option><option value=https://swiest.com/cs/>ƒåe≈°tina</option><option value=https://swiest.com/el/>ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨</option><option value=https://swiest.com/cy/>Cymraeg</option><option value=https://swiest.com/da/>Dansk</option><option value=https://swiest.com/de/>Deutsch</option><option value=https://swiest.com/eo/>Esperanto</option><option value=https://swiest.com/es-es/>Espa√±ol (Espa√±a)</option><option value=https://swiest.com/es-419/>Espa√±ol (Latinoam√©rica)</option><option value=https://swiest.com/et/>Eesti</option><option value=https://swiest.com/eu/>Euskara</option><option value=https://swiest.com/haw/> ª≈ålelo Hawai ªi</option><option value=https://swiest.com/fa/>ŸÅÿßÿ±ÿ≥€å</option><option value=https://swiest.com/fi/>Suomi</option><option value=https://swiest.com/fo/>F√∏royskt</option><option value=https://swiest.com/fr/>Fran√ßais</option><option value=https://swiest.com/fy/>Frysk</option><option value=https://swiest.com/ga/>Gaeilge</option><option value=https://swiest.com/gl/>Galego</option><option value=https://swiest.com/gu/>‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä</option><option value=https://swiest.com/he/>◊¢÷¥◊ë◊®÷¥◊ô◊™</option><option value=https://swiest.com/km/>·ûÄ·ûò·üí·ûñ·ûª·ûá·û∂·üî</option><option value=https://swiest.com/hi/>‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</option><option value=https://swiest.com/hr/>Hrvatski</option><option value=https://swiest.com/ht/>Krey√≤l Ayisyen</option><option value=https://swiest.com/hu/>Magyar</option><option value=https://swiest.com/hy/>’Ä’°’µ’•÷Ä’•’∂</option><option value=https://swiest.com/ig/>√Ås·ª•ÃÄs·ª•ÃÅ √ågb√≤</option><option value=https://swiest.com/id/>Bahasa Indonesia</option><option value=https://swiest.com/is/>√çslenska</option><option value=https://swiest.com/it/>Italiano</option><option value=https://swiest.com/ja/>Êó•Êú¨Ë™û</option><option value=https://swiest.com/jv/>Basa Jawa</option><option value=https://swiest.com/ka/>·É•·Éê·É†·Éó·É£·Éö·Éò</option><option value=https://swiest.com/kk/>“ö–∞–∑–∞“õ—à–∞</option><option value=https://swiest.com/kn/>‡≤ï‡≤®‡≥ç‡≤®‡≤°</option><option value=https://swiest.com/ko/>ÌïúÍµ≠Ïñ¥</option><option value=https://swiest.com/or/>‡¨ì‡¨°‡¨º‡¨ø‡¨Ü</option><option value=https://swiest.com/ckb/>⁄©Ÿàÿ±ÿØ€å</option><option value=https://swiest.com/ky/>–ö—ã—Ä–≥—ã–∑—á–∞</option><option value=https://swiest.com/la/>Latina</option><option value=https://swiest.com/lb/>L√´tzebuergesch</option><option value=https://swiest.com/lo/>‡∫û‡∫≤‡∫™‡∫≤‡∫•‡∫≤‡∫ß</option><option value=https://swiest.com/lt/>Lietuvi≈≥</option><option value=https://swiest.com/lv/>Latvie≈°u</option><option value=https://swiest.com/mk/>–ú–∞–∫–µ–¥–æ–Ω—Å–∫–∏</option><option value=https://swiest.com/ml/>‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç</option><option value=https://swiest.com/mn/>–ú–æ–Ω–≥–æ–ª —Ö—ç–ª</option><option value=https://swiest.com/mr/>‡§Æ‡§∞‡§æ‡§†‡•Ä</option><option value=https://swiest.com/sw/>Kiswahili</option><option value=https://swiest.com/ms/>Bahasa Melayu</option><option value=https://swiest.com/my/>·Äô·Äº·Äî·Ä∫·Äô·Ä¨</option><option value=https://swiest.com/ne/>‡§®‡•á‡§™‡§æ‡§≤‡•Ä</option><option value=https://swiest.com/nl/>Nederlands</option><option value=https://swiest.com/no/>Norsk</option><option value=https://swiest.com/pa/>‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä</option><option value=https://swiest.com/pl/>Polski</option><option value=https://swiest.com/pt-br/>Portugu√™s Brasil</option><option value=https://swiest.com/pt-pt/>Portugu√™s Europeu</option><option value=https://swiest.com/ro/>Rom√¢nƒÉ</option><option value=https://swiest.com/ru/>–†—É—Å—Å–∫–∏–π</option><option value=https://swiest.com/rw/>Kinyarwanda</option><option value=https://swiest.com/si/>‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω</option><option value=https://swiest.com/sk/>Slovenƒçina</option><option value=https://swiest.com/sl/>Sloven≈°ƒçina</option><option value=https://swiest.com/sq/>Shqip</option><option value=https://swiest.com/sr/>–°—Ä–ø—Å–∫–∏ (Srpski)</option><option value=https://swiest.com/su/>Basa Sunda</option><option value=https://swiest.com/sv/>Svenska</option><option value=https://swiest.com/ta/>‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç</option><option value=https://swiest.com/te/>‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</option><option value=https://swiest.com/tg/>–¢–æ“∑–∏–∫”£</option><option value=https://swiest.com/th/>‡πÑ‡∏ó‡∏¢</option><option value=https://swiest.com/tk/>T√ºrkmenler</option><option value=https://swiest.com/tl/>Filipino</option><option value=https://swiest.com/tr/>T√ºrk√ße</option><option value=https://swiest.com/uk/>–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</option><option value=https://swiest.com/ur/>ÿßÿ±ÿØŸà</option><option value=https://swiest.com/uz/>O'zbekcha</option><option value=https://swiest.com/vi/>Ti·∫øng Vi·ªát</option><option value=https://swiest.com/yi/>◊ê◊ô◊ì◊ô◊©</option><option value=https://swiest.com/zh-hk/>Á≤µË™û</option><option value=https://swiest.com/zu/>IsiZulu</option></select></li><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#video>Video</a></li><li><a href=#transcript>Transcript</a></li></ol></nav></div></section><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><ins class=adsbygoogle style=display:block data-ad-client=ca-pub-9206135835124064 data-ad-slot=8754979142 data-ad-format=auto data-full-width-responsive=true></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></aside><a id=back-to-top href=#><img src=/img/top_hu7c2829da96df0e9f8f0191d120020b22_22287_40x0_resize_box_3.png></a><main class="main full-width"><form action=/search/ class="search-form widget"><p><label>Search</label>
<input name=keyword required placeholder="Type something...">
<button title=Search><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg></button></p></form><article class=main-article><header class=article-header><div class=article-details><header class=article-tags><a href=/tags/english/>English
</a><a href=/tags/video-transcripts/>Video Transcripts
</a><a href=/tags/openai/>OpenAI</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/en/fdy7dt3ijgy/>OpenAI Spinning Up in Deep RL Workshop ÔΩú OpenAI</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>2023-11-06</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>129 minute read</time></div></footer></div></header><div class=article-content><p style=text-align:center><a href=https://amzn.to/3Nrdcwk target=_blank>üéÅAmazon Prime</a>
<a href=https://amzn.to/3RIBkxg target=_blank>üìñKindle Unlimited</a>
<a href=https://amzn.to/3Rqmudl target=_blank>üéßAudible Plus</a>
<a href=https://amzn.to/3TuLbbj target=_blank>üéµAmazon Music Unlimited</a>
<a href="https://www.iherb.com/?rcode=EID1574" target=_blank>üåøiHerb</a>
<a href="https://accounts.binance.com/register?ref=72302422" target=_blank>üí∞Binance</a></p></div><div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><ins class=adsbygoogle style=display:block data-ad-client=ca-pub-9206135835124064 data-ad-slot=8754979142 data-ad-format=auto data-full-width-responsive=true></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><section class=article-content><h2 id=video>Video</h2><div class=video-wrapper><iframe loading=lazy src=https://www.youtube.com/embed/fdY7dt3ijgY allowfullscreen title="YouTube Video"></iframe></div><h2 id=transcript>Transcript</h2><p>ÔªøHello, and good morning everyone!</p><p>Hi! I&rsquo;m Josh Achiam, I&rsquo;m a Safety researcher</p><p>here at OpenAI and I&rsquo;m the main author
of Spinning Up in Deep RL up and thank you</p><p>all so much for being here today at OpenAI&rsquo;s 1st Spinning Up Workshop.</p><p>oFor people who are tuning in on the
livestream I&rsquo;d like to let you know that</p><p>there is a minor technical difficulty
and so we will not be able to broadcast</p><p>the slides directly from my computer
into the livestream video so you&rsquo;ll be</p><p>seeing the screen through the camera. In
the event that that&rsquo;s not enough for you</p><p>to see it clearly I just open sourced
the repo that has the PDFs for these</p><p>slides so please go to github.com slash
open AI slash spinning up - workshop and</p><p>you&rsquo;ll find in the RL intro folder RL
intro PDF which will be the presentation</p><p>that I&rsquo;m about to give so hopefully that
makes it easier for you to follow along</p><p>so since this is kind of a new thing
that we&rsquo;re doing I&rsquo;d like to start today</p><p>by talking about what it is and why
we&rsquo;re doing it and what we hope you get</p><p>out of it from being here education at
open AI is this concept that as part of</p><p>our mission we want to make sure that we
provide for the public good and that we</p><p>help foster a global community around
AGI which is the thing that at opening I</p><p>we care the most about and are trying to
figure out how to make sure happens in a</p><p>way that&rsquo;s safe and beneficial for all
of humanity so for those of you who</p><p>aren&rsquo;t already familiar AGI is
artificial general intelligence the idea</p><p>is that this is going to be some very
powerful AI technology that&rsquo;ll have the</p><p>ability to change pretty much everything
about how we do anything something that</p><p>could potentially do most economically
valuable work something that could solve</p><p>tasks that currently only human
intelligence is capable of solving and</p><p>so we think it&rsquo;s really important that
we help people become aware of what AGI</p><p>is and what the technology that&rsquo;ll
likely underlie it is so that you can</p><p>think critically about issues that might
come up in the future and also if you&rsquo;re</p><p>interested
participate because we really need for</p><p>people to step up and help make sure
that this technology is safe and does</p><p>what we wanted to do and doesn&rsquo;t cause
anything harmful or detrimental to the</p><p>world so spinning up is the first thing
that we&rsquo;re launching under this</p><p>education at open AI initiative and the
goal is to help people acquire technical</p><p>skills in the research topics that we
care about so spinning up in deep RL is</p><p>a resource that hopefully all of you
have seen by now it contains a number of</p><p>different pieces including a short intro
to reinforcement learning so what is</p><p>this thing that we&rsquo;re doing so much
research about at open AI an essay about</p><p>how you would go about becoming a
researcher if you&rsquo;re interested in</p><p>joining a curated list of important
papers in the field so this is</p><p>particularly important because since
this is an emerging field there isn&rsquo;t</p><p>really a clear consensus on the best way
to learn it or a textbook that</p><p>completely illuminates the way from
start to finish and a lot of the</p><p>important knowledge right now is still
in research papers so if you want to</p><p>find out the most stuff about this you
have to go digging and hopefully this</p><p>helps you figure out where to look also
a code repo of key algorithms because</p><p>for any of you who have tried hacking in
this field before I&rsquo;m sure you found</p><p>that there were a lot of very confusing
resources out there really excellent</p><p>ones but nonetheless ones that made
non-obvious choices and didn&rsquo;t clearly</p><p>connect what they were doing to why they
were doing it and so we hope that the</p><p>repo that we provide and spinning up in
deep RL is part of something to bridge</p><p>the gap there and of course some
exercises so if you want to actually try</p><p>coding something up there are a few
ideas there for for what to do to get</p><p>you familiar with some of the key pieces
of math or algorithms or what kind of</p><p>bugs you might expect and so why are we
having workshops so in addition to</p><p>putting these resources online we think
it&rsquo;s gonna really help people if we work</p><p>with you one-on-one if we can see you
face-to-face and talk with you and have</p><p>the kind of conversations and share the
ideas that just you know don&rsquo;t come up</p><p>in the sort of open loop control thing
that happens when we put information on</p><p>the Internet
today we&rsquo;d like to have you come away</p><p>from this with a better sense of what
the current capabilities and limitations</p><p>are in deeper I&rsquo;ll tell you a little bit
about what kind of research is out there</p><p>so if you want to go and follow some
line of thinking you know what&rsquo;s been</p><p>done and what hasn&rsquo;t and we&rsquo;d like you
to actually try building and running</p><p>algorithms for deep reinforcement
learning for possibly for the first time</p><p>and show you how to be confident in
doing that so that if you want to keep</p><p>doing it afterwards you&rsquo;re able to all
right so then what is deep reinforcement</p><p>learning why do we need it why do we
care about it</p><p>deep reinforcement learning is the
combination of reinforcement learning</p><p>with deep learning RL reinforcement
learning is about solving problems by</p><p>trial and error and deep learning is
about using these very powerful function</p><p>approximate errs called deep neural
networks to solve problems and deep</p><p>reinforcement learning is just
straightforwardly the combination where</p><p>we&rsquo;re gonna have something that&rsquo;s
learning by trial and error and the</p><p>thing that&rsquo;s getting learned is a deep
neural network that&rsquo;s going to make some</p><p>kind of decision or evaluate some
situation and use that ultimately to in</p><p>some environment make decisions that
lead to rewards where reward is just</p><p>some measure of how good or bad an
outcome was so when would you want to</p><p>use RL RL is useful when for one there&rsquo;s
a sequential decision making problem -</p><p>you don&rsquo;t know what the right thing to
do in that situation is already if you</p><p>have the optimal behavior say from
having watched human experts enough and</p><p>you have just a ton of data on exactly
what to do in every situation then you</p><p>can use the standard tools of say
supervised learning to exactly get some</p><p>machine learning system to duplicate
that behavior but when you don&rsquo;t have</p><p>access to that or when you suspect that
what appears to be expert human behavior</p><p>is in fact suboptimal
in that situation you may want to try</p><p>reinforcement learning instead because
it could discover</p><p>things that wouldn&rsquo;t have otherwise been
known and you also have to be able to</p><p>evaluate whether or not a behavior or an
outcome was good or bad this is pretty</p><p>critical
so RL is good when it&rsquo;s easier to</p><p>evaluate behaviors than to generate them
or to exactly solve for them and when</p><p>would you use deep learning
so the typical paradigm for deep</p><p>learning is that you want to approximate
some very complicated function a</p><p>function that usually requires some
amount of intelligence so for instance</p><p>if a human looks at a picture of a bird
and then knows what species of bird that</p><p>is that&rsquo;s a thing that you can&rsquo;t really
write down a simple mathematical rule to</p><p>do if you want to get a machine to do
that you have to teach it from data and</p><p>other problems that you know you would
want to do this for typically have</p><p>inputs or outputs that are very high
dimensional because it&rsquo;s just quite hard</p><p>to from an image or from a video stream
or from an audio stream go to a decision</p><p>rule without doing some sort of learning
in the middle and also you typically</p><p>want to have lots and lots of data
because getting machine learning systems</p><p>to behave in any reasonable way requires
that you give them sufficient examples</p><p>and there are tons of problems where
this is exactly what you have and in</p><p>those domains deep learning has been
very successful at exceeding whatever</p><p>was previously the state of the art from
any other methods that existed before</p><p>and creating things that are now
standard consumer products things that</p><p>were magic 10 years ago are like
completely normal now the idea that we</p><p>have super excellent image recognition
facial classification that you can talk</p><p>to your phone and it&rsquo;s going to know
what you said and it&rsquo;s not just going to</p><p>come up with some completely random
gobbledygook</p><p>this is getting better because we&rsquo;re
able to leverage this very powerful</p><p>technology that is deep learning for
these problems and so deep RL is when</p><p>you have some very hard high dimensional
problem where you can evaluate behaviors</p><p>and you want to get a machine to learn
how to do it because you can&rsquo;t write</p><p>down how it should in fact behave and
some very simple examples of this are</p><p>say video games we
you want to go from a computer looking</p><p>at an image of the screen so just raw
pixels to a decision rule that scores</p><p>the most possible points in the game or
behaves in a way which is cool or</p><p>interesting or exciting or perhaps a
really sophisticated strategy game like</p><p>go we&rsquo;re really deep thinking and
intuition and creativity is necessary to</p><p>make progress you can&rsquo;t write down a
simple rule for that but you can learn</p><p>it with reinforcement learning or
perhaps you want to control some complex</p><p>humanoid some some robot to run around
and do stuff</p><p>or maybe something which is a little
less silly maybe a little more real</p><p>maybe you want to get robots in a
factory to quickly learn a new task when</p><p>the robot uprising happens it&rsquo;s because
of this we&rsquo;re very sorry for this</p><p>research this was trained by the way
with an algorithm that was developed</p><p>here at open a I called proximal policy
optimization it&rsquo;s one of the algorithms</p><p>and spinning up and if you haven&rsquo;t had
any experience with it then we won&rsquo;t get</p><p>into it in this lecture today but any
other point in the afternoon during the</p><p>hackathon happy to go into detail so
before we proceed into the are L</p><p>specific stuff this is a crowd with a
pretty wide range of backgrounds and so</p><p>I just want to do a very brief recap of
some of the patterns from deep learning</p><p>what do you expect when you set up a
deep learning problem what does that</p><p>look like what do you have to think
about so we typically talk about it in</p><p>terms of the language of finding a model
that is able to give the right outputs</p><p>for certain inputs so in this case the
model is going to be some function of</p><p>the inputs and parameters and the
parameters are adjustable we control</p><p>them we change them and we want to
change them in a way that&rsquo;s going to</p><p>make the model behave according to some
design specification the way that we</p><p>provide the design specification and get
the parameters to satisfy it is by</p><p>setting up some kind of loss function
this tells you in a nutshell how good</p><p>the model is at doing the thing that you
want it to do usually some measure of</p><p>just how close the output from the model
is to the</p><p>I earn output and the critical thing
about this loss function is that it has</p><p>to be differentiable with respect to the
parameters in the model and when you</p><p>have that set up oh and of course
there&rsquo;s data as well so you have a bunch</p><p>of different examples of inputs and
outputs and your loss function reflects</p><p>how well your model performs across all
of them typically as just some average</p><p>overpour data point losses so with this
set up you can then proceed to find the</p><p>optimal model through gradient descent
the idea is that the gradient is a</p><p>mathematical object that tells you how
much the loss changes in response to a</p><p>change in the parameters and then you
want to knowing that change the</p><p>parameters in a way which is fruitful
that is it reduces the loss it reduces</p><p>the measure of error so what makes deep
learning deep what is the deep part it&rsquo;s</p><p>this idea that function composition is
at the core of the models that we make</p><p>and that we consider so function
composition just means that you have a</p><p>bunch of different parametrized
functions and the outputs of one are the</p><p>inputs to the next one and you can
arrange these in many different</p><p>topologies we&rsquo;ll call these
architectures for neural networks the</p><p>very simplest kind is just one where you
have an input layer and then there is a</p><p>matrix that multiplies that and then you
maybe add some bias to that vector and</p><p>then you pass that through a nonlinear
activation function typically this is</p><p>going to squash the outputs from that
first linear transformation into</p><p>something which maybe is in the range
from 0 to 1 or 0 to infinity something</p><p>relatively simple but that non-linearity
happens to do a lot of work and then</p><p>when you have successive layers what it
allows the model to do ultimately is</p><p>represent successively more complex
features internally so you might think</p><p>of the output of each layer as being a
new representation of the original input</p><p>which has maybe rearranged the
information in a way which is easier for</p><p>some kind of final decision making
procedure at the end of the network to</p><p>make the right decision based on aside
from that very simple</p><p>there are also substantially more
complex ones so the other two diagrams</p><p>on this slide are for lsdm networks so
that&rsquo;s in the lower left and the</p><p>transformer network that&rsquo;s on the right
an LS TM network is a recurrent neural</p><p>network the idea is that this is the
kind of network that can accept a time</p><p>series of inputs and produce a time
series of outputs and internally it has</p><p>some very complicated mechanisms for
making sure that information gets</p><p>propagated effectively across time steps
in a hidden state so that when you make</p><p>a decision somewhere in the future you
can remember something that you saw in</p><p>the past and then you can update the
network in a way which is stable and</p><p>reasonable the transformer network is
substantially more complicated and it</p><p>allows networks to do something called
attending over their various inputs so</p><p>at attention it&rsquo;s something which is a
concept that we can all kind of relate</p><p>to when we look at the world we don&rsquo;t
actually process literally every piece</p><p>of data that we take in concurrently we
particularly attend to whatever happens</p><p>to be say in the center of our field of
view or whatever we&rsquo;re thinking about at</p><p>the moment whatever is most urgent and
attention neural networks are able to</p><p>basically do that when they make some
decision on the basis of a lot of data</p><p>they can select out the most important
pieces of the data for making particular</p><p>kinds of decisions and that turns out to
be very helpful in practice a few other</p><p>things about deep learning and this is
mostly just I&rsquo;m checking off some boxes</p><p>if you want depth on this I strongly
recommend that you go see the spinning</p><p>up essay where there are a bunch of
links to papers and other resources that</p><p>will give you detailed information about
this but to check off the boxes we might</p><p>talk about regularizer x&rsquo; so the idea is
that sometimes optimizing your loss</p><p>function picking the model that actually
gives the lowest value of your loss</p><p>function may not be the best thing to do
you may wind up with a phenomenon called</p><p>overfitting where you&rsquo;ve made your model
behave perfectly with respect to the</p><p>data that you showed it but then it does
a terrible job when it&rsquo;s given any other</p><p>data
because it learned a decision rule which</p><p>was entirely too specific but with
regularization you trade off the loss</p><p>against something which has nothing to
do with performance on the particular</p><p>task but just kind of says hey cool your
jets a little bit</p><p>don&rsquo;t be so avid about satisfying that
objective and then it turns out that</p><p>regularization actually leads to models
that do a better job of generalizing to</p><p>unseen data then there are also a couple
of things that make the optimization</p><p>process smoother and easier so you might
do some kind of normalization technique</p><p>where internally there&rsquo;s some output in
the middle of the network where it&rsquo;s</p><p>good to adjustably rescale that and
shift it around and that&rsquo;s better than</p><p>just letting the network do whatever it
would have done if you didn&rsquo;t do this</p><p>kind of normalization it&rsquo;s sort of
spooky and there are some legitimate</p><p>complaints inside the community about
whether or not we really understand why</p><p>this helps but it seems to so it&rsquo;s worth
knowing about also you might use a more</p><p>powerful optimizer than standard
gradient descent this comes up also in</p><p>reinforcement learning actually many of
the things that we&rsquo;ve been talking about</p><p>in these past few slides show up in deep
reinforcement learning which is why I&rsquo;m</p><p>bringing them up
adaptive optimizers do something special</p><p>in figuring out how to tune the learning
rate the amount by which you change each</p><p>parameter at each step of updating in a
way which leads to typically faster</p><p>convergence so you get to the to the
optimum point a little bit sooner or a</p><p>little bit easier there&rsquo;s also the
Reaper amortization trick but that&rsquo;s</p><p>quite complicated and so we won&rsquo;t
actually talk about it it&rsquo;s on the slide</p><p>so that you know where to look all right
that&rsquo;s all this stuff from deep learning</p><p>that I wanted to talk about
now onto reinforcement learning so first</p><p>and foremost we have to talk about how
do you formulate a reinforcement</p><p>learning problem what does that mean
what does that do what are the pieces of</p><p>it how do they fit together we typically
use the language of saying that there&rsquo;s</p><p>an agent that interacts with an
environment so the agent is whatever</p><p>thing is making some kind of decision
the environment is wherever those</p><p>decisions are happening and the thing
that creates the consequences of those</p><p>decisions
and there&rsquo;s this loop where the</p><p>environment has some state and has some
measure of how good it is to be in that</p><p>state that&rsquo;s a reward and the agent gets
to observe the state and possibly the</p><p>reward it uses the reward for learning
whether or not it observes it as a</p><p>subtle technical detail but anyway
okay the agent gets a state observation</p><p>and a reward and then the agent makes
some kind of decision about what action</p><p>to take it picks the action and it
executes sit in the environment and then</p><p>the state of the environment changes
there&rsquo;s a new state of the environment</p><p>the agent perceives it the agent acts
etc the goal of the agent is to figure</p><p>out what decisions will maximize the sum
total of rewards that it&rsquo;ll ever get</p><p>actually it&rsquo;s slightly more specific
than this and there are a couple of</p><p>different formulations that we can
choose and we&rsquo;ll talk about them</p><p>momentarily but that&rsquo;s basically it in a
nutshell we want to maximize this sum of</p><p>rewards that we get and the agent is
going to figure out how to attain that</p><p>goal through trial and error so you just
don&rsquo;t know in advance what the right</p><p>thing to do is so you have to just try
things see what happens see how much</p><p>reward you get and then adjust your
decision on the basis of that so</p><p>reinforcement learning is about
algorithms for doing precisely that but</p><p>before we can talk about the algorithms
we have to introduce a bunch of</p><p>terminology for those of you who have
done the work of going through the</p><p>spinning up material online this will
probably be quite familiar and I&rsquo;m</p><p>mostly going through it for the benefit
of the audience that I expect might</p><p>watch this in the future as a starting
point for this so bear with me I&rsquo;ll try</p><p>to go through this reasonably quickly
but we have to talk about observations</p><p>and actions policies trajectories
rewards and returns what the RL</p><p>optimization problem actually is how we
formalize it and then value in action</p><p>value functions and also advantage
functions so there&rsquo;s a whole lot of</p><p>stuff that you kind of have to know and
unpack in order to really fruitfully</p><p>progress and reinforcement learning and
and these are just those central pieces</p><p>so observations and actions a state is
something which tells you absolutely</p><p>everything about the environment
the agent usually doesn&rsquo;t get access to</p><p>the state there is usually some stuff
that&rsquo;s just hidden from the agent so</p><p>what the agent perceives is called an
observation if the observation contains</p><p>all the information in a state we called
this environment fully observed if it</p><p>doesn&rsquo;t we call it partially observed
and states observations and actions can</p><p>be continuous or discrete for all of the
problems that we care about in deep RL</p><p>the observations are continuous and the
actions might be discrete or continuous</p><p>a policy is a rule for selecting actions
there are a couple of different ways</p><p>that you can get to this kind of rule
we typically classify them as one of two</p><p>kinds stochastic or deterministic a
stochastic policy is a rule for randomly</p><p>selecting an action on the basis of the
most recent observation or possibly</p><p>preceding observations as well a
deterministic policy is just a map</p><p>directly from observation to action and
no randomness involved at all you may be</p><p>wondering why it would be useful to have
a random policy at all because it might</p><p>seem like randomness is just sort of
dangerous but actually it can be quite</p><p>helpful and there are some very
principled ways of optimizing stochastic</p><p>policies and it&rsquo;s a little bit harder to
optimize completely deterministic</p><p>policies there may also be a matter of
robustness in that having a little bit</p><p>of randomness can make you more robust
sometimes to perturbation then having</p><p>learned a brittle specific deterministic
policy so now just to give some sort of</p><p>concrete examples in tensorflow
because I assume that most of you will</p><p>probably have met tensorflow as your
first deep learning library and if not</p><p>pi torch and for those of you who are
stuck with tensorflow I&rsquo;m so sorry you</p><p>probably should have picked pi torch I
know I should have but but here we are</p><p>so in in tensorflow for a stochastic
policy over discrete actions we might</p><p>first set up a placeholder for loading
in observations and then we might set up</p><p>a multi-layer perceptron network and MLP
network so this is just the most basic</p><p>kind of feed-forward neural network the
thing that I talked about earlier which</p><p>is a succession of linear transforms of
inputs followed by nonlinear transforms</p><p>of inputs
in this case the linear transforms take</p><p>you to something of size 64 and there
are two of them and then the activation</p><p>is at an H activation so this gets you
to a range of minus one to one in a nice</p><p>smooth way and and then we produce
logits based on the output from that</p><p>piece of the network so logits are
basically something that proceeds having</p><p>probabilities for particular actions if
you take the softmax of the logit that&rsquo;s</p><p>not a function you&rsquo;re familiar with I
recommend looking it up it&rsquo;s just</p><p>something that exponentiate Sall the
logits and then divides by the sum of of</p><p>those exponentiated logins so so it
normalizes the distribution to to being</p><p>a probability distribution all the
entries have to be greater than zero and</p><p>sum up to 1 so we get logits and then we
get actions by using TF multinomial to</p><p>sample something stochastically assuming
that the probabilities are based on</p><p>taking the softmax of those logits you
can ignore the squeeze that&rsquo;s just there</p><p>for making sure that certain things
actually work and then in the</p><p>deterministic policy let&rsquo;s say we have a
continuous action case so we want to</p><p>output a vector of actions where each
entry can be any real value number we</p><p>will just go from observation to network
to a final layer which is just going to</p><p>be the actions all right so that&rsquo;s
policies let&rsquo;s talk about a trajectory a</p><p>trajectory is a complete sequence of
states and actions through the history</p><p>of an environment the agent starts in a
state takes an action then there&rsquo;s a</p><p>next state next action etc the first
state in the environment is sampled from</p><p>some previous distribution over starting
States and then afterwards state</p><p>transitions are going to be either
deterministic or stochastic but there&rsquo;s</p><p>just some rule in the environment that
given the current state and the current</p><p>action whatever action the agent took
picks what the next state is a</p><p>trajectory is also sometimes called an
episode or a rollout you&rsquo;ll see this</p><p>terminology used completely
interchangeably so just be aware that&rsquo;s</p><p>out there there&rsquo;s I&rsquo;m so sorry in every
new ish field a lot of terminology</p><p>confusion we&rsquo;re different people in
different</p><p>areas of academia worked on it for a
while and use different terms and then</p><p>in the end we&rsquo;re left with just a weird
mishmash notation - you&rsquo;re gonna see</p><p>some notation where states and actions
are notated by s and a and then in code</p><p>you&rsquo;ll see some places where it&rsquo;s X and
u and this is because of the ancient</p><p>eternal conflict between the control
theorists and the reinforcement learning</p><p>theorists and we&rsquo;re just stuck with it
now so that aside let&rsquo;s talk about</p><p>rewards and returns so a reward function
is going to map from the states and</p><p>actions or states and actions and
possibly next States on to just some</p><p>number that tells you good or bad
positive is good negative is bad the</p><p>more positive the better and you have to
if you&rsquo;re a designer setting up a</p><p>reinforcement learning problem you have
to pick with that reward function is</p><p>going to be so you want to make sure
that you incentivize the stuff that you</p><p>want to have happen and disincentivize
stuff that you don&rsquo;t want to have happen</p><p>so as a very simple example suppose that
you want a robot to run forward but you</p><p>don&rsquo;t want it to waste a ton of energy
so maybe you will give it a reward</p><p>proportional to its forward velocity but
you&rsquo;ll penalize it proportionally to the</p><p>some of the action magnet or to the
action magnitude so you&rsquo;ll discourage</p><p>superfluous actions the return of a
trajectory is going to be some</p><p>cumulative reward along it we have two
ways of formulating this and what you&rsquo;re</p><p>going to find in deep reinforcement
learning implementations is that we&rsquo;re</p><p>going to completely conflate which
problem we&rsquo;re trying to solve with the</p><p>other but the finite horizon
undiscounted sum of rewards works when</p><p>you have a finite horizon it doesn&rsquo;t
work when you have an infinite horizon</p><p>because if you have an infinite sum of
things it might diverge unless you do</p><p>some kind of discounting so in this
other case infinite horizon discounted</p><p>some of returns you have a discount
factor gamma between 0 & 1 and that&rsquo;s</p><p>how you down weight things that happen
in the future this makes sure that this</p><p>is a reasonably well-defined quantity
but why would it make sense to discount</p><p>things you probably would rather someone
tell you that they&rsquo;re gonna give you</p><p>$100 today than $100 in a hundred years
right like it&rsquo;s just good to get</p><p>upfront then there&rsquo;s the reward to go
this is closely related it&rsquo;s basically</p><p>just a measure of return starting from a
particular time step or state so the</p><p>reward to go from some point in time is
just the sum of rewards that&rsquo;ll happen</p><p>after that point in time and now we can
talk about the reinforcement learning</p><p>problem just formally we&rsquo;re going to set
up a performance measure for a</p><p>particular policy PI J of Pi which is
the expected value of return for</p><p>whichever formulation we&rsquo;ve picked
according to a distribution over</p><p>trajectories in the environment based on
the choice of policy so what that means</p><p>is that again start states come from a
starting distribution transitions in the</p><p>environment are based on something in
the environment that transition</p><p>distribution P and actions will come
from the policy conditioned on the the</p><p>observations of the states and we want
to find the optimal policy PI star which</p><p>maximizes this now we have to talk about
value functions so value functions are</p><p>measures of how much reward you expect
to get from a particular State or state</p><p>action pair assuming that you&rsquo;re going
to behave a certain way so we have the</p><p>on policy value function and action
value function V PI and Q PI which</p><p>respectively tell you how good it is to
be in a particular state and how good it</p><p>is to be in a particular state action
pair assuming that forever after being</p><p>in those places you act according to the
policy PI and then there&rsquo;s also V star</p><p>and Q star same thing except if you were
to act according to the optimal policy</p><p>it&rsquo;s great to know Q star as we&rsquo;ll talk
about momentarily value and action value</p><p>functions are connected the value is
just the expected action value expecting</p><p>over what option you might take
according to the current policy and the</p><p>advantage function tells you how much
better a given action is than average</p><p>and it&rsquo;s just the difference between Q
and V these value functions satisfy</p><p>recursive bellman equations these are
super important and they&rsquo;re the</p><p>foundation of a bunch of algorithms so
they&rsquo;re really worth knowing and kind of</p><p>just worth grappling with I think that
these can be particularly tricky at</p><p>first I remember the first time that I
met reinforcement learning I was just so</p><p>turned around and lost by these the
notion that there was going to be this</p><p>recursive equation where the definition
of a thing depended on itself was quite</p><p>confusing but it&rsquo;s it&rsquo;s it&rsquo;s worth just
hitting your head on for a while until</p><p>it makes sense but what it&rsquo;s saying is
that the value of being in a particular</p><p>place is going to be as good as whatever
reward you get for being in that place</p><p>plus all the rewards that you&rsquo;ll ever
get for all the places you&rsquo;ll go</p><p>afterwards now why is it great to know Q
star Q star tells you if you&rsquo;re gonna</p><p>act according to the optimal policy
forever after you started in this state</p><p>and took this action and we don&rsquo;t care
what policy this action came from how</p><p>well will you do so that means that if
you want to do the best you possibly can</p><p>do all you need to know is what action
maximizes Q star in a particular state</p><p>and then take that action because that&rsquo;s
gonna be the best action in that state</p><p>and then afterwards you&rsquo;ve assumed that
you&rsquo;re gonna do the best that you can</p><p>ever possibly do so if you have Q star
you basically have the optimal policy so</p><p>this is going to lead us ultimately to
the two different kinds of algorithms</p><p>and reinforcement learning for control
where in one case we&rsquo;ll try to directly</p><p>optimize a policy and in the other case
we&rsquo;ll try to find Q star now if we want</p><p>to find Q star we have to set up a
function approximator for it q theta</p><p>which will represent by some kind of
deep neural network and we&rsquo;re gonna want</p><p>to measure how good is it at
approximating Q star and this is what</p><p>that recursive bellman equation is gonna
be really helpful for because the</p><p>beautiful thing is we don&rsquo;t need to have
acted according to the optimal policy to</p><p>check how well Q theta fits that bellman
equation we just need a bunch of</p><p>examples of state action next state and
reward tuples and if we have enough of</p><p>those over enough of the environment
then we can probably do a pretty good</p><p>job of fitting q theta based on that
bellman equation based on maybe this</p><p>means squared bellman error and then use
that afterwards for control which is</p><p>having a decision-making rule by the way
I apologize if anything has been</p><p>confusing about my using sort of the
terminology of control interchangeably</p><p>with the terminology of reinforcement
learning when I say control I mean</p><p>having the best
policy so now what kinds of RL</p><p>algorithms are out there behold a
taxonomy which is much more restrictive</p><p>than it looks it looks very pretty and
it looks very definitive but it&rsquo;s</p><p>actually masking a lot of subtlety and
you know detailed choices and the fact</p><p>that there&rsquo;s actually a lot more bleed
over between these things than you might</p><p>expect
but at a very high level this is a</p><p>useful picture to start with that we
have two different kinds of RL</p><p>algorithms ones where we have access to
the model of the environment and ones</p><p>where we don&rsquo;t so what that means a
model of the environment is something</p><p>which tells us if we&rsquo;re in a given state
and we take a particular action what&rsquo;s</p><p>gonna happen next the model would
predict what the state of the</p><p>environment will be after that and
that&rsquo;s really useful because if we can</p><p>forward simulate the environment then
that&rsquo;s extremely helpful for evaluating</p><p>our current policy it&rsquo;s extremely
helpful for figuring out what a better</p><p>action would be than the one that we
might want to take so if you don&rsquo;t have</p><p>a model you&rsquo;re quite limited you just
have to figure out how to do well based</p><p>on experiences that you&rsquo;ve seen your
direct interactions of the environment</p><p>you don&rsquo;t get any other information but
if you do have a model it&rsquo;s quite</p><p>potentially powerful although as we&rsquo;ll
discuss the methods for model-based</p><p>reinforcement learning are not quite as
mature so far as the methods for model</p><p>free reinforcement learning so now okay
that last slide was just a ton of</p><p>acronyms maybe not that insightful let&rsquo;s
talk about what these algorithms are</p><p>doing there are three key pieces in any
reinforcement learning algorithm for one</p><p>you&rsquo;re going to run the policy in the
environment you&rsquo;re going to actually try</p><p>things and get to some signal error or
otherwise and then you&rsquo;re going to have</p><p>to reflect and evaluate whether or not
those decisions were good ones whether</p><p>or not those actions were the right ones
you have to figure out how good your</p><p>current policy is so that you can use
that information to improve it so you</p><p>run the policy you evaluate the policy
you improve the policy and there are a</p><p>bunch of different ways of doing that
and we&rsquo;ll go into some depth about how</p><p>different algorithms go about doing that</p><p>so let&rsquo;s start with policy optimization</p><p>minor interlude in the chat last night I
surveyed people to see what they were</p><p>interested in
I asked if people were interested in</p><p>math there&rsquo;s gonna be some math so first
at a very high level zooming out ten</p><p>thousand foot view in policy
optimization we&rsquo;re going to run the</p><p>policy by collecting complete
trajectories or snippets of trajectories</p><p>based on our current stochastic policy
and we&rsquo;re going to explicitly represent</p><p>that stochastic policy with a neural
network that perhaps gives these</p><p>sufficient statistics of the action
distribution or something else that we</p><p>can use to derive that and sample from
it and then we&rsquo;re going to evaluate the</p><p>policy by figuring out the on policy
value function and advantage function</p><p>and we&rsquo;re going to evaluate those things
for all the states and actions in the</p><p>trajectories that we sampled and then
we&rsquo;re going to improve the policy by</p><p>making it more likely that we take the
actions that led to higher advantage and</p><p>making it less likely that we take the
actions that led to lower advantage less</p><p>likely that we take the bad actions how
do we do that we&rsquo;re going to have to</p><p>talk about some math now I realize
there&rsquo;s a chance that most of you maybe</p><p>weren&rsquo;t expecting that we would be doing
any kind of deep mathematical excursion</p><p>but if there&rsquo;s one thing that I want you
to take away from today aside from just</p><p>being excited about deep RL it&rsquo;s a
realization that there are some</p><p>limitations to what deep RL can
currently do and that this is not really</p><p>a hundred percent done as a technology
where you can just apply it to a problem</p><p>without really thinking about what it&rsquo;s
doing under the hood and get a good</p><p>solution it&rsquo;s not a black box technology
yet so if you want to try deep RL on a</p><p>problem and grapple with getting it to
work you do have to kind of understand</p><p>what&rsquo;s going on under the hood and that
means taking a look at some of the gory</p><p>mathematical details understanding how
they connect and forming an intuition</p><p>for how those details will shape the
failure</p><p>most of your algorithm so what we&rsquo;ll
talk about we&rsquo;re just gonna talk about</p><p>vanilla policy gradient we&rsquo;re gonna talk
about how you derive the policy gradient</p><p>and a bunch of different equivalent
expressions for it and then we&rsquo;ll get to</p><p>the pseudocode for the sort of standard
version of vanilla policy gradient which</p><p>includes maybe a few more tricks and
details than the very most basic vanilla</p><p>version apologies for the choice of
words there but all of this stuff is</p><p>critical to understanding more advanced
policy optimization algorithms like Terp</p><p>oh and PPO we won&rsquo;t be covering them in
these slides but again happy to talk</p><p>about them offline during the hackathon
so in policy grading algorithms what we</p><p>want to do is we want to find some kind
of expression for the gradient of the</p><p>policy performance with respect to the
parameters of the policy and we want to</p><p>just directly gradient to send on those
parameters so we&rsquo;re going to move the</p><p>parameters in the direction that
increases performance and is this gonna</p><p>be easy or hard well if we just try
putting the gradient onto the policy</p><p>performance we run into a problem all
the parameters are down here in the</p><p>distribution they&rsquo;re not inside here
where we would like them if we want to</p><p>get something that we can actually use
we&rsquo;ll have to do some messy work to</p><p>bring the gradient inside of an
expectation which we could then form a</p><p>sample estimate of so step one to
getting the gradient symbol somewhere</p><p>helpful we&rsquo;re going to recognize that
this expectation can be rewritten as an</p><p>integral going through all of the events
in trajectory space every possible</p><p>trajectory of the density the
probability mass or density for that</p><p>trajectory based on that policy times
the return that you would get for being</p><p>on that trajectory and now we can bring
the gradient in because the limits of</p><p>this integral don&rsquo;t have anything to do
with the parameters and then we apply</p><p>the log derivative trick so this is a
really helpful mathematical trick comes</p><p>up all over the place and deep
reinforcement learning it&rsquo;s basically</p><p>just this notion that the derivative of
log of sum</p><p>thing is one over that something times
the derivative of that something and we</p><p>rearrange it slightly but it lets us go
from the gradient with respect to theta</p><p>of P to P times gradient log P this is
great because now we have an expectation</p><p>again we have an expectation based on
trajectories sampled according to the</p><p>current policy so if we have that data
we can make a sample estimate certainly</p><p>so the very nice thing here is that what
we did after bringing the gradient</p><p>inside the integral and doing this log
derivative trick is that we now have</p><p>something which is an expectation again
because we&rsquo;re integrating through all</p><p>possible trajectories of the probability
density associated to that trajectory</p><p>times something which is a function of
that trajectory so this is an</p><p>expectation and we can form a sample
estimate of it that we can use in a</p><p>practical algorithm but we&rsquo;re not
completely finished yet because we still</p><p>have to talk about what&rsquo;s the gradient
of that log probability for a trajectory</p><p>how does that depend on the parameters
of the policy so let&rsquo;s go back to the</p><p>picture that we had in the beginning
there&rsquo;s a starting state which is drawn</p><p>from some distribution based on the
environment and then after that you pick</p><p>it the agent picks in action based on PI
theta and it has probability PI theta a</p><p>given s for time step 0 then the
environment picks the next state</p><p>according to whatever distribution it
has over next States given your most</p><p>recent action in the most recent state
by the way this is something that I lost</p><p>over earlier slightly more formalism
details that you don&rsquo;t quite need to</p><p>know but this is called the Markov
property this notion that picking the</p><p>next state only depends on the most
recent thing that happened and doesn&rsquo;t</p><p>depend on the past before it that&rsquo;s the
the Markov property and you&rsquo;ll find a</p><p>whole bunch of math if you go digging
for it but you don&rsquo;t have to for for</p><p>this at the very least so then what we
have is that the probability of the</p><p>trajectory is going to be just the
probability of that first state x the</p><p>probabilities of each transition and
action selection that happens afterwards</p><p>so we get that expression up there at
the top and now if we want to take its</p><p>gradient of its log we just
pretty straightforwardly compute first</p><p>the log of that thing turns that product
into a bunch of sums the gradient goes</p><p>through the sums and now all the terms
that are based on distributions from the</p><p>environment have no dependence on the
parameters of the policy the environment</p><p>doesn&rsquo;t care what the policy is it&rsquo;s
just going to behave in whatever way it</p><p>does so those have no dependence on the
parameters those derivatives are zero</p><p>and what we&rsquo;re left with is just
something which is as some overtime</p><p>steps of gradients of the policy and the
beautiful thing is because we control</p><p>the policy and we have explicitly
represented it as a neural network and</p><p>we can compute all of its gradients this
is a thing that we can calculate so now</p><p>we&rsquo;re at something where we can in fact
calculate a sample estimate of this</p><p>gradient of policy performance and use
that as the basis for a gradient ascent</p><p>algorithm for improving performance but
it&rsquo;s not good enough we&rsquo;re not done yet</p><p>yes the function capital e so so this
this capital e is an expectation and if</p><p>we want to form an estimate for the
expectation so we&rsquo;re not going to</p><p>compute the expectation exactly what
we&rsquo;re going to do is we&rsquo;re going to see</p><p>what happens for a bunch of different
trajectories that are sampled according</p><p>to the distribution specified in that
expectation and then we&rsquo;re just going to</p><p>average them and in the limit as we have
an infinite amount of data that sample</p><p>average becomes exactly equal to the
expectation yes</p><p>absolutely absolutely you can so it is a
bunch of derivatives of the final output</p><p>with respect to each one of the
parameters right because there are many</p><p>inputs to this function and we&rsquo;re going
to have a derivative with respect to all</p><p>of them yes I&rsquo;m sorry can you repeat the
question yes can we tie this explicitly</p><p>to reward so inside the expectation here
we have R of tau so that&rsquo;s the return</p><p>measure that we&rsquo;ve chosen whichever one
we picked either the infinite horizon</p><p>discounted sum of rewards along the
trajectory tau or just the finite</p><p>horizon undiscounted sum of rewards so
that R of tau is the sum of all the</p><p>rewards in a particular trajectory and
that&rsquo;s actually why the the variance of</p><p>this is going to be so unnecessarily
high they&rsquo;re going to be a bunch of</p><p>terms in this sample expression actually
just in that expectation which which</p><p>have expectation zero on average they&rsquo;re
zero they don&rsquo;t contribute anything but</p><p>we sample them anyway and the samples
will have noise on them and so we&rsquo;ll</p><p>just wind up getting the noise we won&rsquo;t
get much signal from them so can we</p><p>eliminate a whole bunch of terms yes we
absolutely can the intuition here is</p><p>that if I give you a reward in the past
and you want to update the action that</p><p>you just took really what you care about
for figuring out whether or not the</p><p>action that you just took was good or
bad are the consequences of that action</p><p>you don&rsquo;t care about what preceded it
that action and what preceded it are</p><p>almost completely uncorrelated there
you&rsquo;re not going to to get anything by</p><p>by updating the likelihood of that
action based on an old reward so that in</p><p>expectation is going to be zero and
knowing that we can now expand out this</p><p>return measure and we&rsquo;re going to
get this in the finite horizon case just</p><p>for simplicity but this analysis also
extends to the infinite horizon case so</p><p>we now have a sum of grad log probs of
the policy times the sum of rewards</p><p>we&rsquo;re gonna pull the sums out of this
expression so that we can just look at a</p><p>policy update at a particular time step
times a reward from a different time</p><p>step and then based on that thing that
we asserted above we&rsquo;re gonna drop all</p><p>the terms that are inconsequential all
of those are zero and so what we&rsquo;re left</p><p>with after we take away all the ones
where T greater than T Prime we&rsquo;re left</p><p>with this sum sum over the time steps
for the policy times a sum over time</p><p>steps for rewards that goes for all of
the time steps after the corresponding</p><p>policy time step and then if we bring
that back in what we&rsquo;re seeing now is</p><p>that we want to for each time step
adjust the probability of the action</p><p>from that time step in proportion to the
sum of rewards that came afterwards only</p><p>the consequences of an action will
affect its update yes so it&rsquo;s not that</p><p>you don&rsquo;t consider past actions the sum
over here in the beginning runs over all</p><p>time steps so every action is going to
get to some update it&rsquo;s just a matter of</p><p>which rewards are used in figuring out
the update for that action and it should</p><p>only be the ones that were consequences
of it yes yes</p><p>um well we do care about the future
right because here we have a sum of</p><p>rewards after a particular time step all
the rewards in the future from that time</p><p>step so so that expectation that&rsquo;s just
saying that an action that happens later</p><p>shouldn&rsquo;t be affected by a reward that
happened before it it should only only</p><p>be affected by the rewards that happen
afterwards so in the in the next slide</p><p>actually we&rsquo;ll see how this expression
that we have down here at the bottom</p><p>connects to the value functions so what
we currently have is what I&rsquo;ll call the</p><p>reward to go policy gradient because
what we&rsquo;re doing is we&rsquo;re adjusting the</p><p>probabilities of action proportionally
to the reward to go what we&rsquo;re going to</p><p>do now is go from that into an
expression that has q pi the action</p><p>value on policy for a state action pair
instead of that reward to go and this</p><p>works because you can break up the
expectation so first we&rsquo;re gonna pull</p><p>the sum over time steps out of this and
then this expectation over trajectories</p><p>this is sort of subtle and and maybe a
little math here then we can go into</p><p>detail on here but I recommend that you
go look on the spinning up website in RL</p><p>intro part 3 there&rsquo;s a link separately
to a proof about this but if we think</p><p>about the average thing that&rsquo;s going to
happen over all trajectories that&rsquo;s</p><p>going to be equivalent to the average
thing that happens over all of the cases</p><p>of something with the first T time steps
of the trajectory we&rsquo;re inside of the</p><p>expectation we&rsquo;ve moved all the stuff
that happens in the future</p><p>and we were able to move it inside past
this one because this only depends on</p><p>time step T this doesn&rsquo;t depend on stuff
after T so only this</p><p>it&rsquo;s gonna be affected by averaging over
the future and then it turns out that</p><p>that expression the average sum of
rewards that you get starting from a</p><p>time step assuming that the state and
action for that time step were fixed</p><p>that&rsquo;s exactly equal to the action value
that&rsquo;s exactly saying how good is it to</p><p>be in a particular state take a
particular action and then forever after</p><p>act according to a particular policy and
now we have this expression for the</p><p>policy gradient at the bottom we&rsquo;re most
of the way through the math okay but</p><p>what is a baseline a baseline is a
really important thing because it&rsquo;s</p><p>another tool in our Arsenal for taking a
policy gradient expression and turning</p><p>it into something which is lower
variance more likely to be useful for</p><p>producing a good update to the policy
and it&rsquo;s also the namesake for opening</p><p>eye baselines
well let&rsquo;s save one of them it&rsquo;s a</p><p>couple of things but we have a
expression here at the top which I claim</p><p>is basically true which is that the
gradient policy gradient is the thing</p><p>that we had before but instead of Q we
subtract out some function of state some</p><p>function b of st and i claim that in
expectation it works out exactly the</p><p>same and so there&rsquo;s a short proof here
for that which is that if we look at the</p><p>expectation for that part of it what
happens if you take the expected</p><p>gradient of the log probability of an
action in a state times some function b</p><p>of st the b doesn&rsquo;t have anything to do
with the action so it&rsquo;s a constant with</p><p>respect to this expectation so we pull
it out and then what we&rsquo;re left with is</p><p>an expectation over actions which will
rewrite and now we have it in</p><p>probability times grad log prop we&rsquo;re
going to reverse the log derivative</p><p>trick from earlier so this is now an
integral over actions of the gradient of</p><p>the probability of that action and we
can pull out the gradients</p><p>we&rsquo;re just sort of reversing the
procedure from earlier this thing this</p><p>integral over all possible actions of
the probabilities of those actions</p><p>that&rsquo;s just going to sum up to one
that&rsquo;s just saying probability</p><p>distribution is normalized all of the
chances together have to come out to</p><p>equaling 100% of sum them up and the
derivative of a constant since that&rsquo;s a</p><p>constant is nothing constant has no rate
of change</p><p>so we get zero so all of the terms of
grad log prob times the baseline in</p><p>expectation or zero so we&rsquo;re free to add
this baseline without changing what the</p><p>policy gradient is in expectation but we
can pick it in ways that are fruitful</p><p>and make the estimate better so the
typical thing to do is to pick the</p><p>baseline to be the value function and
this leads us to kind of our our final</p><p>sort of ultimate form of the policy
gradient the form with advantage</p><p>functions and why is this good why is
this good the advantage function says</p><p>how much better in action is than
average why would you prefer that over</p><p>just how good the action is well let&rsquo;s
say you have two actions one gets you a</p><p>hundred dollars one gets you one hundred
and one dollars you only sample the one</p><p>that gets you one hundred now when
you&rsquo;re trying to update your policy you</p><p>can feel really great about that oh man
100 is a big number I feel great</p><p>I&rsquo;m gonna double down on that action
you&rsquo;re acting sub-optimally if you had</p><p>been picking 5050 on average you would
have gotten a hundred dollars and fifty</p><p>cents and you would have realized that
the advantage of taking the action that</p><p>you picked one hundred dollars and fifty
minus a hundred dollars and fifty cents</p><p>you lost fifty cents should pick the
other action so you prefer to use</p><p>advantages to figure out which actions
to increase the likelihood of as opposed</p><p>to just Q values all right summing it up
we have these four different forms of</p><p>the policy gradient they&rsquo;re all tightly
connected we care about the last one but</p><p>to get to the last one we had to go
through the pain but now that we&rsquo;ve all</p><p>gone through that pain together you&rsquo;re
stronger you can go and you can</p><p>implement this and it&rsquo;ll work and you&rsquo;ll
know why it works and you&rsquo;ll feel good</p><p>about that and if it breaks you can fix
it</p><p>all right so then just to sum it up this
key concept we want to push up the</p><p>probabilities of good actions push down
the probabilities of bad ones and also</p><p>importantly that expectation requires
trajectories sampled from the current</p><p>policy so this is the concept of being
on policy and reinforcement learning</p><p>that if you want to update your policy
you have to use data from that policy</p><p>you can&rsquo;t use data from some other
policy unless you appropriately</p><p>reweighed it but relating data is
complicated and really tricky so it&rsquo;s</p><p>sort of preferred to not do it unless
you are trying to build something new</p><p>and cool and super sample efficient and
you&rsquo;re willing to spend a lot of time</p><p>and effort doing research on making sure
that it actually works but ok so the</p><p>policy gradient expression gives us the
policy improvement step coming back</p><p>coming back a bit oh yeah sure the
question was how do we know what the</p><p>average reward would have been so that
we could figure out how to make the</p><p>advantage function in the first place do
we compute it as we go and and actually</p><p>that&rsquo;s exactly what the next slide is
about which is how do we do that</p><p>business of policy evaluation how do we
find an estimate of the advantage</p><p>function which is actually good and
reasonable if we just have a bunch of</p><p>data where do we get the value function
that we might use to subtract out as a</p><p>baseline and the idea here is that we&rsquo;re
going to learn it from data and</p><p>typically it&rsquo;s going to be by regression
so this will be a subroutine that you&rsquo;ll</p><p>find in most policy optimization
algorithms where you&rsquo;re going to have a</p><p>value function approximator another
neural network and you&rsquo;re going to at</p><p>each epoch of the policy optimization
algorithm update the value network to</p><p>try to match the empirical returns that
you saw so for a particular state the</p><p>value should be more or less the sum of
discounted rewards that you saw off to</p><p>then
and then when you have the value</p><p>function approximator you can use that
to estimate advantages and we&rsquo;ll talk a</p><p>bit about estimating advantages from
value function approximate us on the</p><p>next slide but first you may have
noticed that I pulled a fast one on you</p><p>which is that we went from in all the
preceding slides dealing with the finite</p><p>horizon undiscounted case and then here
in our optimization problem for learning</p><p>the value function I&rsquo;ve dropped in
discount factors why is that the answer</p><p>is because everyone does it this is
where there&rsquo;s not a particularly good</p><p>reason in my opinion that this happens
but pretty much every policy</p><p>optimization algorithm that I&rsquo;m aware of
every every single implementation uses</p><p>discounted value functions and advantage
functions but then treats the policy</p><p>optimization part as undiscounted it
creates some bias it seems to work</p><p>shrug it&rsquo;s perfectly reasonable to do
that so it sometimes seems to be helpful</p><p>to set the discount factor to something
a little smaller than one so keeping it</p><p>completely undiscounted would be gamma
equals 1 for whatever reason with some</p><p>optimization problems there&rsquo;s some some
RL problems it&rsquo;s a little bit harder if</p><p>you pick gamma equals 1 than gamma 0.95
and i can&rsquo;t say that there&rsquo;s a</p><p>particularly good reason for this I
would speculate that like in the</p><p>beginning of training if you pick a very
high discount factor those empirical</p><p>returns will be very noisy and if you
choose a discount factor less than 1</p><p>what you&rsquo;re going to do is you&rsquo;re going
to attenuate some of the noise you&rsquo;ll</p><p>bias that sum of rewards so that
whatever happens soonest matters most</p><p>and if you happen to see a few positive
rewards in a row then you&rsquo;ll latch on to</p><p>that whereas maybe because of noise if
you had really paid attention to</p><p>everything out to infinity you&rsquo;d have
just gotten a bunch of positives and</p><p>negatives and positives and negative and
they would have cancelled out uh I think</p><p>it&rsquo;s it&rsquo;s ok to think about it like that
yeah</p><p>yes yes that after a certain point the
trajectory just ends you get it to time</p><p>step T and then it&rsquo;s over
that&rsquo;s finite horizon infinite horizon</p><p>you go out to infinity alright so then
how do we calculate the advantage</p><p>function given data from trajectories
and a value function approximator so a</p><p>thing that I want to introduce here is
this notion of n step advantage</p><p>estimates so what you&rsquo;re going to do is
you&rsquo;re going to have a thing over on the</p><p>left side that approximates Q pi and a
thing over on the right side that</p><p>approximates V PI so this thing for Q pi
remember that that&rsquo;s supposed to be an</p><p>estimate for how well you&rsquo;ll ever do if
you start in a state take an action and</p><p>then act according to the policy forever
after you can just use the empirical</p><p>return the reward to go from that state
as a sample estimate of the expected</p><p>value which is the Q value but in an N
step advantage estimate what we&rsquo;re going</p><p>to do is we&rsquo;re not going to go all the
way out to the end of the trajectory in</p><p>that sample estimate for Q we&rsquo;re going
to go n steps in and then use the value</p><p>function approximator to assume what&rsquo;s
going to happen for the rest of it and</p><p>this corresponds to a decision about how
much bias or variance we find acceptable</p><p>in this advantage estimator so if you
pick n equals 0 then your advantage</p><p>estimator in that case would be just the
reward plus gamma times the value</p><p>function approximator for the next time
step minus the value function</p><p>approximator for the current time step
and that&rsquo;s gonna be very high bias</p><p>because whatever is wrong with your
value function is not going to be wrong</p><p>with your advantage function but it&rsquo;ll
be really low variance because the only</p><p>thing that&rsquo;s going to have variance to
it is the reward and the stochasticity</p><p>in the next state transition but if on
the other hand you pick n equals</p><p>infinity so for the q approximator you
just take the exact sum of rewards that</p><p>you got in the real trajectory and then
at the end you subtract out the value</p><p>function at st you&rsquo;re going to accept
all of the variance that&rsquo;s in the</p><p>environment
but the nice thing is you don&rsquo;t have</p><p>bias in forming your policy gradient
estimator with this because in</p><p>expectation the Q part is going to be
exactly Q in expectation and the B part</p><p>recall that that was a baseline that we
added with a guarantee of no bias in the</p><p>policy gradient so on expectation that
part falls out and it&rsquo;s fine</p><p>so the bias-variance tradeoff is
typically mitigated through what we call</p><p>generalized advantage estimation so this
is a way of interpolating between all of</p><p>those different possible choices of n
step advantage estimate where we use a</p><p>factor called lambda so this is sort of
like another discount factor as the</p><p>interpolation variable and it&rsquo;s a hyper
parameter and you choose it in each</p><p>implementation that you make and it&rsquo;s
generally good to set it somewhere</p><p>between like 0.9 and 97 usually it&rsquo;s a
set it and forget it in my experience I</p><p>can I can&rsquo;t think of very many cases
when I saw a substantial difference in</p><p>algorithm performance from adjusting it
beyond that kind of narrow range if you</p><p>set it equal to one then you&rsquo;ll get
exactly the case of the N equals</p><p>infinity and if you set it to zero then
you&rsquo;ll get exactly the N equals zero</p><p>case so it&rsquo;s good to kind of leave it in
the range where it&rsquo;s putting a little</p><p>bit more weight on the real empirical
returns than the biased value estimator</p><p>but not all the way to the extreme okay
at long last I give you the pseudocode</p><p>for the full vanilla policy gradient
algorithm that incorporates everything</p><p>that we&rsquo;ve talked about so far what
we&rsquo;re going to do is collect a set of</p><p>trajectories by running the current
policy in the environment and then we&rsquo;ll</p><p>compute the rewards to go so that we can
use them as targets for the value</p><p>function approximator will compute the
advantage function estimates with any</p><p>method of advantage estimation but
typically generalized advantage</p><p>estimation and then we&rsquo;re gonna use
those to estimate the policy gradient</p><p>with that we take a step of gradient
this gradient descent we might use an</p><p>adaptive optimizer like Adam to
accelerate the rate at which we learn</p><p>and then we&rsquo;re going to do the
supervised learning problem of trying to</p><p>get the value function approximator to
match the empirical returns and that&rsquo;s</p><p>how we learn our value function and then
we loop that&rsquo;s vanilla policy gradient</p><p>yeah absolutely
so yes usually you will pick networks</p><p>have the same size for policy and value
function in cases where the environment</p><p>is partially observed you may want to
have a single core recurrent neural</p><p>network that&rsquo;s going to be able to
remember past information and then give</p><p>that corner all Network separate outputs
for policy and value function and then</p><p>you&rsquo;ll train that jointly and it gets a
little bit complicated because I can&rsquo;t</p><p>say that there&rsquo;s any good work in RL
theory that I&rsquo;m aware of that reasons</p><p>about how it alters performance for the
final policy to be simultaneously</p><p>optimizing with respect to both
objectives on the same model but that&rsquo;s</p><p>what you would do in that situation so
so yes typically they&rsquo;ll be about the</p><p>same size unless they&rsquo;re actually
sharing parameters and then they&rsquo;re sort</p><p>of the same model yes</p><p>does the choice of initial policy affect
convergence wonderful question and sadly</p><p>in a lot of cases yeah so this is part
of what goes into my saying that deep</p><p>reinforcement learning is not a
technology that&rsquo;s ready to be used as a</p><p>black box yet so when we do experiments
in deep reinforcement learning we</p><p>typically run the same exact experiment
with different choices of new of seed</p><p>for the random number generators and
what we find is that the seed which in</p><p>the beginning of the algorithm only
changes the initialization of the</p><p>policies and value functions happens to
matter quite significantly some seeds</p><p>learn some seeds don&rsquo;t some seeds learn
much slowly much more slowly than others</p><p>and there&rsquo;s no particularly good reason
for it</p><p>we are generally quite heartened when we
find an algorithm that appears to be</p><p>robust to initial conditions and where
the</p><p>average of the learning curves is quite
narrow we think that&rsquo;s great and it</p><p>doesn&rsquo;t quite happen as often as we
would hope all right do we have any</p><p>other questions about policy gradients
so in the bottom right hand corner there</p><p>that says 47 out of 63 I may have
slightly miscalibrated</p><p>how long parts one and Part two were
relative to the initial time slots of 45</p><p>minutes and 1 hour respectively this is
by far the longer one but since we&rsquo;ve</p><p>been at it for an hour I think this is a
good point to take a 15-minute break and</p><p>we&rsquo;ll pick back up to discuss q-learning
after coffee thank you so much</p><p>we will</p><p>we will be resuming with Joshua Humes
introduction to RL in two minutes</p><p>hello</p><p>hi everyone we&rsquo;re about to get started
for the second part of intro to RL and</p><p>just as a heads up
I prepared entirely too many slides for</p><p>the hour and 45 minutes that I was
scheduled to speak please bear with that</p><p>because you know this is the first time
we&rsquo;re doing this and so I&rsquo;m still</p><p>getting calibrated on what we can get
through in that amount of time but</p><p>everything that I don&rsquo;t cover by 11 a.m.
when I hand over the mic to the next</p><p>speaker I&rsquo;m more than happy to share
with you later today during the</p><p>hackathon so in particular the material
that I expect that we won&rsquo;t quite get to</p><p>will involve an overview of what&rsquo;s been
accomplished recently in deep</p><p>reinforcement learning and where the
challenges and limitations are and what</p><p>the research horizons look like on those
limitations but before we do any of that</p><p>let&rsquo;s continue our discussion from
earlier and talk about the next major</p><p>family for algorithms for deep RL for
control which is to say cue learning so</p><p>there are a lot of algorithms that fall
under this umbrella deep Q learning was</p><p>one of the first algorithms that really
made deep reinforcement learning viable</p><p>and popular speaking from personal
experience I just started my graduate</p><p>student career in 2014 when I heard
about the playing Atari with deep</p><p>reinforcement learning paper I was just
becoming aware of topics in AI and AI</p><p>research and that completely and totally
blew my mind it was the most exciting</p><p>thing that I had ever seen that a
computer could just figure out from</p><p>looking at what was happening on a
screen how to behave how to play a game</p><p>how to do something that I thought
required some human spark of</p><p>understanding and capability for joy and
the in the computer had it it was</p><p>beautiful and amazing and it made me
want to study this and participate in</p><p>taking this technology all the way from
where it was at that point to what it</p><p>could be in the future
anyway q-learning</p><p>so back to this RL loop that we have run
policy evaluate policy improve policy in</p><p>q-learning you run the policy by taking
a step in the environment either</p><p>randomly so there&rsquo;s going to be some
stochasticity in what you do or you&rsquo;re</p><p>going to act in a way which is called
greedy with respect to your current Q</p><p>function approximator so remember what
you&rsquo;re trying to learn is Q star the</p><p>optimal action value function and if you
happen to have Q star then whatever</p><p>action is the maximum or maximizes q
theta in a particular state is the best</p><p>action to take um but when you don&rsquo;t in
fact have q theta equals Q star then the</p><p>the maximizing action probably isn&rsquo;t
great so exploring a little bit by</p><p>acting randomly is going to help you and
then once you&rsquo;ve taken that step in the</p><p>environment so you send an action to it
and you get back a reward in the next</p><p>state you store that transition state
action reward next state in a replay</p><p>buffer you save it for later because
you&rsquo;re going to use it for learning how</p><p>to evaluate the policy which is to say
updating q theta to try to have it fit</p><p>that bellman equation and once you have
that the policy improvement step is just</p><p>looking into q theta and saying what&rsquo;s
the action that maximizes this policy</p><p>improvement is basically implicit in Q
learning and we&rsquo;re gonna structure our</p><p>discussion about Q learning around the
original deep Q networks algorithm but</p><p>pretty much everything in this
discussion is quite general for Q</p><p>learning methods because they all kind
of share this common DNA of you take a</p><p>step in the environment you take some
gradient descent steps on your Q</p><p>function to minimize a mean squared
bellman error and you use the techniques</p><p>that will describe in a minute
experience replay on target networks to</p><p>stabilize the learning procedure so Q
learning updates by bootstrapping so</p><p>what is what is that it&rsquo;s this notion of
how</p><p>are we actually going to fit q2 that
bellman equation so we talked about</p><p>minimizing mean squared bellman error
and it&rsquo;s a useful picture to start with</p><p>and so I&rsquo;m gonna keep using that
terminology although in a few slides I&rsquo;m</p><p>going to tell you something completely
different and ask you to ignore this and</p><p>pretend you never heard it but this is
where all the papers start and this is</p><p>where all the tutorials starts so it&rsquo;s
good to familiarize you what you&rsquo;re</p><p>going to do to update Q is set up this
loss function where you&rsquo;re going to</p><p>average or sum over data from your
replay buffer D and you&rsquo;re gonna have</p><p>these transitions state action next
state reward and you&rsquo;re going to regress</p><p>Q theta against targets Y where those
Y&rsquo;s are obtained basically from that</p><p>bellman back up from that bellman
equation as the reward plus the Q value</p><p>in the next time step and this is based
on the bellman equation for the optimal</p><p>action value function so it&rsquo;s gonna have
that Max over next actions which is to</p><p>say that it&rsquo;s going to assume that you
know if Q theta was optimal if it was Q</p><p>star then whichever action maximized it
in that state would be the best one to</p><p>take and that would be the best value
there so interestingly you don&rsquo;t</p><p>propagate gradients through why even
though why has the dependence on the</p><p>parameters of Q theta and the reasons
for this are kind of mathy so we&rsquo;ll get</p><p>to them in a bit okay
getting this to work so there are two</p><p>main techniques that I mentioned there&rsquo;s
experienced replaying there&rsquo;s target</p><p>networks
the idea behind experience replay is</p><p>just that you want to use a really wide
distribution of data for training your Q</p><p>function you don&rsquo;t want to fit it really
well to a very narrow region of</p><p>transition space because if you do it&rsquo;s
not gonna be good anywhere else and if</p><p>it&rsquo;s not good anywhere else you&rsquo;re not
going to be able to bootstrap it to the</p><p>correct values even in the places where
you&rsquo;ve been trying to fit it</p><p>you&rsquo;ll get nothing which is actually
useful for control so experience replay</p><p>helps you broaden that data distribution
fit q well everywhere gets something</p><p>which is good for control target
networks</p><p>so bootstrapping with function
approximator is super super super</p><p>unstable that thing that we said on the
previous slide where the Y&rsquo;s depend</p><p>exactly on the current Thetas actually
throw that out can&rsquo;t do that that won&rsquo;t</p><p>work
if you try to do it what&rsquo;s gonna happen</p><p>is typically the keys will explode
they&rsquo;ll go to something really large or</p><p>really negative and that&rsquo;ll happen
really fast you won&rsquo;t be able to control</p><p>it even with reasonably well tuned
learning rates you probably won&rsquo;t be</p><p>able to stop it so instead what we&rsquo;re
gonna do is we&rsquo;re gonna have target</p><p>network Q theta Targ and we&rsquo;re gonna
make sure that that network tracks</p><p>reasonably closely to Q theta but
there&rsquo;s going to be a lag so that it</p><p>updates more slowly so that if you make
an update to Q theta which pushes a Q</p><p>value too high or a little too low then
that doesn&rsquo;t immediately propagate into</p><p>Q theta Targ and therefore does not
propagate into the bootstrap so this is</p><p>this wide thing we&rsquo;re gonna call this
the bootstrap and then this tamps down</p><p>on instability grants it why if Q
learning is so horrific ly unstable</p><p>would we want to do it like this in the
first place why wouldn&rsquo;t we just</p><p>differentiate through with respect to
that bootstrap and the answer is it if</p><p>you differentiate all the way through it
tends to not work that well and the</p><p>reason that this thing does the reason
that it works well if you do this kind</p><p>of bootstrapping approach as long as you
take some appropriate precautions has</p><p>something to do with the theory
underlying Q learning and we&rsquo;ll talk</p><p>about that in a few slides but not quite
yet you&rsquo;re spared for now so also</p><p>another note in DQ networks the
particular algorithm that we&rsquo;re talking</p><p>about right now
action space matters a lot so what we</p><p>did in describing that bootstrap we had
a maximization over actions of the q</p><p>function if you have a q function that
accepts as input a continuous state and</p><p>a continuous action and feeds that into
a deep neural network trying to figure</p><p>out the action that maximizes the Q
function output is really hard that</p><p>would be a non-trivial optimization
problem an expensive subroutine so if we</p><p>want to be able to
get that max over actions that&rsquo;s a case</p><p>where we won&rsquo;t really be able to do it
so dq1 will apply specifically to the</p><p>discrete action case where we&rsquo;re able to
use a network architecture that instead</p><p>of taking a continuous action as an
input at the bottom of the network emits</p><p>action values for each possible output
for each possible action at the end of</p><p>the network so a single observation goes
in and then K action values come out</p><p>where K is the number of actions one for
each action and then because there&rsquo;s</p><p>just a finite number of them it&rsquo;s very
easy to figure out which action maximize</p><p>the Q value we can compare all of them
directly so now but we can talk about</p><p>the pseudocode for deep Q learning this
is relatively straightforward based on</p><p>the stuff that we just described there&rsquo;s
one thing which is a little more</p><p>specific than what I mentioned which is
this business of Epsilon greedy</p><p>exploration so I mentioned before that
you&rsquo;re going to explore by sometimes</p><p>taking a completely random action and
sometimes taking the action which is</p><p>greedy which maximizes your current Q
function approximator so epsilon greedy</p><p>is a strategy for doing that where with
probability epsilon where epsilon is</p><p>going to be something small you&rsquo;ll pick
a completely random action so uniform</p><p>random over the K different choices and
with probability 1 minus Epsilon most of</p><p>the time you&rsquo;ll pick the action that&rsquo;s
greedy with respect to your current Q</p><p>function so that&rsquo;s the run policy step
and then after you store that transition</p><p>into the replay buffer and anneal
Epsilon because over time you want to</p><p>explore less and exploit more you want
to rely on the policy as it gets better</p><p>after doing that you&rsquo;re now going to
evaluate the policy by learning Q star</p><p>from the data by improving q theta to be
a better reflector of Q star so that&rsquo;s</p><p>exactly the step of gradient descent
that we described which is that you</p><p>sample some transitions from your replay
buffer from your from your experience</p><p>replay memory and you compute the
bootstraps for those transitions and</p><p>there&rsquo;s a special case for if a
transition ended in a terminal state</p><p>which
that we don&rsquo;t give it a value after that</p><p>particular time step and then we use
those Y values in our bootstraps Q value</p><p>regression update the parameters and
then every once in a while with some</p><p>frequency will copy over the parameters
of the main q network onto the target</p><p>network so that&rsquo;s the target network
lagging the q network ensuring stability</p><p>and that&rsquo;s deep Q learning in a nutshell
this algorithm kicked off everything I</p><p>mean a whole bunch of stuff that
preceded it you can&rsquo;t really point to</p><p>any one moment in the history of a field
that you know had no precedent before</p><p>this there was neural fitted Q before
that there was Q learning with linear</p><p>function approximation and there were
all kinds of algorithms for trying to</p><p>get things to work with nonlinear
function approximation like deep neural</p><p>networks but but but this was the one
that got a lot of people really really</p><p>excited so anyhow caveat emptor buyer
beware this can break this will not work</p><p>on every problem out of the box you&rsquo;ll
try it in some places and it just won&rsquo;t</p><p>work you&rsquo;ll fiddle with hyper parameters
and it still won&rsquo;t work you&rsquo;ll try some</p><p>tricks to stabilize it because there are
pretty much infinity tricks to make deep</p><p>Q learning better at this point and some
of the time that still won&rsquo;t work so</p><p>this picture here is from a recent paper
which I really love and which I strongly</p><p>recommend that you take a look at if you
get interested in seeing some analysis</p><p>of failure modes for algorithms in deep
RL it&rsquo;s called deep reinforcement</p><p>learning and the deadly triad the deadly
triad is a set of traits that deep</p><p>reinforcement learning algorithms might
have which are known to occasionally</p><p>cause divergence and to create
substantial obstacles to theoreticians</p><p>who would like to come up with
algorithms that have provable</p><p>convergence guarantees so the deadly
triad consists of function approximation</p><p>off policy learning and bootstrapping
which are exactly the three things the</p><p>deep Q learning relies on we have
function approximation in the form of</p><p>neural networks we have auth policy
learning in the form of</p><p>spirits replay and we have bootstrapping
in the form of using the target network</p><p>with a one-step backup as the regression
target for q and so deep Q learning</p><p>works a whole lot of the time and then
some of the time it just doesn&rsquo;t so in</p><p>this set of experiments what the
researchers did was they examined deep Q</p><p>learning and a few variants of it a
bleeding on whether they would include a</p><p>target network so here this Q does not
have a target network the regression</p><p>target that it uses is exactly based on
Q theta naught Q theta tark and tried it</p><p>with a target network and then tried a
couple of other tricks that relate to</p><p>how you use the target network to
possibly either estimate the value in</p><p>the bootstrap or select the action in
the bootstrap and those are tricks that</p><p>are known to potentially help they
looked at at all these different cases</p><p>for many different Atari games as the
experimental test bed and they clipped</p><p>the rewards in the environments into a
certain range so that they knew exactly</p><p>mathematically what the ceiling for
possible real Q value would be they</p><p>chose it to be a hundred and they looked
and saw over all the experiments that</p><p>they ran how often did the maximum
absolute learned Q value in an</p><p>experiment exceed the threshold which
they knew was the real true maximum</p><p>possible Q value and the answer was a
lot so this shows that Q learning</p><p>without target networks is very unstable
in that a lot of the time you will get</p><p>this this divergence phenomenon and even
as you include tricks that make it</p><p>progressively more stable you&rsquo;ll still
expect to see divergence every now and</p><p>then so we&rsquo;re gonna dive into a little
bit of math now to kind of get maybe</p><p>some intuition for why this is the case
and what deep Q learning algorithms are</p><p>really trying to do and how that
translates into the algorithm or doesn&rsquo;t</p><p>so we&rsquo;re going to start by taking the
operator view of the bellman equation so</p><p>the optimal bellman operator t&rsquo;east
is a map from cue functions on to other</p><p>cue functions and the value of T star
for a particular state action pair is</p><p>given by the the cue right by the
bellman equation that we saw before the</p><p>optimal cue function is the fixed point
of T star so Q star equals T star Q star</p><p>that&rsquo;s great and T star has this special
thing about it which is that it&rsquo;s a</p><p>contraction map on the space of Q
functions contraction maps have some</p><p>very special properties that we&rsquo;re gonna
talk about now yay</p><p>so the main thing about a contraction
map is this idea that if you have two</p><p>points and you apply the contraction map
to both of them they&rsquo;ll basically be</p><p>closer with respect to some distance
function after you&rsquo;ve applied that map</p><p>to both of them than they were before
so expressed mathematically we have some</p><p>some norm some distance of the norm of a
thing minus the other thing and the norm</p><p>of f of X minus f of Y is going to be
less than or equal to some constant</p><p>factor times the norm of the difference
between x and y that distance between x</p><p>and y and when that beta is less than
one then we have a contraction that&rsquo;s</p><p>saying it&rsquo;s getting closer together it&rsquo;s
shrinking why do we care about</p><p>contractions because they have unique
fixed points and you can get to them by</p><p>just repeatedly applying the operator to
any initial point this is something</p><p>called the binocs 20 room if you&rsquo;re
interested in going on Wikipedia and</p><p>finding something which is going to be
more precise than however I&rsquo;ve typed</p><p>this up but in a nutshell to show you
that they have unique let&rsquo;s forget about</p><p>uniqueness for a moment but at the very
least that repeatedly applying this</p><p>operator will get you to a fixed point
if we look at a sequence of points X and</p><p>we have a contraction map F with modulus
beta and each point in the sequence is</p><p>just yet generated by F of the previous
point and we look at the distance</p><p>between successive iterates what we see
is that it&rsquo;s shrinking as a function of</p><p>the iteration number so in the limit as
the iteration number goes to infinity</p><p>that distance will shrink to zero it
will converge repeatedly applying it</p><p>will get you to the fixed point t star
is a contraction on Q functions so if</p><p>you could represent the entirety of the
Q function that is to say the Q values</p><p>for every state action pair in the
entirety of the environment which for</p><p>all the environments that we care about
in deep reinforcement learning you</p><p>cannot easily do you can only do this
with function approximation which is to</p><p>say you&rsquo;re going to generalize whatever
you choose for the value in one state</p><p>action pair will have some influence on
another you can&rsquo;t completely separate</p><p>them when you do function approximation
but putting that aside so we could</p><p>represent all the action values for
every state action pair and we applied T</p><p>star the operator to that function we
would get a new function Q which is</p><p>closer to optimal than the one that went
in and if we applied it over and over</p><p>and over again we would eventually get
to Q star the fixed point of T star this</p><p>is value iteration
it&rsquo;s a classic algorithm and</p><p>reinforcement learning so before
function approximation before deep when</p><p>you had environments where there were a
discrete number of states and a discrete</p><p>number of actions and you could
represent the Q values in a table of</p><p>elements one for each state action pair
you could compute this exactly and use</p><p>this as a way to get to Q star now when
you live in the problems that we do when</p><p>you&rsquo;re trying to solve high dimensional
complex video games high dimensional</p><p>complex strategy games you can&rsquo;t use the
table yet use a function approximator</p><p>and now your problem is that you can&rsquo;t
compute all of T star Qi and even if you</p><p>could you probably couldn&rsquo;t find a
choice of parameters that would allow</p><p>you to exactly represent it so if you
want to do this kind of value iteration</p><p>you have to do it approximately and this
is roughly what Q learning algorithms</p><p>with function approximation try to do
which is that they push the parameters</p><p>of the network in the direction such
that you move Q theta towards T</p><p>star q theta and sometimes this works
and sometimes it doesn&rsquo;t because when</p><p>you go to this function approximation
setting this operation is not</p><p>necessarily going to be a contraction on
the space of Q functions you might have</p><p>lost that property if you did expect
divergence in fact I expect things to</p><p>blow up horribly if you preserved it or
if you&rsquo;ve done enough tricks to</p><p>stabilize it things will work pretty
well in my experience Q learning</p><p>algorithms and their variants tend to be
extremely sample efficient when they</p><p>work which is quite desirable and it&rsquo;s
very nice if they can recycle off policy</p><p>data because on policy methods sadly
have to throw away tons of it but last</p><p>point on Q learning what you normally
see in deep learning algorithms and deep</p><p>RL algorithms is that paradigm of
there&rsquo;s an objective function and you</p><p>optimize it and you find the model that
optimizes the objective in Q learning</p><p>don&rsquo;t be misled into believing however
many times you see it that the mean</p><p>squared bellman error it&rsquo;s really the
thing that you&rsquo;re optimizing you change</p><p>that function every time you change the
target the thing that you&rsquo;re really</p><p>doing is this sort of approximate value
iteration you&rsquo;re trying to apply an</p><p>approximate operator which is going to
get you to something better you&rsquo;re not</p><p>trying to minimize a loss that&rsquo;s not to
say that there aren&rsquo;t variants of these</p><p>kinds of algorithms that do involve
well-defined loss functions there&rsquo;s a</p><p>whole family of algorithms called
gradient temporal difference methods</p><p>which if you are theoretically inclined
and willing to go down a deep deep deep</p><p>rabbit hole I recommend you check out
talk to me if you want references also</p><p>in the spinning up key papers doc I
believe there&rsquo;s a book in the bonus</p><p>section for classic RL papers and review
papers choppa Sabbath Baris book on RL</p><p>algorithms from 2010 which recaps a lot
of this really great old stuff including</p><p>gradient temporal difference algorithms
so I recommend you check that out if</p><p>you&rsquo;re interested yes I&rsquo;m actually
working on some research on that right</p><p>now
like I</p><p>talk to me offline yes yes yes so so
this thing yes it&rsquo;s called a temporal</p><p>difference error because it is the
difference in the Q value based on the</p><p>next time step versus the current time
step yeah yes absolutely what is the</p><p>difference between off policy and on
policy the on policy algorithms have</p><p>updates which are based on the expected
values of things where the distribution</p><p>and that expectation depends on the
current policy so if you want to form a</p><p>sample estimate of the thing in the
update equation then you first have to</p><p>run the current policy collect
interactions with the environment on the</p><p>current policy and use those samples for
forming that sample estimate that&rsquo;s on</p><p>policy because all the data that you use
has to be generated by the policy that</p><p>you&rsquo;re using at the time in off policy
methods like q-learning what you do when</p><p>you make an update is you use experience
which might have been generated by older</p><p>policies not the current one so the
current policy you could think of as</p><p>being implicitly expressed in the in the
Q function approximator is current value</p><p>but many steps ago it was different and
you got whatever data you got from</p><p>interacting with the environment you put
that in your replay buffer and then many</p><p>steps later you still sample those
states and actions from that replay</p><p>buffer to help you form your your new
update to the current q function so when</p><p>the data was generated by a different
policy that&rsquo;s off policy yes</p><p>in what sort of gaming situation would
we maybe use deep q-learning or like</p><p>what&rsquo;s a use case for it so there&rsquo;s a
fabulous use case actually Facebook</p><p>recently released a paper on their
machine learning and RL learning there</p><p>RL platform called horizon which they
used to train with deep Q learning</p><p>neural networks for making decisions
about when to send you push</p><p>notifications so actually DQ n is in
your phones right now okay then let&rsquo;s</p><p>proceed to the next part which is model
based stuff so I&rsquo;m going to be pretty</p><p>brief about model based stuff there&rsquo;s a
very wide variety of different model</p><p>based algorithms and we&rsquo;re not going to
drill down into them the way that we</p><p>drill down into policy learning and Q
learning but we will give a relatively</p><p>brief overview of some of the more
salient points and a few algorithms that</p><p>I think are particularly interesting so
back to the loop run policy evaluate</p><p>policy improve policy where do models
fit in so recall that a model of the</p><p>environment lets you predict what&rsquo;s
gonna happen next you can use that for</p><p>pretty much any of these while you&rsquo;re
running your policy before you take an</p><p>action you can stop and imagine what&rsquo;s
gonna happen if you try many different</p><p>things you can create partial rollouts
that you can use to evaluate your</p><p>different choices and then you might
pick something different than you would</p><p>have otherwise so that&rsquo;s maybe where it
can appear in running ball and running</p><p>the policy in evaluating the policy you
can use that same kind of approach of</p><p>just simulating look-ahead data to help
you get a maybe a more stable backup for</p><p>your q function or just use some kind of
Monte Carlo tree search style algorithm</p><p>where you&rsquo;re going to propagate Q values
back and figure out like an average case</p><p>Q value and then for improving the
policy you can regress your policy</p><p>network if you have explicitly
represented one towards whatever the</p><p>outputs were from that look-ahead
planning process so if you have a model</p><p>it&rsquo;s very powerful you can use the
a lot of different ways you can embed it</p><p>pretty deeply in into RL the problem is
that models are very hard to learn and</p><p>you usually don&rsquo;t have them so let&rsquo;s say
you have just made a wonderful brand new</p><p>complex physical robot unless you have a
lot of hours to spare and control theory</p><p>expertise you probably do not know how
to fully characterize that and have a</p><p>simulator model which is going to be
accurate in any reasonable way certainly</p><p>not accurate enough for training it in
simulation and then directly applying</p><p>that simulation trained policy into the
real world you may want to try learning</p><p>a policy from data but this can be quite
tricky although there are some really</p><p>exceptional success cases but because
yes uh yes you could make that argument</p><p>so I let&rsquo;s say hardness to learn is not
a fuck oh I suppose sorry the question</p><p>was can you make the same argument for
value functions and I would say that</p><p>hardness to learn in this case should be
interpreted more as has the research</p><p>community figured out really robust
reliable standard methods for doing it</p><p>yet but not necessarily whether there&rsquo;s
some intrinsic quality of hardness</p><p>finding the correct model is a
supervised learning problem if you have</p><p>enough data part of the problem in RL is
that you usually don&rsquo;t have enough data</p><p>and you would have to get it by
interacting with the environment and</p><p>there may be areas in the environment
very critical to decision making which</p><p>you&rsquo;ve just never observed yet so
imagine that you are in a giant maze and</p><p>you can try to learn a model of the maze
as you go but until you&rsquo;ve seen the exit</p><p>your model does not going to be very
helpful for you and navigating except to</p><p>help you perhaps avoid repeating places
that you&rsquo;ve been to already but but yeah</p><p>in practice models tend to be so far
hard to learn so let&rsquo;s look at maybe one</p><p>case study in ways that you can use
models so this is the case of planning</p><p>and/or expert iteration the basic idea
is that you&rsquo;re going to use your model</p><p>from a current state to look ahead into
the future and help guide your decision</p><p>about what action to take
so in planning you might explicitly just</p><p>base your decision about what action to
take on whatever the output from that</p><p>look-ahead process is and your current
value function in expert iteration</p><p>you&rsquo;re not only going to do that but
then you&rsquo;re also going to have a</p><p>explicit representation of a policy
which you&rsquo;ll try to improve by</p><p>regressing it towards the output from
the look-ahead process so as a case</p><p>study consider alpha 0 alpha 0 is an
algorithm which has succeeded at</p><p>achieving superhuman performance in a
wide variety of complex 2-player fully</p><p>observed strategy games particularly
chess go and shogi so this was a</p><p>successor to alphago the algorithm that
beat human grandmasters and go and alpha</p><p>0 at the algorithm level is sort of
beautifully simple you have a neural</p><p>network that emits two things a
probability distribution over moves to</p><p>play P and a value network that says
basically whether or not you&rsquo;re gonna</p><p>win or lose B and you learn this with
this very simple regression approach</p><p>where you&rsquo;re gonna move the value
function to be more like whatever the</p><p>true outcomes from games work and you&rsquo;re
going to update the policy by using a</p><p>model-based look-ahead operator to
figure out what a better policy would</p><p>have been based on your current policy
and value function and you&rsquo;re just going</p><p>to move your current policy towards that
and then there&rsquo;s also some</p><p>regularization very straightforward and
the look ahead is done with Monte Carlo</p><p>tree search so that&rsquo;s just stochastic
lis considering different possible</p><p>outcomes and then aggregating data after
having done partial rollouts down the</p><p>game tree to figure out what would have
been the best thing to do so this is one</p><p>model-based approach now this required
having a perfect model of the</p><p>environment and in games like chess ergo
this is feasible because you could fully</p><p>Express the rules in a way which is easy
to compute and forward simulate</p><p>and you don&rsquo;t have to learn anything
from data and you also don&rsquo;t have</p><p>anything which is partially observed so
your model doesn&rsquo;t have to do anything</p><p>fancy to keep track of what&rsquo;s going on
in the background very straightforward</p><p>and this kind of approach can be very
very powerful but the problem is that</p><p>most conditions are not quite as ideal
as this so another family of approaches</p><p>is where you&rsquo;re going to use the model
for policy evaluation so let&rsquo;s say that</p><p>you have learned a model or perhaps
you&rsquo;re given one but more often than not</p><p>for these algorithms you&rsquo;re trying to
learn it concurrently with experience</p><p>you learn some models and then you&rsquo;re
going to have the agent quote dream in</p><p>them the agent will sample a bunch of
fictitious trajectories inside of the</p><p>simulator and use those as the basis for
a policy improvement step and algorithms</p><p>that are like this
there&rsquo;s model ensemble TRP oh and I want</p><p>to say Mehta policy optimization or
model-based Mehta policy optimization</p><p>then you could also instead of using
this for computing advantages and and a</p><p>policy optimization style improvement
you could use this for Q learning as</p><p>well where perhaps instead of forming
the target based on the bootstrap which</p><p>might be inaccurate on particular
regions of state action space that you</p><p>haven&rsquo;t visited you could use the model
to simulate what the bootstrap might be</p><p>in those cases and use that as your
backup for Q learning so that&rsquo;s an</p><p>approach called model-based value
expansion and these algorithms the gain</p><p>that you get from doing this is
ultimately in-sample efficiency so what</p><p>happens in normal deep RL is that you
use tons and tons of data from</p><p>interacting with the environment to try
to improve your policy or your q</p><p>function and you make progress at
whatever pace when you use the model and</p><p>you offload a whole lot of the
improvement steps on to experience</p><p>collected in the model that frees you up
from having to have collected that</p><p>amount of experience in the real world
as long as your model is good enough if</p><p>your models not good this won&rsquo;t be very
helpful but if it is good and if you</p><p>only needed a little bit of data to
train your model then you can get a lot</p><p>of mileage out of it and your overall RL
algorithm will have used less</p><p>interactions with the real environment
and otherwise this is great for cases</p><p>where interacting with the real
environment is very expensive so for</p><p>instance if you want to train something
on a physical robot that can be an</p><p>expensive process the robot might be
slow the robot might break the robot</p><p>might have all kinds of things where
it&rsquo;s difficult to get it to do that or</p><p>it&rsquo;s difficult to reset it you probably
don&rsquo;t want to have to spend that many</p><p>man-hours waiting around for the robot
to finish its learning procedure so if</p><p>you can offload some of that time into
simulation then it makes life better</p><p>yes is that what you would apply for
self-driving cars that&rsquo;s a good question</p><p>so I&rsquo;m not actually all that familiar
with cases where self-driving cars have</p><p>fruitfully made use of deep RL that&rsquo;s
not to say that they don&rsquo;t I just don&rsquo;t</p><p>know I would imagine that in
self-driving cars it&rsquo;s probably more a</p><p>matter of collecting data from
experienced human experts and then using</p><p>that data as the basis for learning a
behavioral policy but I&rsquo;m also happy to</p><p>you know go through this later and see
what we can find in the literature yes</p><p>what would model-based RL be more geared
towards transfer learning I think it</p><p>could potentially be quite helpful so
certainly when we think about trying to</p><p>get robotics to transfer from say
simulation to reality you know we want</p><p>to make sure that the model used in
simulation is high fidelity with respect</p><p>to reality and if that&rsquo;s the case then
this model you can think about sim to</p><p>real as sort of a model-based approach
and perhaps it&rsquo;s gonna be very helpful</p><p>all right and then there&rsquo;s this other
completely orthogonal way of using</p><p>models which I&rsquo;m really fond of because
it&rsquo;s just sort of weird which is that</p><p>you actually take the model and embed it
inside of a model free agent where the</p><p>model is going to receive inputs from
the from the environment and use that</p><p>with some internal process of perhaps
imagining some futures and then</p><p>transforming whatever representation and
has of those futures into something</p><p>which then becomes side information to
the model free agent so you train the</p><p>model separately from the agent the
module that provides some information</p><p>based on the model to the agent is sort
of decoupled from it except that however</p><p>it&rsquo;s going to process however the model
free agent will process that information</p><p>is based purely on the model free
learning so this is an approach called</p><p>imagination Augmented agents I think
this is really interesting and really</p><p>neat I&rsquo;m not aware of a whole lot of
follow-up work from when this came out I</p><p>want to say last year or the year before
but I just think that because it is so</p><p>different from the other model-based
approaches that&rsquo;s cool whenever there&rsquo;s</p><p>something different it&rsquo;s cool all right
that takes me to what was originally</p><p>intended to be the end of part one but
it&rsquo;s now the end of both parts thank you</p><p>so much</p><p>at this point I would like to turn over
the mic and the stage to Matthias</p><p>Clapperton who is a researcher on the
robotics team at open AI and he&rsquo;ll be</p><p>presenting on the work on the robotics
team for learning how to do complex</p><p>manipulation with deep reinforcement
learning on a real physical robot great</p><p>thank you
we have a computer suite</p><p>yay I think it works okay thank you
cool so hey everybody my name is Matias</p><p>as Josh mentioned I&rsquo;m super excited to
be here and talk a little bit about what</p><p>robotics that openly is doing and to
talk that I&rsquo;m going to present this call</p><p>it&rsquo;s called learning dexterity as I
mentioned this is basically the effort</p><p>of the entire robotics teams for many
months so everything I&rsquo;m kind of talking</p><p>about is not just my work but these are
robotics teams okay cool</p><p>so let&rsquo;s maybe start with talking a
little bit about what robotics at open</p><p>era is actually trying to do and the
ultimate goal I guess robotics at open</p><p>eye has is suppose some form of general
purpose robot so I think this kind of</p><p>picture illustrates as well very well we
have human-like robots today and we know</p><p>that humans can do a very very large
amount of different jobs and skills so</p><p>that can include things like cooking it
can include things like actual labor in</p><p>some form of agricultural thing
maybe it&rsquo;s very precise kind of things</p><p>like surgery or building things and
putting things together in this kind of</p><p>stuff and ideally we would like to have
a robot that has a similar similar level</p><p>of dexterity and a similar level of well
general purpose Ness if you will the way</p><p>robotics looks right now it&rsquo;s very
different from that so we have these</p><p>kind of very specialized robots so an
example I think that is good it&rsquo;s the</p><p>Roomba which is on the lower in the
upper left corner here that can clean</p><p>your house but it can only clean your
house it can only vacuum your house and</p><p>similarly your things like self-driving
cars which to some extent also robots</p><p>that are very good at one thing which is
driving themselves but they cannot do</p><p>anything else and the robots there are
more kind of versatile and more</p><p>complicated they are either very often
controlled by humans so an example for</p><p>that would be doing surgery so we have
robots that can assist humans in that</p><p>but they&rsquo;re always controlled by human
operator which is a surgeon or we have</p><p>more complicated robots in factories but
those are typically just programmed to</p><p>basically blindly execute a given show
secretary so someone sits with the robot</p><p>and figures out how to do a certain
process in a factory and the robot is</p><p>very very stupid and has no idea what&rsquo;s
going on so the question of course is</p><p>how can we kind of step away from that
paradigm and how can we have robots that</p><p>work in an actual physical world and
aware of their surroundings and given</p><p>that this is the spinning out workshop
that&rsquo;s concerned with ll it&rsquo;s not so</p><p>surprising that we think RL may be a
good approach to that and we know that</p><p>RL works really well in certain domains
so I&rsquo;ve picked out two examples here</p><p>that probably most people have seen on
the left side we have alphago zero</p><p>playing against Lisa at all and a game
of Go and as you know alphago zero won</p><p>this game
in fact I think one almost all games</p><p>that it has ever played and the
follow-up versions of alphago zero</p><p>beyond beyond human capabilities when it
comes to playing go similarly we have</p><p>dota 2 so this is some of the work that
the dota team at opening AI has been</p><p>doing for a while we have this door
abort called opening at five that is</p><p>very very good at playing the game dota
2 which is a 5v5 multiplayer game and it</p><p>is approaching like professional levels
so it&rsquo;s it&rsquo;s consistently winning and</p><p>can semi-pros and we are already playing
against some pros in fact we&rsquo;ve done</p><p>that last summer at the International
unfortunately we have not yet won</p><p>against those pros so the question is
how does this work in robotics and of</p><p>course yes like a lot of work in this in
robotics it&rsquo;s not like we we are the</p><p>only ones doing this and I just like to
give a bunch of examples that I think</p><p>are kind of illustrating what people are
typically doing today</p><p>the first approach here is somewhat
reason it&rsquo;s from 2017 and I think it</p><p>looks really cool so you can see the
agent is even able to use certain tools</p><p>so in this case a hammer it can open
doors it can do all sorts of things</p><p>the unfortunate thing here is that all
of this looks really cool about it&rsquo;s</p><p>only in simulation and ultimately in
robotics it doesn&rsquo;t really count if it&rsquo;s</p><p>only in simulation because you want the
physical robot to do something otherwise</p><p>it&rsquo;s not very useful
so the other approach that people have</p><p>been taking is to train on the actual
robot itself so this is a some work from</p><p>2016 where people have been doing
dextrose in hand manipulation so the</p><p>goal of the robot here is to kind of
manipulate this this tube filled with</p><p>coffee beans for some reason into a
target orientation and they do all the</p><p>learning on the on the actual robot and
that of course has the advantage of not</p><p>having to do any form of transfer
because you learn on the robot you</p><p>exactly know how the robot is going to
work and once you have a good policy</p><p>you&rsquo;re done the downside of that of
course is that well you have to run on</p><p>the actual robot so it kind of breaks a
lot on you it&rsquo;s very slow to do you</p><p>can&rsquo;t really scale this up unless you
get a lot of robots which is actually</p><p>something that people are doing so this
is the approach thing by Google and</p><p>typical Google fashion scale it up so
just get a lot of robots and let them do</p><p>it for two months in parallel and then
you can suddenly train on the robot</p><p>because well you have 20 of those doing
it in parallel and it can do very</p><p>meaningful stuff so in this case they
have learned to grasp arbitrary objects</p><p>out of this kind of box that I have
sitting here and this is actually very</p><p>impressive demo like this kind of been
picking stuff is actually very hard the</p><p>thing is still that obviously this does
not really scale all that well because</p><p>this is a relatively simple task yet you
need 20 robots going for two months and</p><p>you will also just have to babysit the
robot all the time right like you&rsquo;ll</p><p>have to repair it when it breaks you&rsquo;ll
have to kind of reset the environment</p><p>when certain objects fall out of the bin
and all of this kind of stuff so it&rsquo;s</p><p>just a lot of work so what we&rsquo;re trying
to do is to kind of combine the benefits</p><p>of those two approaches so training in
simulation and then transferring to the</p><p>physical world which is called sim to
real and I&rsquo;ll be talking a lot more</p><p>about this but before I do that I&rsquo;d like
to introduce you to the test that we</p><p>actually have in mind when we when we do
our research so we decided to do</p><p>dextrose in hand manipulation and the
reason for that is that it is first of</p><p>all very hard to do and then second of
all</p><p>it is something that we&rsquo;re interested in
because we know that our hands these</p><p>universal end effectors right so human
hands are very versatile in what they</p><p>can do it they can be very dexterous you
can do an cooking thing or you can</p><p>operate on a human if you&rsquo;re searching
at least but you can also do very heavy</p><p>lifting with it and you can use tools
made for you nuts hands and these kind</p><p>of things so so this is basically the
motivation for why we choose this kind</p><p>of hand and just kind of tasks because
it&rsquo;s hard and because it&rsquo;s also</p><p>ultimately useful for the channel
purpose robot we would like to build and</p><p>the reason why it&rsquo;s hard I think is
summarized relatively well in this this</p><p>kind of slide so we use a hand called
the shadow Dexter&rsquo;s hand which is</p><p>depicted in this picture it has 24
joints and it has 20 actuators so what</p><p>this means is that your policy it and
every time set has to produce an action</p><p>for 20 individual actuaries and it
actually has to coordinate right like</p><p>you&rsquo;ll have to have different joints
work together to do certain things so</p><p>it&rsquo;s a really high dimensional kind of
control problem that&rsquo;s typically well</p><p>out of reach of what traditional control
problems can solve as I mentioned</p><p>ultimately we wanna run this on real
hardware and so we have to work with the</p><p>real hard way and all its flaws and
issues so this includes things like</p><p>noisy and delayed sensing so that&rsquo;s just
a fact of physical hardware systems</p><p>right like they will not have perfect
information and they will have delays</p><p>and certain certain quirks that you kind
of have to deal with the other issue</p><p>that comes out of this sensing is that
you actually have to handle partial</p><p>observability so in simulation you have
perfect knowledge of everything that&rsquo;s</p><p>going on because well it&rsquo;s your
simulation and you can just read out</p><p>from your simulation what the current
state is but on the physical system you</p><p>can only use what you can actually sense
so obviously certain things like the</p><p>friction for instance of the system
cannot directly be observed and then</p><p>last of all this is actually super hard
to simulate as it turns out the reason</p><p>for that is that you have a lot of
contacts going on so if you have</p><p>something in your hand like you kind of
constantly touch it and contexts are</p><p>notoriously hard to model accurately
first of all and then the hand itself is</p><p>also incredibly complicated so it&rsquo;s 10
actuated which means that you kind of</p><p>have tendons pulling and just causes a
lot of unmodeled kind of things in you</p><p>and your hardware that you have not
modeled in simulation cool so as I</p><p>mentioned we set out to solve this
problem with our seem to real approach</p><p>so we trained in simulation and then we
transfer to the physical hardware and</p><p>while this sounds very easy it is not
very easy because the transfer problem</p><p>as you&rsquo;ll see is actually not very easy
to overcome but before we talk about</p><p>that let&rsquo;s have a look at what what we
can do in simulation and what the policy</p><p>that we train looks like in simulation I
think this also illustrates the task at</p><p>hand so that you can actually understand
later what what the robot is trying to</p><p>do so as you can see you kind of have
this block with colored faces and the</p><p>task is to rotate this block into the
desired target orientation that you have</p><p>and the target is depicted as this kind
of like semi-transparent additional</p><p>block on the right hand side so now it&rsquo;s
trying to bring up the blue face a yeah</p><p>it got it and then kind of moves on to
the next goal and as you can see it just</p><p>kind of involved like it coordinating
its fingers it has to kind of use its</p><p>permit it&rsquo;s kind of using gravity to let
it roll and it&rsquo;s like even in simulation</p><p>this is not super easy to learn the hard
way itself looks like this so this is</p><p>the cage we call it it houses all sorts
of things in the middle of course you</p><p>have the shadow Dexter&rsquo;s hand which is
the robot itself and then you have it</p><p>surrounded by quite a lot of these face
based tracking cameras so we have 12 of</p><p>those in total and what they do is they
provide you with relatively accurate</p><p>sensing in in Cartesian space so we have
LED markers on the hand itself so we</p><p>know where the hand is and we also have
LED markers on the object so we know</p><p>where the object is and those guys
basically they sense the slide of the</p><p>LED and since multiple cameras can kind
of see the same LED marker that it can</p><p>do triangulation and you can recover the
position in in space from that</p><p>information
we also have an alternative setup</p><p>because as I mentioned ultimately we&rsquo;d
like to have something that&rsquo;s more</p><p>general and having a motion capture
system is not very kind of real-world</p><p>like so we also have RGB camera so those
are regular RGB cameras we have three of</p><p>them surrounding the scene and they can
also be used for sensing in fact they</p><p>can be used for post estimation of the
object so you don&rsquo;t even have to have</p><p>any any special kind of sensing on the
object itself the cameras can do it for</p><p>you and the reason why we have three is
just just so they can first kind of</p><p>recover depth information and then
second they can also kind of work around</p><p>occlusions because it&rsquo;s in the hand from
certain angles you cannot sometimes see</p><p>the object because it&rsquo;s kind of covered
by the hand so this is how it looks up</p><p>close when we run things so as you can
see we have the we have the hand with</p><p>the block in it palm and in this case
it&rsquo;s the block that we use for face</p><p>based tracking so you can kind of also
see the LEDs on it that we use this is</p><p>simply much easier to do when when kind
of testing these algorithms so we have</p><p>these kind of world setups all right so
the big question of course is how do we</p><p>do the transfer so I showed you a video
of the policy doing its thing in</p><p>simulation and I showed you the physical
Hardware so we can have all the building</p><p>blocks but how can we actually transfer
it to the physical robot and if you just</p><p>train it in simulation it will not work
at all it&rsquo;s the short version so I&rsquo;ll be</p><p>showing some kind of numbers for that as
well but there you can believe me if I</p><p>say the transfer problem is really the
core issue that we&rsquo;re dealing with here</p><p>and the approach that we&rsquo;re taking is
relatively straightforward actually so</p><p>what we do is we use two main techniques
the first one of course being</p><p>reinforcement learning to learn the
actual control policy and then the</p><p>second technique being the main
randomization to make sure that the</p><p>learn control policy actually transfers
to the physical system and I&rsquo;ll be</p><p>speaking about both of those in a little
bit more detail so let&rsquo;s get started</p><p>with the main randomization so this is a
technique that has been used for a</p><p>little while
pretty popular paper when it comes to</p><p>this is from 2016 in this paper what
they did is they learn to fly a drone</p><p>and the way they approach this is they
trained in only in simulation using</p><p>these kind of randomized buildings so
you can kind of see it has a lot of</p><p>different rooms in it the textures are
very different so the walls look</p><p>different of ceilings of floors and they
train a drone to fly in all of those</p><p>rooms and what they then do is they take
this drone that was only ever flying</p><p>inside a simulation and show that they
can actually fly another completely</p><p>different actual room simply because it
kind of has seen all of this variant</p><p>doing during its training it kind of
like from its perspective what happens</p><p>is that the policy think so justice is
another like randomization it&rsquo;s kind of</p><p>weird but oh well I know how to handle
it so it flies in the actual room and</p><p>people that open either has been using
similar approaches as well so this is</p><p>some work from my colleague Josh Tobin
what he has been doing is he has been</p><p>using domain randomization for grasping
so this is using a robot called the</p><p>fetch
so it&rsquo;s you&rsquo;ll see a better picture in a</p><p>moment but it&rsquo;s basically a a simple
robot armed with a parallel group at the</p><p>end and what he would like to do is pick
up these objects that you kind of see in</p><p>these randomized scenes and by basically
using the same approach so he&rsquo;s</p><p>randomizing all sorts of things like the
looks of the objects of shape of the</p><p>objects the background the color of the
table as you can see he can then use</p><p>this information or this training to
transfer to the physical robot even</p><p>though it has never seen the actual
physical table and what was pretty</p><p>surprising in this research is that it
turns out you don&rsquo;t even need</p><p>photorealistic rendering so as you can
see like this it looks not realistic at</p><p>all it&rsquo;s like pretty computer graphics
and and still it transfers to the</p><p>physical to the physical world so the
important thing here is that you have</p><p>this variety and not necessarily
realistic environments yeah yes so all</p><p>of the the two approaches that I showed
are using</p><p>using vision to learn a policy yes in
this case I think it&rsquo;s actually not</p><p>using the vision to learn a policy
directly I think it&rsquo;s instead just</p><p>predicting the location of the object
and then there&rsquo;s a policy that the</p><p>Kinect can grasp it from that so some
some other work in this domain which i</p><p>think is equally important is physics
randomization and this has been done by</p><p>Jason pang who used to be an intern at
open air in 2017 and he&rsquo;s basically</p><p>using the same idea of randomizing but
now for physics instead of visual</p><p>appearances so it&rsquo;s kind of hard to like
visualize what&rsquo;s going on but what the</p><p>policy in training sees a certain worlds
that are just different so maybe they</p><p>have different masses maybe they have
different frictions of the table maybe</p><p>the robot itself behaves differently and
so on and so forth and what he was able</p><p>to show is that this again is sufficient
to train strictly in simulation and then</p><p>transfer to the physical robot so the
test at hand here is again with the</p><p>fetch robot and it&rsquo;s trying to move this
this park to the goal location which is</p><p>marked in in red and on the left hand
side you see a policy that has been</p><p>trained with those physics
randomizations and on the right hand</p><p>side it has been trained without and as
you can see obviously the one on the</p><p>left hand side does a pretty decent job
it&rsquo;s like relatively precise it can push</p><p>the park where it wants to go and the
one on the right kind of freaks out so</p><p>it shakes very violently in fact the
building was shaking when he was</p><p>deploying this and it cannot really do
with the job and the reason is that it</p><p>well has kind of over fit to the
simulation which simply is not fully</p><p>accurate even though it&rsquo;s calibrated to
be close to the robot and then it</p><p>doesn&rsquo;t generalize to the actual
physical world where&rsquo;s the one with</p><p>physics randomization stars okay of
course so that&rsquo;s the main randomization</p><p>in a nutshell so both the visual
randomization and the physics</p><p>randomization yeah</p><p>yeah it&rsquo;s it&rsquo;s not very realistic
honestly I mean it&rsquo;s realistic in the</p><p>sense that it&rsquo;s the physical so if you
randomize too much your simulation will</p><p>become unstable because you&rsquo;ve set in
certain parameters such that they cannot</p><p>make sense anymore
but it&rsquo;s not very realistic like the</p><p>masses will be very high sometimes it&rsquo;s
like smart to move the puck and it&rsquo;s</p><p>more about diversity again yeah okay
cool</p><p>so I&rsquo;ll now speak about our approach so
what I previously talked about was</p><p>mostly other people&rsquo;s work even though
they&rsquo;re also in the robotics team but</p><p>this is the the learning dexterity
approach that we took so again remember</p><p>the goal is to have the shadow hand
rotate an object in hand and to kind of</p><p>start it off I think it makes sense to
just give you the the overview of the</p><p>entire system and then we&rsquo;ll kind of
dive into some of them details after</p><p>that so again as I mentioned everything
we do is only in simulation so we never</p><p>see the actual physical robot until we
run on it like we&rsquo;ve never seen it so so</p><p>the way it works is that we collect a
lot of data in simulations so we have</p><p>many many simulations running in
parallel which is kind of depicted here</p><p>in box a and all of those are randomized
which is kind of visualized by them</p><p>having different visual appearances but
also think physics randomizations so the</p><p>friction and the masses will also be
randomized and using this collected data</p><p>we basically end up training two
different networks so one of them is a</p><p>policy and the other one is job is a
vision network because we&rsquo;d ultimately</p><p>like to run this from vision alone
without the face base the policy network</p><p>is what is depicted in Box B here and
the way it works is that it takes the</p><p>observed robot state which is the
position of the five fingertips so you</p><p>have doting coaches in a space of 15
dimensions in total so it knows where</p><p>its fingertips are and then also the
pose of the object so that means just at</p><p>the orientation and the rotation in
space sorry the position and the</p><p>rotation in space and this information
is then fed into an LSD and policy so</p><p>it&rsquo;s a recurrent policy and it produces
the next action and we train this in</p><p>simulation using reinforcement learning
the second network that we have which is</p><p>actually distinct they are not
end-to-end</p><p>this is two networks that we train
separately it&rsquo;s a vision Network and the</p><p>rate and vision Network uses works is
that it takes three different images so</p><p>remember we had these three RGB cameras
surrounding so images rendered from the</p><p>perspective of those but again only in
simulation and then using a</p><p>convolutional neural network predicts
the pose of the object from that</p><p>information from those images and again
this is only trained in simulation when</p><p>it comes to actually deploying this to
transfer as you can maybe kind of guess</p><p>is that we can combine those two systems
to get us what we ultimately would like</p><p>so you use the actual cameras to sense
the position or the pose of the object</p><p>using the vision network so you feed it
into that and then by having the object</p><p>pose and the fingertip locations you use
your L SCM policy to produce actions and</p><p>that allows the robot to basically see
what is going on and react accordingly</p><p>and of all only being trained in
simulation yeah potentially honestly we</p><p>have mostly used this approach because
we knew it worked from previous research</p><p>it is almost as accurate as face base
and face base is very very accurate I</p><p>think if you spend a lot of time you
could probably develop something with</p><p>more traditional methods I don&rsquo;t
question it but like we would like to</p><p>have something that&rsquo;s more general again
and having a convolution that conversion</p><p>neural network - it seemed like the most
general approach we could have yeah yeah</p><p>it&rsquo;s kind of interesting so ideally you
would just use whatever the robot has as</p><p>joint sensing so it knows it should know
what its own joints are as it turns out</p><p>the sensor in the shadow hand uses
hall-effect sensing which is a magnetic</p><p>kind of sensor and they interfere quite
a lot so if you think as a close</p><p>together you will actually not know
where your fingers are so that&rsquo;s the</p><p>reason why we don&rsquo;t use it we would like
to use it but it turned out to be not</p><p>precise enough for what we ultimately
wanted to do so we couldn&rsquo;t actually</p><p>rely
but yeah you&rsquo;re right like like this is</p><p>more for more for work around like
ideally the robot should just tell us</p><p>what the joint positions are and then we
wouldn&rsquo;t need the fingertip positions no</p><p>it actually has very limited information
it&rsquo;s very surprising that it works like</p><p>that yeah yes yeah yeah very good
question this is there&rsquo;s a lot of debate</p><p>about this I don&rsquo;t think it does we have
some indication that it doesn&rsquo;t in fact</p><p>it seems to help like the performance
seems to improve over the board like we</p><p>have certain ways of measuring symptom
transfer and when we randomize more we</p><p>tend to get better performance on all
the environments so I don&rsquo;t think it&rsquo;s</p><p>it&rsquo;s compromising actually I think it&rsquo;s
more of an adaptive policy but then</p><p>there&rsquo;s people who disagree so it&rsquo;s
currently a little bit unclear okay cool</p><p>so as I mentioned we need to randomize
and of course we use appearance</p><p>randomization so this is only for the
vision Network so this is basically what</p><p>I&rsquo;ve described before just for our setup
so you can kind of see we have three</p><p>different cameras showing the same scene
and we randomize this scene quite quite</p><p>heavily so the robot changes its color
the background changes its color</p><p>importantly the block itself stays
mostly the same because it actually has</p><p>that color like you cannot randomize the
dye but rarely but we changed the</p><p>material of the of the block as well so
it looks slightly different and then we</p><p>of course have that vision network which
again is relatively straightforward so</p><p>the way it works is it takes those three
camera images then uses convolutions and</p><p>the rest net architecture and spatial
softmax to kind of process them and then</p><p>simply calculates all the things and
produces the final object position on</p><p>object rotation so the pose of the
object and this is simply trained with</p><p>supervised learning because in
simulation you actually have perfect</p><p>ground truth which is another very
convenient thing you actually perfectly</p><p>precisely know where your your object is
you have not to actually sense it at all</p><p>and this is what the model actually sees
so it&rsquo;s actually I think very</p><p>interesting because it looks very very
different from</p><p>randomization and yet it generalizes to
that simply because it has seen enough</p><p>variety that it&rsquo;s kind of okay with with
yet another variety that&rsquo;s kind of weird</p><p>but still within distribution in that
sense so when it comes to the physics</p><p>randomizations that we use we randomized
quite a lot of things as well so we have</p><p>things like object dimensions for
instance we have things like masses</p><p>obviously and then mostly things about
the robot itself so things like the way</p><p>we actuate the robot things like damping
within its joints and all of this stuff</p><p>and the reason for that is that it&rsquo;s
actually very hard to measure this so</p><p>another neat thing is that you can in
this physics randomization actually</p><p>account for your uncertainty so for the
object dimensions we know those with</p><p>relatively little uncertainty because we
can just measure the dimension of the</p><p>block but things like the actuation we
learn much less about and so we kind of</p><p>widen the randomizations for those and
another kind of cool thing is that we</p><p>randomized the gravity vector which may
seem a little bit weird but it basically</p><p>amounts to like when you when you mount
the hand it&rsquo;s not perfectly parallel to</p><p>the to the floor like it will be
slightly angled because of imperfections</p><p>and by randomizing the gravity vector
you kind of get this effect as well like</p><p>it&rsquo;s sometimes slightly angled and it
turned out to be actually very useful</p><p>and then we of course also have noisy
observations and noisy actions simply</p><p>because it&rsquo;s a rare reality of the of
the physical system the policy is very</p><p>very simple so what it gets is the noisy
observations so that&rsquo;s five fingertip</p><p>positions and the poles of the object
and the goal so it&rsquo;s knows what it wants</p><p>to do and then we normalize a little bit
so this is just making sure that things</p><p>have a zero mean and unit variance and
then use one fully connected value layer</p><p>and one lsdm to produce the actual
distribution and from that we sample and</p><p>then perform perform that on the robot
so it&rsquo;s a relatively shallow and</p><p>relatively small network over all the
more so yeah</p><p>they only come in through the simulation
they cannot be observed directly so</p><p>sorry</p><p>they are simply set in the simulation so
the environment has been changed but the</p><p>policy cannot sense this directly it has
to infer this basically because on the</p><p>physical robot it also cannot sense it
like we don&rsquo;t know what it is on the</p><p>physical system so it basically what
what we think it ultimately ends up</p><p>doing is some form of system
identification so when it&rsquo;s running it&rsquo;s</p><p>implicitly inferring certain information
about the environment and then using</p><p>this information to kind of adapt itself
accordingly yeah sorry I couldn&rsquo;t hear</p><p>yeah so so we add Gaussian noise to the
observations and to the actions yeah all</p><p>right so I think I&rsquo;m running a little
bit late actually how bad is this huh</p><p>okay then we have to hurry a little bit
cooler so disappeared of training let me</p><p>speak about this and then I&rsquo;ll show a
video so disappeared training I think is</p><p>very interesting because we use
basically the same system that the dota</p><p>team uses as well so we have a very
large-scale kind of system and the way</p><p>it works is that we have role of workers
who generate a lot of experience and</p><p>then we have an optimizing machine
that&rsquo;s kind of using this information to</p><p>update its policy and we use approximate
policy optimization for that so a non</p><p>policy algorithm as I think josh has
explained earlier today and I think it&rsquo;s</p><p>kind of cool that we use the same system
estera let me skip over some things but</p><p>I think I&rsquo;ve want to show this so this
is when it&rsquo;s running on the physical</p><p>robot as you can see it&rsquo;s using vision
so there are no markers on the actual</p><p>object the robot hand is doing all of
this this is not cut in any way it is</p><p>not sped up again the goal is depicted
in the right corner here so it will try</p><p>to get the e face front and the end face
up top and it will get 250 successful</p><p>rotations in this case so it can do
quite a lot of those and it can run on</p><p>the on the physical system and if I have
enough time one one kind of final thing</p><p>that I think is actually very
interesting is that it actually learns</p><p>certain strategies that happen to have
names so we have thing a pivoting where</p><p>you kind of like use two fingers to
create a rotational axis and then you</p><p>rotate around that and things like
finger gating and the reason why they</p><p>have names is because they are used by
humans as well and they have been kind</p><p>of studied very well they emerge
automatically in our case so we have</p><p>never shown the robot what a human would
do it has kind of discovered that itself</p><p>and the reason why they come up is
simply because it has a human-like</p><p>morphology right like it has a
human-like hand and it just turns out</p><p>that these strategies are equally useful
for humans and robots but they have kind</p><p>of been rediscovered quote-unquote which
i think is a really</p><p>thing so I wanted to mention that and
yet we have some qualitative results</p><p>that show that randomizations are very
important so if you don&rsquo;t randomized you</p><p>get no successes if you randomized you
do it turns out memory is very important</p><p>so you need an LLC M you cannot simply
have a feed for policy and you need a</p><p>lot of experience so for the final
policy we use a hundred years worth of</p><p>data so imagine doing that on the
physical robot like probably not such a</p><p>good idea so but we can get away with it
because we use simulation so we do all</p><p>of this in 50 hours and I think with
that I have to close all right thank you</p><p>great thank you so much Matthias we&rsquo;re
gonna switch out the slides and then</p><p>please welcome to the stage the leader
of the safety team at open AI dario</p><p>Amadei all right just a minute to get
the slides</p><p>right</p><p>very good thing that you&rsquo;re ensuring
that computers in the future will not be</p><p>as malicious so I work on a team at open
AI that thinks about making AI systems</p><p>do what humans want them to do
which is you know kind of very central</p><p>to open the eyes mission and you know
which which we think of as you know</p><p>something that our focus on
distinguishes us from from other</p><p>organizations we think it&rsquo;s very
important particularly as systems get</p><p>more capable to ensure that they you
know both in a narrow and broad sense</p><p>benefit society
so this workshops called spinning up in</p><p>in deep RL so it&rsquo;s useful to step back
and you know think about what is what is</p><p>RL accomplished in the last couple years
and where is it going so you know this</p><p>is actually out of date we should add
add a couple things to it but you know</p><p>if we look at playing games like go if
we look at for about a year ago multi</p><p>agent behaviors where you can use RL and
self play to train agents to sumo</p><p>wrestle each other off a pad we are able
to play competitively against</p><p>professional professional players in
dota 2 the robot results which you just</p><p>saw and you know we should probably add
just in the last week or two you know</p><p>the results we&rsquo;ve seen on StarCraft
which is you know in some ways similar</p><p>to dota but just a different kind of
game with the different kind of</p><p>properties and yet you know that shows
that these techniques are really are</p><p>really pretty general and are are
advancing pretty quickly so you know if</p><p>we step back and reflect on you know
kind of where are things going you know</p><p>some properties that we could point out
of these RL agents that are becoming</p><p>more and more true right that we&rsquo;re not
true five years ago but are becoming</p><p>more and
are true we have systems that have an</p><p>extended interaction with complex
real-time environment they have a very</p><p>high level of autonomy and speed you can
imagine systems like this in the real</p><p>world being used to make decisions
faster than humans can intervene or in</p><p>more complex ways than humans could you
know could hope to understand so</p><p>regulating the economy or financial
system managing large networks of</p><p>computers these are the kinds of things
that as RL technology matures it will be</p><p>better and better better and better able
to do and you know the these systems</p><p>unlike supervised learning systems and
unlike in any interesting way you know</p><p>the simple RL systems were a few years
ago these systems are able to teach</p><p>themselves and discover their own
strategies and in many cases they</p><p>discover non-trivial strategies you know
just like we saw with the robot it kind</p><p>of recapitulating a lot of strategies
that humans use you know we see in go</p><p>and dota and Starcraft a lot of human
strategies that have names you know the</p><p>RL system discovers and recapitulates
but it also sometimes discover</p><p>strategies that a human would never
would never have thought of so if we</p><p>look at what these properties mean
together one thing it means is that the</p><p>connection between us as designers
specifying what we want the system to do</p><p>and what the system actually does in
theory the system does in theory if</p><p>everything is done right the system does
what we want but that that rope it&rsquo;s</p><p>longer it&rsquo;s more afraid it&rsquo;s more
tenuous</p><p>than for just kind of less less
autonomous systems that we&rsquo;ve we&rsquo;ve</p><p>designed in the past
and there are many ways relative to you</p><p>know simple computer systems or machine
learning systems like supervised</p><p>learning for for these systems to go
wrong and so a couple years ago several</p><p>people on most most of whom are now now
now constitute the the open a nice</p><p>safety team started started thinking
about this you know we&rsquo;re worried about</p><p>current systems worried about tomorrow
systems eventually we&rsquo;re worried about</p><p>you
about about building general</p><p>intelligence and what that what that
will mean for the world and making sure</p><p>that those systems are safe so you know
we wrote kind of a position paper and</p><p>this kind of started us thinking about
you know the directions and how to even</p><p>think about this problem of you know do
systems reliably do do what we want them</p><p>to do and the the kind of general
framework and division we came up with</p><p>was okay so you know let&rsquo;s let&rsquo;s let&rsquo;s
narrowly scope the problem we&rsquo;re not</p><p>we&rsquo;re speaking not about kind of wider
or societal impacts although those are</p><p>also important but you know just
narrowly the designer had a clear thing</p><p>they wanted the system to do and then
you know the system gets trained it gets</p><p>deployed it goes through some long
process actual system fails at this</p><p>catastrophic ly and we kind of divide it
up into into a couple things one is you</p><p>know you&rsquo;re you&rsquo;re giving the system
some Direction some objective function</p><p>that it learns from like the reward in
RL there are ways for that to be subtly</p><p>wrong and you can get spectacularly
wrong behavior if that happens you might</p><p>have the right objective function but
your system has problems with robustness</p><p>doesn&rsquo;t generalize well it you know
exhibits on exhibits unpredictable</p><p>behavior as its learning it does
dangerous things even if the final</p><p>policy it&rsquo;s gonna learn makes sense and
then as a reminder that you know there</p><p>like this
all exists on top of kind of software</p><p>implementation that has bugs in and of
itself and so you know these the a and B</p><p>are new but they&rsquo;re layered on top of
the general just the general</p><p>unreliability of software so kind of a
useful way to think about let&rsquo;s put CSI</p><p>because it&rsquo;s not really a machine
learning problem or just you know a</p><p>reminder that this is layered on top of
existing problems but a crude analogy we</p><p>can make is you know it&rsquo;s a bit like the
simple statistical concepts of bias and</p><p>variance right better a better objective
function you know that&rsquo;s that&rsquo;s about</p><p>reducing bias and making sure your aim
for the right target robustness is is</p><p>about making sure that you&rsquo;re narrowly
cluster around the target and that you</p><p>always get what you&rsquo;re intending to get
so we&rsquo;re interested in in both problem</p><p>because I have limited time I&rsquo;m going to
talk about our work on the getting the</p><p>objective function side right I think
you know open AI does more the opening I</p><p>safety team does more of that relative
to other you know other teams say at</p><p>Google brain or deep mind that that
think about these problems and so I&rsquo;ll</p><p>mostly talk about that but increasingly
and maybe I&rsquo;ll have a little bit of time</p><p>to talk about it at the end we&rsquo;re also
thinking about the robustness direction</p><p>and how these two things interact so
just to be clear about what we mean I</p><p>think this this this this video has been
widely circulated so I apologize if for</p><p>people who are already familiar with it
but you know about about a year and a</p><p>half ago we you know we were we were
training lots of flash games using RL</p><p>and you know there there happens to be
this boat race game so you know I I just</p><p>set lots of lots of games running with
with a reward function so the way this</p><p>boat race works is supposed to go along
the course and you&rsquo;re supposed to that</p><p>you&rsquo;re supposed to finish the course but
the way the reward function works and</p><p>it&rsquo;s hard to reach in and write a
different reward function is you get you</p><p>get points for you know these markers
along the way that are mostly along the</p><p>course but it turns out there&rsquo;s this
this little Lagoon in the corner of the</p><p>course where you can go around in
circles and get more and more power-ups</p><p>and that turns out to get you a faster
rate of power up to naturally finishing</p><p>the course there&rsquo;s nothing wrong with RL
here the system did what it was supposed</p><p>to do but it identifies the weakness of
the connection between a reward function</p><p>in the final behavior the reward
function that you specify that you may</p><p>think corresponds to some behavior that
you want may in fact correspond to very</p><p>different behaviors and you get no
feedback on that other than just finding</p><p>out what the system does right when I
first trained this I trained along with</p><p>a bunch of other games
two days later I looked at this I&rsquo;m like</p><p>what what in the world has what in the
world is this doing it doesn&rsquo;t make any</p><p>sense and then I thought about it alone
I&rsquo;m like oh of course that makes sense</p><p>and you know so the more powerful the
system is the more autonomous it is the</p><p>less of human is paying attention to it
the more potential there is this is like</p><p>you know
generate dozens of these examples but</p><p>you know robotic system where we forgot
to make the table totally fixed it has a</p><p>high mass but it&rsquo;s not fixed turns out
to be easy it&rsquo;s hard to send the send</p><p>the puck exactly to the point you want
it to be it&rsquo;s easier to send the puck</p><p>observe if it&rsquo;s gonna be a little to the
right or a little to the left and then</p><p>nudge the table so that it hits it
exactly it&rsquo;s very it&rsquo;s very clever it&rsquo;s</p><p>a correct solution to the problem but
the problem was not the right problem so</p><p>the general approach that we&rsquo;ve kind of
hit on and we&rsquo;ve been pursuing the</p><p>strategy for about a year a year a year
a year and a half is that the this</p><p>training loop is too long right the
human at the beginning says here&rsquo;s a</p><p>mathematical roared function like go go
optimize this then you look back at the</p><p>end of training you might get the right
thing you might not if you don&rsquo;t you</p><p>have to go back to the beginning or you
know maybe the system is already doing</p><p>something dangerous so maybe we should
have humans be involved interactively in</p><p>the training process right when we train
humans to do things it&rsquo;s not just like</p><p>here&rsquo;s your goal go off tell me what you
did you know two weeks later so if we if</p><p>we do this is there a way that we can
use a human to decide what the reward</p><p>function is in a continuous way that&rsquo;s
more reliable that&rsquo;s more naturalistic</p><p>so that the system ends up imbued with
human goals and values but it&rsquo;s able to</p><p>act faster and bigger than human scale
once it&rsquo;s trained it knows what the</p><p>human wants and it does it example of
this is like instead of RL we can learn</p><p>from demonstrations but that kind of has
the same problem a human demonstrates at</p><p>AI system copies it and there&rsquo;s kind of
it&rsquo;s hard it&rsquo;s hard to do better than</p><p>the human it&rsquo;s hard to course-correct
it&rsquo;s hard for the human to say you</p><p>should be doing this instead of this and
traditional RL has has a loop that&rsquo;s too</p><p>long so the kind of first effort we did
in this direction was we called it deep</p><p>RL from human preferences so the idea is
you know I want this thing to do a back</p><p>flip and I you know it&rsquo;s hard to
mathematically specify the reward</p><p>function for a back flip
we tried by looking at all the</p><p>individual joint angles and you know it
turns out it just gives you some</p><p>think very very like you know very
awkward looking but what we do instead</p><p>is and you know this is now running for
the second time but a human looks at the</p><p>behavior of the system and says which of
these is more like a backflip than the</p><p>other the system just starts by acting
randomly it has it has just like a</p><p>random reward function and human gives
it feedback on what what is more like</p><p>what the human wants and then the AI
system you know like the the RL system</p><p>has a reward predictor and it tries to
fit a reward predictor consistent with</p><p>what the human says the human prefers
and then in the background it&rsquo;s running</p><p>a whole bunch of copies of of the RL
environment and those copies optimize</p><p>the reward function that it learns from
the human the human only ever has to</p><p>give feedback on a very small fraction
of the AI systems behavior doesn&rsquo;t have</p><p>to see everything it does just has to
get enough samples to give the to give</p><p>the policy an idea of what the reward
function should be so another way to put</p><p>it is the human trains the reward
function and the reward function trains</p><p>the RL system so what I just said can be
kind of pictured in this the grey part</p><p>is the standard set up for for for
reinforcement learning where you have an</p><p>RL Aughra than the environment they
exchange observations and actions and</p><p>there&rsquo;s a reward that kind of that kind
of you know comes from the ether that</p><p>was ultimately specified by a designer
but that isn&rsquo;t thought about as being</p><p>part of the problem here what we have is
that reward starts out being completely</p><p>random and the human sees examples of
the agent&rsquo;s behavior and feeds them to a</p><p>reward predictor so the reward predictor
is changing and improving and adapting</p><p>over time and the RL system is both
learning from the existing reward</p><p>function and adapting to changes in the
reward function</p><p>so we did several versions of it in our
paper and we found that a simple active</p><p>learning technique helped relative to
random it didn&rsquo;t help by that much but</p><p>but it helped the idea is you train an
ensemble of reward predictors that are</p><p>trained on subsets of the data and that
that allows you to have kind of like</p><p>semi independent predictors and you can
pick examples where the predictors are</p><p>uncertain meaning that those are parts
of the space or situations where there&rsquo;s</p><p>just the reward predictor has more
uncertainty and so would like more</p><p>feedback from the human that helps you
can go much more sophisticated in that</p><p>direction right the system could like
ask the human like what you know like</p><p>you know what what am i doing that&rsquo;s
wrong what am i doing that&rsquo;s not clear</p><p>the human could say to the system like
you know I&rsquo;d like you to produce some</p><p>examples of this right and then it
becomes much more like a like teacher to</p><p>human teacher to human pupil teaching
process and a lot of what we&rsquo;re doing is</p><p>kind of going in that direction but we
kind of have to start so imitation</p><p>learning has the following limitations
when you do imitation learning you</p><p>except for noise reduction which is
usually a small effect you can&rsquo;t perform</p><p>better than the human does so as we&rsquo;ll
see in some future tasks here there are</p><p>cases where learning from preferences
allows you to perform better than how</p><p>the human does the reason for that is
with imitation learning you just do what</p><p>the human does here you learn what the
human wants and once you learn the</p><p>reward function you could do it better
than the human right so consider</p><p>something like you know if I didn&rsquo;t know
how to play go I can teach you the rules</p><p>of go and then you can do RL on the
rules of go and get much better than me</p><p>or you can just copy my moves if you&rsquo;re
just copying my moves you can never do</p><p>better than me if I teach you the rules
and then you use RL to learn how to</p><p>learn how to play you can you can then</p><p>you can then in principle do better</p><p>another another difference is you tend
to get kind of like</p><p>better sample sample you tend to get
like better sample efficiency you can</p><p>come up with strategies that a human
wouldn&rsquo;t would you can come up with</p><p>strategies that human wouldn&rsquo;t have
thought of and many tasks a human just</p><p>can&rsquo;t do so actually this backflip task
it&rsquo;s actually very hard for a human to</p><p>demonstrate that task like you&rsquo;d have to
get a VR setup and if we look at like</p><p>the tasks of the future right where you
know like let&rsquo;s say I want to defend you</p><p>know a large corporate IT network or
something and I want to respond to</p><p>threats in real time that&rsquo;s just
something where I I can&rsquo;t get training</p><p>data from a human I&rsquo;m asking the machine
to do things that a human can&rsquo;t can&rsquo;t do</p><p>which is what we ultimately want AI
systems to be able to do does that kind</p><p>of answer the question yeah so we have
an option in this paper for basically I</p><p>don&rsquo;t know or I think we had separate
options for I don&rsquo;t know or it just</p><p>throws out the data or these two look
about the same in which case it like</p><p>waits them equally in the predictor and
in yeah so that&rsquo;s that&rsquo;s easy to</p><p>incorporate I think ultimately the
communication needs to be in terms of</p><p>language and not in terms of clicking
left or right and then that will kind of</p><p>like make a richer space for doing
things and saying I don&rsquo;t know or like</p><p>show me some other examples these things
aren&rsquo;t comparable at all become much</p><p>more common so the nice thing about this
is given an environment without changing</p><p>the code at all only changing what the
human provides as feedback you can get</p><p>totally different behaviors so in about
half an hour a human can train this this</p><p>are all system this is like simple
simple atari enduro game i can train it</p><p>to do the usual thing which is to to
race ahead of all the other cars but i</p><p>can also train it to go exactly at the
same speed as other cars and when it</p><p>does that you know it&rsquo;s able to actually
get there very very effectively like you</p><p>know stay exactly even with other cars
which isn&rsquo;t it isn&rsquo;t easy you have to go</p><p>at kind of exactly the same speed and
match their speed and so</p><p>exact same code just the human provided
different different feedback one thing</p><p>we show is if we don&rsquo;t give you the
rewards for Atari games we just hide</p><p>this hide them from you humans giving
feedback on basically you know trying to</p><p>get the system to get the highest score
that it can works really well on the</p><p>kind of right of each panel those like
colored bars that are moving that</p><p>represents how much reward the system is
thinks that it&rsquo;s getting or just how</p><p>much how much it thinks to give an
action is good so if you look at the</p><p>breakout case when the ball hits the
paddle instead of so on the Left when</p><p>the ball hits the paddle instead of you
know instant instead of instead of the</p><p>ball going to the bottom it says yeah I
got a lot of reward from that same with</p><p>pong the when it surfaces to get oxygen
and Seaquest it&rsquo;s very very very high</p><p>very high reward level so the predictors
seem to correspond to what you know to</p><p>what human would say is good behavior
which is not surprising because of human</p><p>training them so we did did a bunch of
we did a bunch of experiments and you</p><p>know with fixed reward Atari games your
goal is just to do as good as you would</p><p>if you knew the reward right so you&rsquo;re
like hiding the reward from yourself and</p><p>you&rsquo;re trying to learn the reward from a
human so most of the time it does it</p><p>does almost as good but actually there
are cases where it can do better</p><p>we&rsquo;re in enduro that the algorithm we
used a3c has trouble learning enduro</p><p>because of sparsity of the reward but a
human actually helps to shape the reward</p><p>right in enduro you have to like kind of
like read the control stick to go at a</p><p>certain speed in order to get in order
to get any reward at all so you can</p><p>start you can start to move and the RL
system doesn&rsquo;t give you any reward and</p><p>then you have to keep moving faster and
faster to get reward and some some</p><p>algorithms never figure that out but the
human will basically say okay yeah you</p><p>went ahead you made progress that&rsquo;s
better than when you&rsquo;re not moving and</p><p>so little by little with just with a few
feedback points it can lead the system</p><p>and so the human can shape the reward
and they&rsquo;re actually cases like the the</p><p>curve for enduro in the bottom right
where you can actually do better than</p><p>the human did or you can actually do
better than that</p><p>standard then a standard oral algorithm
did even though you had less information</p><p>instead of knowing the right reward
function you just had a human indicate</p><p>the reward function also works for a
bunch of kind of like simulated robotics</p><p>tasks we haven&rsquo;t really tried it in the
real world relevant to the question</p><p>about demonstrations we&rsquo;ve we actually
followed this up with an effort</p><p>combining human feedback with
demonstrations so what that did is you</p><p>know there&rsquo;s some tasks
human can do it but we liken it you we&rsquo;d</p><p>like the RL system to do it better
however we can initialize from human</p><p>human demonstrations the AI system
copies that but then on top of that on</p><p>top of that initialization we run RL RL
with human preferences so there&rsquo;s no</p><p>reward function there&rsquo;s no like
programmatic reward function anywhere</p><p>it&rsquo;s entirely learning from humans but
the first step the human demonstrates</p><p>and then the second step the AI system
copies in the human says it would be</p><p>better if you could do it this way and
again the second step allows you to</p><p>exceed human performance or do tasks
that humans can&rsquo;t do right the humans</p><p>like this is as well as I know how to do
it the AI system copies that the the</p><p>human says you know ok well I wouldn&rsquo;t
be able to do this myself but if you</p><p>move back and forth really quickly and
shot those two ships that will be better</p><p>than if you didn&rsquo;t do that the AI system
is capable of that and so it can</p><p>bootstrap itself to kind of beyond
beyond human capabilities more recently</p><p>and we don&rsquo;t have any work out on this
but I think we will soon we&rsquo;ve started</p><p>applying this to natural language so in
the last year or so there have been kind</p><p>of big a lot of progress on large
language models like open the eyes GPT</p><p>and google&rsquo;s burt where you just take a
big corpus of text you train just just a</p><p>big transformer model to to predict the
next word or the next token and that</p><p>allows you to generate very coherent
text and can also be fine-tuned to solve</p><p>a lot of linguistic tasks so one one
idea there is can we find two NAT via RL</p><p>from human preferences right I have a
language model it&rsquo;s</p><p>a lot of text some of its happy some of
its sad five five minutes yeah you know</p><p>some of its formal statements or
informal statements some of its jokes</p><p>the language model maybe has some idea
in its internal representation of the</p><p>difference between those things but you
know if I just sample from the language</p><p>model it just kind of gives me random
samples of stuff so can i push this</p><p>language model in directions and to
produce behaviors that only a human can</p><p>specify that can&rsquo;t be specified
programmatically so things like</p><p>statements that rhyme or our-our
statements that are in iambic pentameter</p><p>could you make a system that is you know
from the logic of learning from human</p><p>preferences is a better poet than any
human could be or something like this or</p><p>you know makes makes like very positive
sentiment statements that are you know</p><p>that it&rsquo;s hard to find enough enough
positive sentiment statements to to copy</p><p>from so that&rsquo;s the direction we&rsquo;re going
in and then I think you know like a long</p><p>term vision for it would be you know we
would you know we want a system that</p><p>basically has an ongoing dialogue with
with a human the human asset to do</p><p>something really complicated like
planning and executing the mission to</p><p>Mars you know the system kind of kind of
clarifies ss4 instructions while it&rsquo;s</p><p>learning and while it&rsquo;s doing the task
and we make sure that that things like</p><p>pathological solutions the problem don&rsquo;t
happen one way to get to Mars really</p><p>quickly is you know to escape from Earth
and propel yourself by dropping a bunch</p><p>of like nuclear explosions back at earth
that would work that would get you to</p><p>Mars this project called the Orion
project in the 1950s although plan was</p><p>to detonate the nuclear weapons when
they were like far away from Earth but</p><p>this is not a solution we would favor
how do we make sure that that that AI</p><p>systems don&rsquo;t don&rsquo;t do things like that
cool so I&rsquo;ve only talked about a subset</p><p>of what the safety team is working on
but you know we have around 15 members</p><p>here some of some of these efforts were
done in collaboration with with with</p><p>deep mind and various various academic
and</p><p>we have a number of kind of interns and
faculty affiliates but you know we&rsquo;re a</p><p>safety team is is is continuing to hire
and we&rsquo;re we&rsquo;re interested in you know</p><p>further advancing these and these in
other areas thank you so much sorry oh</p><p>hello everyone we are now at the
conclusion of today&rsquo;s morning talks but</p><p>before we break for lunch I would like
to invite all of the volunteers who are</p><p>joining us today from open AI and
Berkeley and New Haven school to please</p><p>come up to the front so as we proceed
into the afternoon hackathon and</p><p>breakout sessions these will be the
faces that will be around to help you</p><p>that you should ask questions to these
people are all talented researchers or</p><p>contributors or engineers in this space
many of these people are employees of</p><p>open AI ever and we also have I think
the only person here who&rsquo;s not currently</p><p>employed by opening I was previously
employed by open AI so if you want to</p><p>pick our brains about what it&rsquo;s like
here what we do why it matters please</p><p>feel free can we just have everyone get
maybe a sense to introduce themselves</p><p>sure I am Daniel I work on the safety
team as a male engineer working on the</p><p>language fine-tuning project from a
human feedback</p><p>yeah Matthias I&rsquo;ve owned robotics I&rsquo;m
Ethan I&rsquo;m on the safety team working on</p><p>model-based or LM safe exploration with
Josh I&rsquo;m Carl I&rsquo;m on the games team</p><p>primarily studying transfer learning and
procedurally generated environments my</p><p>name is Dylan I&rsquo;m a PhD student at UC
Berkeley and I mainly work on preference</p><p>learning I&rsquo;m Amanda and I&rsquo;m on the
policy team here opening I I marry I</p><p>work on the safety team on safe
exploration alright and another thing</p><p>that I want to say thank you all so much
for being here today something that I</p><p>hope we can do is really make this a
useful experience</p><p>for all of you and I hope that over the
course of the day that you know you give</p><p>us feedback about what you find helpful
and not helpful and what it is that</p><p>you&rsquo;re hoping to get out of this
experience so that we can figure out you</p><p>know how to help you get to that and and
thank you so much please enjoy lunch</p></section><footer class=article-footer><section class=article-tags><a href=/tags/english/>English</a>
<a href=/tags/video-transcripts/>Video Transcripts</a>
<a href=/tags/openai/>OpenAI</a></section></footer></article><div><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin=anonymous></script><ins class=adsbygoogle style=display:block;text-align:center data-ad-layout=in-article data-ad-format=fluid data-ad-client=ca-pub-9206135835124064 data-ad-slot=1055602464></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/en/at2xkqjazns/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/AT2XkqJAZns data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>Towards Epileptic Seizure Prediction with Deep Network ÔΩú Kata Slama ÔΩú OpenAI Scholars Demo Day 2020 ÔΩú OpenAI</h2></div></a></article><article><a href=/en/jzohw-eybtq/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/JZOHW-eYBtQ data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>Introductions by Sam Altman & Greg Brockman ÔΩú OpenAI Scholars Demo Day 2020 ÔΩú OpenAI</h2></div></a></article><article><a href=/en/-fozam9xqs4/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/-FoZAM9xqS4 data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>OpenAI Five vs. OG, Game 2 ÔΩú OpenAI Five Finals (4‚ß∏6) ÔΩú OpenAI</h2></div></a></article><article><a href=/en/u9mjuukhuzk/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/U9mJuUkhUzk data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>OpenAI DevDay, Opening Keynote ÔΩú OpenAI</h2></div></a></article><article><a href=/en/lpe5gwuqa-k/><div class=article-image><img src=/img/related-content.png loading=lazy data-key=en/lpe5Gwuqa-k data-hash style=opacity:.3></div><div class=article-details><h2 class=article-title>Scaling Laws for Language Transfer Learning ÔΩú Christina Kim ÔΩú OpenAI Scholars Demo Day 2021 ÔΩú OpenAI</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2021 -
2023 SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics</section><section class=powerby>As an Amazon Associate I earn from qualifying purchases üõí<br>Built with <a href=https://swiest.com/ target=_blank rel=noopener>(Ôæâ‚óï„ÉÆ‚óï)Ôæâü™Ñüíûüíñü•∞ across the glüåçüåèüåébe</a><br></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Serif&family=Noto+Serif+Armenian&family=Noto+Serif+Bengali&family=Noto+Serif+Devanagari&family=Noto+Serif+Georgian&family=Noto+Serif+Gujarati&family=Noto+Serif+HK&family=Noto+Serif+Hebrew&family=Noto+Serif+JP&family=Noto+Serif+KR&family=Noto+Serif+Kannada&family=Noto+Serif+Khmer&family=Noto+Serif+Lao&family=Noto+Serif+Makasar&family=Noto+Serif+Malayalam&family=Noto+Serif+Myanmar&family=Noto+Serif+Oriya&family=Noto+Serif+SC&family=Noto+Serif+Sinhala&family=Noto+Serif+TC&family=Noto+Serif+Tamil&family=Noto+Serif+Telugu&family=Noto+Serif+Thai&family=Noto+Serif+Tibetan&display=swap" rel=stylesheet></body></html>
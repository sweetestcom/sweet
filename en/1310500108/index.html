<!DOCTYPE html>
<html lang="en" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='The following is a conversation with Sergei Levine, a professor at Berkeley and a world
class researcher in deep learning, reinforcement learning, robotics, and computer vision, including
the development of algorithms for end to end training of neural network policies that combine
perception and control, scalable algorithms for inverse reinforcement learning, and, in
general, deep RL algorithms.
Quick summary of the ads.
Two sponsors, Cash App and ExpressVPN.
Please consider supporting the podcast by downloading Cash App and using code LexPodcast'>
<title>Lex Fridman Podcast - #108 ‚Äì Sergey Levine: Robotics and Machine Learning | SWIEST</title>

<link rel='canonical' href='https://swiest.com/en/1310500108/'>

<link rel="stylesheet" href="/scss/style.min.3d39e72bbd386061e6d416f87b164e3ac72f13b441e2fda037853463b75c184c.css"><script>
    document.oncontextmenu = function(){ return false; };
    document.onselectstart = function(){ return false; };
    document.oncopy = function(){ return false; };
    document.oncut = function(){ return false; };
</script>

<script src="https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>


<script type="text/javascript">
    $(document).ready(function(){
     
     $("#back-to-top").hide();
     
     $(function () {
      $(window).scroll(function(){
       if ($(window).scrollTop()>600){
        $("#back-to-top").fadeIn(500);
       }else{
        $("#back-to-top").fadeOut(500);
       }
     });
     
     $("#back-to-top").click(function(){
      $('body,html').animate({scrollTop:0},500);
       return false;
      });
     });
    });
    </script><meta property='og:title' content='Lex Fridman Podcast - #108 ‚Äì Sergey Levine: Robotics and Machine Learning'>
<meta property='og:description' content='The following is a conversation with Sergei Levine, a professor at Berkeley and a world
class researcher in deep learning, reinforcement learning, robotics, and computer vision, including
the development of algorithms for end to end training of neural network policies that combine
perception and control, scalable algorithms for inverse reinforcement learning, and, in
general, deep RL algorithms.
Quick summary of the ads.
Two sponsors, Cash App and ExpressVPN.
Please consider supporting the podcast by downloading Cash App and using code LexPodcast'>
<meta property='og:url' content='https://swiest.com/en/1310500108/'>
<meta property='og:site_name' content='SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='English' /><meta property='article:tag' content='Podcast' /><meta property='article:tag' content='Lex Fridman Podcast' /><meta property='article:published_time' content='2022-06-16T17:00:00&#43;00:00'/><meta property='article:modified_time' content='2022-06-16T17:00:00&#43;00:00'/>
<meta name="twitter:title" content="Lex Fridman Podcast - #108 ‚Äì Sergey Levine: Robotics and Machine Learning">
<meta name="twitter:description" content="The following is a conversation with Sergei Levine, a professor at Berkeley and a world
class researcher in deep learning, reinforcement learning, robotics, and computer vision, including
the development of algorithms for end to end training of neural network policies that combine
perception and control, scalable algorithms for inverse reinforcement learning, and, in
general, deep RL algorithms.
Quick summary of the ads.
Two sponsors, Cash App and ExpressVPN.
Please consider supporting the podcast by downloading Cash App and using code LexPodcast">
    <link rel="shortcut icon" href="/favicon.ico" />
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin="anonymous"></script>
<script type="text/javascript">amzn_assoc_ad_type = "link_enhancement_widget";amzn_assoc_tracking_id = "swiest00-20";amzn_assoc_linkid = "b583d47a9f44ec47064a228dad7fb822";amzn_assoc_placement = "";amzn_assoc_marketplace = "amazon";amzn_assoc_region = "US";</script><script src="//ws-na.amazon-adsystem.com/widgets/q?ServiceVersion=20070822&Operation=GetScript&ID=OneJS&WS=1&MarketPlace=US"></script>
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "dark");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu50a90395ee466aab210c1019489b5a11_223737_300x0_resize_box_3.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">‚ú®</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics</a></h1>
            <h2 class="site-description">üåçüåèüåé</h2>
        </div>
    </header><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        

        <div class="menu-bottom-section">
                <li id="i18n-switch">  
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M4 5h7" />
  <path d="M9 3v2c0 4.418 -2.239 8 -5 8" />
  <path d="M5 9c-.003 2.144 2.952 3.908 6.7 4" />
  <path d="M12 20l4 -9l4 9" />
  <path d="M19.1 18h-6.2" />
</svg>



                    <select name="language" onchange="window.location.href = this.selectedOptions[0].value">
                        
                            <option value="https://swiest.com/" selected>English</option>
                        
                            <option value="https://swiest.com/af/" >Afrikaans</option>
                        
                            <option value="https://swiest.com/ar/" >ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</option>
                        
                            <option value="https://swiest.com/az/" >Az…ôrbaycan</option>
                        
                            <option value="https://swiest.com/be/" >–±–µ–ª–∞—Ä—É—Å–∫—ñ</option>
                        
                            <option value="https://swiest.com/bg/" >–±—ä–ª–≥–∞—Ä—Å–∫–∏</option>
                        
                            <option value="https://swiest.com/bn/" >‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ</option>
                        
                            <option value="https://swiest.com/ca/" >Catal√†</option>
                        
                            <option value="https://swiest.com/zh-hans/" >ÁÆÄ‰Ωì‰∏≠Êñá</option>
                        
                            <option value="https://swiest.com/zh-hant/" >ÁπÅÈ´î‰∏≠Êñá</option>
                        
                            <option value="https://swiest.com/cs/" >ƒåe≈°tina</option>
                        
                            <option value="https://swiest.com/da/" >Dansk</option>
                        
                            <option value="https://swiest.com/de/" >Deutsch</option>
                        
                            <option value="https://swiest.com/el/" >ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨</option>
                        
                            <option value="https://swiest.com/es/" >Espa√±ol</option>
                        
                            <option value="https://swiest.com/et/" >Eesti</option>
                        
                            <option value="https://swiest.com/fa/" >ŸÅÿßÿ±ÿ≥€å</option>
                        
                            <option value="https://swiest.com/fi/" >Suomi</option>
                        
                            <option value="https://swiest.com/fr/" >Fran√ßais</option>
                        
                            <option value="https://swiest.com/gu/" >‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä</option>
                        
                            <option value="https://swiest.com/he/" >◊¢÷¥◊ë◊®÷¥◊ô◊™</option>
                        
                            <option value="https://swiest.com/hi/" >‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</option>
                        
                            <option value="https://swiest.com/hr/" >Hrvatski</option>
                        
                            <option value="https://swiest.com/hu/" >Magyar</option>
                        
                            <option value="https://swiest.com/hy/" >’Ä’°’µ’•÷Ä’•’∂</option>
                        
                            <option value="https://swiest.com/id/" >Bahasa Indonesia</option>
                        
                            <option value="https://swiest.com/is/" >√çslenska</option>
                        
                            <option value="https://swiest.com/it/" >Italiano</option>
                        
                            <option value="https://swiest.com/ja/" >Êó•Êú¨Ë™û</option>
                        
                            <option value="https://swiest.com/km/" >·ûÄ·ûò·üí·ûñ·ûª·ûá·û∂·üî</option>
                        
                            <option value="https://swiest.com/kn/" >‡≤ï‡≤®‡≥ç‡≤®‡≤°</option>
                        
                            <option value="https://swiest.com/ko/" >ÌïúÍµ≠Ïñ¥</option>
                        
                            <option value="https://swiest.com/ku/" >⁄©Ÿàÿ±ÿØ€å</option>
                        
                            <option value="https://swiest.com/lo/" >‡∫û‡∫≤‡∫™‡∫≤‡∫•‡∫≤‡∫ß</option>
                        
                            <option value="https://swiest.com/lt/" >Lietuvi≈≥</option>
                        
                            <option value="https://swiest.com/lv/" >Latvie≈°u</option>
                        
                            <option value="https://swiest.com/ml/" >‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç</option>
                        
                            <option value="https://swiest.com/mn/" >–ú–æ–Ω–≥–æ–ª</option>
                        
                            <option value="https://swiest.com/mr/" >‡§Æ‡§∞‡§æ‡§†‡•Ä</option>
                        
                            <option value="https://swiest.com/ms/" >Bahasa Melayu</option>
                        
                            <option value="https://swiest.com/my/" >·Äô·Äº·Äî·Ä∫·Äô·Ä¨</option>
                        
                            <option value="https://swiest.com/ne/" >‡§®‡•á‡§™‡§æ‡§≤‡•Ä</option>
                        
                            <option value="https://swiest.com/nl/" >Nederlands</option>
                        
                            <option value="https://swiest.com/no/" >Norsk</option>
                        
                            <option value="https://swiest.com/pa/" >‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä</option>
                        
                            <option value="https://swiest.com/pl/" >Polski</option>
                        
                            <option value="https://swiest.com/pt-br/" >Portugu√™s Brasil</option>
                        
                            <option value="https://swiest.com/pt-pt/" >Portugu√™s</option>
                        
                            <option value="https://swiest.com/ro/" >Rom√¢nƒÉ</option>
                        
                            <option value="https://swiest.com/ru/" >–†—É—Å—Å–∫–∏–π</option>
                        
                            <option value="https://swiest.com/sk/" >Slovenƒçina</option>
                        
                            <option value="https://swiest.com/sl/" >Sloven≈°ƒçina</option>
                        
                            <option value="https://swiest.com/sq/" >Shqip</option>
                        
                            <option value="https://swiest.com/sr/" >–°—Ä–ø—Å–∫–∏</option>
                        
                            <option value="https://swiest.com/sv/" >Svenska</option>
                        
                            <option value="https://swiest.com/sw/" >Kiswahili</option>
                        
                            <option value="https://swiest.com/ta/" >‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç</option>
                        
                            <option value="https://swiest.com/te/" >‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</option>
                        
                            <option value="https://swiest.com/th/" >‡πÑ‡∏ó‡∏¢</option>
                        
                            <option value="https://swiest.com/tl/" >Filipino</option>
                        
                            <option value="https://swiest.com/tr/" >T√ºrk√ße</option>
                        
                            <option value="https://swiest.com/uk/" >–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</option>
                        
                            <option value="https://swiest.com/ur/" >ÿßÿ±ÿØŸà</option>
                        
                            <option value="https://swiest.com/uz/" >O&#39;zbekcha</option>
                        
                            <option value="https://swiest.com/vi/" >Ti·∫øng Vi·ªát</option>
                        
                            <option value="https://swiest.com/zh-hk/" >Á≤µË™û</option>
                        
                    </select>
                </li>
            
            
            
                <li id="dark-mode-toggle">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <span>Dark Mode</span>
                </li>
            
        </div>
    </ol>
</aside>

    

            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/podcast/" >
                Podcast
            </a>
        
            <a href="/categories/lex-fridman-podcast/" >
                Lex Fridman Podcast
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/en/1310500108/">Lex Fridman Podcast - #108 ‚Äì Sergey Levine: Robotics and Machine Learning</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2022-06-16</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    82 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <p>The following is a conversation with Sergei Levine, a professor at Berkeley and a world</p>
<p>class researcher in deep learning, reinforcement learning, robotics, and computer vision, including</p>
<p>the development of algorithms for end to end training of neural network policies that combine</p>
<p>perception and control, scalable algorithms for inverse reinforcement learning, and, in</p>
<p>general, deep RL algorithms.</p>
<p>Quick summary of the ads.</p>
<p>Two sponsors, Cash App and ExpressVPN.</p>
<p>Please consider supporting the podcast by downloading Cash App and using code LexPodcast</p>
<p>and signing up at expressvpn.com slash lexpod.</p>
<p>Click the links, buy the stuff, it&rsquo;s the best way to support this podcast and, in general,</p>
<p>the journey I&rsquo;m on.</p>
<p>If you enjoy this thing, subscribe on YouTube, review it with 5 stars on Apple Podcast, follow</p>
<p>on Spotify, support it on Patreon, or connect with me on Twitter at lexfriedman.</p>
<p>As usual, I&rsquo;ll do a few minutes of ads now and never any ads in the middle that can break</p>
<p>the flow of the conversation.</p>
<p>This show is presented by Cash App, the number one finance app in the App Store.</p>
<p>When you get it, use code lexpodcast.</p>
<p>Cash App lets you send money to friends, buy Bitcoin, and invest in the stock market with</p>
<p>as little as one dollar.</p>
<p>Since Cash App does fractional share trading, let me mention that the order execution algorithm</p>
<p>that works behind the scenes to create the abstraction of fractional orders is an algorithmic</p>
<p>marvel.</p>
<p>So, big props to the Cash App engineers for taking a step up to the next layer of abstraction</p>
<p>over the stock market, making trading more accessible for new investors and diversification</p>
<p>much easier.</p>
<p>So, again, if you get Cash App from the App Store or Google Play and use the code lexpodcast,</p>
<p>you get $10, and Cash App will also donate $10 to FIRST, an organization that is helping</p>
<p>to advance robotics and STEM education for young people around the world.</p>
<p>This show is also sponsored by ExpressVPN.</p>
<p>Get it at expressvpn.com slash lexpod to support this podcast and to get an extra three months</p>
<p>free on a one year package.</p>
<p>I&rsquo;ve been using ExpressVPN for many years.</p>
<p>I love it.</p>
<p>I think ExpressVPN is the best VPN out there.</p>
<p>They told me to say it, but it happens to be true in my humble opinion.</p>
<p>It doesn&rsquo;t log your data, it&rsquo;s crazy fast, and it&rsquo;s easy to use literally just one big</p>
<p>power on button.</p>
<p>Again, it&rsquo;s probably obvious to you, but I should say it again, it&rsquo;s really important</p>
<p>that they don&rsquo;t log your data.</p>
<p>It works on Linux and every other operating system, but Linux, of course, is the best</p>
<p>operating system.</p>
<p>Shout out to my favorite flavor, Ubuntu Mate 2004.</p>
<p>Once again, get it at expressvpn.com slash lexpod to support this podcast and to get</p>
<p>an extra three months free on a one year package.</p>
<p>And now, here&rsquo;s my conversation with Sergey Levine.</p>
<p>What&rsquo;s the difference between a state of the art human, such as you and I, well, I don&rsquo;t</p>
<p>know if we qualify as state of the art humans, but a state of the art human and a state of</p>
<p>the art robot?</p>
<p>That&rsquo;s a very interesting question.</p>
<p>Robot capability is, it&rsquo;s kind of a, I think it&rsquo;s a very tricky thing to understand because</p>
<p>there are some things that are difficult that we wouldn&rsquo;t think are difficult and some things</p>
<p>that are easy that we wouldn&rsquo;t think are easy.</p>
<p>And there&rsquo;s also a really big gap between capabilities of robots in terms of hardware</p>
<p>and their physical capability and capabilities of robots in terms of what they can do autonomously.</p>
<p>There is a little video that I think robotics researchers really like to show, especially</p>
<p>robotics learning researchers like myself, from 2004 from Stanford, which demonstrates</p>
<p>a prototype robot called the PR1, and the PR1 was a robot that was designed as a home</p>
<p>assistance robot.</p>
<p>And there&rsquo;s this beautiful video showing the PR1 tidying up a living room, putting away</p>
<p>toys and at the end bringing a beer to the person sitting on the couch, which looks really</p>
<p>amazing.</p>
<p>And then the punchline is that this robot is entirely controlled by a person.</p>
<p>So in some ways the gap between a state of the art human and state of the art robot,</p>
<p>if the robot has a human brain, is actually not that large.</p>
<p>Now obviously like human bodies are sophisticated and very robust and resilient in many ways,</p>
<p>but on the whole, if we&rsquo;re willing to like spend a bit of money and do a bit of engineering,</p>
<p>we can kind of close the hardware gap almost.</p>
<p>But the intelligence gap, that one is very wide.</p>
<p>And when you say hardware, you&rsquo;re referring to the physical, sort of the actuators, the</p>
<p>actual body of the robot, as opposed to the hardware on which the cognition, the hardware</p>
<p>of the nervous system.</p>
<p>Yes, exactly.</p>
<p>I&rsquo;m referring to the body rather than the mind.</p>
<p>So that means that the kind of the work is cut out for us.</p>
<p>Like while we can still make the body better, we kind of know that the big bottleneck right</p>
<p>now is really the mind.</p>
<p>And how big is that gap?</p>
<p>How big is the difference in your sense of ability to learn, ability to reason, ability</p>
<p>to perceive the world between humans and our best robots?</p>
<p>The gap is very large and the gap becomes larger the more unexpected events can happen</p>
<p>in the world.</p>
<p>So essentially the spectrum along which you can measure the size of that gap is the spectrum</p>
<p>of how open the world is.</p>
<p>If you control everything in the world very tightly, if you put the robot in like a factory</p>
<p>and you tell it where everything is and you rigidly program its motion, then it can do</p>
<p>things, you know, one might even say in a superhuman way.</p>
<p>It can move faster, it&rsquo;s stronger, it can lift up a car and things like that.</p>
<p>But as soon as anything starts to vary in the environment, now it&rsquo;ll trip up.</p>
<p>And if many, many things vary like they would like in your kitchen, for example, then things</p>
<p>are pretty much like wide open.</p>
<p>Now, again, we&rsquo;re going to stick a bit on the philosophical questions, but how much</p>
<p>on the human side of the cognitive abilities in your sense is nature versus nurture?</p>
<p>So how much of it is a product of evolution and how much of it is something we&rsquo;ll learn</p>
<p>from sort of scratch from the day we&rsquo;re born?</p>
<p>I&rsquo;m going to read into your question as asking about the implications of this for AI.</p>
<p>Because I&rsquo;m not a biologist, I can&rsquo;t really like speak authoritatively.</p>
<p>So until we go on it, if it&rsquo;s so, if it&rsquo;s all about learning, then there&rsquo;s more hope</p>
<p>for AI.</p>
<p>So the way that I look at this is that, you know, well, first, of course, biology is very</p>
<p>messy.</p>
<p>And it&rsquo;s if you ask the question, how does a person do something or has a person&rsquo;s mind</p>
<p>do something, you can come up with a bunch of hypotheses and oftentimes you can find</p>
<p>support for many different, often conflicting hypotheses.</p>
<p>One way that we can approach the question of what the implications of this for AI are</p>
<p>is we can think about what&rsquo;s sufficient.</p>
<p>So you know, maybe a person is from birth very, very good at some things like, for example,</p>
<p>recognizing faces.</p>
<p>There&rsquo;s a very strong evolutionary pressure to do that.</p>
<p>If you can recognize your mother&rsquo;s face, then you&rsquo;re more likely to survive and therefore</p>
<p>people are good at this.</p>
<p>But we can also ask like, what&rsquo;s the minimum sufficient thing?</p>
<p>And one of the ways that we can study the minimal sufficient thing is we could, for</p>
<p>example, see what people do in unusual situations.</p>
<p>If you present them with things that evolution couldn&rsquo;t have prepared them for, you know,</p>
<p>our daily lives actually do this to us all the time.</p>
<p>We didn&rsquo;t evolve to deal with, you know, automobiles and space flight and whatever.</p>
<p>So there are all these situations that we can find ourselves in and we do very well</p>
<p>there.</p>
<p>Like I can give you a joystick to control a robotic arm, which you&rsquo;ve never used before</p>
<p>and you might be pretty bad for the first couple of seconds.</p>
<p>But if I tell you like your life depends on using this robotic arm to like open this door,</p>
<p>you&rsquo;ll probably manage it.</p>
<p>Even though you&rsquo;ve never seen this device before, you&rsquo;ve never used the joystick control</p>
<p>us and you&rsquo;ll kind of muddle through it.</p>
<p>And that&rsquo;s not your evolved natural ability.</p>
<p>That&rsquo;s your, your flexibility or your adaptability.</p>
<p>And that&rsquo;s exactly where our current robotic systems really kind of fall flat.</p>
<p>But I wonder how much general, almost what we think of as common sense, pre trained models</p>
<p>underneath all of that.</p>
<p>So that ability to adapt to a joystick is, requires you to have a kind of, you know,</p>
<p>I&rsquo;m human.</p>
<p>So it&rsquo;s hard for me to introspect all the knowledge I have about the world, but it seems</p>
<p>like there might be an iceberg underneath of the amount of knowledge we actually bring</p>
<p>to the table.</p>
<p>That&rsquo;s kind of the open question.</p>
<p>There&rsquo;s absolutely an iceberg of knowledge that we bring to the table, but I think it&rsquo;s</p>
<p>very likely that iceberg of knowledge is actually built up over our lifetimes.</p>
<p>Because we have, you know, we have a lot of prior experience to draw on.</p>
<p>And it kind of makes sense that the right way for us to, you know, to optimize our,</p>
<p>our efficiency, our evolutionary fitness and so on is to utilize all of that experience</p>
<p>to build up the best iceberg we can get.</p>
<p>And that&rsquo;s actually one of the, you know, while that sounds an awful lot like what machine</p>
<p>learning actually does, I think that for modern machine learning, it&rsquo;s actually a really big</p>
<p>challenge to take this unstructured mass of experience and distill out something that</p>
<p>looks like a common sense understanding of the world.</p>
<p>And perhaps part of that isn&rsquo;t, it&rsquo;s not because something about machine learning itself is,</p>
<p>is broken or hard, but because we&rsquo;ve been a little too rigid in subscribing to a very</p>
<p>supervised, very rigid notion of learning, you know, kind of the input output, X&rsquo;s go</p>
<p>to Y&rsquo;s sort of model.</p>
<p>And maybe what we really need to do is to view the world more as like a mass of experience</p>
<p>that is not necessarily providing any rigid supervision, but sort of providing many, many</p>
<p>instances of things that could be.</p>
<p>And then you take that and you distill it into some sort of common sense understanding.</p>
<p>I see what you&rsquo;re, you&rsquo;re painting an optimistic, beautiful picture, especially from the robotics</p>
<p>perspective because that means we just need to invest and build better learning algorithms,</p>
<p>figure out how we can get access to more and more data for those learning algorithms to</p>
<p>extract signal from, and then accumulate that iceberg of knowledge.</p>
<p>It&rsquo;s a beautiful picture.</p>
<p>It&rsquo;s a hopeful one.</p>
<p>I think it&rsquo;s potentially a little bit more than just that.</p>
<p>And this is, this is where we perhaps reach the limits of our current understanding.</p>
<p>But one thing that I think that the research community hasn&rsquo;t really resolved in a satisfactory</p>
<p>way is how much it matters where that experience comes from, like, you know, do you just like</p>
<p>download everything on the internet and cram it into essentially the 21st century analog</p>
<p>of the giant language model and then see what happens or does it actually matter whether</p>
<p>your machine physically experiences the world or in the sense that it actually attempts</p>
<p>things, observes the outcome of its actions and kind of augments its experience that way.</p>
<p>And it chooses which parts of the world it gets to interact with and observe and learn</p>
<p>from.</p>
<p>Right.</p>
<p>It may be that the world is so complex that simply obtaining a large mass of sort of</p>
<p>IID samples of the world is a very difficult way to go.</p>
<p>But if you are actually interacting with the world and essentially performing this sort</p>
<p>of hard negative mining by attempting what you think might work, observing the sometimes</p>
<p>happy and sometimes sad outcomes of that and augmenting your understanding using that experience</p>
<p>and you&rsquo;re just doing this continually for many years, maybe that sort of data in some</p>
<p>sense is actually much more favorable to obtaining a common sense understanding.</p>
<p>One reason we might think that this is true is that, you know, what we associate with</p>
<p>common sense or lack of common sense is often characterized by the ability to reason about</p>
<p>kind of counterfactual questions like, you know, if I were to hear this bottle of water</p>
<p>sitting on the table, everything is fine if I were to knock it over, which I&rsquo;m not going</p>
<p>to do.</p>
<p>But if I were to do that, what would happen?</p>
<p>And I know that nothing good would happen from that.</p>
<p>But if I have a bad understanding of the world, I might think that that&rsquo;s a good way for me</p>
<p>to like, you know, gain more utility.</p>
<p>If I actually go about my daily life doing the things that my current understanding of</p>
<p>the world suggests will give me high utility, in some ways, I&rsquo;ll get exactly the right supervision</p>
<p>to tell me not to do those bad things and to keep doing the good things.</p>
<p>So there&rsquo;s a spectrum between IID, random walk through the space of data, and then there&rsquo;s</p>
<p>and what we humans do, I don&rsquo;t even know if we do it optimal, but that might be beyond.</p>
<p>So this open question that you raised, where do you think systems, intelligent systems</p>
<p>that would be able to deal with this world fall?</p>
<p>Can we do pretty well by reading all of Wikipedia, sort of randomly sampling it like language</p>
<p>models do?</p>
<p>Or do we have to be exceptionally selective and intelligent about which aspects of the</p>
<p>world we interact with?</p>
<p>So I think this is first an open scientific problem, and I don&rsquo;t have like a clear answer,</p>
<p>but I can speculate a little bit.</p>
<p>And what I would speculate is that you don&rsquo;t need to be super, super careful.</p>
<p>I think it&rsquo;s less about like, being careful to avoid the useless stuff, and more about</p>
<p>making sure that you hit on the really important stuff.</p>
<p>So perhaps it&rsquo;s okay, if you spend part of your day, just, you know, guided by your curiosity,</p>
<p>reading interesting regions of your state space, but it&rsquo;s important for you to, you</p>
<p>know, every once in a while, make sure that you really try out the solutions that your</p>
<p>current model of the world suggests might be effective, and observe whether those solutions</p>
<p>are working as you expect or not.</p>
<p>And perhaps some of that is really essential to have kind of a perpetual improvement loop.</p>
<p>This perpetual improvement loop is really like, that&rsquo;s really the key, the key that&rsquo;s</p>
<p>going to potentially distinguish the best current methods from the best methods of tomorrow</p>
<p>in a sense.</p>
<p>How important do you think is exploration or total out of the box thinking exploration</p>
<p>in this space as you jump to totally different domains?</p>
<p>So you kind of mentioned there&rsquo;s an optimization problem, you kind of kind of explore the specifics</p>
<p>of a particular strategy, whatever the thing you&rsquo;re trying to solve.</p>
<p>How important is it to explore totally outside of the strategies that have been working for</p>
<p>you so far?</p>
<p>What&rsquo;s your intuition there?</p>
<p>Yeah, I think it&rsquo;s a very problem dependent kind of question.</p>
<p>And I think that that&rsquo;s actually, you know, in some ways that question gets at one of</p>
<p>the big differences between sort of the classic formulation of a reinforcement learning problem</p>
<p>and some of the sort of more open ended reformulations of that problem that have been explored in</p>
<p>recent years.</p>
<p>So classically reinforcement learning is framed as a problem of maximizing utility, like any</p>
<p>kind of rational AI agent, and then anything you do is in service to maximizing that utility.</p>
<p>But a very interesting kind of way to look at, I&rsquo;m not necessarily saying this is the</p>
<p>best way to look at it, but an interesting alternative way to look at these problems</p>
<p>is as something where you first get to explore the world, however you please, and then afterwards</p>
<p>you will be tasked with doing something.</p>
<p>And that might suggest a somewhat different solution.</p>
<p>So if you don&rsquo;t know what you&rsquo;re going to be tasked with doing, and you just want to</p>
<p>prepare yourself optimally for whatever your uncertain future holds, maybe then you will</p>
<p>choose to attain some sort of coverage, build up sort of an arsenal of cognitive tools,</p>
<p>if you will, such that later on when someone tells you, now your job is to fetch the coffee</p>
<p>for me, you will be well prepared to undertake that task.</p>
<p>And that you see that as the modern formulation of the reinforcement learning problem, as</p>
<p>a kind of the more multitask, the general intelligence kind of formulation.</p>
<p>I think that&rsquo;s one possible vision of where things might be headed.</p>
<p>I don&rsquo;t think that&rsquo;s by any means the mainstream or standard way of doing things, and it&rsquo;s</p>
<p>not like if I had to&hellip;</p>
<p>But I like it.</p>
<p>It&rsquo;s a beautiful vision.</p>
<p>So maybe you actually take a step back.</p>
<p>What is the goal of robotics?</p>
<p>What&rsquo;s the general problem of robotics we&rsquo;re trying to solve?</p>
<p>You actually kind of painted two pictures here.</p>
<p>One of sort of the narrow, one of the general.</p>
<p>What in your view is the big problem of robotics?</p>
<p>And ridiculously philosophical high level questions.</p>
<p>I think that maybe there are two ways I can answer this question.</p>
<p>One is there&rsquo;s a very pragmatic problem, which is like what would make robots, what would</p>
<p>sort of maximize the usefulness of robots?</p>
<p>And there the answer might be something like a system where a system that can perform whatever</p>
<p>task a human user sets for it, within the physical constraints, of course.</p>
<p>If you tell it to teleport to another planet, it probably can&rsquo;t do that.</p>
<p>But if you ask it to do something that&rsquo;s within its physical capability, then potentially</p>
<p>with a little bit of additional training or a little bit of additional trial and error,</p>
<p>it ought to be able to figure it out in much the same way as like a human teleoperator</p>
<p>ought to figure out how to drive the robot to do that.</p>
<p>That&rsquo;s kind of the very pragmatic view of what it would take to kind of solve the robotics</p>
<p>problem, if you will.</p>
<p>But I think that there is a second answer, and that answer is a lot closer to why I want</p>
<p>to work on robotics, which is that I think it&rsquo;s less about what it would take to do a</p>
<p>really good job in the world of robotics, but more the other way around, what robotics</p>
<p>can bring to the table to help us understand artificial intelligence.</p>
<p>So your dream fundamentally is to understand intelligence?</p>
<p>Yes.</p>
<p>And I think that&rsquo;s the dream for many people who actually work in this space.</p>
<p>I think that there&rsquo;s something very pragmatic and very useful about studying robotics, but</p>
<p>I do think that a lot of people that go into this field actually, you know, the things</p>
<p>that they draw inspiration from are the potential for robots to like help us learn about intelligence</p>
<p>and about ourselves.</p>
<p>So that&rsquo;s fascinating that robotics is basically the space by which you can get closer to understanding</p>
<p>the fundamentals of artificial intelligence.</p>
<p>So what is it about robotics that&rsquo;s different from some of the other approaches?</p>
<p>So if we look at some of the early breakthroughs in deep learning or in the computer vision</p>
<p>space and the natural language processing, there&rsquo;s really nice clean benchmarks that</p>
<p>a lot of people competed on and thereby came up with a lot of brilliant ideas.</p>
<p>What&rsquo;s the fundamental difference to you between computer vision purely defined and ImageNet</p>
<p>and kind of the bigger robotics problem?</p>
<p>So there are a couple of things.</p>
<p>One is that with robotics, you kind of have to take away many of the crutches.</p>
<p>So you have to deal with both the particular problems of perception control and so on,</p>
<p>but you also have to deal with the integration of those things.</p>
<p>And you know, classically, we&rsquo;ve always thought of the integration as kind of a separate problem.</p>
<p>So a classic kind of modular engineering approach is that we solve the individual subproblems</p>
<p>and then wire them together and then the whole thing works.</p>
<p>And one of the things that we&rsquo;ve been seeing over the last couple of decades is that, well,</p>
<p>maybe studying the thing as a whole might lead to just like very different solutions</p>
<p>than if we were to study the parts and wire them together.</p>
<p>So the integrative nature of robotics research helps us see, you know, the different perspectives</p>
<p>on the problem.</p>
<p>Another part of the answer is that with robotics, it casts a certain paradox into very clever</p>
<p>relief.</p>
<p>This is sometimes referred to as Moravec&rsquo;s paradox, the idea that in artificial intelligence,</p>
<p>things that are very hard for people can be very easy for machines and vice versa.</p>
<p>Things that are very easy for people can be very hard for machines.</p>
<p>So you know, integral and differential calculus is pretty difficult to learn for people.</p>
<p>But if you program a computer, do it, it can derive derivatives and integrals for you all</p>
<p>day long without any trouble.</p>
<p>Whereas some things like, you know, drinking from a cup of water, very easy for a person</p>
<p>to do, very hard for a robot to deal with.</p>
<p>And sometimes when we see such blatant discrepancies, that gives us a really strong hint that we&rsquo;re</p>
<p>missing something important.</p>
<p>So if we really try to zero in on those discrepancies, we might find that little bit that we&rsquo;re missing.</p>
<p>And it&rsquo;s not that we need to make machines better or worse at math and better at drinking</p>
<p>water, but just that by studying those discrepancies, we might find some new insight.</p>
<p>So that could be in any space, it doesn&rsquo;t have to be robotics.</p>
<p>But you&rsquo;re saying, I mean, it&rsquo;s kind of interesting that robotics seems to have a lot of those</p>
<p>discrepancies.</p>
<p>So the Hans Marvak paradox is probably referring to the space of the physical interaction,</p>
<p>like you said, object manipulation, walking, all the kind of stuff we do in the physical</p>
<p>world.</p>
<p>How do you make sense if you were to try to disentangle the Marvak paradox, like why is</p>
<p>there such a gap in our intuition about it?</p>
<p>Why do you think manipulating objects is so hard from everything you&rsquo;ve learned from applying</p>
<p>reinforcement learning in this space?</p>
<p>Yeah, I think that one reason is maybe that for many of the other problems that we&rsquo;ve</p>
<p>studied in AI and computer science and so on, the notion of input output and supervision</p>
<p>is much, much cleaner.</p>
<p>So computer vision, for example, deals with very complex inputs.</p>
<p>But it&rsquo;s comparatively a bit easier, at least up to some level of abstraction, to cast it</p>
<p>as a very tightly supervised problem.</p>
<p>It&rsquo;s comparatively much, much harder to cast robotic manipulation as a very tightly supervised</p>
<p>problem.</p>
<p>You can do it, it just doesn&rsquo;t seem to work all that well.</p>
<p>So you could say that, well, maybe we get a labeled data set where we know exactly which</p>
<p>motor commands to send, and then we train on that.</p>
<p>But for various reasons, that&rsquo;s not actually such a great solution.</p>
<p>And it also doesn&rsquo;t seem to be even remotely similar to how people and animals learn to</p>
<p>do things, because we&rsquo;re not told by our parents, here&rsquo;s how you fire your muscles in order</p>
<p>to walk.</p>
<p>So we do get some guidance, but the really low level detailed stuff we figure out mostly</p>
<p>on our own.</p>
<p>And that&rsquo;s what you mean by tightly coupled, that every single little sub action gets a</p>
<p>supervised signal of whether it&rsquo;s a good one or not.</p>
<p>Right.</p>
<p>So while in computer vision, you could sort of imagine up to a level of abstraction that</p>
<p>maybe somebody told you this is a car and this is a cat and this is a dog, in motor</p>
<p>control, it&rsquo;s very clear that that was not the case.</p>
<p>If we look at sort of the sub spaces of robotics, that, again, as you said, robotics integrates</p>
<p>all of them together, and we get to see how this beautiful mess interplays.</p>
<p>But so there&rsquo;s nevertheless still perception.</p>
<p>So it&rsquo;s the computer vision problem, broadly speaking, understanding the environment.</p>
<p>And there&rsquo;s also maybe you can correct me on this kind of categorization of the space,</p>
<p>and there&rsquo;s prediction in trying to anticipate what things are going to do into the future</p>
<p>in order for you to be able to act in that world.</p>
<p>And then there&rsquo;s also this game theoretic aspect of how your actions will change the</p>
<p>behavior of others.</p>
<p>In this kind of space, what, and this is bigger than reinforcement learning, this is just</p>
<p>broadly looking at the problem of robotics, what&rsquo;s the hardest problem here?</p>
<p>Or is there, or is what you said true that when you start to look at all of them together,</p>
<p>that&rsquo;s a whole nother thing, like you can&rsquo;t even say which one individually is harder</p>
<p>because all of them together, you should only be looking at them all together.</p>
<p>I think when you look at them all together, some things actually become easier.</p>
<p>And I think that&rsquo;s actually pretty important.</p>
<p>So we had back in 2014, we had some work, basically our first work on end to end reinforcement</p>
<p>learning for robotic manipulation skills from vision, which at the time was something that</p>
<p>seemed a little inflammatory and controversial in the robotics world.</p>
<p>But other than the inflammatory and controversial part of it, the point that we were actually</p>
<p>trying to make in that work is that for the particular case of combining perception and</p>
<p>control, you could actually do better if you treat them together than if you try to separate</p>
<p>them.</p>
<p>And the way that we tried to demonstrate this is we picked a fairly simple motor control</p>
<p>task where a robot had to insert a little red trapezoid into a trapezoidal hole.</p>
<p>And we had our separated solution, which involved first detecting the hole using a pose detector</p>
<p>and then actuating the arm to put it in.</p>
<p>And then our intent solution, which just mapped pixels to the torques.</p>
<p>And one of the things we observed is that if you use the intent solution, essentially</p>
<p>the pressure on the perception part of the model is actually lower.</p>
<p>Like it doesn&rsquo;t have to figure out exactly where the thing is in 3D space.</p>
<p>It just needs to figure out where it is, you know, distributing the errors in such a way</p>
<p>that the horizontal difference matters more than the vertical difference because vertically</p>
<p>it just pushes it down all the way until it can&rsquo;t go any further.</p>
<p>And their perceptual errors are a lot less harmful, whereas perpendicular to the direction</p>
<p>of motion, perceptual errors are much more harmful.</p>
<p>So the point is that if you combine these two things, you can trade off errors between</p>
<p>the components optimally to best accomplish the task.</p>
<p>And the components can actually be weaker while still leading to better overall performance.</p>
<p>It&rsquo;s a profound idea.</p>
<p>I mean, in the space of pegs and things like that, it&rsquo;s quite simple.</p>
<p>It almost is tempting to overlook, but that seems to be at least intuitively an idea that</p>
<p>should generalize to basically all aspects of perception and control, that one strengthens</p>
<p>the other.</p>
<p>Yeah.</p>
<p>And we, you know, people who have studied sort of perceptual heuristics in humans and</p>
<p>animals find things like that all the time.</p>
<p>So one very well known example of this is something called the gaze heuristic, which</p>
<p>is a little trick that you can use to intercept a flying object.</p>
<p>So if you want to catch a ball, for instance, you could try to localize it in 3D space,</p>
<p>estimate its velocity, estimate the effect of wind resistance, solve a complex system</p>
<p>of differential equations in your head.</p>
<p>Or you can maintain a running speed so that the object stays in the same position as in</p>
<p>your field of view.</p>
<p>So if it dips a little bit, you speed up.</p>
<p>If it rises a little bit, you slow down.</p>
<p>And if you follow the simple rule, you&rsquo;ll actually arrive at exactly the place where</p>
<p>the object lands and you&rsquo;ll catch it.</p>
<p>And humans use it when they play baseball, human pilots use it when they fly airplanes</p>
<p>to figure out if they&rsquo;re about to collide with somebody, frogs use this to catch insects</p>
<p>and so on and so on.</p>
<p>So this is something that actually happens in nature.</p>
<p>And I&rsquo;m sure this is just one instance of it that we were able to identify just because</p>
<p>all the scientists were able to identify because it&rsquo;s so prevalent, but there are probably</p>
<p>many others.</p>
<p>Do you have a, just so we can zoom in as we talk about robotics, do you have a canonical</p>
<p>problem, sort of a simple, clean, beautiful representative problem in robotics that you</p>
<p>think about when you&rsquo;re thinking about some of these problems?</p>
<p>We talked about robotic manipulation, to me that seems intuitively, at least the robotics</p>
<p>community has converged towards that as a space that&rsquo;s the canonical problem.</p>
<p>If you agree, then maybe do you zoom in in some particular aspect of that problem that</p>
<p>you just like?</p>
<p>Like if we solve that problem perfectly, it&rsquo;ll unlock a major step towards human level intelligence.</p>
<p>I don&rsquo;t think I have like a really great answer to that.</p>
<p>And I think partly the reason I don&rsquo;t have a great answer kind of has to do with the,</p>
<p>it has to do with the fact that the difficulty is really in the flexibility and adaptability</p>
<p>rather than in doing a particular thing really, really well.</p>
<p>So it&rsquo;s hard to just say like, oh, if you can, I don&rsquo;t know, like shuffle a deck of</p>
<p>cards as fast as like a Vegas casino dealer, then you&rsquo;ll be very proficient.</p>
<p>It&rsquo;s really the ability to quickly figure out how to do some arbitrary new thing well</p>
<p>enough to like, you know, to move on to the next arbitrary thing.</p>
<p>But the source of newness and uncertainty, have you found problems in which it&rsquo;s easy</p>
<p>to generate new newnessnesses?</p>
<p>New types of newness.</p>
<p>Yeah.</p>
<p>So a few years ago, so if you had asked me this question around like 2016, maybe I would</p>
<p>have probably said that robotic grasping is a really great example of that because it&rsquo;s</p>
<p>a task with great real world utility.</p>
<p>Like you will get a lot of money if you can do it well.</p>
<p>What is robotic grasping?</p>
<p>Picking up any object with a robotic hand.</p>
<p>Exactly.</p>
<p>So you will get a lot of money if you do it well, because lots of people want to run warehouses</p>
<p>with robots and it&rsquo;s highly non trivial because very different objects will require very different</p>
<p>grasping strategies.</p>
<p>But actually since then, people have gotten really good at building systems to solve this</p>
<p>problem to the point where I&rsquo;m not actually sure how much more progress we can make with</p>
<p>that as like the main guiding thing.</p>
<p>But it&rsquo;s kind of interesting to see the kind of methods that have actually worked well</p>
<p>in that space because robotic grasping classically used to be regarded very much as kind of almost</p>
<p>like a geometry problem.</p>
<p>So people who have studied the history of computer vision will find this very familiar</p>
<p>that it&rsquo;s kind of in the same way that in the early days of computer vision, people</p>
<p>thought of it very much as like an inverse graphics thing.</p>
<p>In robotic grasping, people thought of it as an inverse physics problem essentially.</p>
<p>You look at what&rsquo;s in front of you, figure out the shapes, then use your best estimate</p>
<p>of the laws of physics to figure out where to put your fingers on, you pick up the thing.</p>
<p>And it turns out that works really well for robotic grasping instantiated in many different</p>
<p>recent works, including our own, but also ones from many other labs is to use learning</p>
<p>methods with some combination of either exhaustive simulation or like actual real world trial</p>
<p>and error.</p>
<p>And it turns out that those things actually work really well and then you don&rsquo;t have to</p>
<p>worry about solving geometry problems or physics problems.</p>
<p>What are, just by the way, in the grasping, what are the difficulties that have been worked</p>
<p>on?</p>
<p>So one is like the materials of things, maybe occlusions on the perception side.</p>
<p>Why is it such a difficult, why is picking stuff up such a difficult problem?</p>
<p>Yeah, it&rsquo;s a difficult problem because the number of things that you might have to deal</p>
<p>with or the variety of things that you have to deal with is extremely large.</p>
<p>And oftentimes things that work for one class of objects won&rsquo;t work for other classes of</p>
<p>objects.</p>
<p>So if you, if you get really good at picking up boxes and now you have to pick up plastic</p>
<p>bags, you know, you just need to employ a very different strategy.</p>
<p>And there are many properties of objects that are more than just their geometry that has</p>
<p>to do with, you know, the bits that are easier to pick up, the bits that are hard to pick</p>
<p>up, the bits that are more flexible, the bits that will cause the thing to pivot and bend</p>
<p>and drop out of your hand versus the bits that result in a nice secure grasp.</p>
<p>Things that are flexible, things that if you pick them up the wrong way, they&rsquo;ll fall upside</p>
<p>down and the contents will spill out.</p>
<p>So there&rsquo;s all these little details that come up, but the task is still kind of can be characterized</p>
<p>as one task.</p>
<p>Like there&rsquo;s a very clear notion of you did it or you didn&rsquo;t do it.</p>
<p>So in terms of spilling things, there creeps in this notion that starts to sound and feel</p>
<p>like common sense reasoning.</p>
<p>Do you think solving the general problem of robotics requires common sense reasoning,</p>
<p>requires general intelligence, this kind of human level capability of, you know, like</p>
<p>you said, be robust and deal with uncertainty, but also be able to sort of reason and assimilate</p>
<p>different pieces of knowledge that you have?</p>
<p>Yeah.</p>
<p>What are your thoughts on the needs?</p>
<p>Of common sense reasoning in the space of the general robotics problem?</p>
<p>So I&rsquo;m going to slightly dodge that question and say that I think maybe actually it&rsquo;s the</p>
<p>other way around is that studying robotics can help us understand how to put common sense</p>
<p>into our AI systems.</p>
<p>One way to think about common sense is that, and why our current systems might lack common</p>
<p>sense is that common sense is an emergent property of actually having to interact with</p>
<p>a particular world, a particular universe, and get things done in that universe.</p>
<p>So you might think that, for instance, like an image captioning system, maybe it looks</p>
<p>at pictures of the world and it types out English sentences.</p>
<p>So it kind of deals with our world.</p>
<p>And then you can easily construct situations where image captioning systems do things that</p>
<p>defy common sense, like give it a picture of a person wearing a fur coat and we&rsquo;ll say</p>
<p>it&rsquo;s a teddy bear.</p>
<p>But I think what&rsquo;s really happening in those settings is that the system doesn&rsquo;t actually</p>
<p>live in our world.</p>
<p>It lives in its own world that consists of pixels and English sentences and doesn&rsquo;t actually</p>
<p>consist of having to put on a fur coat in the winter so you don&rsquo;t get cold.</p>
<p>So perhaps the reason for the disconnect is that the systems that we have now simply inhabit</p>
<p>a different universe.</p>
<p>And if we build AI systems that are forced to deal with all of the messiness and complexity</p>
<p>of our universe, maybe they will have to acquire common sense to essentially maximize their</p>
<p>utility.</p>
<p>Whereas the systems we&rsquo;re building now don&rsquo;t have to do that.</p>
<p>They can take some shortcuts.</p>
<p>That&rsquo;s fascinating.</p>
<p>You&rsquo;ve a couple of times already sort of reframed the role of robotics in this whole thing.</p>
<p>And for some reason, I don&rsquo;t know if my way of thinking is common, but I thought like</p>
<p>we need to understand and solve intelligence in order to solve robotics.</p>
<p>And you&rsquo;re kind of framing it as, no, robotics is one of the best ways to just study artificial</p>
<p>intelligence and build sort of like, robotics is like the right space in which you get to</p>
<p>explore some of the fundamental learning mechanisms, fundamental sort of multimodal multitask aggregation</p>
<p>of knowledge mechanisms that are required for general intelligence.</p>
<p>It&rsquo;s really interesting way to think about it, but let me ask about learning.</p>
<p>Can the general sort of robotics, the epitome of the robotics problem be solved purely through</p>
<p>learning, perhaps end to end learning, sort of learning from scratch as opposed to injecting</p>
<p>human expertise and rules and heuristics and so on?</p>
<p>I think that in terms of the spirit of the question, I would say yes.</p>
<p>I mean, I think that though in some ways it&rsquo;s maybe like an overly sharp dichotomy, I think</p>
<p>that in some ways when we build algorithms, at some point a person does something, a person</p>
<p>turned on the computer, a person implemented a TensorFlow.</p>
<p>But yeah, I think that in terms of the point that you&rsquo;re getting at, I do think the answer</p>
<p>is yes.</p>
<p>I think that we can solve many problems that have previously required meticulous manual</p>
<p>engineering through automated optimization techniques.</p>
<p>And actually one thing I will say on this topic is I don&rsquo;t think this is actually a</p>
<p>very radical or very new idea.</p>
<p>I think people have been thinking about automated optimization techniques as a way to do control</p>
<p>for a very, very long time.</p>
<p>And in some ways what&rsquo;s changed is really more the name.</p>
<p>So today we would say that, oh, my robot does machine learning, it does reinforcement learning.</p>
<p>Maybe in the 1960s you&rsquo;d say, oh, my robot is doing optimal control.</p>
<p>And maybe the difference between typing out a system of differential equations and doing</p>
<p>feedback linearization versus training a neural net, maybe it&rsquo;s not such a large difference.</p>
<p>It&rsquo;s just pushing the optimization deeper and deeper into the thing.</p>
<p>Well, it&rsquo;s interesting you think that way, but especially with deep learning that the</p>
<p>accumulation of sort of experiences in data form to form deep representations starts to</p>
<p>feel like knowledge as opposed to optimal control.</p>
<p>So this feels like there&rsquo;s an accumulation of knowledge through the learning process.</p>
<p>Yes.</p>
<p>Yeah.</p>
<p>So I think that is a good point.</p>
<p>That one big difference between learning based systems and classic optimal control systems</p>
<p>is that learning based systems in principle should get better and better the more they</p>
<p>do something.</p>
<p>Right.</p>
<p>And I do think that that&rsquo;s actually a very, very powerful difference.</p>
<p>So if we look back at the world of expert systems and symbolic AI and so on of using</p>
<p>logic to accumulate expertise, human expertise, human encoded expertise, do you think that</p>
<p>will have a role at some point?</p>
<p>The deep learning, machine learning, reinforcement learning has shown incredible results and</p>
<p>breakthroughs and just inspired thousands, maybe millions of researchers.</p>
<p>But there&rsquo;s this less popular now, but it used to be popular idea of symbolic AI.</p>
<p>Do you think that will have a role?</p>
<p>I think in some ways the descendants of symbolic AI actually already have a role.</p>
<p>So this is the highly biased history from my perspective.</p>
<p>You say that, well, initially we thought that rational decision making involves logical</p>
<p>manipulation.</p>
<p>So you have some model of the world expressed in terms of logic.</p>
<p>You have some query, like what action do I take in order for X to be true?</p>
<p>And then you manipulate your logical symbolic representation to get an answer.</p>
<p>What that turned into somewhere in the 1990s is, well, instead of building kind of predicates</p>
<p>and statements that have true or false values, we&rsquo;ll build probabilistic systems where things</p>
<p>have probabilities associated and probabilities of being true and false.</p>
<p>And that turned into Bayes nets.</p>
<p>And that provided sort of a boost to what were really still essentially logical inference</p>
<p>systems, just probabilistic logical inference systems.</p>
<p>And then people said, well, let&rsquo;s actually learn the individual probabilities inside</p>
<p>these models.</p>
<p>And then people said, well, let&rsquo;s not even specify the nodes in the models, let&rsquo;s just</p>
<p>put a big neural net in there.</p>
<p>But in many ways, I see these as actually kind of descendants from the same idea.</p>
<p>It&rsquo;s essentially instantiating rational decision making by means of some inference process</p>
<p>and learning by means of an optimization process.</p>
<p>So in a sense, I would say, yes, that it has a place.</p>
<p>And in many ways that place is, it already holds that place.</p>
<p>It&rsquo;s already in there.</p>
<p>Yeah.</p>
<p>It&rsquo;s just quite different.</p>
<p>It looks slightly different than it was before.</p>
<p>Yeah.</p>
<p>But there are some things that we can think about that make this a little bit more obvious.</p>
<p>Like if I train a big neural net model to predict what will happen in response to my</p>
<p>robot&rsquo;s actions, and then I run probabilistic inference, meaning I invert that model to</p>
<p>figure out the actions that lead to some plausible outcome, like to me, that seems like a kind</p>
<p>of logic.</p>
<p>You have a model of the world that just happens to be expressed by a neural net, and you are</p>
<p>doing some inference procedure, some sort of manipulation on that model to figure out</p>
<p>the answer to a query that you have.</p>
<p>It&rsquo;s the interpretability.</p>
<p>It&rsquo;s the explainability, though, that seems to be lacking more so because the nice thing</p>
<p>about sort of expert systems is you can follow the reasoning of the system that to us mere</p>
<p>humans is somehow compelling.</p>
<p>It&rsquo;s just I don&rsquo;t know what to make of this fact that there&rsquo;s a human desire for intelligence</p>
<p>systems to be able to convey in a poetic way to us why it made the decisions it did, like</p>
<p>tell a convincing story.</p>
<p>And perhaps that&rsquo;s like a silly human thing, like we shouldn&rsquo;t expect that of intelligence</p>
<p>systems.</p>
<p>I&rsquo;m super happy that there is intelligence systems out there.</p>
<p>But if I were to sort of psychoanalyze the researchers at the time, I would say expert</p>
<p>systems connected to that part, that desire of AI researchers for systems to be explainable.</p>
<p>I mean, maybe on that topic, do you have a hope that sort of inferences of learning based</p>
<p>systems will be as explainable as the dream was with expert systems, for example?</p>
<p>I think it&rsquo;s a very complicated question because I think that in some ways the question of</p>
<p>explainability is kind of very closely tied to the question of like performance, like,</p>
<p>you know, why do you want your system to explain itself so that when it screws up, you can</p>
<p>kind of figure out why it did it.</p>
<p>But in some ways that&rsquo;s a much bigger problem, actually.</p>
<p>Like your system might screw up and then it might screw up in how it explains itself.</p>
<p>Or you might have some bug somewhere so that it&rsquo;s not actually doing what it was supposed</p>
<p>to do.</p>
<p>So, you know, maybe a good way to view that problem is really as a problem, as a bigger</p>
<p>problem of verification and validation, of which explainability is sort of one component.</p>
<p>I see.</p>
<p>I just see it differently.</p>
<p>I see explainability, you put it beautifully, I think you actually summarize the field of</p>
<p>explainability.</p>
<p>But to me, there&rsquo;s another aspect of explainability, which is like storytelling that has nothing</p>
<p>to do with errors or with, like, it uses errors as elements of its story as opposed to a fundamental</p>
<p>need to be explainable when errors occur.</p>
<p>It&rsquo;s just that for other intelligent systems to be in our world, we seem to want to tell</p>
<p>each other stories.</p>
<p>And that&rsquo;s true in the political world, that&rsquo;s true in the academic world.</p>
<p>And that, you know, neural networks are less capable of doing that, or perhaps they&rsquo;re</p>
<p>equally capable of storytelling and storytelling.</p>
<p>Maybe it doesn&rsquo;t matter what the fundamentals of the system are.</p>
<p>You just need to be a good storyteller.</p>
<p>Maybe one specific story I can tell you about in that space is actually about some work</p>
<p>that was done by my former collaborator, who&rsquo;s now a professor at MIT named Jacob Andreas.</p>
<p>Jacob actually works in natural language processing, but he had this idea to do a little bit of</p>
<p>work in reinforcement learning on how natural language can basically structure the internals</p>
<p>of policies trained with RL.</p>
<p>And one of the things he did is he set up a model that attempts to perform some task</p>
<p>that&rsquo;s defined by a reward function, but the model reads in a natural language instruction.</p>
<p>So this is a pretty common thing to do in instruction following.</p>
<p>So you tell it like, you know, go to the red house and then it&rsquo;s supposed to go to the red house.</p>
<p>But then one of the things that Jacob did is he treated that sentence, not as a command</p>
<p>from a person, but as a representation of the internal kind of a state of the mind of</p>
<p>this policy, essentially.</p>
<p>So that when it was faced with a new task, what it would do is it would basically try</p>
<p>to think of possible language descriptions, attempt to do them and see if they led to</p>
<p>the right outcome.</p>
<p>So it would kind of think out loud, like, you know, I&rsquo;m faced with this new task.</p>
<p>What am I going to do?</p>
<p>Let me go to the red house.</p>
<p>Oh, that didn&rsquo;t work.</p>
<p>Let me go to the blue room or something.</p>
<p>Let me go to the green plant.</p>
<p>And once it got some reward, it would say, oh, go to the green plant.</p>
<p>That&rsquo;s what&rsquo;s working.</p>
<p>I&rsquo;m going to go to the green plant.</p>
<p>And then you could look at the string that it came up with, and that was a description</p>
<p>of how it thought it should solve the problem.</p>
<p>So you could do, you could basically incorporate language as internal state and you can start</p>
<p>getting some handle on these kinds of things.</p>
<p>And then what I was kind of trying to get to is that also, if you add to the reward</p>
<p>function, the convincingness of that story.</p>
<p>So I have another reward signal of like people who review that story, how much they like</p>
<p>it.</p>
<p>So that, you know, initially that could be a hyperparameter sort of hard coded heuristic</p>
<p>type of thing, but it&rsquo;s an interesting notion of the convincingness of the story becoming</p>
<p>part of the reward function, the objective function of the explainability.</p>
<p>That&rsquo;s in the world of sort of Twitter and fake news, that might be a scary notion that</p>
<p>the nature of truth may not be as important as the convincingness of the, how convincing</p>
<p>you are in telling the story around the facts.</p>
<p>Well, let me ask the basic question.</p>
<p>You&rsquo;re one of the world class researchers in reinforcement learning, deep reinforcement</p>
<p>learning, certainly in the robotic space.</p>
<p>What is reinforcement learning?</p>
<p>I think that what reinforcement learning refers to today is really just the kind of the modern</p>
<p>incarnation of learning based control.</p>
<p>So classically reinforcement learning has a much more narrow definition, which is that</p>
<p>it&rsquo;s literally learning from reinforcement, like the thing does something and then it</p>
<p>gets a reward or punishment.</p>
<p>But really I think the way the term is used today is it&rsquo;s used to refer more broadly to</p>
<p>learning based control.</p>
<p>So some kind of system that&rsquo;s supposed to be controlling something and it uses data</p>
<p>to get better.</p>
<p>And what does control mean?</p>
<p>So this action is the fundamental element there.</p>
<p>It means making rational decisions.</p>
<p>And rational decisions are decisions that maximize a measure of utility.</p>
<p>And sequentially, so you made decisions time and time and time again.</p>
<p>Now like it&rsquo;s easier to see that kind of idea in the space of maybe games and the space</p>
<p>of robotics.</p>
<p>Do you see it bigger than that?</p>
<p>Is it applicable?</p>
<p>Like where are the limits of the applicability of reinforcement learning?</p>
<p>Yeah, so rational decision making is essentially the encapsulation of the AI problem viewed</p>
<p>through a particular lens.</p>
<p>So any problem that we would want a machine to do, an intelligent machine, can likely</p>
<p>be represented as a decision making problem.</p>
<p>Learning images is a decision making problem, although not a sequential one typically.</p>
<p>Controlling a chemical plant is a decision making problem.</p>
<p>Deciding what videos to recommend on YouTube is a decision making problem.</p>
<p>And one of the really appealing things about reinforcement learning is if it does encapsulate</p>
<p>the range of all these decision making problems, perhaps working on reinforcement learning</p>
<p>is one of the ways to reach a very broad swath of AI problems.</p>
<p>What is the fundamental difference between reinforcement learning and maybe supervised</p>
<p>machine learning?</p>
<p>So reinforcement learning can be viewed as a generalization of supervised machine learning.</p>
<p>You can certainly cast supervised learning as a reinforcement learning problem.</p>
<p>You can just say your loss function is the negative of your reward.</p>
<p>But you have stronger assumptions.</p>
<p>You have the assumption that someone actually told you what the correct answer was, that</p>
<p>your data was IID and so on.</p>
<p>So you could view reinforcement learning as essentially relaxing some of those assumptions.</p>
<p>Now that&rsquo;s not always a very productive way to look at it because if you actually have</p>
<p>a supervised learning problem, you&rsquo;ll probably solve it much more effectively by using supervised</p>
<p>learning methods because it&rsquo;s easier.</p>
<p>But you can view reinforcement learning as a generalization of that.</p>
<p>No, for sure.</p>
<p>But they&rsquo;re fundamentally different.</p>
<p>That&rsquo;s a mathematical statement.</p>
<p>That&rsquo;s absolutely correct.</p>
<p>But it seems that reinforcement learning, the kind of tools we bring to the table today</p>
<p>of today.</p>
<p>So maybe down the line, everything will be a reinforcement learning problem.</p>
<p>Just like you said, image classification should be mapped to a reinforcement learning problem.</p>
<p>But today, the tools and ideas, the way we think about them are different, sort of supervised</p>
<p>learning has been used very effectively to solve basic narrow AI problems.</p>
<p>Reinforcement learning kind of represents the dream of AI.</p>
<p>It&rsquo;s very much so in the research space now in sort of captivating the imagination of</p>
<p>people of what we can do with intelligent systems, but it hasn&rsquo;t yet had as wide of</p>
<p>an impact as the supervised learning approaches.</p>
<p>So my question comes from the more practical sense, like what do you see is the gap between</p>
<p>the more general reinforcement learning and the very specific, yes, it&rsquo;s a question decision</p>
<p>making with one step in the sequence of the supervised learning?</p>
<p>So from a practical standpoint, I think that one thing that is potentially a little tough</p>
<p>now, and this is I think something that we&rsquo;ll see, this is a gap that we might see closing</p>
<p>over the next couple of years, is the ability of reinforcement learning algorithms to effectively</p>
<p>utilize large amounts of prior data.</p>
<p>So one of the reasons why it&rsquo;s a bit difficult today to use reinforcement learning for all</p>
<p>the things that we might want to use it for is that in most of the settings where we want</p>
<p>to do rational decision making, it&rsquo;s a little bit tough to just deploy some policy that</p>
<p>does crazy stuff and learns purely through trial and error.</p>
<p>It&rsquo;s much easier to collect a lot of data, a lot of logs of some other policy that you&rsquo;ve</p>
<p>got, and then maybe if you can get a good policy out of that, then you deploy it and</p>
<p>let it kind of fine tune a little bit.</p>
<p>But algorithmically, it&rsquo;s quite difficult to do that.</p>
<p>So I think that once we figure out how to get reinforcement learning to bootstrap effectively</p>
<p>from large data sets, then we&rsquo;ll see very, very rapid growth in applications of these</p>
<p>technologies.</p>
<p>So this is what&rsquo;s referred to as off policy reinforcement learning or offline RL or batch</p>
<p>RL.</p>
<p>And I think we&rsquo;re seeing a lot of research right now that&rsquo;s bringing us closer and closer</p>
<p>to that.</p>
<p>Can you maybe paint the picture of the different methods?</p>
<p>So you said off policy, what&rsquo;s value based reinforcement learning?</p>
<p>What&rsquo;s policy based?</p>
<p>What&rsquo;s model based?</p>
<p>What&rsquo;s off policy, on policy?</p>
<p>What are the different categories of reinforcement learning?</p>
<p>Okay.</p>
<p>So one way we can think about reinforcement learning is that it&rsquo;s, in some very fundamental</p>
<p>way, it&rsquo;s about learning models that can answer kind of what if questions.</p>
<p>So what would happen if I take this action that I hadn&rsquo;t taken before?</p>
<p>And you do that, of course, from experience, from data.</p>
<p>And oftentimes you do it in a loop.</p>
<p>So you build a model that answers these what if questions, use it to figure out the best</p>
<p>action you can take, and then go and try taking that and see if the outcome agrees with what</p>
<p>you predicted.</p>
<p>So the different kinds of techniques basically refer to different ways of doing it.</p>
<p>So model based methods answer a question of what state you would get, basically what would</p>
<p>happen to the world if you were to take a certain action.</p>
<p>Value based methods, they answer the question of what value you would get, meaning what</p>
<p>utility you would get.</p>
<p>But in a sense, they&rsquo;re not really all that different because they&rsquo;re both really just</p>
<p>answering these what if questions.</p>
<p>Now unfortunately for us, with current machine learning methods, answering what if questions</p>
<p>can be really hard because they are really questions about things that didn&rsquo;t happen.</p>
<p>If you wanted to answer what if questions about things that did happen, you wouldn&rsquo;t</p>
<p>need a learn model.</p>
<p>You would just like repeat the thing that worked before.</p>
<p>And that&rsquo;s really a big part of why RL is a little bit tough.</p>
<p>So if you have a purely on policy kind of online process, then you ask these what if</p>
<p>questions, you make some mistakes, then you go and try doing those mistaken things.</p>
<p>And then you observe kind of the counter examples that will teach you not to do those things</p>
<p>again.</p>
<p>If you have a bunch of off policy data and you just want to synthesize the best policy</p>
<p>you can out of that data, then you really have to deal with the challenges of making</p>
<p>these counterfactual.</p>
<p>First of all, what&rsquo;s a policy?</p>
<p>A policy is a model or some kind of function that maps from observations of the world to</p>
<p>actions.</p>
<p>So in reinforcement learning, we often refer to the current configuration of the world</p>
<p>as the state.</p>
<p>So we say the state kind of encompasses everything you need to fully define where the world is</p>
<p>at the moment.</p>
<p>And depending on how we formulate the problem, we might say you either get to see the state</p>
<p>or you get to see an observation, which is some snapshot, some piece of the state.</p>
<p>So policy just includes everything in it in order to be able to act in this world.</p>
<p>Yes.</p>
<p>And so what does off policy mean?</p>
<p>Yeah, so the terms on policy and off policy refer to how you get your data.</p>
<p>So if you get your data from somebody else who was doing some other stuff, maybe you</p>
<p>get your data from some manually programmed system that was just running in the world</p>
<p>before that&rsquo;s referred to as off policy data.</p>
<p>But if you got the data by actually acting in the world based on what your current policy</p>
<p>thinks is good, we call that on policy data.</p>
<p>And obviously on policy data is more useful to you because if your current policy makes</p>
<p>some bad decisions, you will actually see that those decisions are bad.</p>
<p>Off policy data, however, might be much easier to obtain because maybe that&rsquo;s all the logged</p>
<p>data that you have from before.</p>
<p>So we talk about offline, talked about autonomous vehicles so you can envision off policy kind</p>
<p>of approaches in robotic spaces where there&rsquo;s already a ton of robots out there, but they</p>
<p>don&rsquo;t get the luxury of being able to explore based on a reinforcement learning framework.</p>
<p>So how do we make, again, open question, but how do we make off policy methods work?</p>
<p>Yeah.</p>
<p>So this is something that has been kind of a big open problem for a while.</p>
<p>And in the last few years, people have made a little bit of progress on that.</p>
<p>You know, I can tell you about, and it&rsquo;s not by any means solved yet, but I can tell you</p>
<p>some of the things that, for example, we&rsquo;ve done to try to address some of the challenges.</p>
<p>It turns out that one really big challenge with off policy reinforcement learning is</p>
<p>that you can&rsquo;t really trust your models to give accurate predictions for any possible</p>
<p>action.</p>
<p>So if I&rsquo;ve never tried to, if in my data set I never saw somebody steering the car off</p>
<p>the road onto the sidewalk, my value function or my model is probably not going to predict</p>
<p>the right thing if I ask what would happen if I were to steer the car off the road onto</p>
<p>the sidewalk.</p>
<p>So one of the important things you have to do to get off policy RL to work is you have</p>
<p>to be able to figure out whether a given action will result in a trustworthy prediction or</p>
<p>not.</p>
<p>And you can use a kind of distribution estimation methods, kind of density estimation methods</p>
<p>to try to figure that out.</p>
<p>So you could figure out that, well, this action, my model is telling me that it&rsquo;s great, but</p>
<p>it looks totally different from any action I&rsquo;ve taken before, so my model is probably</p>
<p>not correct.</p>
<p>And you can incorporate regularization terms into your learning objective that will essentially</p>
<p>tell you not to ask those questions that your model is unable to answer.</p>
<p>What would lead to breakthroughs in this space, do you think?</p>
<p>Like what&rsquo;s needed?</p>
<p>Is this a data set question?</p>
<p>Do we need to collect big benchmark data sets that allow us to explore the space?</p>
<p>Is it a new kinds of methodologies?</p>
<p>Like what&rsquo;s your sense?</p>
<p>Or maybe coming together in a space of robotics and defining the right problem to be working</p>
<p>on?</p>
<p>I think for off policy reinforcement learning in particular, it&rsquo;s very much an algorithms</p>
<p>question right now.</p>
<p>And this is something that I think is great because an algorithms question is that that</p>
<p>just takes some very smart people to get together and think about it really hard, whereas if</p>
<p>it was like a data problem or a hardware problem, that would take some serious engineering.</p>
<p>So that&rsquo;s why I&rsquo;m pretty excited about that problem because I think that we&rsquo;re in a position</p>
<p>where we can make some real progress on it just by coming up with the right algorithms.</p>
<p>In terms of which algorithms they could be, the problems at their core are very related</p>
<p>to problems in things like causal inference.</p>
<p>Because what you&rsquo;re really dealing with is situations where you have a model, a statistical</p>
<p>model, that&rsquo;s trying to make predictions about things that it hadn&rsquo;t seen before.</p>
<p>And if it&rsquo;s a model that&rsquo;s generalizing properly, that&rsquo;ll make good predictions.</p>
<p>If it&rsquo;s a model that picks up on spurious correlations, that will not generalize properly.</p>
<p>And then you have an arsenal of tools you can use.</p>
<p>You could, for example, figure out what are the regions where it&rsquo;s trustworthy, or on</p>
<p>the other hand, you could try to make it generalize better somehow, or some combination of the</p>
<p>two.</p>
<p>Is there room for mixing where most of it, like 90, 95% is off policy, you already have</p>
<p>the data set, and then you get to send the robot out to do a little exploration?</p>
<p>What&rsquo;s that role of mixing them together?</p>
<p>Yeah, absolutely.</p>
<p>I think that this is something that you actually described very well at the beginning of our</p>
<p>discussion when you talked about the iceberg.</p>
<p>This is the iceberg.</p>
<p>The 99% of your prior experience, that&rsquo;s your iceberg.</p>
<p>You&rsquo;d use that for off policy reinforcement learning.</p>
<p>And then, of course, if you&rsquo;ve never opened that particular kind of door with that particular</p>
<p>lock before, then you have to go out and fiddle with it a little bit.</p>
<p>And that&rsquo;s that additional 1% to help you figure out a new task.</p>
<p>And I think that&rsquo;s actually a pretty good recipe going forward.</p>
<p>Is this, to you, the most exciting space of reinforcement learning now?</p>
<p>Or is there, what&rsquo;s, and maybe taking a step back, not just now, but what&rsquo;s, to you, is</p>
<p>the most beautiful idea, apologize for the romanticized question, but the beautiful idea</p>
<p>or concept in reinforcement learning?</p>
<p>In general, I actually think that one of the things that is a very beautiful idea in reinforcement</p>
<p>learning is just the idea that you can obtain a near optimal control or near optimal policy</p>
<p>without actually having a complete model of the world.</p>
<p>This is, you know, it&rsquo;s something that feels perhaps kind of obvious if you just hear the</p>
<p>term reinforcement learning or you think about trial and error learning.</p>
<p>But from a controls perspective, it&rsquo;s a very weird thing because classically, you know,</p>
<p>we think about engineered systems and controlling engineered systems as the problem of writing</p>
<p>down some equations and then figuring out given these equations, you know, basically</p>
<p>solve for X, figure out the thing that maximizes its performance.</p>
<p>And the theory of reinforcement learning actually gives us a mathematically principled framework</p>
<p>to think, to reason about, you know, optimizing some quantity when you don&rsquo;t actually know</p>
<p>the equations that govern that system.</p>
<p>And I don&rsquo;t, to me, that&rsquo;s actually seems kind of, you know, very elegant, not something</p>
<p>that sort of becomes immediately obvious, at least in the mathematical sense.</p>
<p>Does it make sense to you that it works at all?</p>
<p>Well, I think it makes sense when you take some time to think about it, but it is a little</p>
<p>surprising.</p>
<p>Well, then taking a step into the more deeper representations, which is also very surprising</p>
<p>of sort of the richness of the state space, the space of environments that this kind of</p>
<p>approach can operate in, can you maybe say what is deep reinforcement learning?</p>
<p>Well, deep reinforcement learning simply refers to taking reinforcement learning algorithms</p>
<p>and combining them with high capacity neural net representations.</p>
<p>Which is, you know, kind of, it might at first seem like a pretty arbitrary thing, just take</p>
<p>these two components and stick them together.</p>
<p>But the reason that it&rsquo;s something that has become so important in recent years is that</p>
<p>reinforcement learning, it kind of faces an exacerbated version of a problem that has</p>
<p>faced many other machine learning techniques.</p>
<p>So if we go back to like, you know, the early two thousands or the late nineties, we&rsquo;ll</p>
<p>see a lot of research on machine learning methods that have some very appealing mathematical</p>
<p>properties like they reduce the convex optimization problems, for instance, but they require very</p>
<p>special inputs.</p>
<p>They require a representation of the input that is clean in some way.</p>
<p>Like for example, clean in the sense that the classes in your multi class classification</p>
<p>problems separate linearly.</p>
<p>So they have some kind of good representation and we call this a feature representation.</p>
<p>And for a long time, people were very worried about features in the world of supervised</p>
<p>learning because somebody had to actually build those features so you couldn&rsquo;t just</p>
<p>take an image and plug it into your logistic regression or your SVM or something.</p>
<p>How to take that image and process it using some handwritten code.</p>
<p>And then neural nets came along and they could actually learn the features and suddenly we</p>
<p>could apply learning directly to the raw inputs, which was great for images, but it was even</p>
<p>more great for all the other fields where people hadn&rsquo;t come up with good features yet.</p>
<p>And one of those fields actually reinforcement learning because in reinforcement learning,</p>
<p>the notion of features, if you don&rsquo;t use neural nets and you have to design your own features</p>
<p>is very, very opaque.</p>
<p>Like it&rsquo;s very hard to imagine, let&rsquo;s say I&rsquo;m playing chess or go.</p>
<p>What is a feature with which I can represent the value function for go or even the optimal</p>
<p>policy for go linearly?</p>
<p>Like I don&rsquo;t even know how to start thinking about it.</p>
<p>And people tried all sorts of things that would write down, you know, an expert chess</p>
<p>player looks for whether the knight is in the middle of the board or not.</p>
<p>So that&rsquo;s a feature is knight in middle of board.</p>
<p>And they would write these like long lists of kind of arbitrary made up stuff.</p>
<p>And that was really kind of getting us nowhere.</p>
<p>And that&rsquo;s a little, chess is a little more accessible than the robotics problem.</p>
<p>Absolutely.</p>
<p>Right.</p>
<p>There&rsquo;s at least experts in the different features for chess, but still like the neural</p>
<p>network there, to me, that&rsquo;s, I mean, you put it eloquently and almost made it seem</p>
<p>like a natural step to add neural networks, but the fact that neural networks are able</p>
<p>to discover features in the control problem, it&rsquo;s very interesting.</p>
<p>It&rsquo;s hopeful.</p>
<p>I&rsquo;m not sure what to think about it, but it feels hopeful that the control problem has</p>
<p>features to be learned.</p>
<p>Like I guess my question is, is it surprising to you how far the deep side of deep reinforcement</p>
<p>learning was able to like what the space of problems has been able to tackle from, especially</p>
<p>in games with alpha star and alpha zero and just the representation power there and in</p>
<p>the robotics space and what is your sense of the limits of this representation power</p>
<p>and the control context?</p>
<p>I think that in regard to the limits that here, I think that one thing that makes it</p>
<p>a little hard to fully answer this question is because in settings where we would like</p>
<p>to push these things to the limit, we encounter other bottlenecks.</p>
<p>So like the reason that I can&rsquo;t get my robot to learn how to like, I don&rsquo;t know, do the</p>
<p>dishes in the kitchen, it&rsquo;s not because it&rsquo;s neural net is not big enough.</p>
<p>It&rsquo;s because when you try to actually do trial and error learning, reinforcement learning,</p>
<p>directly in the real world where you have the potential to gather these large, highly</p>
<p>varied and complex data sets, you start running into other problems.</p>
<p>Like one problem you run into very quickly, it&rsquo;ll first sound like a very pragmatic problem,</p>
<p>but it actually turns out to be a pretty deep scientific problem.</p>
<p>Take the robot, put it in your kitchen, have it try to learn to do the dishes with trial</p>
<p>and error.</p>
<p>It&rsquo;ll break all your dishes and then we&rsquo;ll have no more dishes to clean.</p>
<p>Now you might think this is a very practical issue, but there&rsquo;s something to this, which</p>
<p>is that if you have a person trying to do this, a person will have some degree of common</p>
<p>sense.</p>
<p>They&rsquo;ll break one dish, they&rsquo;ll be a little more careful with the next one, and if they</p>
<p>break all of them, they&rsquo;re going to go and get more or something like that.</p>
<p>So there&rsquo;s all sorts of scaffolding that comes very naturally to us for our learning process.</p>
<p>Like if I have to learn something through trial and error, I have the common sense to</p>
<p>know that I have to try multiple times.</p>
<p>If I screw something up, I ask for help or I reset things or something like that.</p>
<p>And all of that is kind of outside of the classic reinforcement learning problem formulation.</p>
<p>There are other things that can also be categorized as kind of scaffolding, but are very important.</p>
<p>Like for example, where do you get your reward function?</p>
<p>If I want to learn how to pour a cup of water, well, how do I know if I&rsquo;ve done it correctly?</p>
<p>Now that probably requires an entire computer vision system to be built just to determine</p>
<p>that, and that seems a little bit inelegant.</p>
<p>So there are all sorts of things like this that start to come up when we think through</p>
<p>what we really need to get reinforcement learning to happen at scale in the real world.</p>
<p>And many of these things actually suggest a little bit of a shortcoming in the problem</p>
<p>formulation and a few deeper questions that we have to resolve.</p>
<p>That&rsquo;s really interesting.</p>
<p>I talked to David Silver about AlphaZero, and it seems like there&rsquo;s no, again, we haven&rsquo;t</p>
<p>hit the limit at all in the context where there&rsquo;s no broken dishes.</p>
<p>So in the case of Go, you can, it&rsquo;s really about just scaling compute.</p>
<p>So again, like the bottleneck is the amount of money you&rsquo;re willing to invest in compute</p>
<p>and then maybe the different, the scaffolding around how difficult it is to scale compute</p>
<p>maybe, but there, there&rsquo;s no limit.</p>
<p>And it&rsquo;s interesting, now we&rsquo;ll move to the real world and there&rsquo;s the broken dishes,</p>
<p>there&rsquo;s all the, and the reward function, like you mentioned, that&rsquo;s really nice.</p>
<p>So what, how do we push forward there?</p>
<p>Do you think there&rsquo;s, there&rsquo;s this kind of a sample efficiency question that people bring</p>
<p>up of, you know, not having to break a hundred thousand dishes.</p>
<p>Is this an algorithm question?</p>
<p>Is this a data selection like question?</p>
<p>What do you think?</p>
<p>How do we, how do we not break too many dishes?</p>
<p>Yeah.</p>
<p>Well, one way we can think about that is that maybe we need to be better at, at reusing</p>
<p>our data, building that, that iceberg.</p>
<p>So perhaps, perhaps it&rsquo;s too much to hope that you can have a machine that&rsquo;s in isolation</p>
<p>in the vacuum without anything else, can just master complex tasks in like in minutes the</p>
<p>way that people do, but perhaps it also doesn&rsquo;t have to, perhaps what it really needs to do</p>
<p>is have an existence, a lifetime where it does many things and the previous things that</p>
<p>it has done, prepare it to do new things more efficiently.</p>
<p>And you know, the study of these kinds of questions typically falls under categories</p>
<p>like multitask learning or meta learning, but they all fundamentally deal with the same</p>
<p>general theme, which is use experience for doing other things to learn to do new things</p>
<p>efficiently and quickly.</p>
<p>So what do you think about if we just look at the one particular case study of a Tesla</p>
<p>autopilot that has quickly approaching towards a million vehicles on the road where some</p>
<p>percentage of the time, 30, 40% of the time is driven using the computer vision, multitask</p>
<p>hydranet, right?</p>
<p>And then the other percent, that&rsquo;s what they call it, hydranet.</p>
<p>The other percent is human controlled.</p>
<p>In the human side, how can we use that data?</p>
<p>What&rsquo;s your sense?</p>
<p>What&rsquo;s the signal?</p>
<p>Do you have ideas in this autonomous vehicle space when people can lose their lives?</p>
<p>You know, it&rsquo;s a safety critical environment.</p>
<p>So how do we use that data?</p>
<p>So I think that actually the kind of problems that come up when we want systems that are</p>
<p>reliable and that can kind of understand the limits of their capabilities, they&rsquo;re actually</p>
<p>very similar to the kind of problems that come up when we&rsquo;re doing off policy reinforcement</p>
<p>learning.</p>
<p>So as I mentioned before, in off policy reinforcement learning, the big problem is you need to know</p>
<p>when you can trust the predictions of your model, because if you&rsquo;re trying to evaluate</p>
<p>some pattern of behavior for which your model doesn&rsquo;t give you an accurate prediction, then</p>
<p>you shouldn&rsquo;t use that to modify your policy.</p>
<p>It&rsquo;s actually very similar to the problem that we&rsquo;re faced when we actually then deploy</p>
<p>that thing and we want to decide whether we trust it in the moment or not.</p>
<p>So perhaps we just need to do a better job of figuring out that part, and that&rsquo;s a very</p>
<p>deep research question, of course, but it&rsquo;s also a question that a lot of people are working</p>
<p>on.</p>
<p>So I&rsquo;m pretty optimistic that we can make some progress on that over the next few years.</p>
<p>What&rsquo;s the role of simulation in reinforcement learning, deep reinforcement learning, reinforcement</p>
<p>learning?</p>
<p>Like how essential is it?</p>
<p>It&rsquo;s been essential for the breakthroughs so far for some interesting breakthroughs.</p>
<p>Do you think it&rsquo;s a crutch that we rely on?</p>
<p>I mean, again, this connects to our off policy discussion, but do you think we can ever get</p>
<p>rid of simulation or do you think simulation will actually take over?</p>
<p>We&rsquo;ll create more and more realistic simulations that will allow us to solve actual real world</p>
<p>problems, like transfer the models we learn in simulation to real world problems.</p>
<p>I think that simulation is a very pragmatic tool that we can use to get a lot of useful</p>
<p>stuff to work right now, but I think that in the long run, we will need to build machines</p>
<p>that can learn from real data because that&rsquo;s the only way that we&rsquo;ll get them to improve</p>
<p>perpetually because if we can&rsquo;t have our machines learn from real data, if they have to rely</p>
<p>on simulated data, eventually the simulator becomes the bottleneck.</p>
<p>In fact, this is a general thing.</p>
<p>If your machine has any bottleneck that is built by humans and that doesn&rsquo;t improve from</p>
<p>data, it will eventually be the thing that holds it back.</p>
<p>And if you&rsquo;re entirely reliant on your simulator, that&rsquo;ll be the bottleneck.</p>
<p>If you&rsquo;re entirely reliant on a manually designed controller, that&rsquo;s going to be the bottleneck.</p>
<p>So simulation is very useful.</p>
<p>It&rsquo;s very pragmatic, but it&rsquo;s not a substitute for being able to utilize real experience.</p>
<p>And by the way, this is something that I think is quite relevant now, especially in the context</p>
<p>of some of the things we&rsquo;ve discussed, because some of these kind of scaffolding issues that</p>
<p>I mentioned, things like the broken dishes and the unknown reward function, like these</p>
<p>are not problems that you would ever stumble on when working in a purely simulated kind</p>
<p>of environment, but they become very apparent when we try to actually run these things in</p>
<p>the real world.</p>
<p>To throw a brief wrench into our discussion, let me ask, do you think we&rsquo;re living in a</p>
<p>simulation?</p>
<p>Oh, I have no idea.</p>
<p>Do you think that&rsquo;s a useful thing to even think about, about the fundamental physics</p>
<p>nature of reality?</p>
<p>Or another perspective, the reason I think the simulation hypothesis is interesting is</p>
<p>to think about how difficult is it to create sort of a virtual reality game type situation</p>
<p>that will be sufficiently convincing to us humans or sufficiently enjoyable that we wouldn&rsquo;t</p>
<p>want to leave.</p>
<p>I mean, that&rsquo;s actually a practical engineering challenge.</p>
<p>And I personally really enjoy virtual reality, but it&rsquo;s quite far away.</p>
<p>I kind of think about what would it take for me to want to spend more time in virtual reality</p>
<p>versus the real world.</p>
<p>And that&rsquo;s a sort of a nice clean question because at that point, if I want to live in</p>
<p>a virtual reality, that means we&rsquo;re just a few years away where a majority of the population</p>
<p>lives in a virtual reality.</p>
<p>And that&rsquo;s how we create the simulation, right?</p>
<p>You don&rsquo;t need to actually simulate the quantum gravity and just every aspect of the universe.</p>
<p>And that&rsquo;s an interesting question for reinforcement learning too, is if we want to make sufficiently</p>
<p>realistic simulations that may blend the difference between sort of the real world and the simulation,</p>
<p>thereby just some of the things we&rsquo;ve been talking about, kind of the problems go away</p>
<p>if we can create actually interesting, rich simulations.</p>
<p>It&rsquo;s an interesting question.</p>
<p>And it actually, I think your question casts your previous question in a very interesting</p>
<p>light, because in some ways asking whether we can, well, the more kind of practical version</p>
<p>is like, you know, can we build simulators that are good enough to train essentially</p>
<p>AI systems that will work in the world?</p>
<p>And it&rsquo;s kind of interesting to think about this, about what this implies, if true, it</p>
<p>kind of implies that it&rsquo;s easier to create the universe than it is to create a brain.</p>
<p>And that seems like, put this way, it seems kind of weird.</p>
<p>The aspect of the simulation most interesting to me is the simulation of other humans.</p>
<p>That seems to be a complexity that makes the robotics problem harder.</p>
<p>Now I don&rsquo;t know if every robotics person agrees with that notion.</p>
<p>Just as a quick aside, what are your thoughts about when the human enters the picture of</p>
<p>the robotics problem?</p>
<p>How does that change the reinforcement learning problem, the learning problem in general?</p>
<p>Yeah, I think that&rsquo;s a, it&rsquo;s a kind of a complex question.</p>
<p>And I guess my hope for a while had been that if we build these robotic learning systems</p>
<p>that are multitask, that utilize lots of prior data and that learn from their own experience,</p>
<p>the bit where they have to interact with people will be perhaps handled in much the same way</p>
<p>as all the other bits.</p>
<p>So if they have prior experience of interacting with people and they can learn from their</p>
<p>own experience of interacting with people for this new task, maybe that&rsquo;ll be enough.</p>
<p>Now, of course, if it&rsquo;s not enough, there are many other things we can do and there&rsquo;s</p>
<p>quite a bit of research in that area.</p>
<p>But I think it&rsquo;s worth a shot to see whether the multi agent interaction, the ability to</p>
<p>understand that other beings in the world have their own goals and tensions and thoughts</p>
<p>and so on, whether that kind of understanding can emerge automatically from simply learning</p>
<p>to do things with and maximize utility.</p>
<p>That information arises from the data.</p>
<p>You&rsquo;ve said something about gravity, that you don&rsquo;t need to explicitly inject anything</p>
<p>into the system.</p>
<p>They can be learned from the data.</p>
<p>And gravity is an example of something that could be learned from data, so like the physics</p>
<p>of the world.</p>
<p>What are the limits of what we can learn from data?</p>
<p>Do you really think we can?</p>
<p>So a very simple, clean way to ask that is, do you really think we can learn gravity from</p>
<p>just data, the idea, the laws of gravity?</p>
<p>So something that I think is a common kind of pitfall when thinking about prior knowledge</p>
<p>and learning is to assume that just because we know something, then that it&rsquo;s better to</p>
<p>tell the machine about that rather than have it figured out on its own.</p>
<p>In many cases, things that are important that affect many of the events that the machine</p>
<p>will experience are actually pretty easy to learn.</p>
<p>If every time you drop something, it falls down, yeah, you might get the Newton&rsquo;s version,</p>
<p>not Einstein&rsquo;s version, but it&rsquo;ll be pretty good and it will probably be sufficient for</p>
<p>you to act rationally in the world because you see the phenomenon all the time.</p>
<p>So things that are readily apparent from the data, we might not need to specify those by</p>
<p>hand.</p>
<p>It might actually be easier to let the machine figure them out.</p>
<p>It just feels like that there might be a space of many local minima in terms of theories</p>
<p>of this world that we would discover and get stuck on, that Newtonian mechanics is not necessarily</p>
<p>easy to come by.</p>
<p>Yeah.</p>
<p>And in fact, in some fields of science, for example, human civilization is itself full</p>
<p>of these local optima.</p>
<p>So for example, if you think about how people tried to figure out biology and medicine for</p>
<p>the longest time, the kind of rules, the kind of principles that serve us very well in our</p>
<p>day to day lives actually serve us very poorly in understanding medicine and biology.</p>
<p>We had kind of very superstitious and weird ideas about how the body worked until the</p>
<p>advent of the modern scientific method.</p>
<p>So that does seem to be a failing of this approach, but it&rsquo;s also a failing of human</p>
<p>intelligence arguably.</p>
<p>Maybe a small aside, but some, you know, the idea of self play is fascinating in reinforcement</p>
<p>learning sort of these competitive, creating a competitive context in which agents can</p>
<p>play against each other in a, sort of at the same skill level and thereby increasing each</p>
<p>other skill level.</p>
<p>It seems to be this kind of self improving mechanism is exceptionally powerful in the</p>
<p>context where it could be applied.</p>
<p>First of all, is that beautiful to you that this mechanism work as well as it does?</p>
<p>And also can we generalize to other contexts like in the robotic space or anything that&rsquo;s</p>
<p>applicable to the real world?</p>
<p>I think that it&rsquo;s a very interesting idea, but I suspect that the bottleneck to actually</p>
<p>generalizing it to the robotic setting is actually going to be the same as the bottleneck</p>
<p>for everything else that we need to be able to build machines that can get better and</p>
<p>better through natural interaction with the world.</p>
<p>And once we can do that, then they can go out and play with, they can play with each</p>
<p>other, they can play with people, they can play with the natural environment.</p>
<p>But before we get there, we&rsquo;ve got all these other problems we&rsquo;ve got, we have to get out</p>
<p>of the way.</p>
<p>So there&rsquo;s no shortcut around that.</p>
<p>You have to interact with a natural environment that.</p>
<p>Well because in a, in a self play setting, you still need a mediating mechanism.</p>
<p>So the, the reason that, you know, self play works for a board game is because the rules</p>
<p>of that board game mediate the interaction between the agents.</p>
<p>So the kind of intelligent behavior that will emerge depends very heavily on the nature</p>
<p>of that mediating mechanism.</p>
<p>So on the side of reward functions, that&rsquo;s coming up with good reward functions seems</p>
<p>to be the thing that we associate with general intelligence, like human beings seem to value</p>
<p>the idea of developing our own reward functions of, you know, at arriving at meaning and so</p>
<p>on.</p>
<p>And yet for reinforcement learning, we often kind of specify that&rsquo;s the given.</p>
<p>What&rsquo;s your sense of how we develop reward, you know, good reward functions?</p>
<p>Yeah, I think that&rsquo;s a very complicated and very deep question.</p>
<p>And you&rsquo;re completely right that classically in reinforcement learning, this question,</p>
<p>I guess, kind of been treated as an on issue that you sort of treat the reward as this</p>
<p>external thing that comes from some other bit of your biology and you kind of don&rsquo;t</p>
<p>worry about it.</p>
<p>And I do think that that&rsquo;s actually, you know, a little bit of a mistake that we should worry</p>
<p>about it.</p>
<p>And we can approach it in a few different ways.</p>
<p>We can approach it, for instance, by thinking of rewards as a communication medium.</p>
<p>We can say, well, how does a person communicate to a robot what its objective is?</p>
<p>You can approach it also as a sort of more of an intrinsic motivation medium.</p>
<p>You could say, can we write down kind of a general objective that leads to good capability?</p>
<p>Like for example, can you write down some objectives such that even in the absence of</p>
<p>any other task, if you maximize that objective, you&rsquo;ll sort of learn useful things.</p>
<p>This is something that has sometimes been called unsupervised reinforcement learning,</p>
<p>which I think is a really fascinating area of research, especially today.</p>
<p>We&rsquo;ve done a bit of work on that recently.</p>
<p>One of the things we&rsquo;ve studied is whether we can have some notion of unsupervised reinforcement</p>
<p>learning by means of, you know, information theoretic quantities, like for instance, minimizing</p>
<p>a Bayesian measure of surprise.</p>
<p>This is an idea that was, you know, pioneered actually in the computational neuroscience</p>
<p>community by folks like Carl Friston.</p>
<p>And we&rsquo;ve done some work recently that shows that you can actually learn pretty interesting</p>
<p>skills by essentially behaving in a way that allows you to make accurate predictions about</p>
<p>the world.</p>
<p>Like do the things that will lead to you getting the right answer for prediction.</p>
<p>But you can, you know, by doing this, you can sort of discover stable niches in the</p>
<p>world.</p>
<p>You can discover that if you&rsquo;re playing Tetris, then correctly, you know, clearing the rows</p>
<p>will let you play Tetris for longer and keep the board nice and clean, which sort of satisfies</p>
<p>some desire for order in the world.</p>
<p>And as a result, get some degree of leverage over your domain.</p>
<p>So we&rsquo;re exploring that pretty actively.</p>
<p>Is there a role for a human notion of curiosity in itself being the reward, sort of discovering</p>
<p>new things about the world?</p>
<p>So one of the things that I&rsquo;m pretty interested in is actually whether discovering new things</p>
<p>can actually be an emergent property of some other objective that quantifies capability.</p>
<p>So new things for the sake of new things maybe is not, maybe might not by itself be the right</p>
<p>answer, but perhaps we can figure out an objective for which discovering new things is actually</p>
<p>the natural consequence.</p>
<p>That&rsquo;s something we&rsquo;re working on right now, but I don&rsquo;t have a clear answer for you there</p>
<p>yet that&rsquo;s still a work in progress.</p>
<p>You mean just that it&rsquo;s a curious observation to see sort of creative patterns of curiosity</p>
<p>on the way to optimize for a particular task?</p>
<p>On the way to optimize for a particular measure of capability.</p>
<p>Is there ways to understand or anticipate unexpected unintended consequences of particular</p>
<p>reward functions, sort of anticipate the kind of strategies that might be developed and</p>
<p>try to avoid highly detrimental strategies?</p>
<p>So classically, this is something that has been pretty hard in reinforcement learning</p>
<p>because it&rsquo;s difficult for a designer to have good intuition about, you know, what a learning</p>
<p>algorithm will come up with when they give it some objective.</p>
<p>There are ways to mitigate that.</p>
<p>One way to mitigate it is to actually define an objective that says like, don&rsquo;t do weird</p>
<p>stuff.</p>
<p>You can actually quantify it.</p>
<p>You can say just like, don&rsquo;t enter situations that have low probability under the distribution</p>
<p>of states you&rsquo;ve seen before.</p>
<p>It turns out that that&rsquo;s actually one very good way to do off policy reinforcement learning</p>
<p>actually.</p>
<p>So we can do some things like that.</p>
<p>If we slowly venture in speaking about reward functions into greater and greater levels</p>
<p>of intelligence, there&rsquo;s, I mean, Stuart Russell thinks about this, the alignment of AI systems</p>
<p>with us humans.</p>
<p>So how do we ensure that AGI systems align with us humans?</p>
<p>It&rsquo;s kind of a reward function question of specifying the behavior of AI systems such</p>
<p>that their success aligns with this, with the broader intended success interest of human</p>
<p>beings.</p>
<p>Do you have thoughts on this?</p>
<p>Do you have kind of concerns of where reinforcement learning fits into this, or are you really</p>
<p>focused on the current moment of us being quite far away and trying to solve the robotics</p>
<p>problem?</p>
<p>I don&rsquo;t have a great answer to this, but, you know, and I do think that this is a problem</p>
<p>that&rsquo;s important to figure out.</p>
<p>For my part, I&rsquo;m actually a bit more concerned about the other side of the, of this equation</p>
<p>that, you know, maybe rather than unintended consequences for objectives that are specified</p>
<p>too well, I&rsquo;m actually more worried right now about unintended consequences for objectives</p>
<p>that are not optimized well enough, which might become a very pressing problem when</p>
<p>we, for instance, try to use these techniques for safety critical systems like cars and</p>
<p>aircraft and so on.</p>
<p>I think at some point we&rsquo;ll face the issue of objectives being optimized too well, but</p>
<p>right now I think we&rsquo;re, we&rsquo;re more likely to face the issue of them not being optimized</p>
<p>well enough.</p>
<p>But you don&rsquo;t think unintended consequences can arise even when you&rsquo;re far from optimality,</p>
<p>sort of like on the path to it?</p>
<p>Oh no, I think unintended consequences can absolutely arise.</p>
<p>It&rsquo;s just, I think right now the bottleneck for improving reliability, safety and things</p>
<p>like that is more with systems that like need to work better, that need to optimize their</p>
<p>objectives better.</p>
<p>Do you have thoughts, concerns about existential threats of human level intelligence that have,</p>
<p>if we put on our hat of looking in 10, 20, 100, 500 years from now, do you have concerns</p>
<p>about existential threats of AI systems?</p>
<p>I think there are absolutely existential threats for AI systems, just like there are for any</p>
<p>powerful technology.</p>
<p>But I think that the, these kinds of problems can take many forms and, and some of those</p>
<p>forms will come down to, you know, people with nefarious intent.</p>
<p>Some of them will come down to AI systems that have some fatal flaws.</p>
<p>And some of them will, will of course come down to AI systems that are too capable in</p>
<p>some way.</p>
<p>But among this set of potential concerns, I would actually be much more concerned about</p>
<p>the first two right now, and principally the one with nefarious humans, because, you know,</p>
<p>just through all of human history, actually it&rsquo;s the nefarious humans that have been the</p>
<p>problem, not the nefarious machines, than I am about the others.</p>
<p>And I think that right now the best that I can do to make sure things go well is to build</p>
<p>the best technology I can and also hopefully promote responsible use of that technology.</p>
<p>Do you think RL Systems has something to teach us humans?</p>
<p>You said nefarious humans getting us in trouble.</p>
<p>I mean, machine learning systems have in some ways have revealed to us the ethical flaws</p>
<p>in our data.</p>
<p>In that same kind of way, can reinforcement learning teach us about ourselves?</p>
<p>Has it taught something?</p>
<p>What have you learned about yourself from trying to build robots and reinforcement learning</p>
<p>systems?</p>
<p>I&rsquo;m not sure what I&rsquo;ve learned about myself, but maybe part of the answer to your question</p>
<p>might become a little bit more apparent once we see more widespread deployment of reinforcement</p>
<p>learning for decision making support in domains like healthcare, education, social media,</p>
<p>etc.</p>
<p>And I think we will see some interesting stuff emerge there.</p>
<p>We will see, for instance, what kind of behaviors these systems come up with in situations where</p>
<p>there is interaction with humans and where they have a possibility of influencing human</p>
<p>behavior.</p>
<p>I think we&rsquo;re not quite there yet, but maybe in the next few years we&rsquo;ll see some interesting</p>
<p>stuff come out in that area.</p>
<p>I hope outside the research space, because the exciting space where this could be observed</p>
<p>is sort of large companies that deal with large data, and I hope there&rsquo;s some transparency.</p>
<p>One of the things that&rsquo;s unclear when I look at social networks and just online is why</p>
<p>an algorithm did something or whether even an algorithm was involved.</p>
<p>And that&rsquo;d be interesting from a research perspective, just to observe the results of</p>
<p>algorithms, to open up that data, or to at least be sufficiently transparent about the</p>
<p>behavior of these AI systems in the real world.</p>
<p>What&rsquo;s your sense?</p>
<p>I don&rsquo;t know if you looked at the blog post, Bitter Lesson, by Rich Sutton, where it looks</p>
<p>at sort of the big lesson of researching AI and reinforcement learning is that simple</p>
<p>methods, general methods that leverage computation seem to work well.</p>
<p>So basically don&rsquo;t try to do any kind of fancy algorithms, just wait for computation to get</p>
<p>fast.</p>
<p>Do you share this kind of intuition?</p>
<p>I think the high level idea makes a lot of sense.</p>
<p>I&rsquo;m not sure that my takeaway would be that we don&rsquo;t need to work on algorithms.</p>
<p>I think that my takeaway would be that we should work on general algorithms.</p>
<p>And actually, I think that this idea of needing to better automate the acquisition of experience</p>
<p>in the real world actually follows pretty naturally from Rich Sutton&rsquo;s conclusion.</p>
<p>So if the claim is that automated general methods plus data leads to good results, then</p>
<p>it makes sense that we should build general methods and we should build the kind of methods</p>
<p>that we can deploy and get them to go out there and collect their experience autonomously.</p>
<p>I think that one place where I think that the current state of things falls a little</p>
<p>bit short of that is actually the going out there and collecting the data autonomously,</p>
<p>which is easy to do in a simulated board game, but very hard to do in the real world.</p>
<p>Yeah, it keeps coming back to this one problem, right?</p>
<p>Your mind is focused there now in this real world.</p>
<p>It just seems scary, the step of collecting the data, and it seems unclear to me how we</p>
<p>can do it effectively.</p>
<p>Well, you know, seven billion people in the world, each of them had to do that at some</p>
<p>point in their lives.</p>
<p>And we should leverage that experience that they&rsquo;ve all done.</p>
<p>We should be able to try to collect that kind of data.</p>
<p>Okay, big questions.</p>
<p>Maybe stepping back through your life, what book or books, technical or fiction or philosophical,</p>
<p>had a big impact on the way you saw the world, on the way you thought about in the world,</p>
<p>your life in general?</p>
<p>And maybe what books, if it&rsquo;s different, would you recommend people consider reading on their</p>
<p>own intellectual journey?</p>
<p>It could be within reinforcement learning, but it could be very much bigger.</p>
<p>I don&rsquo;t know if this is like a scientifically, like, particularly meaningful answer.</p>
<p>But like, the honest answer is that I actually found a lot of the work by Isaac Asimov to</p>
<p>be very inspiring when I was younger.</p>
<p>I don&rsquo;t know if that has anything to do with AI necessarily.</p>
<p>You don&rsquo;t think it had a ripple effect in your life?</p>
<p>Maybe it did.</p>
<p>But yeah, I think that a vision of a future where, well, first of all, artificial, I might</p>
<p>say artificial intelligence system, artificial robotic systems have, you know, kind of a</p>
<p>big place, a big role in society, and where we try to imagine the sort of the limiting</p>
<p>case of technological advancement and how that might play out in our future history.</p>
<p>But yeah, I think that that was in some way influential.</p>
<p>I don&rsquo;t really know how.</p>
<p>I would recommend it.</p>
<p>I mean, if nothing else, you&rsquo;d be well entertained.</p>
<p>When did you first yourself like fall in love with the idea of artificial intelligence,</p>
<p>get captivated by this field?</p>
<p>So my honest answer here is actually that I only really started to think about it as</p>
<p>something that I might want to do actually in graduate school pretty late.</p>
<p>And a big part of that was that until, you know, somewhere around 2009, 2010, it just</p>
<p>wasn&rsquo;t really high on my priority list because I didn&rsquo;t think that it was something where</p>
<p>we&rsquo;re going to see very substantial advances in my lifetime.</p>
<p>And you know, maybe in terms of my career, the time when I really decided I wanted to</p>
<p>work on this was when I actually took a seminar course that was taught by Professor Andrew</p>
<p>Ng.</p>
<p>And, you know, at that point, I, of course, had like a decent understanding of the technical</p>
<p>things involved.</p>
<p>But one of the things that really resonated with me was when he said in the opening lecture</p>
<p>something to the effect of like, well, he used to have graduate students come to him</p>
<p>and talk about how they want to work on AI, and he would kind of chuckle and give them</p>
<p>some math problem to deal with.</p>
<p>But now he&rsquo;s actually thinking that this is an area where we might see like substantial</p>
<p>advances in our lifetime.</p>
<p>And that kind of got me thinking because, you know, in some abstract sense, yeah, like</p>
<p>you can kind of imagine that, but in a very real sense, when someone who had been working</p>
<p>on that kind of stuff their whole career suddenly says that, yeah, like that had some effect</p>
<p>on me.</p>
<p>Yeah, this might be a special moment in the history of the field.</p>
<p>That this is where we might see some interesting breakthroughs.</p>
<p>So in the space of advice, somebody who&rsquo;s interested in getting started in machine learning</p>
<p>or reinforcement learning, what advice would you give to maybe an undergraduate student</p>
<p>or maybe even younger, how, what are the first steps to take and further on what are the</p>
<p>steps to take on that journey?</p>
<p>So something that I think is important to do is to not be afraid to like spend time</p>
<p>imagining the kind of outcome that you might like to see.</p>
<p>So you know, one outcome might be a successful career, a large paycheck or something, or</p>
<p>state of the art results on some benchmark, but hopefully that&rsquo;s not the thing that&rsquo;s</p>
<p>like the main driving force for somebody.</p>
<p>But I think that if someone who is a student considering a career in AI like takes a little</p>
<p>while, sits down and thinks like, what do I really want to see?</p>
<p>What I want to see a machine do?</p>
<p>What do I want to see a robot do?</p>
<p>What do I want to do?</p>
<p>What do I want to see a natural language system, which is like, imagine, you know, imagine</p>
<p>it almost like a commercial for a future product or something or like, like something that</p>
<p>you&rsquo;d like to see in the world and then actually sit down and think about the steps that are</p>
<p>necessary to get there.</p>
<p>And hopefully that thing is not a better number on image net classification.</p>
<p>It&rsquo;s like, it&rsquo;s probably like an actual thing that we can&rsquo;t do today that would be really</p>
<p>awesome.</p>
<p>Whether it&rsquo;s a robot Butler or a, you know, a really awesome healthcare decision making</p>
<p>support system, whatever it is that you find inspiring.</p>
<p>And I think that thinking about that and then backtracking from there and imagining the</p>
<p>steps needed to get there will actually lead to much better research.</p>
<p>It&rsquo;ll lead to rethinking the assumptions.</p>
<p>It&rsquo;ll lead to working on the bottlenecks that other people aren&rsquo;t working on.</p>
<p>And then naturally to turn to you, we&rsquo;ve talked about reward functions and you just give an</p>
<p>advice on looking forward, how you&rsquo;d like to see, what kind of change you would like</p>
<p>to make in the world.</p>
<p>What do you think, ridiculous, big question, what do you think is the meaning of life?</p>
<p>What is the meaning of your life?</p>
<p>What gives you fulfillment, purpose, happiness and meaning?</p>
<p>That&rsquo;s a very big question.</p>
<p>What&rsquo;s the reward function under which you are operating?</p>
<p>Yeah.</p>
<p>I think one thing that does give, you know, if not meaning, at least satisfaction is some</p>
<p>degree of confidence that I&rsquo;m working on a problem that really matters.</p>
<p>I feel like it&rsquo;s less important to me to like actually solve a problem, but it&rsquo;s quite nice</p>
<p>to take things to spend my time on that I believe really matter.</p>
<p>And I try pretty hard to look for that.</p>
<p>I don&rsquo;t know if it&rsquo;s easy to answer this, but if you&rsquo;re successful, what does that look</p>
<p>like?</p>
<p>What&rsquo;s the big dream?</p>
<p>Now, of course, success is built on top of success and you keep going forever, but what</p>
<p>is the dream?</p>
<p>Yeah.</p>
<p>So one very concrete thing or maybe as concrete as it&rsquo;s going to get here is to see machines</p>
<p>that actually get better and better the longer they exist in the world.</p>
<p>And that kind of seems like on the surface, one might even think that that&rsquo;s something</p>
<p>that we have today, but I think we really don&rsquo;t.</p>
<p>I think that there is an ending complexity in the universe and to date, all of the machines</p>
<p>that we&rsquo;ve been able to build don&rsquo;t sort of improve up to the limit of that complexity.</p>
<p>They hit a wall somewhere.</p>
<p>Maybe they hit a wall because they&rsquo;re in a simulator that has, that is only a very limited,</p>
<p>very pale imitation of the real world, or they hit a wall because they rely on a label</p>
<p>data set, but they never hit the wall of like running out of stuff to see.</p>
<p>So I&rsquo;d like to build a machine that can go as far as possible.</p>
<p>Runs up against the ceiling of the complexity of the universe.</p>
<p>Yes.</p>
<p>Well, I don&rsquo;t think there&rsquo;s a better way to end it, Sergey.</p>
<p>Thank you so much.</p>
<p>It&rsquo;s a huge honor.</p>
<p>I can&rsquo;t wait to see the amazing work that you have to publish and in education space</p>
<p>in terms of reinforcement learning.</p>
<p>Thank you for inspiring the world.</p>
<p>Thank you for the great research you do.</p>
<p>Thank you.</p>
<p>Thanks for listening to this conversation with Sergey Levine and thank you to our sponsors,</p>
<p>Cash App and ExpressVPN.</p>
<p>Please consider supporting this podcast by downloading Cash App and using code LexPodcast</p>
<p>and signing up at expressvpn.com slash LexPod.</p>
<p>Click all the links, buy all the stuff, it&rsquo;s the best way to support this podcast and the</p>
<p>journey I&rsquo;m on.</p>
<p>If you enjoy this thing, subscribe on YouTube, review it with five stars on Apple Podcast,</p>
<p>support it on Patreon, or connect with me on Twitter at Lex Friedman, spelled somehow</p>
<p>if you can figure out how without using the letter E, just F R I D M A N.</p>
<p>And now let me leave you with some words from Salvador Dali.</p>
<p>Intelligence without ambition is a bird without wings.</p>
<p>Thank you for listening and hope to see you next time.</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/english/">English</a>
        
            <a href="/tags/podcast/">Podcast</a>
        
            <a href="/tags/lex-fridman-podcast/">Lex Fridman Podcast</a>
        
    </section>


    </footer>


    
</article>

    

    

<aside class="related-content--wrapper">
    <h2 class="section-title">Related content</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/en/1310500372/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500372" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #368 - Eliezer Yudkowsky: Dangers of AI and the End of Human Civilization</h2>
            
            <footer class="article-time">
                <time datetime=''>Mar 30, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500371/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500371" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #367 - Sam Altman: OpenAI CEO on GPT-4, ChatGPT, and the Future of AI</h2>
            
            <footer class="article-time">
                <time datetime=''>Mar 26, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500370/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500370" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #366 - Shannon Curry: Johnny Depp &amp; Amber Heard Trial, Marriage, Dating &amp; Love</h2>
            
            <footer class="article-time">
                <time datetime=''>Mar 22, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500369/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500369" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #365 - Sam Harris: Trump, Pandemic, Twitter, Elon, Bret, IDW, Kanye, AI &amp; UFOs</h2>
            
            <footer class="article-time">
                <time datetime=''>Mar 15, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500368/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500368" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #364 - Chris Voss: FBI Hostage Negotiator</h2>
            
            <footer class="article-time">
                <time datetime=''>Mar 11, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    <div class="disqus-container">
    <div id="disqus_thread"></div>
<script type="application/javascript">
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "swiest" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

<style>
    .disqus-container {
        background-color: var(--card-background);
        border-radius: var(--card-border-radius);
        box-shadow: var(--shadow-l1);
        padding: var(--card-padding);
    }
</style>

<script>
    window.addEventListener('onColorSchemeChange', (e) => {
        if (typeof DISQUS == 'object') {
            DISQUS.reset({
                reload: true
            });
        }
    })
</script>

    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2021 - 
        
        2023 SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics
    </section>
    
    <section class="powerby">
        

        As an Amazon Associate I earn from qualifying purchases üõí<br/>

        Built with <a href="https://swiest.com/" target="_blank" rel="noopener">(Ôæâ‚óï„ÉÆ‚óï)Ôæâü™Ñüíûüíñü•∞ across the glüåçüåèüåébe</a> <br />
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        27,978.97k words in English ‚úçÔ∏è
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>


<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans+Hebrew&family=Noto+Serif&family=Noto+Serif+Armenian&family=Noto+Serif+Bengali&family=Noto+Serif+Gujarati&family=Noto+Serif+JP&family=Noto+Serif+KR&family=Noto+Serif+Kannada&family=Noto+Serif+Lao&family=Noto+Serif+Malayalam&family=Noto+Serif+SC&family=Noto+Serif+TC&family=Noto+Serif+Tamil&family=Noto+Serif+Telugu&family=Noto+Serif+Thai&display=swap" rel="stylesheet">

    </body>
</html>

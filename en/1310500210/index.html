<!DOCTYPE html>
<html lang="en" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='The following is a conversation with Eshan Mizra,
research scientist at Facebook AI Research,
who works on self supervised machine learning
in the domain of computer vision,
or in other words, making AI systems understand
the visual world with minimal help from us humans.
Transformers and self attention has been successfully used
by OpenAI&amp;rsquo;s DPT3 and other language models
to do self supervised learning in the domain of language.
Eshan, together with Yann LeCun and others,'>
<title>Lex Fridman Podcast - #206 - Ishan Misra: Self-Supervised Deep Learning in Computer Vision | SWIEST</title>

<link rel='canonical' href='https://swiest.com/en/1310500210/'>

<link rel="stylesheet" href="/scss/style.min.91b18679590f4ceed910ade4d64b1e7375cc0770ef5b8c1d822f42424f9ff2c8.css"><script>
    document.oncontextmenu = function(){ return false; };
    document.onselectstart = function(){ return false; };
    document.oncopy = function(){ return false; };
    document.oncut = function(){ return false; };
</script>

<script src="https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>


<script type="text/javascript">
    $(document).ready(function(){
     
     $("#back-to-top").hide();
     
     $(function () {
      $(window).scroll(function(){
       if ($(window).scrollTop()>600){
        $("#back-to-top").fadeIn(500);
       }else{
        $("#back-to-top").fadeOut(500);
       }
     });
     
     $("#back-to-top").click(function(){
      $('body,html').animate({scrollTop:0},500);
       return false;
      });
     });
    });
    </script><meta property='og:title' content='Lex Fridman Podcast - #206 - Ishan Misra: Self-Supervised Deep Learning in Computer Vision'>
<meta property='og:description' content='The following is a conversation with Eshan Mizra,
research scientist at Facebook AI Research,
who works on self supervised machine learning
in the domain of computer vision,
or in other words, making AI systems understand
the visual world with minimal help from us humans.
Transformers and self attention has been successfully used
by OpenAI&amp;rsquo;s DPT3 and other language models
to do self supervised learning in the domain of language.
Eshan, together with Yann LeCun and others,'>
<meta property='og:url' content='https://swiest.com/en/1310500210/'>
<meta property='og:site_name' content='SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='English' /><meta property='article:tag' content='Podcast' /><meta property='article:tag' content='Lex Fridman Podcast' /><meta property='article:published_time' content='2022-09-26T17:00:00&#43;00:00'/><meta property='article:modified_time' content='2022-09-26T17:00:00&#43;00:00'/>
<meta name="twitter:title" content="Lex Fridman Podcast - #206 - Ishan Misra: Self-Supervised Deep Learning in Computer Vision">
<meta name="twitter:description" content="The following is a conversation with Eshan Mizra,
research scientist at Facebook AI Research,
who works on self supervised machine learning
in the domain of computer vision,
or in other words, making AI systems understand
the visual world with minimal help from us humans.
Transformers and self attention has been successfully used
by OpenAI&amp;rsquo;s DPT3 and other language models
to do self supervised learning in the domain of language.
Eshan, together with Yann LeCun and others,">
    <link rel="shortcut icon" href="/favicon.ico" />
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin="anonymous"></script>
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "dark");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu307e6a33fa6fd661ccda3b77024ef5c2_252345_300x0_resize_box_3.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">‚ú®</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics</a></h1>
            <h2 class="site-description">üåçüåèüåé</h2>
        </div>
    </header><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        
        
        <li >
            <a href='/chart/podcastchart.html' target="_blank">
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-apple-podcast" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M18.364 18.364a9 9 0 1 0 -12.728 0" />
  <path d="M11.766 22h.468a2 2 0 0 0 1.985 -1.752l.5 -4a2 2 0 0 0 -1.985 -2.248h-1.468a2 2 0 0 0 -1.985 2.248l.5 4a2 2 0 0 0 1.985 1.752z" />
  <path d="M12 9m-2 0a2 2 0 1 0 4 0a2 2 0 1 0 -4 0" />
</svg>
                
                <span>Podcast Charts</span>
            </a>
        </li>
        

        <div>
            <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064"
                crossorigin="anonymous"></script>
            
            <ins class="adsbygoogle"
                style="display:block"
                data-ad-client="ca-pub-9206135835124064"
                data-ad-slot="8754979142"
                data-ad-format="auto"
                data-full-width-responsive="true"></ins>
            <script>
                 (adsbygoogle = window.adsbygoogle || []).push({});
            </script>
        </div>

        <div class="menu-bottom-section">
                <li id="i18n-switch">  
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M4 5h7" />
  <path d="M9 3v2c0 4.418 -2.239 8 -5 8" />
  <path d="M5 9c-.003 2.144 2.952 3.908 6.7 4" />
  <path d="M12 20l4 -9l4 9" />
  <path d="M19.1 18h-6.2" />
</svg>



                    <select name="language" onchange="window.location.href = this.selectedOptions[0].value">
                        
                            <option value="https://swiest.com/" selected>English</option>
                        
                            <option value="https://swiest.com/af/" >Afrikaans</option>
                        
                            <option value="https://swiest.com/am/" >·ä†·àõ·à≠·äõ</option>
                        
                            <option value="https://swiest.com/ar/" >ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</option>
                        
                            <option value="https://swiest.com/az/" >Az…ôrbaycan</option>
                        
                            <option value="https://swiest.com/be/" >–±–µ–ª–∞—Ä—É—Å–∫—ñ</option>
                        
                            <option value="https://swiest.com/bg/" >–±—ä–ª–≥–∞—Ä—Å–∫–∏</option>
                        
                            <option value="https://swiest.com/bn/" >‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ</option>
                        
                            <option value="https://swiest.com/bo/" >‡Ωñ‡Ωº‡Ωë‡ºã‡Ω¶‡æê‡Ωë‡ºã</option>
                        
                            <option value="https://swiest.com/bs/" >Bosanski</option>
                        
                            <option value="https://swiest.com/ca/" >Catal√†</option>
                        
                            <option value="https://swiest.com/zh-hans/" >ÁÆÄ‰Ωì‰∏≠Êñá</option>
                        
                            <option value="https://swiest.com/zh-hant/" >ÁπÅÈ´î‰∏≠Êñá</option>
                        
                            <option value="https://swiest.com/cs/" >ƒåe≈°tina</option>
                        
                            <option value="https://swiest.com/el/" >ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨</option>
                        
                            <option value="https://swiest.com/cy/" >Cymraeg</option>
                        
                            <option value="https://swiest.com/da/" >Dansk</option>
                        
                            <option value="https://swiest.com/de/" >Deutsch</option>
                        
                            <option value="https://swiest.com/eo/" >Esperanto</option>
                        
                            <option value="https://swiest.com/es-es/" >Espa√±ol (Espa√±a)</option>
                        
                            <option value="https://swiest.com/es-419/" >Espa√±ol (Latinoam√©rica)</option>
                        
                            <option value="https://swiest.com/et/" >Eesti</option>
                        
                            <option value="https://swiest.com/eu/" >Euskara</option>
                        
                            <option value="https://swiest.com/haw/" > ª≈ålelo Hawai ªi</option>
                        
                            <option value="https://swiest.com/fa/" >ŸÅÿßÿ±ÿ≥€å</option>
                        
                            <option value="https://swiest.com/fi/" >Suomi</option>
                        
                            <option value="https://swiest.com/fo/" >F√∏royskt</option>
                        
                            <option value="https://swiest.com/fr/" >Fran√ßais</option>
                        
                            <option value="https://swiest.com/fy/" >Frysk</option>
                        
                            <option value="https://swiest.com/ga/" >Gaeilge</option>
                        
                            <option value="https://swiest.com/gl/" >Galego</option>
                        
                            <option value="https://swiest.com/gu/" >‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä</option>
                        
                            <option value="https://swiest.com/he/" >◊¢÷¥◊ë◊®÷¥◊ô◊™</option>
                        
                            <option value="https://swiest.com/km/" >·ûÄ·ûò·üí·ûñ·ûª·ûá·û∂·üî</option>
                        
                            <option value="https://swiest.com/hi/" >‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</option>
                        
                            <option value="https://swiest.com/hr/" >Hrvatski</option>
                        
                            <option value="https://swiest.com/ht/" >Krey√≤l Ayisyen</option>
                        
                            <option value="https://swiest.com/hu/" >Magyar</option>
                        
                            <option value="https://swiest.com/hy/" >’Ä’°’µ’•÷Ä’•’∂</option>
                        
                            <option value="https://swiest.com/ig/" >√Ås·ª•ÃÄs·ª•ÃÅ √ågb√≤</option>
                        
                            <option value="https://swiest.com/id/" >Bahasa Indonesia</option>
                        
                            <option value="https://swiest.com/is/" >√çslenska</option>
                        
                            <option value="https://swiest.com/it/" >Italiano</option>
                        
                            <option value="https://swiest.com/ja/" >Êó•Êú¨Ë™û</option>
                        
                            <option value="https://swiest.com/jv/" >Basa Jawa</option>
                        
                            <option value="https://swiest.com/ka/" >·É•·Éê·É†·Éó·É£·Éö·Éò</option>
                        
                            <option value="https://swiest.com/kk/" >“ö–∞–∑–∞“õ—à–∞</option>
                        
                            <option value="https://swiest.com/kn/" >‡≤ï‡≤®‡≥ç‡≤®‡≤°</option>
                        
                            <option value="https://swiest.com/ko/" >ÌïúÍµ≠Ïñ¥</option>
                        
                            <option value="https://swiest.com/or/" >‡¨ì‡¨°‡¨º‡¨ø‡¨Ü</option>
                        
                            <option value="https://swiest.com/ckb/" >⁄©Ÿàÿ±ÿØ€å</option>
                        
                            <option value="https://swiest.com/ky/" >–ö—ã—Ä–≥—ã–∑—á–∞</option>
                        
                            <option value="https://swiest.com/la/" >Latina</option>
                        
                            <option value="https://swiest.com/lb/" >L√´tzebuergesch</option>
                        
                            <option value="https://swiest.com/lo/" >‡∫û‡∫≤‡∫™‡∫≤‡∫•‡∫≤‡∫ß</option>
                        
                            <option value="https://swiest.com/lt/" >Lietuvi≈≥</option>
                        
                            <option value="https://swiest.com/lv/" >Latvie≈°u</option>
                        
                            <option value="https://swiest.com/mk/" >–ú–∞–∫–µ–¥–æ–Ω—Å–∫–∏</option>
                        
                            <option value="https://swiest.com/ml/" >‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç</option>
                        
                            <option value="https://swiest.com/mn/" >–ú–æ–Ω–≥–æ–ª —Ö—ç–ª</option>
                        
                            <option value="https://swiest.com/mr/" >‡§Æ‡§∞‡§æ‡§†‡•Ä</option>
                        
                            <option value="https://swiest.com/sw/" >Kiswahili</option>
                        
                            <option value="https://swiest.com/ms/" >Bahasa Melayu</option>
                        
                            <option value="https://swiest.com/my/" >·Äô·Äº·Äî·Ä∫·Äô·Ä¨</option>
                        
                            <option value="https://swiest.com/ne/" >‡§®‡•á‡§™‡§æ‡§≤‡•Ä</option>
                        
                            <option value="https://swiest.com/nl/" >Nederlands</option>
                        
                            <option value="https://swiest.com/no/" >Norsk</option>
                        
                            <option value="https://swiest.com/pa/" >‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä</option>
                        
                            <option value="https://swiest.com/pl/" >Polski</option>
                        
                            <option value="https://swiest.com/pt-br/" >Portugu√™s Brasil</option>
                        
                            <option value="https://swiest.com/pt-pt/" >Portugu√™s Europeu</option>
                        
                            <option value="https://swiest.com/ro/" >Rom√¢nƒÉ</option>
                        
                            <option value="https://swiest.com/ru/" >–†—É—Å—Å–∫–∏–π</option>
                        
                            <option value="https://swiest.com/rw/" >Kinyarwanda</option>
                        
                            <option value="https://swiest.com/si/" >‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω</option>
                        
                            <option value="https://swiest.com/sk/" >Slovenƒçina</option>
                        
                            <option value="https://swiest.com/sl/" >Sloven≈°ƒçina</option>
                        
                            <option value="https://swiest.com/sq/" >Shqip</option>
                        
                            <option value="https://swiest.com/sr/" >–°—Ä–ø—Å–∫–∏ (Srpski)</option>
                        
                            <option value="https://swiest.com/su/" >Basa Sunda</option>
                        
                            <option value="https://swiest.com/sv/" >Svenska</option>
                        
                            <option value="https://swiest.com/ta/" >‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç</option>
                        
                            <option value="https://swiest.com/te/" >‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</option>
                        
                            <option value="https://swiest.com/tg/" >–¢–æ“∑–∏–∫”£</option>
                        
                            <option value="https://swiest.com/th/" >‡πÑ‡∏ó‡∏¢</option>
                        
                            <option value="https://swiest.com/tl/" >Filipino</option>
                        
                            <option value="https://swiest.com/tr/" >T√ºrk√ße</option>
                        
                            <option value="https://swiest.com/uk/" >–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</option>
                        
                            <option value="https://swiest.com/ur/" >ÿßÿ±ÿØŸà</option>
                        
                            <option value="https://swiest.com/uz/" >O&#39;zbekcha</option>
                        
                            <option value="https://swiest.com/vi/" >Ti·∫øng Vi·ªát</option>
                        
                            <option value="https://swiest.com/yi/" >◊ê◊ô◊ì◊ô◊©</option>
                        
                            <option value="https://swiest.com/zh-hk/" >Á≤µË™û</option>
                        
                            <option value="https://swiest.com/zu/" >IsiZulu</option>
                        
                    </select>
                </li>
            
            
            
                <li id="dark-mode-toggle">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <span>Dark Mode</span>
                </li>
            
        </div>
    </ol>
</aside>

    

            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/podcast/" >
                Podcast
            </a>
        
            <a href="/categories/lex-fridman-podcast/" >
                Lex Fridman Podcast
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/en/1310500210/">Lex Fridman Podcast - #206 - Ishan Misra: Self-Supervised Deep Learning in Computer Vision</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2022-09-26</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    135 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>
<div>
    <ul>
       <a href="https://amzn.to/471i0jl" target="_blank">üéÅAmazon Prime</a>
       <a href="https://amzn.to/3QDVlVf" target="_blank">üìñKindle Unlimited</a>
       <a href="https://amzn.to/3FqzNoB" target="_blank">üéßAudible Plus</a>
       <a href="https://amzn.to/3tMT3dm" target="_blank">üéµAmazon Music Unlimited</a>
       <a href="https://www.iherb.com/?rcode=EID1574" target="_blank">üåøiHerb</a>
    </ul>
</div>
<div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064"
     crossorigin="anonymous"></script>
    
    <ins class="adsbygoogle"
         style="display:block"
         data-ad-client="ca-pub-9206135835124064"
         data-ad-slot="8754979142"
         data-ad-format="auto"
         data-full-width-responsive="true"></ins>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>


    <section class="article-content">
    
    
    <p>The following is a conversation with Eshan Mizra,</p>
<p>research scientist at Facebook AI Research,</p>
<p>who works on self supervised machine learning</p>
<p>in the domain of computer vision,</p>
<p>or in other words, making AI systems understand</p>
<p>the visual world with minimal help from us humans.</p>
<p>Transformers and self attention has been successfully used</p>
<p>by OpenAI&rsquo;s DPT3 and other language models</p>
<p>to do self supervised learning in the domain of language.</p>
<p>Eshan, together with Yann LeCun and others,</p>
<p>is trying to achieve the same success</p>
<p>in the domain of images and video.</p>
<p>The goal is to leave a robot</p>
<p>watching YouTube videos all night,</p>
<p>and in the morning, come back to a much smarter robot.</p>
<p>I read the blog post, Self Supervised Learning,</p>
<p>The Dark Matter of Intelligence by Eshan and Yann LeCun,</p>
<p>and then listened to Eshan&rsquo;s appearance</p>
<p>on the excellent Machine Learning Street Talk podcast,</p>
<p>and I knew I had to talk to him.</p>
<p>By the way, if you&rsquo;re interested in machine learning and AI,</p>
<p>I cannot recommend the ML Street Talk podcast highly enough.</p>
<p>Those guys are great.</p>
<p>Quick mention of our sponsors.</p>
<p>Onnit, The Information, Grammarly, and Athletic Greens.</p>
<p>Check them out in the description to support this podcast.</p>
<p>As a side note, let me say that,</p>
<p>for those of you who may have been listening</p>
<p>for quite a while, this podcast used to be called</p>
<p>Artificial Intelligence Podcast,</p>
<p>because my life passion has always been,</p>
<p>will always be artificial intelligence,</p>
<p>both narrowly and broadly defined.</p>
<p>My goal with this podcast is still</p>
<p>to have many conversations with world class researchers</p>
<p>in AI, math, physics, biology, and all the other sciences,</p>
<p>but I also want to talk to historians, musicians, athletes,</p>
<p>and of course, occasionally comedians.</p>
<p>In fact, I&rsquo;m trying out doing this podcast</p>
<p>three times a week now to give me more freedom</p>
<p>with guest selection and maybe get a chance</p>
<p>to have a bit more fun.</p>
<p>Speaking of fun, in this conversation,</p>
<p>I challenge the listener to count the number of times</p>
<p>the word banana is mentioned.</p>
<p>Ishan and I use the word banana as the canonical example</p>
<p>at the core of the hard problem of computer vision</p>
<p>and maybe the hard problem of consciousness.</p>
<p>This is the Lex Friedman Podcast,</p>
<p>and here is my conversation with Ishan Mizra.</p>
<p>What is self supervised learning?</p>
<p>And maybe even give the bigger basics</p>
<p>of what is supervised and semi supervised learning,</p>
<p>and maybe why is self supervised learning</p>
<p>a better term than unsupervised learning?</p>
<p>Let&rsquo;s start with supervised learning.</p>
<p>So typically for machine learning systems,</p>
<p>the way they&rsquo;re trained is you get a bunch of humans,</p>
<p>the humans point out particular concepts.</p>
<p>So if it&rsquo;s in the case of images,</p>
<p>you want the humans to come and tell you</p>
<p>what is present in the image,</p>
<p>draw boxes around them, draw masks of like things,</p>
<p>pixels, which are of particular categories or not.</p>
<p>For NLP, again, there are like lots</p>
<p>of these particular tasks, say about sentiment analysis,</p>
<p>about entailment and so on.</p>
<p>So typically for supervised learning,</p>
<p>we get a big corpus of such annotated or labeled data.</p>
<p>And then we feed that to a system</p>
<p>and the system is really trying to mimic.</p>
<p>So it&rsquo;s taking this input of the data</p>
<p>and then trying to mimic the output.</p>
<p>So it looks at an image and the human has tagged</p>
<p>that this image contains a banana.</p>
<p>And now the system is basically trying to mimic that.</p>
<p>So that&rsquo;s its learning signal.</p>
<p>And so for supervised learning,</p>
<p>we try to gather lots of such data</p>
<p>and we train these machine learning models</p>
<p>to imitate the input output.</p>
<p>And the hope is basically by doing so,</p>
<p>now on unseen or like new kinds of data,</p>
<p>this model can automatically learn</p>
<p>to predict these concepts.</p>
<p>So this is a standard sort of supervised setting.</p>
<p>For semi supervised setting,</p>
<p>the idea typically is that you have,</p>
<p>of course, all of the supervised data,</p>
<p>but you have lots of other data,</p>
<p>which is unsupervised or which is like not labeled.</p>
<p>Now, the problem basically with supervised learning</p>
<p>and why you actually have all of these alternate</p>
<p>sort of learning paradigms is,</p>
<p>supervised learning just does not scale.</p>
<p>So if you look at for computer vision,</p>
<p>the sort of largest,</p>
<p>one of the most popular data sets is ImageNet, right?</p>
<p>So the entire ImageNet data set has about 22,000 concepts</p>
<p>and about 14 million images.</p>
<p>So these concepts are basically just nouns</p>
<p>and they&rsquo;re annotated on images.</p>
<p>And this entire data set was a mammoth data collection</p>
<p>effort that actually gave rise</p>
<p>to a lot of powerful learning algorithms</p>
<p>is credited with like sort of the rise</p>
<p>of deep learning as well.</p>
<p>But this data set took about 22 human years</p>
<p>to collect, to annotate.</p>
<p>And it&rsquo;s not even that many concepts, right?</p>
<p>It&rsquo;s not even that many images,</p>
<p>14 million is nothing really.</p>
<p>Like you have about, I think 400 million images or so,</p>
<p>or even more than that uploaded to most of the popular</p>
<p>sort of social media websites today.</p>
<p>So now supervised learning just doesn&rsquo;t scale.</p>
<p>If I want to now annotate more concepts,</p>
<p>if I want to have various types of fine grained concepts,</p>
<p>then it won&rsquo;t really scale.</p>
<p>So now you come up to these sort of different</p>
<p>learning paradigms, for example, semi supervised learning,</p>
<p>where the idea is you, of course,</p>
<p>you have this annotated corpus of supervised data</p>
<p>and you have lots of these unlabeled images.</p>
<p>And the idea is that the algorithm should basically try</p>
<p>to measure some kind of consistency</p>
<p>or really try to measure some kind of signal</p>
<p>on this sort of unlabeled data</p>
<p>to make itself more confident</p>
<p>about what it&rsquo;s really trying to predict.</p>
<p>So by access to this, lots of unlabeled data,</p>
<p>the idea is that the algorithm actually learns</p>
<p>to be more confident and actually gets better</p>
<p>at predicting these concepts.</p>
<p>And now we come to the other extreme,</p>
<p>which is like self supervised learning.</p>
<p>The idea basically is that the machine or the algorithm</p>
<p>should really discover concepts or discover things</p>
<p>about the world or learn representations about the world</p>
<p>which are useful without access</p>
<p>to explicit human supervision.</p>
<p>So the word supervision is still</p>
<p>in the term self supervised.</p>
<p>So what is the supervision signal?</p>
<p>And maybe that perhaps is when Yann LeCun</p>
<p>and you argue that unsupervised</p>
<p>is the incorrect terminology here.</p>
<p>So what is the supervision signal</p>
<p>when the humans aren&rsquo;t part of the picture</p>
<p>or not a big part of the picture?</p>
<p>Right, so self supervised,</p>
<p>the reason that it has the term supervised in itself</p>
<p>is because you&rsquo;re using the data itself as supervision.</p>
<p>So because the data serves as its own source of supervision,</p>
<p>it&rsquo;s self supervised in that way.</p>
<p>Now, the reason a lot of people,</p>
<p>I mean, we did it in that blog post with Yann,</p>
<p>but a lot of other people have also argued</p>
<p>for using this term self supervised.</p>
<p>So starting from like 94 from Virginia Desas group,</p>
<p>I think UCSD, and now she&rsquo;s at UCSD.</p>
<p>Jeetendra Malik has said this a bunch of times as well.</p>
<p>So you have supervised,</p>
<p>and then unsupervised basically means everything</p>
<p>which is not supervised,</p>
<p>but that includes stuff like semi supervised,</p>
<p>that includes other like transductive learning,</p>
<p>lots of other sort of settings.</p>
<p>So that&rsquo;s the reason like now people are preferring</p>
<p>this term self supervised</p>
<p>because it explicitly says what&rsquo;s happening.</p>
<p>The data itself is the source of supervision</p>
<p>and any sort of learning algorithm</p>
<p>which tries to extract just sort of data supervision signals</p>
<p>from the data itself is a self supervised algorithm.</p>
<p>But there is within the data,</p>
<p>a set of tricks which unlock the supervision.</p>
<p>So can you give maybe some examples</p>
<p>and there&rsquo;s innovation ingenuity required</p>
<p>to unlock that supervision.</p>
<p>The data doesn&rsquo;t just speak to you some ground truth,</p>
<p>you have to do some kind of trick.</p>
<p>So I don&rsquo;t know what your favorite domain is.</p>
<p>So you specifically specialize in visual learning,</p>
<p>but is there favorite examples,</p>
<p>maybe in language or other domains?</p>
<p>Perhaps the most successful applications</p>
<p>have been in NLP, not language processing.</p>
<p>So the idea basically being that you can train models</p>
<p>that can you have a sentence and you mask out certain words.</p>
<p>And now these models learn to predict the masked out words.</p>
<p>So if you have like the cat jumped over the dog,</p>
<p>so you can basically mask out cat.</p>
<p>And now you&rsquo;re essentially asking the model</p>
<p>to predict what was missing, what did I mask out?</p>
<p>So the model is going to predict basically a distribution</p>
<p>over all the possible words that it knows.</p>
<p>And probably it has like if it&rsquo;s a well trained model,</p>
<p>it has a sort of higher probability density</p>
<p>for this word cat.</p>
<p>For vision, I would say the sort of more,</p>
<p>I mean, the easier example,</p>
<p>which is not as widely used these days,</p>
<p>is basically say, for example, video prediction.</p>
<p>So video is again, a sequence of things.</p>
<p>So you can ask the model,</p>
<p>so if you have a video of say 10 seconds,</p>
<p>you can feed in the first nine seconds to a model</p>
<p>and then ask it, hey, what happens basically</p>
<p>in the 10 second, can you predict what&rsquo;s going to happen?</p>
<p>And the idea basically is because the model</p>
<p>is predicting something about the data itself.</p>
<p>Of course, you didn&rsquo;t need any human</p>
<p>to tell you what was happening</p>
<p>because the 10 second video was naturally captured.</p>
<p>Because the model is predicting what&rsquo;s happening there,</p>
<p>it&rsquo;s going to automatically learn something</p>
<p>about the structure of the world, how objects move,</p>
<p>object permanence, and these kinds of things.</p>
<p>So like, if I have something at the edge of the table,</p>
<p>it will fall down.</p>
<p>Things like these, which you really don&rsquo;t have to sit</p>
<p>and annotate.</p>
<p>In a supervised learning setting,</p>
<p>I would have to sit and annotate.</p>
<p>This is a cup, now I move this cup, this is still a cup,</p>
<p>and now I move this cup, it&rsquo;s still a cup,</p>
<p>and then it falls down, and this is a fallen down cup.</p>
<p>So I won&rsquo;t have to annotate all of these things</p>
<p>in a self supervised setting.</p>
<p>Isn&rsquo;t that kind of a brilliant little trick</p>
<p>of taking a series of data that is consistent</p>
<p>and removing one element in that series,</p>
<p>and then teaching the algorithm to predict that element?</p>
<p>Isn&rsquo;t that, first of all, that&rsquo;s quite brilliant.</p>
<p>It seems to be applicable in anything</p>
<p>that has the constraint of being a sequence</p>
<p>that is consistent with the physical reality.</p>
<p>The question is, are there other tricks like this</p>
<p>that can generate the self supervision signal?</p>
<p>So sequence is possibly the most widely used one in NLP.</p>
<p>For vision, the one that is actually used for images,</p>
<p>which is very popular these days,</p>
<p>is basically taking an image,</p>
<p>and now taking different crops of that image.</p>
<p>So you can basically decide to crop,</p>
<p>say the top left corner,</p>
<p>and you crop, say the bottom right corner,</p>
<p>and asking a network to basically present it with a choice,</p>
<p>saying that, okay, now you have this image,</p>
<p>you have this image, are these the same or not?</p>
<p>And so the idea basically is that because different crop,</p>
<p>like in an image, different parts of the image</p>
<p>are going to be related.</p>
<p>So for example, if you have a chair and a table,</p>
<p>basically these things are going to be close by,</p>
<p>versus if you take, again,</p>
<p>if you have like a zoomed in picture of a chair,</p>
<p>if you&rsquo;re taking different crops,</p>
<p>it&rsquo;s going to be different parts of the chair.</p>
<p>So the idea basically is that different crops</p>
<p>of the image are related,</p>
<p>and so the features or the representations</p>
<p>that you get from these different crops</p>
<p>should also be related.</p>
<p>So this is possibly the most like widely used trick</p>
<p>these days for self supervised learning and computer vision.</p>
<p>So again, using the consistency that&rsquo;s inherent</p>
<p>to physical reality in visual domain,</p>
<p>that&rsquo;s, you know, parts of an image are consistent,</p>
<p>and then in the language domain,</p>
<p>or anything that has sequences,</p>
<p>like language or something that&rsquo;s like a time series,</p>
<p>then you can chop up parts in time.</p>
<p>It&rsquo;s similar to the story of RNNs and CNNs,</p>
<p>of RNNs and ConvNets.</p>
<p>You and Yann LeCun wrote the blog post in March, 2021,</p>
<p>titled, Self Supervised Learning,</p>
<p>The Dark Matter of Intelligence.</p>
<p>Can you summarize this blog post</p>
<p>and maybe explain the main idea or set of ideas?</p>
<p>The blog post was mainly about sort of just telling,</p>
<p>I mean, this is really a accepted fact,</p>
<p>I would say for a lot of people now,</p>
<p>that self supervised learning is something</p>
<p>that is going to play an important role</p>
<p>for machine learning algorithms</p>
<p>that come in the future, and even now.</p>
<p>Let me just comment that we don&rsquo;t yet</p>
<p>have a good understanding of what dark matter is.</p>
<p>That&rsquo;s true.</p>
<p>So the idea basically being&hellip;</p>
<p>So maybe the metaphor doesn&rsquo;t exactly transfer,</p>
<p>but maybe it&rsquo;s actually perfectly transfers,</p>
<p>that we don&rsquo;t know, we have an inkling</p>
<p>that it&rsquo;ll be a big part</p>
<p>of whatever solving intelligence looks like.</p>
<p>Right, so I think self supervised learning,</p>
<p>the way it&rsquo;s done right now is,</p>
<p>I would say like the first step towards</p>
<p>what it probably should end up like learning</p>
<p>or what it should enable us to do.</p>
<p>So the idea for that particular piece was,</p>
<p>self supervised learning is going to be a very powerful way</p>
<p>to learn common sense about the world,</p>
<p>or like stuff that is really hard to label.</p>
<p>For example, like is this piece</p>
<p>over here heavier than the cup?</p>
<p>Now, for all these kinds of things,</p>
<p>you&rsquo;ll have to sit and label these things.</p>
<p>So supervised learning is clearly not going to scale.</p>
<p>So what is the thing that&rsquo;s actually going to scale?</p>
<p>It&rsquo;s probably going to be an agent</p>
<p>that can either actually interact with it to lift it up,</p>
<p>or observe me doing it.</p>
<p>So if I&rsquo;m basically lifting these things up,</p>
<p>it can probably reason about,</p>
<p>hey, this is taking him more time to lift up,</p>
<p>or the velocity is different,</p>
<p>whereas the velocity for this is different,</p>
<p>probably this one is heavier.</p>
<p>So essentially, by observations of the data,</p>
<p>you should be able to infer a lot of things about the world</p>
<p>without someone explicitly telling you,</p>
<p>this is heavy, this is not,</p>
<p>this is something that can pour,</p>
<p>this is something that cannot pour,</p>
<p>this is somewhere that you can sit,</p>
<p>this is not somewhere that you can sit.</p>
<p>But you just mentioned ability to interact with the world.</p>
<p>There&rsquo;s so many questions that are yet,</p>
<p>that are still open, which is,</p>
<p>how do you select the set of data</p>
<p>over which the self supervised learning process works?</p>
<p>How much interactivity like in the active learning</p>
<p>or the machine teaching context is there?</p>
<p>What are the reward signals?</p>
<p>Like how much actual interaction there is</p>
<p>with the physical world?</p>
<p>That kind of thing.</p>
<p>So that could be a huge question.</p>
<p>And then on top of that,</p>
<p>which I have a million questions about,</p>
<p>which we don&rsquo;t know the answers to,</p>
<p>but it&rsquo;s worth talking about is,</p>
<p>how much reasoning is involved?</p>
<p>How much accumulation of knowledge</p>
<p>versus something that&rsquo;s more akin to learning</p>
<p>or whether that&rsquo;s the same thing.</p>
<p>But so we&rsquo;re like, it is truly dark matter.</p>
<p>We don&rsquo;t know how exactly to do it.</p>
<p>But we are, I mean, a lot of us are actually convinced</p>
<p>that it&rsquo;s going to be a sort of major thing</p>
<p>in machine learning.</p>
<p>So let me reframe it then,</p>
<p>that human supervision cannot be at large scale</p>
<p>the source of the solution to intelligence.</p>
<p>So the machines have to discover the supervision</p>
<p>in the natural signal of the world.</p>
<p>I mean, the other thing is also</p>
<p>that humans are not particularly good labelers.</p>
<p>They&rsquo;re not very consistent.</p>
<p>For example, like what&rsquo;s the difference</p>
<p>between a dining table and a table?</p>
<p>Is it just the fact that one,</p>
<p>like if you just look at a particular table,</p>
<p>what makes us say one is dining table</p>
<p>and the other is not?</p>
<p>Humans are not particularly consistent.</p>
<p>They&rsquo;re not like very good sources of supervision</p>
<p>for a lot of these kinds of edge cases.</p>
<p>So it may be also the fact that if we want an algorithm</p>
<p>or want a machine to solve a particular task for us,</p>
<p>we can maybe just specify the end goal</p>
<p>and like the stuff in between,</p>
<p>we really probably should not be specifying</p>
<p>because we&rsquo;re not maybe going to confuse it a lot actually.</p>
<p>Well, humans can&rsquo;t even answer the meaning of life.</p>
<p>So I&rsquo;m not sure if we&rsquo;re good supervisors</p>
<p>of the end goal either.</p>
<p>So let me ask you about categories.</p>
<p>Humans are not very good at telling the difference</p>
<p>between what is and isn&rsquo;t a table, like you mentioned.</p>
<p>Do you think it&rsquo;s possible,</p>
<p>let me ask you like pretend you&rsquo;re Plato.</p>
<p>Is it possible to create a pretty good taxonomy</p>
<p>of objects in the world?</p>
<p>It seems like a lot of approaches in machine learning</p>
<p>kind of assume a hopeful vision</p>
<p>that it&rsquo;s possible to construct a perfect taxonomy</p>
<p>or it exists perhaps out of our reach,</p>
<p>but we can always get closer and closer to it.</p>
<p>Or is that a hopeless pursuit?</p>
<p>I think it&rsquo;s hopeless in some way.</p>
<p>So the thing is for any particular categorization</p>
<p>that you create,</p>
<p>if you have a discrete sort of categorization,</p>
<p>I can always take the nearest two concepts</p>
<p>or I can take a third concept and I can blend it in</p>
<p>and I can create a new category.</p>
<p>So if you were to enumerate N categories,</p>
<p>I will always find an N plus one category for you.</p>
<p>That&rsquo;s not going to be in the N categories.</p>
<p>And I can actually create not just N plus one,</p>
<p>I can very easily create far more than N categories.</p>
<p>The thing is a lot of things we talk about</p>
<p>are actually compositional.</p>
<p>So it&rsquo;s really hard for us to come and sit</p>
<p>and enumerate all of these out.</p>
<p>And they compose in various weird ways, right?</p>
<p>Like you have like a croissant and a donut come together</p>
<p>to form a cronut.</p>
<p>So if you were to like enumerate all the foods up until,</p>
<p>I don&rsquo;t know, whenever the cronut was about 10 years ago</p>
<p>or 15 years ago,</p>
<p>then this entire thing called cronut would not exist.</p>
<p>Yeah, I remember there was the most awesome video</p>
<p>of a cat wearing a monkey costume.</p>
<p>Yeah, yes.</p>
<p>People should look it up, it&rsquo;s great.</p>
<p>So is that a monkey or is that a cat?</p>
<p>It&rsquo;s a very difficult philosophical question.</p>
<p>So there is a concept of similarity between objects.</p>
<p>So you think that can take us very far?</p>
<p>Just kind of getting a good function,</p>
<p>a good way to tell which parts of things are similar</p>
<p>and which parts of things are very different.</p>
<p>I think so, yeah.</p>
<p>So you don&rsquo;t necessarily need to name everything</p>
<p>or assign a name to everything to be able to use it, right?</p>
<p>So there are like lots of&hellip;</p>
<p>Shakespeare said that, what&rsquo;s in a name?</p>
<p>What&rsquo;s in a name, yeah, okay.</p>
<p>And I mean, lots of like, for example, animals, right?</p>
<p>They don&rsquo;t have necessarily a well formed</p>
<p>like syntactic language,</p>
<p>but they&rsquo;re able to go about their day perfectly.</p>
<p>The same thing happens for us.</p>
<p>So, I mean, we probably look at things and we figure out,</p>
<p>oh, this is similar to something else that I&rsquo;ve seen before.</p>
<p>And then I can probably learn how to use it.</p>
<p>So I haven&rsquo;t seen all the possible doorknobs in the world.</p>
<p>But if you show me,</p>
<p>like I was able to get into this particular place</p>
<p>fairly easily, I&rsquo;ve never seen that particular doorknob.</p>
<p>So I of course related to all the doorknobs that I&rsquo;ve seen</p>
<p>and I know exactly how it&rsquo;s going to open.</p>
<p>I have a pretty good idea of how it&rsquo;s going to open.</p>
<p>And I think this kind of translation between experiences</p>
<p>only happens because of similarity.</p>
<p>Because I&rsquo;m able to relate it to a doorknob.</p>
<p>If I related it to a hairdryer,</p>
<p>I would probably be stuck still outside, not able to get in.</p>
<p>Again, a bit of a philosophical question,</p>
<p>but can similarity take us all the way</p>
<p>to understanding a thing?</p>
<p>Can having a good function that compares objects</p>
<p>get us to understand something profound</p>
<p>about singular objects?</p>
<p>I think I&rsquo;ll ask you a question back.</p>
<p>What does it mean to understand objects?</p>
<p>Well, let me tell you what that&rsquo;s similar to.</p>
<p>No, so there&rsquo;s an idea of sort of reasoning</p>
<p>by analogy kind of thing.</p>
<p>I think understanding is the process of placing that thing</p>
<p>in some kind of network of knowledge that you have.</p>
<p>That it perhaps is fundamentally related to other concepts.</p>
<p>So it&rsquo;s not like understanding is fundamentally related</p>
<p>by composition of other concepts</p>
<p>and maybe in relation to other concepts.</p>
<p>And maybe deeper and deeper understanding</p>
<p>is maybe just adding more edges to that graph somehow.</p>
<p>So maybe it is a composition of similarities.</p>
<p>I mean, ultimately, I suppose it is a kind of embedding</p>
<p>in that wisdom space.</p>
<p>Yeah, okay, wisdom space is good.</p>
<p>I think, I do think, right?</p>
<p>So similarity does get you very, very far.</p>
<p>Is it the answer to everything?</p>
<p>I mean, I don&rsquo;t even know what everything is,</p>
<p>but it&rsquo;s going to take us really far.</p>
<p>And I think the thing is things are similar</p>
<p>in very different contexts, right?</p>
<p>So an elephant is similar to, I don&rsquo;t know,</p>
<p>another sort of wild animal.</p>
<p>Let&rsquo;s just pick, I don&rsquo;t know, lion in a different way</p>
<p>because they&rsquo;re both four legged creatures.</p>
<p>They&rsquo;re also land animals.</p>
<p>But of course they&rsquo;re very different</p>
<p>in a lot of different ways.</p>
<p>So elephants are like herbivores, lions are not.</p>
<p>So similarity and particularly dissimilarity</p>
<p>also actually helps us understand a lot about things.</p>
<p>And so that&rsquo;s actually why I think</p>
<p>discrete categorization is very hard.</p>
<p>Just like forming this particular category of elephant</p>
<p>and a particular category of lion,</p>
<p>maybe it&rsquo;s good for just like taxonomy,</p>
<p>biological taxonomies.</p>
<p>But when it comes to other things which are not as maybe,</p>
<p>for example, like grilled cheese, right?</p>
<p>I have a grilled cheese,</p>
<p>I dip it in tomato and I keep it outside.</p>
<p>Now, is that still a grilled cheese</p>
<p>or is that something else?</p>
<p>Right, so categorization is still very useful</p>
<p>for solving problems.</p>
<p>But is your intuition then sort of the self supervised</p>
<p>should be the, to borrow Jan Lekun&rsquo;s terminology,</p>
<p>should be the cake and then categorization,</p>
<p>the classification, maybe the supervised like layer</p>
<p>should be just like the thing on top,</p>
<p>the cherry or the icing or whatever.</p>
<p>So if you make it the cake,</p>
<p>it gets in the way of learning.</p>
<p>If you make it the cake,</p>
<p>then you won&rsquo;t be able to sit and annotate everything.</p>
<p>That&rsquo;s as simple as it is.</p>
<p>Like that&rsquo;s my very practical view on it.</p>
<p>It&rsquo;s just, I mean, in my PhD,</p>
<p>I sat down and annotated like a bunch of cards</p>
<p>for one of my projects.</p>
<p>And very quickly, I was just like, it was in a video</p>
<p>and I was basically drawing boxes around all these cards.</p>
<p>And I think I spent about a week doing all of that</p>
<p>and I barely got anything done.</p>
<p>And basically this was, I think my first year of my PhD</p>
<p>or like a second year of my master&rsquo;s.</p>
<p>And then by the end of it, I&rsquo;m like, okay,</p>
<p>this is just hopeless.</p>
<p>I can keep doing it.</p>
<p>And when I&rsquo;d done that, someone came up to me</p>
<p>and they basically told me, oh, this is a pickup truck.</p>
<p>This is not a card.</p>
<p>And that&rsquo;s when like, aha, this actually makes sense</p>
<p>because a pickup truck is not really like,</p>
<p>what was I annotating?</p>
<p>Was I annotating anything that is mobile</p>
<p>or was I annotating particular sedans</p>
<p>or was I annotating SUVs?</p>
<p>What was I doing?</p>
<p>By the way, the annotation was bounding boxes?</p>
<p>Bounding boxes, yeah.</p>
<p>There&rsquo;s so many deep, profound questions here</p>
<p>that you&rsquo;re almost cheating your way out of</p>
<p>by doing self supervised learning, by the way,</p>
<p>which is like, what makes for an object?</p>
<p>As opposed to solve intelligence,</p>
<p>maybe you don&rsquo;t ever need to answer that question.</p>
<p>I mean, this is the question</p>
<p>that anyone that&rsquo;s ever done annotation</p>
<p>because it&rsquo;s so painful gets to ask,</p>
<p>like, why am I drawing very careful line around this object?</p>
<p>Like, what is the value?</p>
<p>I remember when I first saw semantic segmentation</p>
<p>where you have like instant segmentation</p>
<p>where you have a very exact line</p>
<p>around the object in a 2D plane</p>
<p>of a fundamentally 3D object projected on a 2D plane.</p>
<p>So you&rsquo;re drawing a line around a car</p>
<p>that might be occluded.</p>
<p>There might be another thing in front of it,</p>
<p>but you&rsquo;re still drawing the line</p>
<p>of the part of the car that you see.</p>
<p>How is that the car?</p>
<p>Why is that the car?</p>
<p>Like, I had like an existential crisis every time.</p>
<p>Like, how&rsquo;s that going to help us understand</p>
<p>a solved computer vision?</p>
<p>I&rsquo;m not sure I have a good answer to what&rsquo;s better.</p>
<p>And I&rsquo;m not sure I share the confidence that you have</p>
<p>that self supervised learning can take us far.</p>
<p>I think I&rsquo;m more and more convinced</p>
<p>that it&rsquo;s a very important component,</p>
<p>but I still feel like we need to understand</p>
<p>what makes like this dream of maybe what it&rsquo;s called</p>
<p>like symbolic AI of arriving,</p>
<p>like once you have this common sense base,</p>
<p>be able to play with these concepts and build graphs</p>
<p>or hierarchies of concepts on top</p>
<p>in order to then like form a deep sense</p>
<p>of this three dimensional world or four dimensional world</p>
<p>and be able to reason and then project that onto 2D plane</p>
<p>in order to interpret a 2D image.</p>
<p>Can I ask you just an out there question?</p>
<p>I remember, I think Andre Karpathy had a blog post</p>
<p>about computer vision, like being really hard.</p>
<p>I forgot what the title was, but it was many, many years ago.</p>
<p>And he had, I think President Obama stepping on a scale</p>
<p>and there was humor and there was a bunch of people laughing</p>
<p>and whatever.</p>
<p>And there&rsquo;s a lot of interesting things about that image</p>
<p>and I think Andre highlighted a bunch of things</p>
<p>about the image that us humans are able</p>
<p>to immediately understand.</p>
<p>Like the idea, I think of gravity</p>
<p>and that you have the concept of a weight.</p>
<p>You immediately project because of our knowledge of pose</p>
<p>and how human bodies are constructed,</p>
<p>you understand how the forces are being applied</p>
<p>with the human body.</p>
<p>The really interesting other thing</p>
<p>that you&rsquo;re able to understand,</p>
<p>there&rsquo;s multiple people looking at each other in the image.</p>
<p>You&rsquo;re able to have a mental model</p>
<p>of what the people are thinking about.</p>
<p>You&rsquo;re able to infer like,</p>
<p>oh, this person is probably thinks,</p>
<p>like is laughing at how humorous the situation is.</p>
<p>And this person is confused about what the situation is</p>
<p>because they&rsquo;re looking this way.</p>
<p>We&rsquo;re able to infer all of that.</p>
<p>So that&rsquo;s human vision.</p>
<p>How difficult is computer vision?</p>
<p>Like in order to achieve that level of understanding</p>
<p>and maybe how big of a part</p>
<p>does self supervised learning play in that, do you think?</p>
<p>And do you still, you know, back,</p>
<p>that was like over a decade ago,</p>
<p>I think Andre and I think a lot of people agreed</p>
<p>is computer vision is really hard.</p>
<p>Do you still think computer vision is really hard?</p>
<p>I think it is, yes.</p>
<p>And getting to that kind of understanding,</p>
<p>I mean, it&rsquo;s really out there.</p>
<p>So if you ask me to solve just that particular problem,</p>
<p>I can do it the supervised learning route.</p>
<p>I can always construct a data set and basically predict,</p>
<p>oh, is there humor in this or not?</p>
<p>And of course I can do it.</p>
<p>Actually, that&rsquo;s a good question.</p>
<p>Do you think you can, okay, okay.</p>
<p>Do you think you can do human supervised annotation of humor?</p>
<p>To some extent, yes.</p>
<p>I&rsquo;m sure it will work.</p>
<p>I mean, it won&rsquo;t be as bad as like randomly guessing.</p>
<p>I&rsquo;m sure it can still predict whether it&rsquo;s humorous or not</p>
<p>in some way.</p>
<p>Yeah, maybe like Reddit upvotes is the signal.</p>
<p>I don&rsquo;t know.</p>
<p>I mean, it won&rsquo;t do a great job, but it&rsquo;ll do something.</p>
<p>It may actually be like, it may find certain things</p>
<p>which are not humorous, humorous as well,</p>
<p>which is going to be bad for us.</p>
<p>But I mean, it&rsquo;ll do, it won&rsquo;t be random.</p>
<p>Yeah, kind of like my sense of humor.</p>
<p>Okay, so fine.</p>
<p>So you can, that particular problem, yes.</p>
<p>But the general problem you&rsquo;re saying is hard.</p>
<p>The general problem is hard.</p>
<p>And I mean, self supervised learning</p>
<p>is not the answer to everything.</p>
<p>Of course it&rsquo;s not.</p>
<p>I think if you have machines that are going to communicate</p>
<p>with humans at the end of it,</p>
<p>you want to understand what the algorithm is doing, right?</p>
<p>You want it to be able to produce an output</p>
<p>that you can decipher, that you can understand,</p>
<p>or it&rsquo;s actually useful for something else,</p>
<p>which again is a human.</p>
<p>So at some point in this sort of entire loop,</p>
<p>a human steps in.</p>
<p>And now this human needs to understand what&rsquo;s going on.</p>
<p>And at that point, this entire notion of language</p>
<p>or semantics really comes in.</p>
<p>If the machine just spits out something</p>
<p>and if we can&rsquo;t understand it,</p>
<p>then it&rsquo;s not really that useful for us.</p>
<p>So self supervised learning is probably going to be useful</p>
<p>for a lot of the things before that part,</p>
<p>before the machine really needs to communicate</p>
<p>a particular kind of output with a human.</p>
<p>Because, I mean, otherwise,</p>
<p>how is it going to do that without language?</p>
<p>Or some kind of communication.</p>
<p>But you&rsquo;re saying that it&rsquo;s possible to build</p>
<p>a big base of understanding or whatever,</p>
<p>of what&rsquo;s a better? Concepts.</p>
<p>Of concepts. Concepts, yeah.</p>
<p>Like common sense concepts. Right.</p>
<p>Supervised learning in the context of computer vision</p>
<p>is something you&rsquo;ve focused on,</p>
<p>but that&rsquo;s a really hard domain.</p>
<p>And it&rsquo;s kind of the cutting edge</p>
<p>of what we&rsquo;re, as a community, working on today.</p>
<p>Can we take a little bit of a step back</p>
<p>and look at language?</p>
<p>Can you summarize the history of success</p>
<p>of self supervised learning in natural language processing,</p>
<p>language modeling?</p>
<p>What are transformers?</p>
<p>What is the masking, the sentence completion</p>
<p>that you mentioned before?</p>
<p>How does it lead us to understand anything?</p>
<p>Semantic meaning of words,</p>
<p>syntactic role of words and sentences?</p>
<p>So I&rsquo;m, of course, not the expert on NLP.</p>
<p>I kind of follow it a little bit from the sides.</p>
<p>So the main sort of reason</p>
<p>why all of this masking stuff works is,</p>
<p>I think it&rsquo;s called the distributional hypothesis in NLP.</p>
<p>The idea basically being that words</p>
<p>that occur in the same context</p>
<p>should have similar meaning.</p>
<p>So if you have the blank jumped over the blank,</p>
<p>it basically, whatever is like in the first blank</p>
<p>is basically an object that can actually jump,</p>
<p>is going to be something that can jump.</p>
<p>So a cat or a dog, or I don&rsquo;t know, sheep, something,</p>
<p>all of these things can basically be in that particular context.</p>
<p>And now, so essentially the idea is that</p>
<p>if you have words that are in the same context</p>
<p>and you predict them,</p>
<p>you&rsquo;re going to learn lots of useful things</p>
<p>about how words are related,</p>
<p>because you&rsquo;re predicting by looking at their context</p>
<p>where the word is going to be.</p>
<p>So in this particular case, the blank jumped over the fence.</p>
<p>So now if it&rsquo;s a sheep, the sheep jumped over the fence,</p>
<p>the dog jumped over the fence.</p>
<p>So essentially the algorithm or the representation</p>
<p>basically puts together these two concepts together.</p>
<p>So it says, okay, dogs are going to be kind of related to sheep</p>
<p>because both of them occur in the same context.</p>
<p>Of course, now you can decide</p>
<p>depending on your particular application downstream,</p>
<p>you can say that dogs are absolutely not related to sheep</p>
<p>because well, I don&rsquo;t, I really care about dog food,</p>
<p>for example, I&rsquo;m a dog food person</p>
<p>and I really want to give this dog food</p>
<p>to this particular animal.</p>
<p>So depending on what your downstream application is,</p>
<p>of course, this notion of similarity or this notion</p>
<p>or this common sense that you&rsquo;ve learned</p>
<p>may not be applicable.</p>
<p>But the point is basically that this,</p>
<p>just predicting what the blanks are</p>
<p>is going to take you really, really far.</p>
<p>So there&rsquo;s a nice feature of language</p>
<p>that the number of words in a particular language</p>
<p>is very large, but it&rsquo;s finite</p>
<p>and it&rsquo;s actually not that large</p>
<p>in the grand scheme of things.</p>
<p>I still got it because we take it for granted.</p>
<p>So first of all, when you say masking,</p>
<p>you&rsquo;re talking about this very process of the blank,</p>
<p>of removing words from a sentence</p>
<p>and then having the knowledge of what word went there</p>
<p>in the initial data set,</p>
<p>that&rsquo;s the ground truth that you&rsquo;re training on</p>
<p>and then you&rsquo;re asking the neural network</p>
<p>to predict what goes there.</p>
<p>That&rsquo;s like a little trick.</p>
<p>It&rsquo;s a really powerful trick.</p>
<p>The question is how far that takes us.</p>
<p>And the other question is, is there other tricks?</p>
<p>Because to me, it&rsquo;s very possible</p>
<p>there&rsquo;s other very fascinating tricks.</p>
<p>I&rsquo;ll give you an example in autonomous driving,</p>
<p>there&rsquo;s a bunch of tricks</p>
<p>that give you the self supervised signal back.</p>
<p>For example, very similar to sentences, but not really,</p>
<p>which is you have signals from humans driving the car</p>
<p>because a lot of us drive cars to places.</p>
<p>And so you can ask the neural network to predict</p>
<p>what&rsquo;s going to happen the next two seconds</p>
<p>for a safe navigation through the environment.</p>
<p>And the signal comes from the fact</p>
<p>that you also have knowledge of what happened</p>
<p>in the next two seconds, because you have video of the data.</p>
<p>The question in autonomous driving, as it is in language,</p>
<p>can we learn how to drive autonomously</p>
<p>based on that kind of self supervision?</p>
<p>Probably the answer is no.</p>
<p>The question is how good can we get?</p>
<p>And the same with language, how good can we get?</p>
<p>And are there other tricks?</p>
<p>Like we get sometimes super excited by this trick</p>
<p>that works really well.</p>
<p>But I wonder, it&rsquo;s almost like mining for gold.</p>
<p>I wonder how many signals there are in the data</p>
<p>that could be leveraged that are like there.</p>
<p>I just wanted to kind of linger on that</p>
<p>because sometimes it&rsquo;s easy to think</p>
<p>that maybe this masking process is self supervised learning.</p>
<p>No, it&rsquo;s only one method.</p>
<p>So there could be many, many other methods,</p>
<p>many tricky methods, maybe interesting ways</p>
<p>to leverage human computation in very interesting ways</p>
<p>that might actually border on semi supervised learning,</p>
<p>something like that.</p>
<p>Obviously the internet is generated by humans</p>
<p>at the end of the day.</p>
<p>So all that to say is what&rsquo;s your sense</p>
<p>in this particular context of language,</p>
<p>how far can that masking process take us?</p>
<p>So it has stood the test of time, right?</p>
<p>I mean, so Word2vec, the initial sort of NLP technique</p>
<p>that was using this to now, for example,</p>
<p>like all the BERT and all these big models that we get,</p>
<p>BERT and Roberta, for example,</p>
<p>all of them are still sort of based</p>
<p>on the same principle of masking.</p>
<p>It&rsquo;s taken us really far.</p>
<p>I mean, you can actually do things like,</p>
<p>oh, these two sentences are similar or not,</p>
<p>whether this particular sentence follows this other sentence</p>
<p>in terms of logic, so entailment,</p>
<p>you can do a lot of these things</p>
<p>with just this masking trick.</p>
<p>So I&rsquo;m not sure if I can predict how far it can take us,</p>
<p>because when it first came out, when Word2vec was out,</p>
<p>I don&rsquo;t think a lot of us would have imagined</p>
<p>that this would actually help us do some kind</p>
<p>of entailment problems and really that well.</p>
<p>And so just the fact that by just scaling up</p>
<p>the amount of data that we&rsquo;re training on</p>
<p>and using better and more powerful neural network</p>
<p>architectures has taken us from that to this,</p>
<p>is just showing you how maybe poor predictors we are,</p>
<p>as humans, how poor we are at predicting</p>
<p>how successful a particular technique is going to be.</p>
<p>So I think I can say something now,</p>
<p>but like 10 years from now,</p>
<p>I look completely stupid basically predicting this.</p>
<p>In the language domain, is there something in your work</p>
<p>that you find useful and insightful</p>
<p>and transferable to computer vision,</p>
<p>but also just, I don&rsquo;t know, beautiful and profound</p>
<p>that I think carries through to the vision domain?</p>
<p>I mean, the idea of masking has been very powerful.</p>
<p>It has been used in vision as well for predicting,</p>
<p>like you say, the next sort of if you have</p>
<p>and sort of frames and you predict</p>
<p>what&rsquo;s going to happen in the next frame.</p>
<p>So that&rsquo;s been very powerful.</p>
<p>In terms of modeling, like in just terms</p>
<p>in terms of architecture, I think you would have asked</p>
<p>about transformers a while back.</p>
<p>That has really become like,</p>
<p>it has become super exciting for computer vision now.</p>
<p>Like in the past, I would say year and a half,</p>
<p>it&rsquo;s become really powerful.</p>
<p>What&rsquo;s a transformer?</p>
<p>Right.</p>
<p>I mean, the core part of a transformer</p>
<p>is something called the self attention model.</p>
<p>So it came out of Google</p>
<p>and the idea basically is that if you have N elements,</p>
<p>what you&rsquo;re creating is a way for all of these N elements</p>
<p>to talk to each other.</p>
<p>So the idea basically is that you are paying attention.</p>
<p>Each element is paying attention</p>
<p>to each of the other element.</p>
<p>And basically by doing this,</p>
<p>it&rsquo;s really trying to figure out,</p>
<p>you&rsquo;re basically getting a much better view of the data.</p>
<p>So for example, if you have a sentence of like four words,</p>
<p>the point is if you get a representation</p>
<p>or a feature for this entire sentence,</p>
<p>it&rsquo;s constructed in a way such that each word</p>
<p>has paid attention to everything else.</p>
<p>Now, the reason it&rsquo;s like different from say,</p>
<p>what you would do in a ConvNet</p>
<p>is basically that in the ConvNet,</p>
<p>you would only pay attention to a local window.</p>
<p>So each word would only pay attention</p>
<p>to its next neighbor or like one neighbor after that.</p>
<p>And the same thing goes for images.</p>
<p>In images, you would basically pay attention to pixels</p>
<p>in a three cross three or a seven cross seven neighborhood.</p>
<p>And that&rsquo;s it.</p>
<p>Whereas with the transformer, the self attention mainly,</p>
<p>the sort of idea is that each element</p>
<p>needs to pay attention to each other element.</p>
<p>And when you say attention,</p>
<p>maybe another way to phrase that</p>
<p>is you&rsquo;re considering a context,</p>
<p>a wide context in terms of the wide context of the sentence</p>
<p>in understanding the meaning of a particular word</p>
<p>and in computer vision that&rsquo;s understanding</p>
<p>a larger context to understand the local pattern</p>
<p>of a particular local part of an image.</p>
<p>Right, so basically if you have say,</p>
<p>again, a banana in the image,</p>
<p>you&rsquo;re looking at the full image first.</p>
<p>So whether it&rsquo;s like, you know,</p>
<p>you&rsquo;re looking at all the pixels that are off a kitchen</p>
<p>or for dining table and so on.</p>
<p>And then you&rsquo;re basically looking at the banana also.</p>
<p>Yeah, by the way, in terms of,</p>
<p>if we were to train the funny classifier,</p>
<p>there&rsquo;s something funny about the word banana.</p>
<p>Just wanted to anticipate that.</p>
<p>I am wearing a banana shirt, so yeah.</p>
<p>Is there bananas on it?</p>
<p>Okay, so masking has worked for the vision context as well.</p>
<p>And so this transformer idea has worked as well.</p>
<p>So basically looking at all the elements</p>
<p>to understand a particular element</p>
<p>has been really powerful in vision.</p>
<p>The reason is like a lot of things</p>
<p>when you&rsquo;re looking at them in isolation.</p>
<p>So if you look at just a blob of pixels,</p>
<p>so Antonio Torralba at MIT used to have</p>
<p>this like really famous image,</p>
<p>which I looked at when I was a PhD student.</p>
<p>But he would basically have a blob of pixels</p>
<p>and he would ask you, hey, what is this?</p>
<p>And it looked basically like a shoe</p>
<p>or like it could look like a TV remote.</p>
<p>It could look like anything.</p>
<p>And it turns out it was a beer bottle.</p>
<p>But I&rsquo;m not sure it was one of these three things,</p>
<p>but basically he showed you the full picture</p>
<p>and then it was very obvious what it was.</p>
<p>But the point is just by looking at</p>
<p>that particular local window, you couldn&rsquo;t figure it out.</p>
<p>Because of resolution, because of other things,</p>
<p>it&rsquo;s just not easy always to just figure it out</p>
<p>by looking at just the neighborhood of pixels,</p>
<p>what these pixels are.</p>
<p>And the same thing happens for language as well.</p>
<p>For the parameters that have to learn</p>
<p>something about the data,</p>
<p>you need to give it the capacity</p>
<p>to learn the essential things.</p>
<p>Like if it&rsquo;s not actually able to receive the signal at all,</p>
<p>then it&rsquo;s not gonna be able to learn that signal.</p>
<p>And in order to understand images, to understand language,</p>
<p>you have to be able to see words in their full context.</p>
<p>Okay, what is harder to solve, vision or language?</p>
<p>Visual intelligence or linguistic intelligence?</p>
<p>So I&rsquo;m going to say computer vision is harder.</p>
<p>My reason for this is basically that</p>
<p>language of course has a big structure to it</p>
<p>because we developed it.</p>
<p>Whereas vision is something that is common</p>
<p>in a lot of animals.</p>
<p>Everyone is able to get by a lot of these animals</p>
<p>on earth are actually able to get by without language.</p>
<p>And a lot of these animals we also deem to be intelligent.</p>
<p>So clearly intelligence does have</p>
<p>like a visual component to it.</p>
<p>And yes, of course, in the case of humans,</p>
<p>it of course also has a linguistic component.</p>
<p>But it means that there is something far more fundamental</p>
<p>about vision than there is about language.</p>
<p>And I&rsquo;m sorry to anyone who disagrees,</p>
<p>but yes, this is what I feel.</p>
<p>So that&rsquo;s being a little bit reflected in the challenges</p>
<p>that have to do with the progress</p>
<p>of self supervised learning, would you say?</p>
<p>Or is that just a peculiar accidents</p>
<p>of the progress of the AI community</p>
<p>that we focused on like,</p>
<p>or we discovered self attention and transformers</p>
<p>in the context of language first?</p>
<p>So like the self supervised learning success</p>
<p>was actually for vision has not much to do</p>
<p>with the transformers part.</p>
<p>I would say it&rsquo;s actually been independent a little bit.</p>
<p>I think it&rsquo;s just that the signal was a little bit different</p>
<p>for vision than there was for like NLP</p>
<p>and probably NLP folks discovered it before.</p>
<p>So for vision, the main success</p>
<p>has basically been this like crops so far,</p>
<p>like taking different crops of images.</p>
<p>Whereas for NLP, it was this masking thing.</p>
<p>But also the level of success</p>
<p>is still much higher for language.</p>
<p>It has.</p>
<p>So that has a lot to do with,</p>
<p>I mean, I can get into a lot of details.</p>
<p>For this particular question, let&rsquo;s go for it, okay.</p>
<p>So the first thing is language is very structured.</p>
<p>So you are going to produce a distribution</p>
<p>over a finite vocabulary.</p>
<p>English has a finite number of words.</p>
<p>It&rsquo;s actually not that large.</p>
<p>And you need to produce basically,</p>
<p>when you&rsquo;re doing this masking thing,</p>
<p>all you need to do is basically tell me</p>
<p>which one of these like 50,000 words it is.</p>
<p>That&rsquo;s it.</p>
<p>Now for vision, let&rsquo;s imagine doing the same thing.</p>
<p>Okay, we&rsquo;re basically going to blank out</p>
<p>a particular part of the image</p>
<p>and we ask the network or this neural network</p>
<p>to predict what is present in this missing patch.</p>
<p>It&rsquo;s combinatorially large, right?</p>
<p>You have 256 pixel values.</p>
<p>If you&rsquo;re even producing basically a seven cross seven</p>
<p>or a 14 cross 14 like window of pixels,</p>
<p>at each of these 169 or each of these 49 locations,</p>
<p>you have 256 values to predict.</p>
<p>And so it&rsquo;s really, really large.</p>
<p>And very quickly, the kind of like prediction problems</p>
<p>that we&rsquo;re setting up are going to be extremely</p>
<p>like interactable for us.</p>
<p>And so the thing is for NLP, it has been really successful</p>
<p>because we are very good at predicting,</p>
<p>like doing this like distribution over a finite set.</p>
<p>And the problem is when this set becomes really large,</p>
<p>we are going to become really, really bad</p>
<p>at making these predictions</p>
<p>and at solving basically this particular set of problems.</p>
<p>So if you were to do it exactly in the same way</p>
<p>as NLP for vision, there is very limited success.</p>
<p>The way stuff is working right now</p>
<p>is actually not by predicting these masks.</p>
<p>It&rsquo;s basically by saying that you take these two</p>
<p>like crops from the image,</p>
<p>you get a feature representation from it.</p>
<p>And just saying that these two features,</p>
<p>so they&rsquo;re like vectors,</p>
<p>just saying that the distance between these vectors</p>
<p>should be small.</p>
<p>And so it&rsquo;s a very different way of learning</p>
<p>from the visual signal than there is from NLP.</p>
<p>Okay, the other reason is the distributional hypothesis</p>
<p>that we talked about for NLP, right?</p>
<p>So a word given its context,</p>
<p>basically the context actually supplies</p>
<p>a lot of meaning to the word.</p>
<p>Now, because there are just finite number of words</p>
<p>and there is a finite way in like which we compose them.</p>
<p>Of course, the same thing holds for pixels,</p>
<p>but in language, there&rsquo;s a lot of structure, right?</p>
<p>So I always say whatever,</p>
<p>the dash jumped over the fence, for example.</p>
<p>There are lots of these sentences that you&rsquo;ll get.</p>
<p>And from this, you can actually look at</p>
<p>this particular sentence might occur</p>
<p>in a lot of different contexts as well.</p>
<p>This exact same sentence</p>
<p>might occur in a different context.</p>
<p>So the sheep jumped over the fence,</p>
<p>the cat jumped over the fence,</p>
<p>the dog jumped over the fence.</p>
<p>So you immediately get a lot of these words,</p>
<p>which are because this particular token itself</p>
<p>has so much meaning,</p>
<p>you get a lot of these tokens or these words,</p>
<p>which are actually going to have sort of</p>
<p>this related meaning across given this context.</p>
<p>Whereas for vision, it&rsquo;s much harder</p>
<p>because just by like pure,</p>
<p>like the way we capture images,</p>
<p>lighting can be different.</p>
<p>There might be like different noise in the sensor.</p>
<p>So the thing is you&rsquo;re capturing a physical phenomenon</p>
<p>and then you&rsquo;re basically going through</p>
<p>a very complicated pipeline of like image processing.</p>
<p>And then you&rsquo;re translating that into</p>
<p>some kind of like digital signal.</p>
<p>Whereas with language, you write it down</p>
<p>and you transfer it to a digital signal,</p>
<p>almost like it&rsquo;s a lossless like transfer.</p>
<p>And each of these tokens are very, very well defined.</p>
<p>There could be a little bit of an argument there</p>
<p>because language as written down</p>
<p>is a projection of thought.</p>
<p>This is one of the open questions is</p>
<p>if you perfectly can solve language,</p>
<p>are you getting close to being able to solve easily</p>
<p>with flying colors past the towing test kind of thing.</p>
<p>So that&rsquo;s, it&rsquo;s similar, but different</p>
<p>and the computer vision problem is in the 2D plane</p>
<p>is a projection with three dimensional world.</p>
<p>So perhaps there are similar problems there.</p>
<p>Maybe this is a good.</p>
<p>I mean, I think what I&rsquo;m saying is NLP is not easy.</p>
<p>Of course, don&rsquo;t get me wrong.</p>
<p>Like abstract thought expressed in knowledge</p>
<p>or knowledge basically expressed in language</p>
<p>is really hard to understand, right?</p>
<p>I mean, we&rsquo;ve been communicating with language for so long</p>
<p>and it is of course a very complicated concept.</p>
<p>The thing is at least getting like somewhat reasonable,</p>
<p>like being able to solve some kind of reasonable tasks</p>
<p>with language, I would say slightly easier</p>
<p>than it is with computer vision.</p>
<p>Yeah, I would say, yeah.</p>
<p>So that&rsquo;s well put.</p>
<p>I would say getting impressive performance on language</p>
<p>is easier.</p>
<p>I feel like for both language and computer vision,</p>
<p>there&rsquo;s going to be this wall of like,</p>
<p>like this hump you have to overcome</p>
<p>to achieve superhuman level performance</p>
<p>or human level performance.</p>
<p>And I feel like for language, that wall is farther away.</p>
<p>So you can get pretty nice.</p>
<p>You can do a lot of tricks.</p>
<p>You can show really impressive performance.</p>
<p>You can even fool people that you&rsquo;re tweeting</p>
<p>or you write blog posts writing</p>
<p>or your question answering has intelligence behind it.</p>
<p>But to truly demonstrate understanding of dialogue,</p>
<p>of continuous long form dialogue</p>
<p>that would require perhaps big breakthroughs.</p>
<p>In the same way in computer vision,</p>
<p>I think the big breakthroughs need to happen earlier</p>
<p>to achieve impressive performance.</p>
<p>This might be a good place to, you already mentioned it,</p>
<p>but what is contrastive learning</p>
<p>and what are energy based models?</p>
<p>Contrastive learning is sort of the paradigm of learning</p>
<p>where the idea is that you are learning this embedding space</p>
<p>or so you&rsquo;re learning this sort of vector space</p>
<p>of all your concepts.</p>
<p>And the way you learn that is basically by contrasting.</p>
<p>So the idea is that you have a sample,</p>
<p>you have another sample that&rsquo;s related to it.</p>
<p>So that&rsquo;s called the positive</p>
<p>and you have another sample that&rsquo;s not related to it.</p>
<p>So that&rsquo;s negative.</p>
<p>So for example, let&rsquo;s just take an NLP</p>
<p>or in a simple example in computer vision.</p>
<p>So you have an image of a cat, you have an image of a dog</p>
<p>and for whatever application that you&rsquo;re doing,</p>
<p>say you&rsquo;re trying to figure out what the pets are,</p>
<p>you&rsquo;re saying that these two images are related.</p>
<p>So image of a cat and dog are related,</p>
<p>but now you have another third image of a banana</p>
<p>because you don&rsquo;t like that word.</p>
<p>So now you basically have this banana.</p>
<p>Thank you for speaking to the crowd.</p>
<p>And so you take both of these images</p>
<p>and you take the image from the cat,</p>
<p>the image from the dog,</p>
<p>you get a feature from both of them.</p>
<p>And now what you&rsquo;re training the network to do</p>
<p>is basically pull both of these features together</p>
<p>while pushing them away from the feature of a banana.</p>
<p>So this is the contrastive part.</p>
<p>So you&rsquo;re contrasting against the banana.</p>
<p>So there&rsquo;s always this notion of a negative and a positive.</p>
<p>Now, energy based models are like one way</p>
<p>that Jan sort of explains a lot of these methods.</p>
<p>So Jan basically, I think a couple of years</p>
<p>or more than that, like when I joined Facebook,</p>
<p>Jan used to keep mentioning this word, energy based models.</p>
<p>And of course I had no idea what he was talking about.</p>
<p>So then one day I caught him in one of the conference rooms</p>
<p>and I&rsquo;m like, can you please tell me what this is?</p>
<p>So then like very patiently,</p>
<p>he sat down with like a marker and a whiteboard.</p>
<p>And his idea basically is that</p>
<p>rather than talking about probability distributions,</p>
<p>you can talk about energies of models.</p>
<p>So models are trying to minimize certain energies</p>
<p>in certain space,</p>
<p>or they&rsquo;re trying to maximize a certain kind of energy.</p>
<p>And the idea basically is that</p>
<p>you can explain a lot of the contrastive models,</p>
<p>GANs, for example,</p>
<p>which are like Generative Adversarial Networks.</p>
<p>A lot of these modern learning methods</p>
<p>or VAEs, which are Variational Autoencoders,</p>
<p>you can really explain them very nicely</p>
<p>in terms of an energy function</p>
<p>that they&rsquo;re trying to minimize or maximize.</p>
<p>And so by putting this common sort of language</p>
<p>for all of these models,</p>
<p>what looks very different in machine learning</p>
<p>that, oh, VAEs are very different from what GANs are,</p>
<p>are very, very different from what contrastive models are,</p>
<p>you actually get a sense of like,</p>
<p>oh, these are actually very, very related.</p>
<p>It&rsquo;s just that the way or the mechanism</p>
<p>in which they&rsquo;re sort of maximizing</p>
<p>or minimizing this energy function is slightly different.</p>
<p>It&rsquo;s revealing the commonalities</p>
<p>between all these approaches</p>
<p>and putting a sexy word on top of it, like energy.</p>
<p>And so similarities,</p>
<p>two things that are similar have low energy.</p>
<p>Like the low energy signifying similarity.</p>
<p>Right, exactly.</p>
<p>So basically the idea is that if you were to imagine</p>
<p>like the embedding as a manifold, a 2D manifold,</p>
<p>you would get a hill or like a high sort of peak</p>
<p>in the energy manifold,</p>
<p>wherever two things are not related.</p>
<p>And basically you would have like a dip</p>
<p>where two things are related.</p>
<p>So you&rsquo;d get a dip in the manifold.</p>
<p>And in the self supervised context,</p>
<p>how do you know two things are related</p>
<p>and two things are not related?</p>
<p>Right.</p>
<p>So this is where all the sort of ingenuity or tricks</p>
<p>comes in, right?</p>
<p>So for example, like you can take</p>
<p>the fill in the blank problem,</p>
<p>or you can take in the context problem.</p>
<p>And what you can say is two words</p>
<p>that are in the same context are related.</p>
<p>Two words that are in different contexts are not related.</p>
<p>For images, basically two crops</p>
<p>from the same image are related.</p>
<p>And whereas a third image is not related at all.</p>
<p>Or for a video, it can be two frames</p>
<p>from that video are related</p>
<p>because they&rsquo;re likely to contain</p>
<p>the same sort of concepts in them.</p>
<p>Whereas a third frame</p>
<p>from a different video is not related.</p>
<p>So it basically is, it&rsquo;s a very general term.</p>
<p>Contrastive learning is nothing really</p>
<p>to do with self supervised learning.</p>
<p>It actually is very popular in for example,</p>
<p>like any kind of metric learning</p>
<p>or any kind of embedding learning.</p>
<p>So it&rsquo;s also used in supervised learning.</p>
<p>And the thing is because we are not really using labels</p>
<p>to get these positive or negative pairs,</p>
<p>it can basically also be used for self supervised learning.</p>
<p>So you mentioned one of the ideas</p>
<p>in the vision context that works</p>
<p>is to have different crops.</p>
<p>So you could think of that as a way</p>
<p>to sort of manipulating the data</p>
<p>to generate examples that are similar.</p>
<p>Obviously, there&rsquo;s a bunch of other techniques.</p>
<p>You mentioned lighting as a very,</p>
<p>in images lighting is something that varies a lot</p>
<p>and you can artificially change those kinds of things.</p>
<p>There&rsquo;s the whole broad field of data augmentation,</p>
<p>which manipulates images in order to increase arbitrarily</p>
<p>the size of the data set.</p>
<p>First of all, what is data augmentation?</p>
<p>And second of all, what&rsquo;s the role of data augmentation</p>
<p>in self supervised learning and contrastive learning?</p>
<p>So data augmentation is just a way like you said,</p>
<p>it&rsquo;s basically a way to augment the data.</p>
<p>So you have say n samples.</p>
<p>And what you do is you basically define</p>
<p>some kind of transforms for the sample.</p>
<p>So you take your say image</p>
<p>and then you define a transform</p>
<p>where you can just increase say the colors</p>
<p>like the colors or the brightness of the image</p>
<p>or increase or decrease the contrast of the image</p>
<p>for example, or take different crops of it.</p>
<p>So data augmentation is just a process</p>
<p>to like basically perturb the data</p>
<p>or like augment the data, right?</p>
<p>And so it has played a fundamental role</p>
<p>for computer vision for self supervised learning especially.</p>
<p>The way most of the current methods work</p>
<p>contrastive or otherwise is by taking an image</p>
<p>in the case of images is by taking an image</p>
<p>and then computing basically two perturbations of it.</p>
<p>So these can be two different crops of the image</p>
<p>with like different types of lighting</p>
<p>or different contrast or different colors.</p>
<p>So you jitter the colors a little bit and so on.</p>
<p>And now the idea is basically because it&rsquo;s the same object</p>
<p>or because it&rsquo;s like related concepts</p>
<p>in both of these perturbations,</p>
<p>you want the features from both of these perturbations</p>
<p>to be similar.</p>
<p>So now you can use a variety of different ways</p>
<p>to enforce this constraint,</p>
<p>like these features being similar.</p>
<p>You can do this by contrastive learning.</p>
<p>So basically, both of these things are positives,</p>
<p>a third sort of image is negative.</p>
<p>You can do this basically by like clustering.</p>
<p>For example, you can say that both of these images should,</p>
<p>the features from both of these images</p>
<p>should belong in the same cluster because they&rsquo;re related,</p>
<p>whereas image like another image</p>
<p>should belong to a different cluster.</p>
<p>So there&rsquo;s a variety of different ways</p>
<p>to basically enforce this particular constraint.</p>
<p>By the way, when you say features,</p>
<p>it means there&rsquo;s a very large neural network</p>
<p>that extracting patterns from the image</p>
<p>and the kind of patterns that extracts</p>
<p>should be either identical or very similar.</p>
<p>That&rsquo;s what that means.</p>
<p>So the neural network basically takes in the image</p>
<p>and then outputs a set of like,</p>
<p>basically a vector of like numbers,</p>
<p>and that&rsquo;s the feature.</p>
<p>And you want this feature for both of these</p>
<p>like different crops that you computed to be similar.</p>
<p>So you want this vector to be identical</p>
<p>in its like entries, for example.</p>
<p>Be like literally close</p>
<p>in this multi dimensional space to each other.</p>
<p>And like you said,</p>
<p>close can mean part of the same cluster or something like that</p>
<p>in this large space.</p>
<p>First of all, that,</p>
<p>I wonder if there is connection</p>
<p>to the way humans learn to this,</p>
<p>almost like maybe subconsciously,</p>
<p>in order to understand a thing,</p>
<p>you kind of have to see it from two, three multiple angles.</p>
<p>I wonder, I have a lot of friends</p>
<p>who are neuroscientists maybe and cognitive scientists.</p>
<p>I wonder if that&rsquo;s in there somewhere.</p>
<p>Like in order for us to place a concept in its proper place,</p>
<p>we have to basically crop it in all kinds of ways,</p>
<p>do basic data augmentation on it</p>
<p>in whatever very clever ways that the brain likes to do.</p>
<p>Right.</p>
<p>Like spinning around in our minds somehow</p>
<p>that that is very effective.</p>
<p>So I think for some of them, we like need to do it.</p>
<p>So like babies, for example, pick up objects,</p>
<p>like move them and put them close to their eye and whatnot.</p>
<p>But for certain other things,</p>
<p>actually we are good at imagining it as well, right?</p>
<p>So if you, I have never seen, for example,</p>
<p>an elephant from the top.</p>
<p>I&rsquo;ve never basically looked at it from like top down.</p>
<p>But if you showed me a picture of it,</p>
<p>I could very well tell you that that&rsquo;s an elephant.</p>
<p>So I think some of it, we&rsquo;re just like,</p>
<p>we naturally build it or transfer it from other objects</p>
<p>that we&rsquo;ve seen to imagine what it&rsquo;s going to look like.</p>
<p>Has anyone done that with augmentation?</p>
<p>Like imagine all the possible things</p>
<p>that are occluded or not there,</p>
<p>but not just like normal things, like wild things,</p>
<p>but they&rsquo;re nevertheless physically consistent.</p>
<p>So, I mean, people do kind of like</p>
<p>occlusion based augmentation as well.</p>
<p>So you place in like a random like box, gray box</p>
<p>to sort of mask out a certain part of the image.</p>
<p>And the thing is basically you&rsquo;re kind of occluding it.</p>
<p>For example, you place it say on half of a person&rsquo;s face.</p>
<p>So basically saying that, you know,</p>
<p>something below their nose is occluded</p>
<p>because it&rsquo;s grayed out.</p>
<p>So, you know, I meant like, you have like, what is it?</p>
<p>A table and you can&rsquo;t see behind the table.</p>
<p>And you imagine there&rsquo;s a bunch of elves</p>
<p>with bananas behind the table.</p>
<p>Like, I wonder if there&rsquo;s useful</p>
<p>to have a wild imagination for the network</p>
<p>because that&rsquo;s possible or maybe not elves,</p>
<p>but like puppies and kittens or something like that.</p>
<p>Just have a wild imagination</p>
<p>and like constantly be generating that wild imagination.</p>
<p>Because in terms of data augmentation,</p>
<p>as currently applied, it&rsquo;s super ultra, very boring.</p>
<p>It&rsquo;s very basic data augmentation.</p>
<p>I wonder if there&rsquo;s a benefit to being wildly imaginable</p>
<p>while trying to be consistent with physical reality.</p>
<p>I think it&rsquo;s a kind of a chicken and egg problem, right?</p>
<p>Because to have like amazing data augmentation,</p>
<p>you need to understand what the scene is.</p>
<p>And what we&rsquo;re trying to do data augmentation</p>
<p>to learn what a scene is anyway.</p>
<p>So it&rsquo;s basically just keeps going on.</p>
<p>Before you understand it,</p>
<p>just put elves with bananas</p>
<p>until you know it&rsquo;s not to be true.</p>
<p>Just like children have a wild imagination</p>
<p>until the adults ruin it all.</p>
<p>Okay, so what are the different kinds of data augmentation</p>
<p>that you&rsquo;ve seen to be effective in visual intelligence?</p>
<p>For like vision,</p>
<p>it&rsquo;s a lot of these image filtering operations.</p>
<p>So like blurring the image,</p>
<p>you know, all the kind of Instagram filters</p>
<p>that you can think of.</p>
<p>So like arbitrarily like make the red super red,</p>
<p>make the green super greens, like saturate the image.</p>
<p>Rotation, cropping.</p>
<p>Rotation, cropping, exactly.</p>
<p>All of these kinds of things.</p>
<p>Like I said, lighting is a really interesting one to me.</p>
<p>Like that feels like really complicated to do.</p>
<p>I mean, they don&rsquo;t,</p>
<p>the augmentations that we work on aren&rsquo;t like</p>
<p>that involved,</p>
<p>they&rsquo;re not going to be like</p>
<p>physically realistic versions of lighting.</p>
<p>It&rsquo;s not that you&rsquo;re assuming</p>
<p>that there&rsquo;s a light source up</p>
<p>and then you&rsquo;re moving it to the right</p>
<p>and then what does the thing look like?</p>
<p>It&rsquo;s really more about like brightness of the image,</p>
<p>overall brightness of the image</p>
<p>or overall contrast of the image and so on.</p>
<p>But this is a really important point to me.</p>
<p>I always thought that data augmentation</p>
<p>holds an important key</p>
<p>to big improvements in machine learning.</p>
<p>And it seems that it is an important aspect</p>
<p>of self supervised learning.</p>
<p>So I wonder if there&rsquo;s big improvements to be achieved</p>
<p>on much more intelligent kinds of data augmentation.</p>
<p>For example, currently,</p>
<p>maybe you can correct me if I&rsquo;m wrong,</p>
<p>data augmentation is not parameterized.</p>
<p>Yeah.</p>
<p>You&rsquo;re not learning.</p>
<p>To me, it seems like data augmentation potentially</p>
<p>should involve more learning</p>
<p>than the learning process itself.</p>
<p>Right.</p>
<p>You&rsquo;re almost like thinking of like generative kind of,</p>
<p>it&rsquo;s the elves with bananas.</p>
<p>You&rsquo;re trying to,</p>
<p>it&rsquo;s like very active imagination</p>
<p>of messing with the world</p>
<p>and teaching that mechanism for messing with the world</p>
<p>to be realistic.</p>
<p>Right.</p>
<p>Because that feels like,</p>
<p>I mean, it&rsquo;s imagination.</p>
<p>It&rsquo;s just, as you said,</p>
<p>it feels like us humans are able to,</p>
<p>maybe sometimes subconsciously,</p>
<p>imagine before we see the thing,</p>
<p>imagine what we&rsquo;re expecting to see,</p>
<p>like maybe several options.</p>
<p>And especially, we probably forgot,</p>
<p>but when we were younger,</p>
<p>probably the possibilities were wilder, more numerous.</p>
<p>And then as we get older,</p>
<p>we become to understand the world</p>
<p>and the possibilities of what we might see</p>
<p>becomes less and less and less.</p>
<p>So I wonder if you think there&rsquo;s a lot of breakthroughs</p>
<p>yet to be had in data augmentation.</p>
<p>And maybe also can you just comment on the stuff we have,</p>
<p>is that a big part of self supervised learning?</p>
<p>Yes.</p>
<p>So data augmentation is like key to self supervised learning</p>
<p>that has like the kind of augmentation that we&rsquo;re using.</p>
<p>And basically the fact that we&rsquo;re trying to learn</p>
<p>these neural networks that are predicting these features</p>
<p>from images that are robust under data augmentation</p>
<p>has been the key for visual self supervised learning.</p>
<p>And they play a fairly fundamental role to it.</p>
<p>Now, the irony of all of this is that</p>
<p>for like deep learning purists will say</p>
<p>the entire point of deep learning is that</p>
<p>you feed in the pixels to the neural network</p>
<p>and it should figure out the patterns on its own.</p>
<p>So if it really wants to look at edges,</p>
<p>it should look at edges.</p>
<p>You shouldn&rsquo;t really like really go</p>
<p>and handcraft these like features, right?</p>
<p>You shouldn&rsquo;t go tell it that look at edges.</p>
<p>So data augmentation</p>
<p>should basically be in the same category, right?</p>
<p>Why should we tell the network</p>
<p>or tell this entire learning paradigm</p>
<p>what kinds of data augmentation that we&rsquo;re looking for?</p>
<p>We are encoding a very sort of human specific bias there</p>
<p>that we know things are like,</p>
<p>if you change the contrast of the image,</p>
<p>it should still be an apple</p>
<p>or it should still see apple, not banana.</p>
<p>And basically if we change like colors,</p>
<p>it should still be the same kind of concept.</p>
<p>Of course, this is not one,</p>
<p>this is doesn&rsquo;t feel like super satisfactory</p>
<p>because a lot of our human knowledge</p>
<p>or our human supervision</p>
<p>is actually going into the data augmentation.</p>
<p>So although we are calling it self supervised learning,</p>
<p>a lot of the human knowledge</p>
<p>is actually being encoded in the data augmentation process.</p>
<p>So it&rsquo;s really like,</p>
<p>we&rsquo;ve kind of sneaked away the supervision at the input</p>
<p>and we&rsquo;re like really designing</p>
<p>these nice list of data augmentations</p>
<p>that are working very well.</p>
<p>Of course, the idea is that it&rsquo;s much easier</p>
<p>to design a list of data augmentation than it is to do.</p>
<p>So humans are doing nevertheless doing less and less work</p>
<p>and maybe leveraging their creativity more and more.</p>
<p>And when we say data augmentation is not parameterized,</p>
<p>it means it&rsquo;s not part of the learning process.</p>
<p>Do you think it&rsquo;s possible to integrate</p>
<p>some of the data augmentation into the learning process?</p>
<p>I think so.</p>
<p>And in fact, it will be really beneficial for us</p>
<p>because a lot of these data augmentations</p>
<p>that we use in vision are very extreme.</p>
<p>For example, like when you have certain concepts,</p>
<p>again, a banana, you take the banana</p>
<p>and then basically you change the color of the banana, right?</p>
<p>So you make it a purple banana.</p>
<p>Now this data augmentation process</p>
<p>is actually independent of the,</p>
<p>like it has no notion of what is present in the image.</p>
<p>So it can change this color arbitrarily.</p>
<p>It can make it a red banana as well.</p>
<p>And now what we&rsquo;re doing is we&rsquo;re telling</p>
<p>the neural network that this red banana</p>
<p>and so a crop of this image which has the red banana</p>
<p>and a crop of this image where I changed the color</p>
<p>to a purple banana should be,</p>
<p>the features should be the same.</p>
<p>Now bananas aren&rsquo;t red or purple mostly.</p>
<p>So really the data augmentation process</p>
<p>should take into account what is present in the image</p>
<p>and what are the kinds of physical realities</p>
<p>that are possible.</p>
<p>It shouldn&rsquo;t be completely independent of the image.</p>
<p>So you might get big gains if you,</p>
<p>instead of being drastic, do subtle augmentation</p>
<p>but realistic augmentation.</p>
<p>Right, realistic.</p>
<p>I&rsquo;m not sure if it&rsquo;s subtle, but like realistic for sure.</p>
<p>If it&rsquo;s realistic, then even subtle augmentation</p>
<p>will give you big benefits.</p>
<p>Exactly, yeah.</p>
<p>And it will be like for particular domains</p>
<p>you might actually see like,</p>
<p>if for example, now we&rsquo;re doing medical imaging,</p>
<p>there are going to be certain kinds</p>
<p>of like geometric augmentation</p>
<p>which are not really going to be very valid</p>
<p>for the human body.</p>
<p>So if you were to like actually loop in data augmentation</p>
<p>into the learning process,</p>
<p>it will actually be much more useful.</p>
<p>Now this actually does take us</p>
<p>to maybe a semi supervised kind of a setting</p>
<p>because you do want to understand</p>
<p>what is it that you&rsquo;re trying to solve.</p>
<p>So currently self supervised learning</p>
<p>kind of operates in the wild, right?</p>
<p>So you do the self supervised learning</p>
<p>and the purists and all of us basically say that,</p>
<p>okay, this should learn useful representations</p>
<p>and they should be useful for any kind of end task,</p>
<p>no matter it&rsquo;s like banana recognition</p>
<p>or like autonomous driving.</p>
<p>Now it&rsquo;s a tall order.</p>
<p>Maybe the first baby step for us should be that,</p>
<p>okay, if you&rsquo;re trying to loop in this data augmentation</p>
<p>into the learning process,</p>
<p>then we at least need to have some sense</p>
<p>of what we&rsquo;re trying to do.</p>
<p>Are we trying to distinguish</p>
<p>between different types of bananas</p>
<p>or are we trying to distinguish between banana and apple</p>
<p>or are we trying to do all of these things at once?</p>
<p>And so some notion of like what happens at the end</p>
<p>might actually help us do much better at this side.</p>
<p>Let me ask you a ridiculous question.</p>
<p>If I were to give you like a black box,</p>
<p>like a choice to have an arbitrary large data set</p>
<p>of real natural data</p>
<p>versus really good data augmentation algorithms,</p>
<p>which would you like to train in a self supervised way on?</p>
<p>So natural data from the internet are arbitrary large,</p>
<p>so unlimited data,</p>
<p>or it&rsquo;s like more controlled good data augmentation</p>
<p>on the finite data set.</p>
<p>The thing is like,</p>
<p>because our learning algorithms for vision right now</p>
<p>really rely on data augmentation,</p>
<p>even if you were to give me</p>
<p>like an infinite source of like image data,</p>
<p>I still need a good data augmentation algorithm.</p>
<p>You need something that tells you</p>
<p>that two things are similar.</p>
<p>Right.</p>
<p>And so something,</p>
<p>because you&rsquo;ve given me an arbitrary large data set,</p>
<p>I still need to use data augmentation</p>
<p>to take that image construct,</p>
<p>like these two perturbations of it,</p>
<p>and then learn from it.</p>
<p>So the thing is our learning paradigm</p>
<p>is very primitive right now.</p>
<p>Yeah.</p>
<p>Even if you were to give me lots of images,</p>
<p>it&rsquo;s still not really useful.</p>
<p>A good data augmentation algorithm</p>
<p>is actually going to be more useful.</p>
<p>So you can like reduce down the amount of data</p>
<p>that you give me by like 10 times,</p>
<p>but if you were to give me</p>
<p>a good data augmentation algorithm,</p>
<p>that would probably do better</p>
<p>than giving me like 10 times the size of that data,</p>
<p>but me having to rely on</p>
<p>like a very primitive data augmentation algorithm.</p>
<p>Like through tagging and all those kinds of things,</p>
<p>is there a way to discover things</p>
<p>that are semantically similar on the internet?</p>
<p>Obviously there is, but they might be extremely noisy.</p>
<p>And the difference might be farther away</p>
<p>than you would be comfortable with.</p>
<p>So, I mean, yes, tagging will help you a lot.</p>
<p>It&rsquo;ll actually go a very long way</p>
<p>in figuring out what images are related or not.</p>
<p>And then, so, but then the purists would argue</p>
<p>that when you&rsquo;re using human tags,</p>
<p>because these tags are like supervision,</p>
<p>is it really self supervised learning now?</p>
<p>Because you&rsquo;re using human tags</p>
<p>to figure out which images are like similar.</p>
<p>Hashtag no filter means a lot of things.</p>
<p>Yes.</p>
<p>I mean, there are certain tags</p>
<p>which are going to be applicable pretty much to anything.</p>
<p>So they&rsquo;re pretty useless for learning.</p>
<p>But I mean, certain tags are actually like</p>
<p>the Eiffel Tower, for example,</p>
<p>or the Taj Mahal, for example.</p>
<p>These tags are like very indicative of what&rsquo;s going on.</p>
<p>And they are, I mean, they are human supervision.</p>
<p>Yeah.</p>
<p>This is one of the tasks of discovering</p>
<p>from human generated data strong signals</p>
<p>that could be leveraged for self supervision.</p>
<p>Like humans are doing so much work already.</p>
<p>Like many years ago, there was something that was called,</p>
<p>I guess, human computation back in the day.</p>
<p>Humans are doing so much work.</p>
<p>It&rsquo;d be exciting to discover ways to leverage</p>
<p>the work they&rsquo;re doing to teach machines</p>
<p>without any extra effort from them.</p>
<p>An example could be, like we said, driving,</p>
<p>humans driving and machines can learn from the driving.</p>
<p>I always hope that there could be some supervision signal</p>
<p>discovered in video games,</p>
<p>because there&rsquo;s so many people that play video games</p>
<p>that it feels like so much effort is put into video games,</p>
<p>into playing video games,</p>
<p>and you can design video games somewhat cheaply</p>
<p>to include whatever signals you want.</p>
<p>It feels like that could be leverage somehow.</p>
<p>So people are using that.</p>
<p>Like there are actually folks right here in UT Austin,</p>
<p>like Philip Granbull is a professor at UT Austin.</p>
<p>He&rsquo;s been like working on video games</p>
<p>as a source of supervision.</p>
<p>I mean, it&rsquo;s really fun.</p>
<p>Like as a PhD student,</p>
<p>getting to basically play video games all day.</p>
<p>Yeah, but so I do hope that kind of thing scales</p>
<p>and like ultimately boils down to discovering</p>
<p>some undeniably very good signal.</p>
<p>It&rsquo;s like masking in NLP.</p>
<p>But that said, there&rsquo;s non contrastive methods.</p>
<p>What do non contrastive energy based</p>
<p>self supervised learning methods look like?</p>
<p>And why are they promising?</p>
<p>So like I said about contrastive learning,</p>
<p>you have this notion of a positive and a negative.</p>
<p>Now, the thing is, this entire learning paradigm</p>
<p>really requires access to a lot of negatives</p>
<p>to learn a good sort of feature space.</p>
<p>The idea is if I tell you, okay,</p>
<p>so a cat and a dog are similar,</p>
<p>and they&rsquo;re very different from a banana.</p>
<p>The thing is, this is a fairly simple analogy, right?</p>
<p>Because bananas look visually very different</p>
<p>from what cats and dogs do.</p>
<p>So very quickly, if this is the only source</p>
<p>of supervision that I&rsquo;m giving you,</p>
<p>your learning is not going to be like,</p>
<p>after a point, the neural network</p>
<p>is really not going to learn a lot.</p>
<p>Because the negative that you&rsquo;re getting</p>
<p>is going to be so random.</p>
<p>So it can be, oh, a cat and a dog are very similar,</p>
<p>but they&rsquo;re very different from a Volkswagen Beetle.</p>
<p>Now, like this car looks very different</p>
<p>from these animals again.</p>
<p>So the thing is in contrastive learning,</p>
<p>the quality of the negative sample really matters a lot.</p>
<p>And so what has happened is basically that</p>
<p>typically these methods that are contrastive</p>
<p>really require access to lots of negatives,</p>
<p>which becomes harder and harder to sort of scale</p>
<p>when designing a learning algorithm.</p>
<p>So that&rsquo;s been one of the reasons</p>
<p>why non contrastive methods have become like popular</p>
<p>and why people think that they&rsquo;re going to be more useful.</p>
<p>So a non contrastive method, for example,</p>
<p>like clustering is one non contrastive method.</p>
<p>The idea basically being that you have</p>
<p>two of these samples, so the cat and dog</p>
<p>or two crops of this image,</p>
<p>they belong to the same cluster.</p>
<p>And so essentially you&rsquo;re basically doing clustering online</p>
<p>when you&rsquo;re learning this network,</p>
<p>and which is very different from having access</p>
<p>to a lot of negatives explicitly.</p>
<p>The other way which has become really popular</p>
<p>is something called self distillation.</p>
<p>So the idea basically is that you have a teacher network</p>
<p>and a student network,</p>
<p>and the teacher network produces a feature.</p>
<p>So it takes in the image</p>
<p>and basically the neural network figures out the patterns</p>
<p>gets the feature out.</p>
<p>And there&rsquo;s another neural network</p>
<p>which is the student neural network</p>
<p>and that also produces a feature.</p>
<p>And now all you&rsquo;re doing is basically saying</p>
<p>that the features produced by the teacher network</p>
<p>and the student network should be very similar.</p>
<p>That&rsquo;s it.</p>
<p>There is no notion of a negative anymore.</p>
<p>And that&rsquo;s it.</p>
<p>So it&rsquo;s all about similarity maximization</p>
<p>between these two features.</p>
<p>And so all I need to now do is figure out</p>
<p>how to have these two sorts of parallel networks,</p>
<p>a student network and a teacher network.</p>
<p>And basically researchers have figured out</p>
<p>very cheap methods to do this.</p>
<p>So you can actually have for free really</p>
<p>two types of neural networks.</p>
<p>They&rsquo;re kind of related,</p>
<p>but they&rsquo;re different enough that you can actually</p>
<p>basically have a learning problem set up.</p>
<p>So you can ensure that they always remain different enough.</p>
<p>So the thing doesn&rsquo;t collapse into something boring.</p>
<p>Exactly.</p>
<p>So the main sort of enemy of self supervised learning,</p>
<p>any kind of similarity maximization technique is collapse.</p>
<p>It&rsquo;s a collapse means that you learn the same feature</p>
<p>representation for all the images in the world,</p>
<p>which is completely useless.</p>
<p>Everything&rsquo;s a banana.</p>
<p>Everything is a banana.</p>
<p>Everything is a cat.</p>
<p>Everything is a car.</p>
<p>And so all we need to do is basically come up with ways</p>
<p>to prevent collapse.</p>
<p>Contrastive learning is one way of doing it.</p>
<p>And then for example, like clustering or self distillation</p>
<p>or other ways of doing it.</p>
<p>We also had a recent paper where we used like</p>
<p>de correlation between like two sets of features</p>
<p>to prevent collapse.</p>
<p>So that&rsquo;s inspired a little bit by like Horace Barlow&rsquo;s</p>
<p>neuroscience principles.</p>
<p>By the way, I should comment that whoever counts</p>
<p>the number of times the word banana, apple, cat and dog</p>
<p>were using this conversation wins the internet.</p>
<p>I wish you luck.</p>
<p>What is Suave and the main improvement proposed</p>
<p>in the paper on supervised learning of visual features</p>
<p>by contrasting cluster assignments?</p>
<p>Suave basically is a clustering based technique,</p>
<p>which is for again, the same thing for self supervised</p>
<p>learning in vision where we have two crops.</p>
<p>And the idea basically is that you want the features</p>
<p>from these two crops of an image to lie in the same cluster</p>
<p>and basically crops that are coming from different images</p>
<p>to be in different clusters.</p>
<p>Now, typically in a sort of,</p>
<p>if you were to do this clustering,</p>
<p>you would perform clustering offline.</p>
<p>What that means is you would,</p>
<p>if you have a dataset of N examples,</p>
<p>you would run over all of these N examples,</p>
<p>get features for them, perform clustering.</p>
<p>So basically get some clusters</p>
<p>and then repeat the process again.</p>
<p>So this is offline basically because I need to do one pass</p>
<p>through the data to compute its clusters.</p>
<p>Suave is basically just a simple way of doing this online.</p>
<p>So as you&rsquo;re going through the data,</p>
<p>you&rsquo;re actually computing these clusters online.</p>
<p>And so of course there is like a lot of tricks involved</p>
<p>in how to do this in a robust manner without collapsing,</p>
<p>but this is this sort of key idea to it.</p>
<p>Is there a nice way to say what is the key methodology</p>
<p>of the clustering that enables that?</p>
<p>Right, so the idea basically is that</p>
<p>when you have N samples,</p>
<p>we assume that we have access to,</p>
<p>like there are always K clusters in a dataset.</p>
<p>K is a fixed number.</p>
<p>So for example, K is 3000.</p>
<p>And so if you have any,</p>
<p>when you look at any sort of small number of examples,</p>
<p>all of them must belong to one of these K clusters.</p>
<p>And we impose this equipartition constraint.</p>
<p>What this means is that basically</p>
<p>your entire set of N samples</p>
<p>should be equally partitioned into K clusters.</p>
<p>So all your K clusters are basically equal,</p>
<p>they have equal contribution to these N samples.</p>
<p>And this ensures that we never collapse.</p>
<p>So collapse can be viewed as a way</p>
<p>in which all samples belong to one cluster, right?</p>
<p>So all this, if all features become the same,</p>
<p>then you have basically just one mega cluster.</p>
<p>You don&rsquo;t even have like 10 clusters or 3000 clusters.</p>
<p>So Suave basically ensures that at each point,</p>
<p>all these 3000 clusters are being used</p>
<p>in the clustering process.</p>
<p>And that&rsquo;s it.</p>
<p>Basically just figure out how to do this online.</p>
<p>And again, basically just make sure</p>
<p>that two crops from the same image belong to the same cluster</p>
<p>and others don&rsquo;t.</p>
<p>And the fact they have a fixed K makes things simpler.</p>
<p>Fixed K makes things simpler.</p>
<p>Our clustering is not like really hard clustering,</p>
<p>it&rsquo;s soft clustering.</p>
<p>So basically you can be 0.2 to cluster number one</p>
<p>and 0.8 to cluster number two.</p>
<p>So it&rsquo;s not really hard.</p>
<p>So essentially, even though we have like 3000 clusters,</p>
<p>we can actually represent a lot of clusters.</p>
<p>What is SEER, S E E R?</p>
<p>And what are the key results and insights in the paper,</p>
<p>Self Supervised Pre Training of Visual Features in the Wild?</p>
<p>What is this big, beautiful SEER system?</p>
<p>SEER, so I&rsquo;ll first go to Suave</p>
<p>because Suave is actually like one</p>
<p>of the key components for SEER.</p>
<p>So Suave was, when we use Suave,</p>
<p>it was demonstrated on ImageNet.</p>
<p>So typically like self supervised methods,</p>
<p>the way we sort of operate is like in the research community,</p>
<p>we kind of cheat.</p>
<p>So we take ImageNet, which of course I talked about</p>
<p>as having lots of labels.</p>
<p>And then we throw away the labels,</p>
<p>like throw away all the hard work that went behind</p>
<p>basically the labeling process.</p>
<p>And we pretend that it is unsupervised.</p>
<p>But the problem here is that we have,</p>
<p>like when we collected these images,</p>
<p>the ImageNet dataset has a particular distribution</p>
<p>of concepts, right?</p>
<p>So these images are very curated.</p>
<p>And what that means is these images, of course,</p>
<p>belong to a certain set of noun concepts.</p>
<p>And also ImageNet has this bias that all images</p>
<p>contain an object, which is like very big</p>
<p>and it&rsquo;s typically in the center.</p>
<p>So when you&rsquo;re talking about a dog, it&rsquo;s a well framed dog,</p>
<p>it&rsquo;s towards the center of the image.</p>
<p>So a lot of the data augmentation,</p>
<p>a lot of the sort of hidden assumptions</p>
<p>in self supervised learning,</p>
<p>actually really exploit this bias of ImageNet.</p>
<p>And so, I mean, a lot of my work,</p>
<p>a lot of work from other people always uses ImageNet</p>
<p>sort of as the benchmark to show the success</p>
<p>of self supervised learning.</p>
<p>So you&rsquo;re implying that there&rsquo;s particular limitations</p>
<p>to this kind of dataset?</p>
<p>Yes, I mean, it&rsquo;s basically because our data augmentation</p>
<p>that we designed, like all data augmentation</p>
<p>that we designed for self supervised learning in vision</p>
<p>are kind of overfit to ImageNet.</p>
<p>But you&rsquo;re saying a little bit hard coded</p>
<p>like the cropping.</p>
<p>Exactly, the cropping parameters,</p>
<p>the kind of lighting that we&rsquo;re using,</p>
<p>the kind of blurring that we&rsquo;re using.</p>
<p>Yeah, but you would, for more in the wild dataset,</p>
<p>you would need to be clever or more careful</p>
<p>in setting the range of parameters</p>
<p>and those kinds of things.</p>
<p>So for SEER, our main goal was twofold.</p>
<p>One, basically to move away from ImageNet for training.</p>
<p>So the images that we used were like uncurated images.</p>
<p>Now there&rsquo;s a lot of debate</p>
<p>whether they&rsquo;re actually curated or not,</p>
<p>but I&rsquo;ll talk about that later.</p>
<p>But the idea was basically,</p>
<p>these are going to be random internet images</p>
<p>that we&rsquo;re not going to filter out</p>
<p>based on like particular categories.</p>
<p>So we did not say that, oh, images that belong to dogs</p>
<p>and cats should be the only images</p>
<p>that come in this dataset, banana.</p>
<p>And basically, other images should be thrown out.</p>
<p>So we didn&rsquo;t do any of that.</p>
<p>So these are random internet images.</p>
<p>And of course, it also goes back to like the problem</p>
<p>of scale that you talked about.</p>
<p>So these were basically about a billion or so images.</p>
<p>And for context ImageNet,</p>
<p>the ImageNet version that we use</p>
<p>was 1 million images earlier.</p>
<p>So this is basically going like</p>
<p>three orders of magnitude more.</p>
<p>The idea was basically to see</p>
<p>if we can train a very large convolutional model</p>
<p>in a self supervised way on this uncurated,</p>
<p>but really large set of images.</p>
<p>And how well would this model do?</p>
<p>So is self supervised learning really overfit to ImageNet</p>
<p>or can it actually work in the wild?</p>
<p>And it was also out of curiosity,</p>
<p>what kind of things will this model learn?</p>
<p>Will it actually be able to still figure out</p>
<p>different types of objects and so on?</p>
<p>Would there be particular kinds of tasks</p>
<p>that would actually do better than an ImageNet train model?</p>
<p>And so for Sear, one of our main findings was that</p>
<p>we can actually train very large models</p>
<p>in a completely self supervised way</p>
<p>on lots of internet images</p>
<p>without really necessarily filtering them out.</p>
<p>Which was in itself a good thing</p>
<p>because it&rsquo;s a fairly simple process, right?</p>
<p>So you get images which are uploaded</p>
<p>and you basically can immediately use them</p>
<p>to train a model in an unsupervised way.</p>
<p>You don&rsquo;t really need to sit and filter them out.</p>
<p>These images can be cartoons, these can be memes,</p>
<p>these can be actual pictures uploaded by people.</p>
<p>And you don&rsquo;t really care about what these images are.</p>
<p>You don&rsquo;t even care about what concepts they contain.</p>
<p>So this was a very sort of simple setup.</p>
<p>What image selection mechanism would you say</p>
<p>is there like inherent in some aspect of the process?</p>
<p>So you&rsquo;re kind of implying that there&rsquo;s almost none,</p>
<p>but what is there would you say if you were to introspect?</p>
<p>Right, so it&rsquo;s not like uncurated can basically</p>
<p>like one way of imagining uncurated</p>
<p>is basically you have like cameras</p>
<p>that can take pictures at random viewpoints.</p>
<p>When people upload pictures to the internet,</p>
<p>they are typically going to care about the framing of it.</p>
<p>They&rsquo;re not going to upload, say,</p>
<p>the picture of a zoomed in wall, for example.</p>
<p>Well, when you say internet, do you mean social networks?</p>
<p>Yes. Okay.</p>
<p>So these are not going to be like pictures</p>
<p>of like a zoomed in table or a zoomed in wall.</p>
<p>So it&rsquo;s not really completely uncurated</p>
<p>because people do have the like photographer&rsquo;s bias</p>
<p>where they do want to keep things</p>
<p>towards the center a little bit,</p>
<p>or like really have like nice looking things</p>
<p>and so on in the picture.</p>
<p>So that&rsquo;s the kind of bias that typically exists</p>
<p>in this data set and also the user base, right?</p>
<p>You&rsquo;re not going to get lots of pictures</p>
<p>from different parts of the world</p>
<p>because there are certain parts of the world</p>
<p>where people may not actually be uploading</p>
<p>a lot of pictures to the internet</p>
<p>or may not even have access to a lot of internet.</p>
<p>So this is a giant data set and a giant neural network.</p>
<p>I don&rsquo;t think we&rsquo;ve talked about what architectures</p>
<p>work well for SSL, for self supervised learning.</p>
<p>For SEER and for SWAB, we were using convolutional networks,</p>
<p>but recently in a work called Dyno,</p>
<p>we&rsquo;ve basically started using transformers for vision.</p>
<p>Both seem to work really well, Connets and transformers.</p>
<p>And depending on what you want to do,</p>
<p>you might choose to use a particular formulation.</p>
<p>So for SEER, it was a Connet.</p>
<p>It was particularly a RegNet model,</p>
<p>which was also a work from Facebook.</p>
<p>RegNets are like really good when it comes to compute</p>
<p>versus like accuracy.</p>
<p>So because it was a very efficient model,</p>
<p>compute and memory wise efficient,</p>
<p>and basically it worked really well in terms of scaling.</p>
<p>So we used a very large RegNet model</p>
<p>and trained it on a billion images.</p>
<p>Can you maybe quickly comment on what RegNets are?</p>
<p>It comes from this paper, Designing Network Design Spaces.</p>
<p>This is a super interesting concept</p>
<p>that emphasizes how to create efficient neural networks,</p>
<p>large neural networks.</p>
<p>So one of the sort of key takeaways from this paper,</p>
<p>which the authors, like whenever you hear them</p>
<p>present this work, they keep saying is,</p>
<p>a lot of neural networks are characterized</p>
<p>in terms of flops, right?</p>
<p>Flops basically being the floating point operations.</p>
<p>And people really love to use flops to say,</p>
<p>this model is like really computationally heavy,</p>
<p>or like our model is computationally cheap and so on.</p>
<p>Now it turns out that flops are really not a good indicator</p>
<p>of how well a particular network is,</p>
<p>like how efficient it is really.</p>
<p>And what a better indicator is, is the activation</p>
<p>or the memory that is being used by this particular model.</p>
<p>And so designing, like one of the key findings</p>
<p>from this paper was basically that you need to design</p>
<p>network families or neural network architectures</p>
<p>that are actually very efficient in the memory space as well,</p>
<p>not just in terms of pure flops.</p>
<p>So RegNet is basically a network architecture family</p>
<p>that came out of this paper that is particularly good</p>
<p>at both flops and the sort of memory required for it.</p>
<p>And of course it builds upon like earlier work,</p>
<p>like ResNet being like the sort of more popular inspiration</p>
<p>for it, where you have residual connections.</p>
<p>But one of the things in this work is basically</p>
<p>they also use like squeeze excitation blocks.</p>
<p>So it&rsquo;s a lot of nice sort of technical innovation</p>
<p>in all of this from prior work,</p>
<p>and a lot of the ingenuity of these particular authors</p>
<p>in how to combine these multiple building blocks.</p>
<p>But the key constraint was optimize for both flops</p>
<p>and memory when you&rsquo;re basically doing this,</p>
<p>don&rsquo;t just look at flops.</p>
<p>And that allows you to what have a,</p>
<p>sort of have very large networks through this process,</p>
<p>can optimize for low, like for efficiency, for low memory.</p>
<p>Also in just in terms of pure hardware,</p>
<p>they fit very well on GPU memory.</p>
<p>So they can be like really powerful neural network</p>
<p>architectures with lots of parameters, lots of flops,</p>
<p>but also because they&rsquo;re like efficient in terms of</p>
<p>the amount of memory that they&rsquo;re using,</p>
<p>you can actually fit a lot of these on like a,</p>
<p>you can fit a very large model on a single GPU for example.</p>
<p>Would you say that the choice of architecture</p>
<p>matters more than the choice of maybe data augmentation</p>
<p>techniques?</p>
<p>Is there a possibility to say what matters more?</p>
<p>You kind of imply that you can probably go really far</p>
<p>with just using basic conv nuts.</p>
<p>All right, I think like data and data augmentation,</p>
<p>the algorithm being used for the self supervised training</p>
<p>matters a lot more than the particular kind of architecture.</p>
<p>With different types of architecture,</p>
<p>you will get different like properties in the resulting</p>
<p>sort of representation.</p>
<p>But really, I mean, the secret sauce is in the augmentation</p>
<p>and the algorithm being used to train them.</p>
<p>The architectures, I mean, at this point,</p>
<p>a lot of them perform very similarly,</p>
<p>depending on like the particular task that you care about,</p>
<p>they have certain advantages and disadvantages.</p>
<p>Is there something interesting to be said about what it</p>
<p>takes with Sears to train a giant neural network?</p>
<p>You&rsquo;re talking about a huge amount of data,</p>
<p>a huge neural network.</p>
<p>Is there something interesting to be said of how to</p>
<p>effectively train something like that fast?</p>
<p>Lots of GPUs.</p>
<p>Okay.</p>
<p>I mean, so the model was like a billion parameters.</p>
<p>And it was trained on a billion images.</p>
<p>So if like, basically the same number of parameters</p>
<p>as the number of images, and it took a while.</p>
<p>I don&rsquo;t remember the exact number, it&rsquo;s in the paper,</p>
<p>but it took a while.</p>
<p>I guess I&rsquo;m trying to get at is,</p>
<p>when you&rsquo;re thinking of scaling this kind of thing,</p>
<p>I mean, one of the exciting possibilities of self</p>
<p>supervised learning is the several orders of magnitude</p>
<p>scaling of everything, both the neural network</p>
<p>and the size of the data.</p>
<p>And so the question is,</p>
<p>do you think there&rsquo;s some interesting tricks to do large</p>
<p>scale distributed compute,</p>
<p>or is that really outside of even deep learning?</p>
<p>That&rsquo;s more about like hardware engineering.</p>
<p>I think more and more there is like this,</p>
<p>a lot of like systems are designed,</p>
<p>basically taking into account</p>
<p>the machine learning needs, right?</p>
<p>So because whenever you&rsquo;re doing this kind of</p>
<p>distributed training, there is a lot of intercommunication</p>
<p>between nodes.</p>
<p>So like gradients or the model parameters are being passed.</p>
<p>So you really want to minimize communication costs</p>
<p>when you really want to scale these models up.</p>
<p>You want basically to be able to do as much,</p>
<p>like as limited amount of communication as possible.</p>
<p>So currently like a dominant paradigm</p>
<p>is synchronized sort of training.</p>
<p>So essentially after every sort of gradient step,</p>
<p>all you basically have like a synchronization step</p>
<p>between all the sort of compute chips</p>
<p>that you&rsquo;re going on with.</p>
<p>I think asynchronous training was popular,</p>
<p>but it doesn&rsquo;t seem to perform as well.</p>
<p>But in general, I think that&rsquo;s sort of the,</p>
<p>I guess it&rsquo;s outside my scope as well.</p>
<p>But the main thing is like minimize the amount of</p>
<p>synchronization steps that you have.</p>
<p>That has been the key takeaway, at least in my experience.</p>
<p>The others I have no idea about, how to design the chip.</p>
<p>Yeah, there&rsquo;s very few things that I see Jim Keller&rsquo;s eyes</p>
<p>light up as much as talking about giant computers doing</p>
<p>like that fast communication that you&rsquo;re talking to well</p>
<p>when they&rsquo;re training machine learning systems.</p>
<p>What is VSSL, V I S S L, the PyTorch based SSL library?</p>
<p>What are the use cases that you might have?</p>
<p>VSSL basically was born out of a lot of us at Facebook</p>
<p>are doing the self supervised learning research.</p>
<p>So it&rsquo;s a common framework in which we have like a lot of</p>
<p>self supervised learning methods implemented for vision.</p>
<p>It&rsquo;s also, it has in itself like a benchmark of tasks</p>
<p>that you can evaluate the self supervised representations on.</p>
<p>So the use case for it is basically for anyone who&rsquo;s either</p>
<p>trying to evaluate their self supervised model</p>
<p>or train their self supervised model,</p>
<p>or a researcher who&rsquo;s trying to build</p>
<p>a new self supervised technique.</p>
<p>So it&rsquo;s basically supposed to be all of these things.</p>
<p>So as a researcher before VSSL, for example,</p>
<p>or like when we started doing this work fairly seriously</p>
<p>at Facebook, it was very hard for us to go and implement</p>
<p>every self supervised learning model,</p>
<p>test it out in a like sort of consistent manner.</p>
<p>The experimental setup was very different</p>
<p>across different groups.</p>
<p>Even when someone said that they were reporting</p>
<p>image net accuracy, it could mean lots of different things.</p>
<p>So with VSSL, we tried to really sort of standardize that</p>
<p>as much as possible.</p>
<p>And there was a paper like we did in 2019</p>
<p>just about benchmarking.</p>
<p>And so VSSL basically builds upon a lot of this kind of work</p>
<p>that we did about like benchmarking.</p>
<p>And then every time we try to like,</p>
<p>we come up with a self supervised learning method,</p>
<p>a lot of us try to push that into VSSL as well,</p>
<p>just so that it basically is like the central piece</p>
<p>where a lot of these methods can reside.</p>
<p>Just out of curiosity, people may be,</p>
<p>so certainly outside of Facebook, but just researchers,</p>
<p>or just even people that know how to program in Python</p>
<p>and know how to use PyTorch, what would be the use case?</p>
<p>What would be a fun thing to play around with VSSL on?</p>
<p>Like what&rsquo;s a fun thing to play around</p>
<p>with self supervised learning on, would you say?</p>
<p>Is there a good Hello World program?</p>
<p>Like is it always about big size that&rsquo;s important to have,</p>
<p>or is there fun little smaller case playgrounds</p>
<p>to play around with?</p>
<p>So we&rsquo;re trying to like push something towards that.</p>
<p>I think there are a few setups out there,</p>
<p>but nothing like super standard on the smaller scale.</p>
<p>I mean, ImageNet in itself is actually pretty big also.</p>
<p>So that is not something</p>
<p>which is like feasible for a lot of people.</p>
<p>But we are trying to like push up</p>
<p>with like smaller sort of use cases.</p>
<p>The thing is, at a smaller scale,</p>
<p>a lot of the observations</p>
<p>or a lot of the algorithms that work</p>
<p>don&rsquo;t necessarily translate into the medium</p>
<p>or the larger scale.</p>
<p>So it&rsquo;s really tricky to come up</p>
<p>with a good small scale setup</p>
<p>where a lot of your empirical observations</p>
<p>will really translate to the other setup.</p>
<p>So it&rsquo;s been really challenging.</p>
<p>I&rsquo;ve been trying to do that for a little bit as well</p>
<p>because it does take time to train stuff on ImageNet.</p>
<p>It does take time to train on like more images,</p>
<p>but pretty much every time I&rsquo;ve tried to do that,</p>
<p>it&rsquo;s been unsuccessful</p>
<p>because all the observations I draw</p>
<p>from my set of experiments on a smaller data set</p>
<p>don&rsquo;t translate into ImageNet</p>
<p>or like don&rsquo;t translate into another sort of data set.</p>
<p>So it&rsquo;s been hard for us to figure this one out,</p>
<p>but it&rsquo;s an important problem.</p>
<p>So there&rsquo;s this really interesting idea</p>
<p>of learning across multiple modalities.</p>
<p>You have a CVPR 2021 best paper candidate</p>
<p>titled audio visual instance discrimination</p>
<p>with cross modal agreement.</p>
<p>What are the key results, insights in this paper</p>
<p>and what can you say in general</p>
<p>about the promise and power of multimodal learning?</p>
<p>For this paper, it actually came as a little bit</p>
<p>of a shock to me at how well it worked.</p>
<p>So I can describe what the problem set up was.</p>
<p>So it&rsquo;s been used in the past by lots of folks</p>
<p>like for example, Andrew Owens from MIT,</p>
<p>Alyosha Efros from Berkeley,</p>
<p>Andrew Zisserman from Oxford.</p>
<p>So a lot of these people have been</p>
<p>sort of showing results in this.</p>
<p>Of course, I was aware of this result,</p>
<p>but I wasn&rsquo;t really sure how well it would work in practice</p>
<p>for like other sort of downstream tasks.</p>
<p>So the results kept getting better.</p>
<p>And I wasn&rsquo;t sure if like a lot of our insights</p>
<p>from self supervised learning would translate</p>
<p>into this multimodal learning problem.</p>
<p>So multimodal learning is when you have like,</p>
<p>when you have multiple modalities.</p>
<p>That&rsquo;s not even cool.</p>
<p>Okay, so the particular modalities</p>
<p>that we worked on in this work were audio and video.</p>
<p>So the idea was basically, if you have a video,</p>
<p>you have its corresponding audio track.</p>
<p>And you want to use both of these signals,</p>
<p>the audio signal and the video signal</p>
<p>to learn a good representation for video</p>
<p>and good representation for audio.</p>
<p>Like this podcast.</p>
<p>Like this podcast, exactly.</p>
<p>So what we did in this work was basically train</p>
<p>two different neural networks,</p>
<p>one on the video signal, one on the audio signal.</p>
<p>And what we wanted is basically the features</p>
<p>that we get from both of these neural networks</p>
<p>should be similar.</p>
<p>So it should basically be able to produce</p>
<p>the same kinds of features from the video</p>
<p>and the same kinds of features from the audio.</p>
<p>Now, why is this useful?</p>
<p>Well, for a lot of these objects that we have,</p>
<p>there is a characteristic sound, right?</p>
<p>So trains, when they go by,</p>
<p>they make a particular kind of sound.</p>
<p>Boats make a particular kind of sound.</p>
<p>People, when they&rsquo;re jumping around,</p>
<p>will like shout, whatever.</p>
<p>Bananas don&rsquo;t make a sound.</p>
<p>So where you can&rsquo;t learn anything about bananas there.</p>
<p>Or when humans mentioned bananas.</p>
<p>Well, yes, when they say the word banana, then.</p>
<p>So you can&rsquo;t trust basically anything</p>
<p>that comes out of a human&rsquo;s mouth as a source,</p>
<p>that source of audio is useless.</p>
<p>The typical use case is basically like,</p>
<p>for example, someone playing a musical instrument.</p>
<p>So guitars have a particular kind of sound and so on.</p>
<p>So because a lot of these things are correlated,</p>
<p>the idea in multimodal learning</p>
<p>is to take these two kinds of modalities,</p>
<p>video and audio, and learn a common embedding space,</p>
<p>a common feature space where both of these</p>
<p>related modalities can basically be close together.</p>
<p>And again, you use contrastive learning for this.</p>
<p>So in contrastive learning, basically the video</p>
<p>and the corresponding audio are positives.</p>
<p>And you can take any other video or any other audio</p>
<p>and that becomes a negative.</p>
<p>And so basically that&rsquo;s it.</p>
<p>It&rsquo;s just a simple application of contrastive learning.</p>
<p>The main sort of finding from this work for us</p>
<p>was basically that you can actually learn</p>
<p>very, very powerful feature representations,</p>
<p>very, very powerful video representations.</p>
<p>So you can learn the sort of video network</p>
<p>that we ended up learning can actually be used</p>
<p>for downstream, for example, recognizing human actions</p>
<p>or recognizing different types of sounds, for example.</p>
<p>So this was sort of the key finding.</p>
<p>Can you give kind of an example of a human action</p>
<p>or like just so we can build up intuition</p>
<p>of what kind of thing?</p>
<p>Right, so there is this data set called kinetics,</p>
<p>for example, which has like 400 different types</p>
<p>of human actions.</p>
<p>So people jumping, people doing different kinds of sports</p>
<p>or different types of swimming.</p>
<p>So like different strokes and swimming, golf and so on.</p>
<p>So there are like just different types of actions</p>
<p>right there.</p>
<p>And the point is this kind of video network</p>
<p>that you learn in a self supervised way</p>
<p>can be used very easily to kind of recognize</p>
<p>these different types of actions.</p>
<p>It can also be used for recognizing</p>
<p>different types of objects.</p>
<p>And what we did is we tried to visualize</p>
<p>whether the network can figure out</p>
<p>where the sound is coming from.</p>
<p>So basically, give it a video</p>
<p>and basically play say of a person just strumming a guitar,</p>
<p>but of course, there is no audio in this.</p>
<p>And now you give it this sound of a guitar.</p>
<p>And you ask like basically try to visualize</p>
<p>where the network thinks the sound is coming from.</p>
<p>And that can kind of basically draw like</p>
<p>when you visualize it,</p>
<p>you can see that it&rsquo;s basically focusing on the guitar.</p>
<p>Yeah, that&rsquo;s surreal.</p>
<p>And the same thing, for example,</p>
<p>for certain people&rsquo;s voices,</p>
<p>like famous celebrities voices,</p>
<p>it can actually figure out where their mouth is.</p>
<p>So it can actually distinguish different people&rsquo;s voices,</p>
<p>for example, a little bit as well.</p>
<p>Without that ever being annotated in any way.</p>
<p>Right, so this is all what it had discovered.</p>
<p>We never pointed out that this is a guitar</p>
<p>and this is the kind of sound it produces.</p>
<p>It can actually naturally figure that out</p>
<p>because it&rsquo;s seen so many correlations of this sound</p>
<p>coming with this kind of like an object</p>
<p>that it basically learns to associate this sound</p>
<p>with this kind of an object.</p>
<p>Yeah, that&rsquo;s really fascinating, right?</p>
<p>That&rsquo;s really interesting.</p>
<p>So the idea with this kind of network</p>
<p>is then you then fine tune it for a particular task.</p>
<p>So this is forming like a really good knowledge base</p>
<p>within a neural network based on which you could then</p>
<p>the train a little bit more to accomplish a specific task.</p>
<p>Well, so you don&rsquo;t need a lot of videos of humans</p>
<p>doing actions annotated.</p>
<p>You can just use a few of them to basically get your.</p>
<p>How much insight do you draw from the fact</p>
<p>that it can figure out where the sound is coming from?</p>
<p>I&rsquo;m trying to see, so that&rsquo;s kind of very,</p>
<p>it&rsquo;s very CVPR beautiful, right?</p>
<p>It&rsquo;s a cool little insight.</p>
<p>I wonder how profound that is.</p>
<p>Does it speak to the idea that multiple modalities</p>
<p>are somehow much bigger than the sum of their parts?</p>
<p>Or is it really, really useful to have multiple modalities?</p>
<p>Or is it just that cool thing that there&rsquo;s parts</p>
<p>of our world that can be revealed like effectively</p>
<p>through multiple modalities,</p>
<p>but most of it is really all about vision</p>
<p>or about one of the modalities.</p>
<p>I would say a little tending more towards the second part.</p>
<p>So most of it can be sort of figured out with one modality,</p>
<p>but having an extra modality always helps you.</p>
<p>So in this case, for example,</p>
<p>like one thing is when you&rsquo;re,</p>
<p>if you observe someone cutting something</p>
<p>and you don&rsquo;t have any sort of sound there,</p>
<p>whether it&rsquo;s an apple or whether it&rsquo;s an onion,</p>
<p>it&rsquo;s very hard to figure that out.</p>
<p>But if you hear someone cutting it,</p>
<p>it&rsquo;s very easy to figure it out because apples and onions</p>
<p>make a very different kind of characteristics</p>
<p>on when they&rsquo;re cut.</p>
<p>So you really figure this out based on audio,</p>
<p>it&rsquo;s much easier.</p>
<p>So your life will become much easier</p>
<p>when you have access to different kinds of modalities.</p>
<p>And the other thing is, so I like to relate it in this way,</p>
<p>it may be like completely wrong,</p>
<p>but the distributional hypothesis in NLP,</p>
<p>where context basically gives kind of meaning to that word,</p>
<p>sound kind of does that too.</p>
<p>So if you have the same sound,</p>
<p>so that&rsquo;s the same context across different videos,</p>
<p>you&rsquo;re very likely to be observing the same kind of concept.</p>
<p>So that&rsquo;s the kind of reason</p>
<p>why it figures out the guitar thing, right?</p>
<p>It observed the same sound across multiple different videos</p>
<p>and it figures out maybe this is the common factor</p>
<p>that&rsquo;s actually doing it.</p>
<p>I wonder, I used to have this argument with my dad a bunch</p>
<p>for creating general intelligence,</p>
<p>whether smell is an important,</p>
<p>like if that&rsquo;s important sensory information,</p>
<p>mostly we&rsquo;re talking about like falling in love</p>
<p>with an AI system and for him,</p>
<p>smell and touch are important.</p>
<p>And I was arguing that it&rsquo;s not at all.</p>
<p>It&rsquo;s important, it&rsquo;s nice and everything,</p>
<p>but like you can fall in love with just language really,</p>
<p>but a voice is very powerful and vision is next</p>
<p>and smell is not that important.</p>
<p>Can I ask you about this process of active learning?</p>
<p>You mentioned interactivity.</p>
<p>Right.</p>
<p>Is there some value</p>
<p>within the self supervised learning context</p>
<p>to select parts of the data in intelligent ways</p>
<p>such that they would most benefit the learning process?</p>
<p>So I think so.</p>
<p>I mean, I know I&rsquo;m talking to an active learning fan here,</p>
<p>so of course I know the answer.</p>
<p>First you were talking bananas</p>
<p>and now you&rsquo;re talking about active learning.</p>
<p>I love it.</p>
<p>I think Yannakun told me that active learning</p>
<p>is not that interesting.</p>
<p>I think back then I didn&rsquo;t want to argue with him too much,</p>
<p>but when we talk again,</p>
<p>we&rsquo;re gonna spend three hours arguing about active learning.</p>
<p>My sense was you can go extremely far with active learning,</p>
<p>perhaps farther than anything else.</p>
<p>Like to me, there&rsquo;s this kind of intuition</p>
<p>that similar to data augmentation,</p>
<p>you can get a lot from the data,</p>
<p>from intelligent optimized usage of the data.</p>
<p>I&rsquo;m trying to speak generally in such a way</p>
<p>that includes data augmentation</p>
<p>and active learning,</p>
<p>that there&rsquo;s something about maybe interactive exploration</p>
<p>of the data that at least is part</p>
<p>of the solution to intelligence, like an important part.</p>
<p>I don&rsquo;t know what your thoughts are</p>
<p>on active learning in general.</p>
<p>I actually really like active learning.</p>
<p>So back in the day we did this largely ignored CVPR paper</p>
<p>called learning by asking questions.</p>
<p>So the idea was basically you would train an agent</p>
<p>that would ask a question about the image.</p>
<p>It would get an answer</p>
<p>and basically then it would update itself.</p>
<p>It would see the next image.</p>
<p>It would decide what&rsquo;s the next hardest question</p>
<p>that I can ask to learn the most.</p>
<p>And the idea was basically because it was being smart</p>
<p>about the kinds of questions it was asking,</p>
<p>it would learn in fewer samples.</p>
<p>It would be more efficient at using data.</p>
<p>And we did find to some extent</p>
<p>that it was actually better than randomly asking questions.</p>
<p>Kind of weird thing about active learning</p>
<p>is it&rsquo;s also a chicken and egg problem</p>
<p>because when you look at an image,</p>
<p>to ask a good question about the image,</p>
<p>you need to understand something about the image.</p>
<p>You can&rsquo;t ask a completely arbitrarily random question.</p>
<p>It may not even apply to that particular image.</p>
<p>So there is some amount of understanding or knowledge</p>
<p>that basically keeps getting built</p>
<p>when you&rsquo;re doing active learning.</p>
<p>So I think active learning by itself is really good.</p>
<p>And the main thing we need to figure out is basically</p>
<p>how do we come up with a technique</p>
<p>to first model what the model knows</p>
<p>and also model what the model does not know.</p>
<p>I think that&rsquo;s the sort of beauty of it.</p>
<p>Because when you know that there are certain things</p>
<p>that you don&rsquo;t know anything about,</p>
<p>asking a question about those concepts</p>
<p>is actually going to bring you the most value.</p>
<p>And I think that&rsquo;s the sort of key challenge.</p>
<p>Now, self supervised learning by itself,</p>
<p>like selecting data for it and so on,</p>
<p>that&rsquo;s actually really useful.</p>
<p>But I think that&rsquo;s a very narrow view</p>
<p>of looking at active learning.</p>
<p>If you look at it more broadly,</p>
<p>it is basically about if the model has a knowledge</p>
<p>about N concepts,</p>
<p>and it is weak basically about certain things.</p>
<p>So it needs to ask questions</p>
<p>either to discover new concepts</p>
<p>or to basically increase its knowledge</p>
<p>about these N concepts.</p>
<p>So at that level, it&rsquo;s a very powerful technique.</p>
<p>I actually do think it&rsquo;s going to be really useful.</p>
<p>Even in like simple things such as like data labeling,</p>
<p>it&rsquo;s super useful.</p>
<p>So here is like one simple way</p>
<p>that you can use active learning.</p>
<p>For example, you have your self supervised model,</p>
<p>which is very good at predicting similarities</p>
<p>and dissimilarities between things.</p>
<p>And so if you label a picture as basically say a banana,</p>
<p>now you know that all the images</p>
<p>that are very similar to this image</p>
<p>are also likely to contain bananas.</p>
<p>So probably when you want to understand</p>
<p>what else is a banana,</p>
<p>you&rsquo;re not going to use these other images.</p>
<p>You&rsquo;re actually going to use an image</p>
<p>that is not completely dissimilar,</p>
<p>but somewhere in between,</p>
<p>which is not super similar to this image,</p>
<p>but not super dissimilar either.</p>
<p>And that&rsquo;s going to tell you a lot more</p>
<p>about what this concept of a banana is.</p>
<p>So that&rsquo;s kind of a heuristic.</p>
<p>I wonder if it&rsquo;s possible to also learn ways</p>
<p>to discover the most likely,</p>
<p>the most beneficial image.</p>
<p>So like, so not just looking a thing</p>
<p>that&rsquo;s somewhat similar to a banana,</p>
<p>but not exactly similar,</p>
<p>but have some kind of more complicated learning system,</p>
<p>like learned discovering mechanism</p>
<p>that tells you what image to look for.</p>
<p>Like how, yeah, like actually in a self supervised way,</p>
<p>learning strictly a function that says,</p>
<p>is this image going to be very useful to me</p>
<p>given what I currently know?</p>
<p>I think there&rsquo;s a lot of synergy there.</p>
<p>It&rsquo;s just, I think, yeah, it&rsquo;s going to be explored.</p>
<p>I think very much related to that.</p>
<p>I kind of think of what Tesla Autopilot is doing</p>
<p>currently as kind of active learning.</p>
<p>There&rsquo;s something that Andre Capati and their team</p>
<p>are calling a data engine.</p>
<p>So you&rsquo;re basically deploying a bunch of instantiations</p>
<p>of a neural network into the wild,</p>
<p>and they&rsquo;re collecting a bunch of edge cases</p>
<p>that are then sent back for annotation for particular,</p>
<p>and edge cases as defined as near failure</p>
<p>or some weirdness on a particular task</p>
<p>that&rsquo;s then sent back.</p>
<p>It&rsquo;s that not exactly a banana,</p>
<p>but almost the banana cases sent back for annotation.</p>
<p>And then there&rsquo;s this loop that keeps going</p>
<p>and you keep retraining and retraining.</p>
<p>And the active learning step there,</p>
<p>or whatever you want to call it,</p>
<p>is the cars themselves that are sending you back the data.</p>
<p>Like, what the hell happened here?</p>
<p>This was weird.</p>
<p>What are your thoughts about that sort of deployment</p>
<p>of neural networks in the wild?</p>
<p>Another way to ask a question from first is your thoughts.</p>
<p>And maybe if you want to comment,</p>
<p>is there applications for autonomous driving,</p>
<p>like computer vision based autonomous driving,</p>
<p>applications of self supervised learning</p>
<p>in the context of computer vision based autonomous driving?</p>
<p>So I think so.</p>
<p>I think for self supervised learning</p>
<p>to be used in autonomous driving,</p>
<p>there are lots of opportunities.</p>
<p>I mean, just like pure consistency in predictions</p>
<p>is one way, right?</p>
<p>So because you have this nice sequence of data</p>
<p>that is coming in, a video stream of it,</p>
<p>associated of course with the actions</p>
<p>that say the car took,</p>
<p>you can form a very nice predictive model</p>
<p>of what&rsquo;s happening.</p>
<p>So for example, like all the way,</p>
<p>like one way possibly in which how they&rsquo;re figuring out</p>
<p>what data to get labeled is basically</p>
<p>through prediction uncertainty, right?</p>
<p>So you predict that the car was going to turn right.</p>
<p>So this was the action that was going to happen,</p>
<p>say in the shadow mode.</p>
<p>And now the driver turned left.</p>
<p>And this is a really big surprise.</p>
<p>So basically by forming these good predictive models,</p>
<p>you are, I mean, these are kind of self supervised models.</p>
<p>Prediction models are basically being trained</p>
<p>just by looking at what&rsquo;s going to happen next</p>
<p>and asking them to predict what&rsquo;s going to happen next.</p>
<p>So I would say this is really like one use</p>
<p>of self supervised learning.</p>
<p>It&rsquo;s a predictive model</p>
<p>and you&rsquo;re learning a predictive model</p>
<p>basically just by looking at what data you have.</p>
<p>Is there something about that active learning context</p>
<p>that you find insights from?</p>
<p>Like that kind of deployment of the system,</p>
<p>seeing cases where it doesn&rsquo;t perform as you expected</p>
<p>and then retraining the system based on that?</p>
<p>I think that, I mean, that really resonates with me.</p>
<p>It&rsquo;s super smart to do it that way.</p>
<p>Because I mean, the thing is with any kind</p>
<p>of like practical system, like autonomous driving,</p>
<p>there are those edge cases that are the things</p>
<p>that are actually the problem, right?</p>
<p>I mean, highway driving or like freeway driving</p>
<p>has basically been like,</p>
<p>there has been a lot of success in that particular part</p>
<p>of autonomous driving for a long time.</p>
<p>I would say like since the eighties or something.</p>
<p>Now the point is all these failure cases</p>
<p>are the sort of reason why autonomous driving</p>
<p>hasn&rsquo;t become like super, super mainstream and available</p>
<p>like in every possible car right now.</p>
<p>And so basically by really scaling this problem out</p>
<p>by really trying to get all of these edge cases out</p>
<p>as quickly as possible,</p>
<p>and then just like using those to improve your model,</p>
<p>that&rsquo;s super smart.</p>
<p>And prediction uncertainty to do that</p>
<p>is like one really nice way of doing it.</p>
<p>Let me put you on the spot.</p>
<p>So we mentioned offline Jitendra,</p>
<p>he thinks that the Tesla computer vision approach</p>
<p>or really any approach for autonomous driving</p>
<p>is very far away.</p>
<p>How many years away,</p>
<p>if you have to bet all your money on it,</p>
<p>are we to solving autonomous driving</p>
<p>with this kind of computer vision only</p>
<p>machine learning based approach?</p>
<p>Okay, so what does solving autonomous driving mean?</p>
<p>Does it mean solving it in the US?</p>
<p>Does it mean solving it in India?</p>
<p>Because I can tell you</p>
<p>that very different types of driving happening.</p>
<p>Not India, not Russia.</p>
<p>In the United States, autonomous,</p>
<p>so what solving means is when the car says it has control,</p>
<p>it is fully liable.</p>
<p>You can go to sleep, it&rsquo;s driving by itself.</p>
<p>So this is highway and city driving,</p>
<p>but not everywhere, but mostly everywhere.</p>
<p>And it&rsquo;s, let&rsquo;s say significantly better,</p>
<p>like say five times less accidents than humans.</p>
<p>Sufficiently safer such that the public feels</p>
<p>like that transition is enticing beneficial</p>
<p>both for our safety and financial</p>
<p>and all those kinds of things.</p>
<p>Okay, so first disclaimer,</p>
<p>I&rsquo;m not an expert in autonomous driving.</p>
<p>So let me put it out there.</p>
<p>I would say like at least five to 10 years.</p>
<p>This would be my guess from now.</p>
<p>Yeah, I&rsquo;m actually very impressed.</p>
<p>Like when I sat in a friend&rsquo;s Tesla recently</p>
<p>and of course, like looking on that screen,</p>
<p>it basically shows all the detections and everything.</p>
<p>The car is doing as you&rsquo;re driving by</p>
<p>and that&rsquo;s super distracting for me as a person</p>
<p>because all I keep looking at is like the bounding boxes</p>
<p>in the cars it&rsquo;s tracking and it&rsquo;s really impressive.</p>
<p>Like especially when it&rsquo;s raining and it&rsquo;s able to do that,</p>
<p>that was the most impressive part for me.</p>
<p>It&rsquo;s actually able to get through rain and do that.</p>
<p>And one of the reasons why like a lot of us believed</p>
<p>and I would put myself in that category</p>
<p>is LIDAR based sort of technology for autonomous driving</p>
<p>was the key driver, right?</p>
<p>So Waymo was using it for the longest time.</p>
<p>And Tesla then decided to go this completely other route</p>
<p>that we are not going to even use LIDAR.</p>
<p>So their initial system I think was camera and radar based</p>
<p>and now they&rsquo;re actually moving</p>
<p>to a completely like vision based system.</p>
<p>And so that was just like, it sounded completely crazy.</p>
<p>Like LIDAR is very useful in cases</p>
<p>where you have low visibility.</p>
<p>Of course it comes with its own set of complications.</p>
<p>But now to see that happen in like on a live Tesla</p>
<p>that basically just proves everyone wrong</p>
<p>I would say in a way.</p>
<p>And that&rsquo;s just working really well.</p>
<p>I think there were also like a lot of advancements</p>
<p>in camera technology.</p>
<p>Now there were like, I know at CMU when I was there</p>
<p>there was a particular kind of camera</p>
<p>that had been developed that was really good</p>
<p>at basically low visibility setting.</p>
<p>So like lots of snow and lots of rain</p>
<p>it could actually still have a very reasonable visibility.</p>
<p>And I think there are lots of these kinds of innovations</p>
<p>that will happen on the sensor side itself</p>
<p>which is actually going to make this very easy</p>
<p>in the future.</p>
<p>And so maybe that&rsquo;s actually why I&rsquo;m more optimistic</p>
<p>about vision based self, like autonomous driving.</p>
<p>I was going to call it self supervised driving, but.</p>
<p>Vision based autonomous driving.</p>
<p>That&rsquo;s the reason I&rsquo;m quite optimistic about it</p>
<p>because I think there are going to be lots</p>
<p>of these advances on the sensor side itself.</p>
<p>So acquiring this data</p>
<p>we&rsquo;re actually going to get much better about it.</p>
<p>And then of course, once we&rsquo;re able to scale out</p>
<p>and get all of these edge cases in</p>
<p>as like Andre described</p>
<p>I think that&rsquo;s going to make us go very far away.</p>
<p>Yeah, so it&rsquo;s funny.</p>
<p>I&rsquo;m very much with you on the five to 10 years</p>
<p>maybe 10 years</p>
<p>but you made it, I&rsquo;m not sure how you made it sound</p>
<p>but for some people that seem</p>
<p>that might seem like really far away.</p>
<p>And then for other people, it might seem like very close.</p>
<p>There&rsquo;s a lot of fundamental questions</p>
<p>about how much game theory is in this whole thing.</p>
<p>So like, how much is this simply a collision avoidance</p>
<p>problem and how much of it is you still interacting</p>
<p>with other humans in the scene</p>
<p>and you&rsquo;re trying to create an experience</p>
<p>that&rsquo;s compelling.</p>
<p>So you want to get from point A to point B quickly</p>
<p>you want to navigate the scene in a safe way</p>
<p>but you also want to show some level of aggression</p>
<p>because well, certainly this is why you&rsquo;re screwed in India</p>
<p>because you have to show aggression.</p>
<p>Or Jersey or New Jersey.</p>
<p>Or Jersey, right.</p>
<p>So like, or New York or basically any major city</p>
<p>but I think it&rsquo;s probably Elon</p>
<p>that I talked the most about this</p>
<p>which is a surprise to the level of which</p>
<p>they&rsquo;re not considering human beings</p>
<p>as a huge problem in this, as a source of problem.</p>
<p>Like the driving is fundamentally a robot on robot</p>
<p>versus the environment problem</p>
<p>versus like you can just consider humans</p>
<p>not part of the problem.</p>
<p>I used to think humans are almost certainly</p>
<p>have to be modeled really well.</p>
<p>Pedestrians and cyclists and humans inside other cars</p>
<p>you have to have like mental models for them.</p>
<p>You cannot just see it as objects</p>
<p>but more and more it&rsquo;s like the</p>
<p>it&rsquo;s the same kind of intuition breaking thing</p>
<p>that&rsquo;s self supervised learning does, which is</p>
<p>well maybe through the learning</p>
<p>you&rsquo;ll get all the human like human information you need.</p>
<p>Right?</p>
<p>Like maybe you&rsquo;ll get it just with enough data.</p>
<p>You don&rsquo;t need to have explicit good models</p>
<p>of human behavior.</p>
<p>Maybe you get it through the data.</p>
<p>So, I mean my skepticism also just knowing</p>
<p>a lot of automotive companies</p>
<p>and how difficult it is to be innovative.</p>
<p>I was skeptical that they would be able at scale</p>
<p>to convert the driving scene across the world</p>
<p>into digital form such that you can create</p>
<p>this data engine at scale.</p>
<p>And the fact that Tesla is at least getting there</p>
<p>or are already there makes me think that</p>
<p>it&rsquo;s now starting to be coupled</p>
<p>to this self supervised learning vision</p>
<p>which is like if that&rsquo;s gonna work</p>
<p>if through purely this process you can get really far</p>
<p>then maybe you can solve driving that way.</p>
<p>I don&rsquo;t know.</p>
<p>I tend to believe we don&rsquo;t give enough credit</p>
<p>to the how amazing humans are both at driving</p>
<p>and at supervising autonomous systems.</p>
<p>And also we don&rsquo;t, this is, I wish we were.</p>
<p>I wish there was much more driver sensing inside Teslas</p>
<p>and much deeper consideration of human factors</p>
<p>like understanding psychology and drowsiness</p>
<p>and all those kinds of things</p>
<p>when the car does more and more of the work.</p>
<p>How to keep utilizing the little human supervision</p>
<p>that are needed to keep this whole thing safe.</p>
<p>I mean it&rsquo;s a fascinating dance of human robot interaction.</p>
<p>To me autonomous driving for a long time</p>
<p>is a human robot interaction problem.</p>
<p>It is not a robotics problem or computer vision problem.</p>
<p>Like you have to have a human in the loop.</p>
<p>But so which is why I think it&rsquo;s 10 years plus.</p>
<p>But I do think there&rsquo;ll be a bunch of cities and contexts</p>
<p>where geo restricted it will work really, really damn well.</p>
<p>So I think for me that gets five if I&rsquo;m being optimistic</p>
<p>and it&rsquo;s going to be five for a lot of cases</p>
<p>and 10 plus, yeah, I agree with you.</p>
<p>10 plus basically if we want to recover most of the,</p>
<p>say, contiguous United States or something.</p>
<p>Oh, interesting.</p>
<p>So my optimistic is five and pessimistic is 30.</p>
<p>30.</p>
<p>I have a long tail on this one.</p>
<p>I haven&rsquo;t watched enough driving videos.</p>
<p>I&rsquo;ve watched enough pedestrians to think like we may be,</p>
<p>like there&rsquo;s a small part of me still, not a small,</p>
<p>like a pretty big part of me that thinks</p>
<p>we will have to build AGI to solve driving.</p>
<p>Oh, well.</p>
<p>Like there&rsquo;s something to me,</p>
<p>like because humans are part of the picture,</p>
<p>deeply part of the picture,</p>
<p>and also human society is part of the picture</p>
<p>in that human life is at stake.</p>
<p>Anytime a robot kills a human,</p>
<p>it&rsquo;s not clear to me that that&rsquo;s not a problem</p>
<p>that machine learning will also have to solve.</p>
<p>Like it has to, you have to integrate that</p>
<p>into the whole thing.</p>
<p>Just like Facebook or social networks,</p>
<p>one thing is to say how to make</p>
<p>a really good recommender system.</p>
<p>And then the other thing is to integrate</p>
<p>into that recommender system,</p>
<p>all the journalists that will write articles</p>
<p>about that recommender system.</p>
<p>Like you have to consider the society</p>
<p>within which the AI system operates.</p>
<p>And in order to, and like politicians too,</p>
<p>this is the regulatory stuff for autonomous driving.</p>
<p>It&rsquo;s kind of fascinating that the more successful</p>
<p>your AI system becomes,</p>
<p>the more it gets integrated in society</p>
<p>and the more precious politicians</p>
<p>and the public and the clickbait journalists</p>
<p>and all the different fascinating forces</p>
<p>of our society start acting on it.</p>
<p>And then it&rsquo;s no longer how good you are</p>
<p>at doing the initial task.</p>
<p>It&rsquo;s also how good you are at navigating human nature,</p>
<p>which is a fascinating space.</p>
<p>What do you think are the limits of deep learning?</p>
<p>If you allow me, we&rsquo;ll zoom out a little bit</p>
<p>into the big question of artificial intelligence.</p>
<p>You said dark matter of intelligence is self supervised</p>
<p>learning, but there could be more.</p>
<p>What do you think the limits of self supervised learning</p>
<p>and just learning in general, deep learning are?</p>
<p>I think like for deep learning in particular,</p>
<p>because self supervised learning is I would say</p>
<p>a little bit more vague right now.</p>
<p>So I wouldn&rsquo;t, like for something that&rsquo;s so vague,</p>
<p>it&rsquo;s hard to predict what its limits are going to be.</p>
<p>But like I said, I think anywhere you want to interact</p>
<p>with human self supervised learning kind of hits a boundary</p>
<p>very quickly because you need to have an interface</p>
<p>to be able to communicate with the human.</p>
<p>So really like if you have just like vacuous concepts</p>
<p>or like just like nebulous concepts discovered</p>
<p>by a network, it&rsquo;s very hard to communicate those</p>
<p>with the human without like inserting some kind</p>
<p>of human knowledge or some kind of like human bias there.</p>
<p>In general, I think for deep learning,</p>
<p>the biggest challenge is just like data efficiency.</p>
<p>Even with self supervised learning,</p>
<p>even with anything else, if you just see</p>
<p>a single concept once, like one image of like,</p>
<p>I don&rsquo;t know, whatever you want to call it,</p>
<p>like any concept, it&rsquo;s really hard for these methods</p>
<p>to generalize by looking at just one or two samples</p>
<p>of things and that has been a real challenge.</p>
<p>I think that&rsquo;s actually why like these edge cases,</p>
<p>for example, for Tesla are actually that important.</p>
<p>Because if you see just one instance of the car failing</p>
<p>and if you just annotate that and you get that</p>
<p>into your data set, you have like very limited guarantee</p>
<p>that it&rsquo;s not going to happen again.</p>
<p>And you&rsquo;re actually going to be able to recognize</p>
<p>this kind of instance in a very different scenario.</p>
<p>So like when it was snowing, so you got that thing labeled</p>
<p>when it was snowing, but now when it&rsquo;s raining,</p>
<p>you&rsquo;re actually not able to get it.</p>
<p>Or you basically have the same scenario</p>
<p>in a different part of the world.</p>
<p>So the lighting was different or so on.</p>
<p>So it&rsquo;s just really hard for these models,</p>
<p>like deep learning especially to do that.</p>
<p>What&rsquo;s your intuition?</p>
<p>How do we solve handwritten digit recognition problem</p>
<p>when we only have one example for each number?</p>
<p>It feels like humans are using something like learning.</p>
<p>Right.</p>
<p>I think we are good at transferring knowledge a little bit.</p>
<p>We are just better at like for a lot of these problems</p>
<p>where we are generalizing from a single sample</p>
<p>or recognizing from a single sample,</p>
<p>we are using a lot of our own domain knowledge</p>
<p>and a lot of our like inductive bias</p>
<p>into that one sample to generalize it.</p>
<p>So I&rsquo;ve never seen you write the number nine, for example.</p>
<p>And if you were to write it, I would still get it.</p>
<p>And if you were to write a different kind of alphabet</p>
<p>and like write it in two different ways,</p>
<p>I would still probably be able to figure out</p>
<p>that these are the same two characters.</p>
<p>It&rsquo;s just that I have been very used</p>
<p>to seeing handwritten digits in my life.</p>
<p>The other sort of problem with any deep learning system</p>
<p>or any kind of machine learning system is like,</p>
<p>it&rsquo;s guarantees, right?</p>
<p>There are no guarantees for it.</p>
<p>Now you can argue that humans also don&rsquo;t have any guarantees.</p>
<p>Like there is no guarantee that I can recognize a cat</p>
<p>in every scenario.</p>
<p>I&rsquo;m sure there are going to be lots of cats</p>
<p>that I don&rsquo;t recognize, lots of scenarios</p>
<p>in which I don&rsquo;t recognize cats in general.</p>
<p>But I think from just a sort of application perspective,</p>
<p>you do need guarantees, right?</p>
<p>We call these things algorithms.</p>
<p>Now algorithms, like traditional CS algorithms</p>
<p>have guarantees.</p>
<p>Sorting is a guarantee.</p>
<p>If you were to call sort on a particular array of numbers,</p>
<p>you are guaranteed that it&rsquo;s going to be sorted.</p>
<p>Otherwise it&rsquo;s a bug.</p>
<p>Now for machine learning,</p>
<p>it&rsquo;s very hard to characterize this.</p>
<p>We know for a fact that a cat recognition model</p>
<p>is not going to recognize cats,</p>
<p>every cat in the world in every circumstance.</p>
<p>I think most people would agree with that statement,</p>
<p>but we are still okay with it.</p>
<p>We still don&rsquo;t call this as a bug.</p>
<p>Whereas in traditional computer science</p>
<p>or traditional science,</p>
<p>like if you have this kind of failure case existing,</p>
<p>then you think of it as like something is wrong.</p>
<p>I think there is this sort of notion</p>
<p>of nebulous correctness for machine learning.</p>
<p>And that&rsquo;s something we just need to be very comfortable</p>
<p>with.</p>
<p>And for deep learning,</p>
<p>or like for a lot of these machine learning algorithms,</p>
<p>it&rsquo;s not clear how do we characterize</p>
<p>this notion of correctness.</p>
<p>I think limitation in our understanding,</p>
<p>or at least a limitation in our phrasing of this.</p>
<p>And if we were to come up with better ways</p>
<p>to understand this limitation,</p>
<p>then it would actually help us a lot.</p>
<p>Do you think there&rsquo;s a distinction</p>
<p>between the concept of learning</p>
<p>and the concept of reasoning?</p>
<p>Do you think it&rsquo;s possible for neural networks to reason?</p>
<p>So I think of it slightly differently.</p>
<p>So for me, learning is whenever</p>
<p>I can like make a snap judgment.</p>
<p>So if you show me a picture of a dog,</p>
<p>I can immediately say it&rsquo;s a dog.</p>
<p>But if you give me like a puzzle,</p>
<p>like whatever a Goldsberg machine</p>
<p>of like things going to happen,</p>
<p>then I have to reason because I&rsquo;ve never,</p>
<p>it&rsquo;s a very complicated setup.</p>
<p>I&rsquo;ve never seen that particular setup.</p>
<p>And I really need to draw and like imagine in my head</p>
<p>what&rsquo;s going to happen to figure it out.</p>
<p>So I think, yes, neural networks are really good</p>
<p>at recognition, but they&rsquo;re not very good at reasoning.</p>
<p>Because they have seen something before</p>
<p>or seen something similar before, they&rsquo;re very good</p>
<p>at making those sort of snap judgments.</p>
<p>But if you were to give them a very complicated thing</p>
<p>that they&rsquo;ve not seen before,</p>
<p>they have very limited ability right now</p>
<p>to compose different things.</p>
<p>Like, oh, I&rsquo;ve seen this particular part before.</p>
<p>I&rsquo;ve seen this particular part before.</p>
<p>And now probably like this is how</p>
<p>they&rsquo;re going to work in tandem.</p>
<p>It&rsquo;s very hard for them to come up</p>
<p>with these kinds of things.</p>
<p>Well, there&rsquo;s a certain aspect to reasoning</p>
<p>that you can maybe convert into the process of programming.</p>
<p>And so there&rsquo;s the whole field of program synthesis</p>
<p>and people have been applying machine learning</p>
<p>to the problem of program synthesis.</p>
<p>And the question is, can they, the step of composition,</p>
<p>why can&rsquo;t that be learned?</p>
<p>You know, this step of like building things on top of you,</p>
<p>like little intuitions, concepts on top of each other,</p>
<p>can that be learnable?</p>
<p>What&rsquo;s your intuition there?</p>
<p>Or like, I guess similar set of techniques,</p>
<p>do you think that will be applicable?</p>
<p>So I think it is, of course, it is learnable</p>
<p>because like we are prime examples of machines</p>
<p>that have like, or individuals that have learned this, right?</p>
<p>Like humans have learned this.</p>
<p>So it is, of course, it is a technique</p>
<p>that is very easy to learn.</p>
<p>I think where we are kind of hitting a wall</p>
<p>basically with like current machine learning</p>
<p>is the fact that when the network learns</p>
<p>all of this information,</p>
<p>we basically are not able to figure out</p>
<p>how well it&rsquo;s going to generalize to an unseen thing.</p>
<p>And we have no, like a priori, no way of characterizing that.</p>
<p>And I think that&rsquo;s basically telling us a lot about,</p>
<p>like a lot about the fact that we really don&rsquo;t know</p>
<p>what this model has learned and how well it&rsquo;s basically,</p>
<p>because we don&rsquo;t know how well it&rsquo;s going to transfer.</p>
<p>There&rsquo;s also a sense in which it feels like</p>
<p>we humans may not be aware of how much like background,</p>
<p>how good our background model is,</p>
<p>how much knowledge we just have slowly building</p>
<p>on top of each other.</p>
<p>It feels like neural networks</p>
<p>are constantly throwing stuff out.</p>
<p>Like you&rsquo;ll do some incredible thing</p>
<p>where you&rsquo;re learning a particular task in computer vision,</p>
<p>you celebrate your state of the art successes</p>
<p>and you throw that out.</p>
<p>Like, it feels like it&rsquo;s,</p>
<p>you&rsquo;re never using stuff you&rsquo;ve learned</p>
<p>for your future successes in other domains.</p>
<p>And humans are obviously doing that exceptionally well,</p>
<p>still throwing stuff away in their mind,</p>
<p>but keeping certain kernels of truth.</p>
<p>Right, so I think we&rsquo;re like,</p>
<p>continual learning is sort of the paradigm</p>
<p>for this in machine learning.</p>
<p>And I don&rsquo;t think it&rsquo;s a very well explored paradigm.</p>
<p>We have like things in deep learning, for example,</p>
<p>catastrophic forgetting is like one of the standard things.</p>
<p>The thing basically being that if you teach a network</p>
<p>like to recognize dogs,</p>
<p>and now you teach that same network to recognize cats,</p>
<p>it basically forgets how to recognize dogs.</p>
<p>So it forgets very quickly.</p>
<p>I mean, and whereas a human,</p>
<p>if you were to teach someone to recognize dogs</p>
<p>and then to recognize cats,</p>
<p>they don&rsquo;t forget immediately how to recognize these dogs.</p>
<p>I think that&rsquo;s basically sort of what you&rsquo;re trying to get.</p>
<p>Yeah, I just, I wonder if like</p>
<p>the long term memory mechanisms</p>
<p>or the mechanisms that store not just memories,</p>
<p>but concepts that allow you to the reason</p>
<p>and compose concepts,</p>
<p>if those things will look very different</p>
<p>than neural networks,</p>
<p>or if you can do that within a single neural network</p>
<p>with some particular sort of architecture quirks,</p>
<p>that seems to be a really open problem.</p>
<p>And of course I go up and down on that</p>
<p>because there&rsquo;s something so compelling to the symbolic AI</p>
<p>or to the ideas of logic based sort of expert systems.</p>
<p>You have like human interpretable facts</p>
<p>that built on top of each other.</p>
<p>It&rsquo;s really annoying like with self supervised learning</p>
<p>that the AI is not very explainable.</p>
<p>Like you can&rsquo;t like understand</p>
<p>all the beautiful things it has learned.</p>
<p>You can&rsquo;t ask it like questions,</p>
<p>but then again, maybe that&rsquo;s a stupid thing</p>
<p>for us humans to want.</p>
<p>Right, I think whenever we try to like understand it,</p>
<p>we are putting our own subjective human bias into it.</p>
<p>Yeah.</p>
<p>And I think that&rsquo;s the sort of problem</p>
<p>with self supervised learning,</p>
<p>the goal is that it should learn naturally from the data.</p>
<p>So now if you try to understand it,</p>
<p>you are using your own preconceived notions</p>
<p>of what this model has learned.</p>
<p>And that&rsquo;s the problem.</p>
<p>High level question.</p>
<p>What do you think it takes to build a system</p>
<p>with superhuman, maybe let&rsquo;s say human level</p>
<p>or superhuman level general intelligence?</p>
<p>We&rsquo;ve already kind of started talking about this,</p>
<p>but what&rsquo;s your intuition?</p>
<p>Like, does this thing have to have a body?</p>
<p>Does it have to interact richly with the world?</p>
<p>Does it have to have some more human elements</p>
<p>like self awareness?</p>
<p>I think emotion.</p>
<p>I think emotion is something which is like,</p>
<p>it&rsquo;s not really attributed typically</p>
<p>in standard machine learning.</p>
<p>It&rsquo;s not something we think about,</p>
<p>like there is NLP, there is vision,</p>
<p>there is no like emotion.</p>
<p>Emotion is never a part of all of this.</p>
<p>And that just seems a little bit weird to me.</p>
<p>I think the reason basically being that there is surprise</p>
<p>and like, basically emotion is like one of the reasons</p>
<p>emotions arise is like what happens</p>
<p>and what do you expect to happen, right?</p>
<p>There is like a mismatch between these things.</p>
<p>And so that gives rise to like,</p>
<p>I can either be surprised or I can be saddened</p>
<p>or I can be happy and all of this.</p>
<p>And so this basically indicates</p>
<p>that I already have a predictive model in my head</p>
<p>and something that I predicted or something</p>
<p>that I thought was likely to happen.</p>
<p>And then there was something that I observed</p>
<p>that happened that there was a disconnect</p>
<p>between these two things.</p>
<p>And that basically is like maybe one of the reasons</p>
<p>like you have a lot of emotions.</p>
<p>Yeah, I think, so I talk to people a lot about them</p>
<p>like Lisa Feldman Barrett.</p>
<p>I think that&rsquo;s an interesting concept of emotion</p>
<p>but I have a sense that emotion primarily</p>
<p>in the way we think about it,</p>
<p>which is the display of emotion</p>
<p>is a communication mechanism between humans.</p>
<p>So it&rsquo;s a part of basically human to human interaction,</p>
<p>an important part, but just the part.</p>
<p>So it&rsquo;s like, I would throw it into the full mix</p>
<p>of communication.</p>
<p>And to me, communication can be done with objects</p>
<p>that don&rsquo;t look at all like humans.</p>
<p>Okay.</p>
<p>I&rsquo;ve seen our ability to anthropomorphize</p>
<p>our ability to connect with things that look like a Roomba</p>
<p>our ability to connect.</p>
<p>First of all, let&rsquo;s talk about other biological systems</p>
<p>like dogs, our ability to love things</p>
<p>that are very different than humans.</p>
<p>But they do display emotion, right?</p>
<p>I mean, dogs do display emotion.</p>
<p>So they don&rsquo;t have to be anthropomorphic</p>
<p>for them to like display the kind of emotions</p>
<p>that we don&rsquo;t.</p>
<p>Exactly.</p>
<p>So, I mean, but then the word emotion starts to lose.</p>
<p>So then we have to be, I guess specific, but yeah.</p>
<p>So have rich flavorful communication.</p>
<p>Communication, yeah.</p>
<p>Yeah, so like, yes, it&rsquo;s full of emotion.</p>
<p>It&rsquo;s full of wit and humor and moods</p>
<p>and all those kinds of things, yeah.</p>
<p>So you&rsquo;re talking about like flavor.</p>
<p>Flavor, yeah.</p>
<p>Okay, let&rsquo;s call it that.</p>
<p>So there&rsquo;s content and then there is flavor</p>
<p>and I&rsquo;m talking about the flavor.</p>
<p>Do you think it needs to have a body?</p>
<p>Do you think like to interact with the physical world?</p>
<p>Do you think you can understand the physical world</p>
<p>without being able to directly interact with it?</p>
<p>I don&rsquo;t think so, yeah.</p>
<p>I think at some point we will need to bite the bullet</p>
<p>and actually interact with the physical,</p>
<p>as much as I like working on like passive computer vision</p>
<p>where I just like sit in my arm chair</p>
<p>and look at videos and learn.</p>
<p>I do think that we will need to have some kind of embodiment</p>
<p>or some kind of interaction</p>
<p>to figure out things about the world.</p>
<p>What about consciousness?</p>
<p>Do you think, how often do you think about consciousness</p>
<p>when you think about your work?</p>
<p>You could think of it</p>
<p>as the more simple thing of self awareness,</p>
<p>of being aware that you are a perceiving,</p>
<p>sensing, acting thing in this world.</p>
<p>Or you can think about the bigger version of that,</p>
<p>which is consciousness,</p>
<p>which is having it feel like something to be that entity,</p>
<p>the subjective experience of being in this world.</p>
<p>So I think of self awareness a little bit more</p>
<p>than like the broader goal of it,</p>
<p>because I think self awareness is pretty critical</p>
<p>for like any kind of like any kind of AGI</p>
<p>or whatever you want to call it that we build,</p>
<p>because it needs to contextualize what it is</p>
<p>and what role it&rsquo;s playing</p>
<p>with respect to all the other things that exist around it.</p>
<p>I think that requires self awareness.</p>
<p>It needs to understand that it&rsquo;s an autonomous car, right?</p>
<p>And what does that mean?</p>
<p>What are its limitations?</p>
<p>What are the things that it is supposed to do and so on?</p>
<p>What is its role in some way?</p>
<p>Or, I mean, these are the kinds of things</p>
<p>that we kind of expect from it, I would say.</p>
<p>And so that&rsquo;s the level of self awareness</p>
<p>that&rsquo;s, I would say, basically required at least,</p>
<p>if not more than that.</p>
<p>Yeah, I tend to, on the emotion side,</p>
<p>believe that it has to have,</p>
<p>it has to be able to display consciousness.</p>
<p>Display consciousness, what do you mean by that?</p>
<p>Meaning like for us humans to connect with each other</p>
<p>or to connect with other living entities,</p>
<p>I think we need to feel,</p>
<p>like in order for us to truly feel</p>
<p>like that there&rsquo;s another being there,</p>
<p>we have to believe that they&rsquo;re conscious.</p>
<p>And so we won&rsquo;t ever connect with something</p>
<p>that doesn&rsquo;t have elements of consciousness.</p>
<p>Now I tend to think that that&rsquo;s easier to achieve</p>
<p>than it may sound,</p>
<p>because we anthropomorphize stuff so hard.</p>
<p>Like you have a mug that just like has wheels</p>
<p>and like rotates every once in a while and makes a sound.</p>
<p>I think a couple of days in,</p>
<p>especially if you don&rsquo;t hang out with humans,</p>
<p>you might start to believe that mug on wheels is conscious.</p>
<p>So I think we anthropomorphize pretty effectively</p>
<p>as human beings.</p>
<p>But I do think that it&rsquo;s in the same bucket</p>
<p>that we&rsquo;ll call emotion,</p>
<p>that show that you&rsquo;re,</p>
<p>I think of consciousness as the capacity to suffer.</p>
<p>And if you&rsquo;re an entity that&rsquo;s able to feel things</p>
<p>in the world and to communicate that to others,</p>
<p>I think that&rsquo;s a really powerful way</p>
<p>to interact with humans.</p>
<p>And in order to create an AGI system,</p>
<p>I believe you should be able to richly interact with humans.</p>
<p>Like humans would need to want to interact with you.</p>
<p>Like it can&rsquo;t be like,</p>
<p>it&rsquo;s the self supervised learning versus like,</p>
<p>like the robot shouldn&rsquo;t have to pay you</p>
<p>to interact with me.</p>
<p>So like it should be a natural fun thing.</p>
<p>And then you&rsquo;re going to scale up significantly</p>
<p>how much interaction it gets.</p>
<p>It&rsquo;s the Alexa prize,</p>
<p>which they were trying to get me to be a judge</p>
<p>on their contest.</p>
<p>Let&rsquo;s see if I want to do that.</p>
<p>But their challenge is to talk to you,</p>
<p>make the human sufficiently interested</p>
<p>that the human keeps talking for 20 minutes.</p>
<p>To Alexa?</p>
<p>To Alexa, yeah.</p>
<p>And right now they&rsquo;re not even close to that</p>
<p>because it just gets so boring when you&rsquo;re like,</p>
<p>when the intelligence is not there,</p>
<p>it gets very not interesting to talk to it.</p>
<p>And so the robot needs to be interesting.</p>
<p>And one of the ways it can be interesting</p>
<p>is display the capacity to love, to suffer.</p>
<p>And I would say that essentially means</p>
<p>the capacity to display consciousness.</p>
<p>Like it is an entity, much like a human being.</p>
<p>Of course, what that really means,</p>
<p>I don&rsquo;t know if that&rsquo;s fundamentally a robotics problem</p>
<p>or some kind of problem that we&rsquo;re not yet even aware.</p>
<p>Like if it is truly a hard problem of consciousness,</p>
<p>I tend to maybe optimistically think it&rsquo;s a,</p>
<p>we can pretty effectively fake it till we make it.</p>
<p>So we can display a lot of human like elements for a while.</p>
<p>And that will be sufficient to form</p>
<p>really close connections with humans.</p>
<p>What&rsquo;s used the most beautiful idea</p>
<p>in self supervised learning?</p>
<p>Like when you sit back with, I don&rsquo;t know,</p>
<p>with a glass of wine and an armchair</p>
<p>and just at a fireplace,</p>
<p>just thinking how beautiful this world that you get</p>
<p>to explore is, what do you think</p>
<p>is the especially beautiful idea?</p>
<p>The fact that like object level,</p>
<p>what objects are and some notion of objectness emerges</p>
<p>from these models by just like self supervised learning.</p>
<p>So for example, like one of the things like the dyno paper</p>
<p>that I was a part of at Facebook is the object sort</p>
<p>of boundaries emerge from these representations.</p>
<p>So if you have like a dog running in the field,</p>
<p>the boundaries around the dog,</p>
<p>the network is basically able to figure out</p>
<p>what the boundaries of this dog are automatically.</p>
<p>And it was never trained to do that.</p>
<p>It was never trained to, no one taught it</p>
<p>that this is a dog and these pixels belong to a dog.</p>
<p>It&rsquo;s able to group these things together automatically.</p>
<p>So that&rsquo;s one.</p>
<p>I think in general, that entire notion that this dumb idea</p>
<p>that you take like these two crops of an image</p>
<p>and then you say that the features should be similar,</p>
<p>that has resulted in something like this,</p>
<p>like the model is able to figure out</p>
<p>what the dog pixels are and so on.</p>
<p>That just seems like so surprising.</p>
<p>And I mean, I don&rsquo;t think a lot of us even understand</p>
<p>how that is happening really.</p>
<p>And it&rsquo;s something we are taking for granted,</p>
<p>maybe like a lot in terms of how we&rsquo;re setting up</p>
<p>these algorithms, but it&rsquo;s just,</p>
<p>it&rsquo;s a very beautiful and powerful idea.</p>
<p>So it&rsquo;s really fundamentally telling us something about</p>
<p>that there is so much signal in the pixels</p>
<p>that we can be super dumb about it.</p>
<p>How about how we are setting up</p>
<p>the self sequencing problem.</p>
<p>And despite being like super dumb about it,</p>
<p>we&rsquo;ll actually get very good,</p>
<p>like we&rsquo;ll actually get something that is able to do</p>
<p>very like surprising things.</p>
<p>I wonder if there&rsquo;s other like objectness</p>
<p>of other concepts that can emerge.</p>
<p>I don&rsquo;t know if you follow Francois Chollet,</p>
<p>he had the competition for intelligence</p>
<p>that basically it&rsquo;s kind of like an IQ test,</p>
<p>but for machines, but for an IQ test,</p>
<p>you have to have a few concepts that you want to apply.</p>
<p>One of them is objectness.</p>
<p>I wonder if those concepts can emerge</p>
<p>through self supervised learning on billions of images.</p>
<p>I think something like object permanence</p>
<p>can definitely emerge, right?</p>
<p>So that&rsquo;s like a fundamental concept which we have,</p>
<p>maybe not through images, through video,</p>
<p>but that&rsquo;s another concept that should be emerging from it</p>
<p>because it&rsquo;s not something that,</p>
<p>like if we don&rsquo;t teach humans that this isn&rsquo;t,</p>
<p>this is like about this concept of object permanence,</p>
<p>it actually emerges.</p>
<p>And the same thing for like animals, like dogs,</p>
<p>I think actually permanence automatically</p>
<p>is something that they are born with.</p>
<p>So I think it should emerge from the data.</p>
<p>It should emerge basically very quickly.</p>
<p>I wonder if ideas like symmetry, rotation,</p>
<p>these kinds of things might emerge.</p>
<p>So I think rotation, probably yes.</p>
<p>Yeah, rotation, yes.</p>
<p>I mean, there&rsquo;s some constraints in the architecture itself,</p>
<p>but it&rsquo;s interesting if all of them could be,</p>
<p>like counting was another one, being able to kind of</p>
<p>understand that there&rsquo;s multiple objects</p>
<p>of the same kind in the image and be able to count them.</p>
<p>I wonder if all of that could be,</p>
<p>if constructed correctly, they can emerge</p>
<p>because then you can transfer those concepts</p>
<p>to then interpret images at a deeper level.</p>
<p>Right.</p>
<p>Counting, I do believe, I mean, it should be possible.</p>
<p>You don&rsquo;t know like yet,</p>
<p>but I do think it&rsquo;s not that far in the realm of possibility.</p>
<p>Yeah, that&rsquo;d be interesting</p>
<p>if using self supervised learning on images</p>
<p>can then be applied to then solving those kinds of IQ tests,</p>
<p>which seem currently to be kind of impossible.</p>
<p>What idea do you believe might be true</p>
<p>that most people think is not true</p>
<p>or don&rsquo;t agree with you on?</p>
<p>Is there something like that?</p>
<p>So this is going to be a little controversial,</p>
<p>but okay, sure.</p>
<p>I don&rsquo;t believe in simulation.</p>
<p>Like actually using simulation to do things very much.</p>
<p>Just to clarify, because this is a podcast</p>
<p>where you talk about, are we living in a simulation often?</p>
<p>You&rsquo;re referring to using simulation to construct worlds</p>
<p>that you then leverage for machine learning.</p>
<p>Right, yeah.</p>
<p>For example, like one example would be like</p>
<p>to train an autonomous car driving system.</p>
<p>You basically first build a simulator,</p>
<p>which builds like the environment of the world.</p>
<p>And then you basically have a lot of like,</p>
<p>you train your machine learning system in that.</p>
<p>So I believe it is possible,</p>
<p>but I think it&rsquo;s a really expensive way of doing things.</p>
<p>And at the end of it, you do need the real world.</p>
<p>So I&rsquo;m not sure.</p>
<p>So maybe for certain settings,</p>
<p>like maybe the payout is so large,</p>
<p>like for autonomous driving, the payout is so large</p>
<p>that you can actually invest that much money to build it.</p>
<p>But I think as a general sort of principle,</p>
<p>it does not apply to a lot of concepts.</p>
<p>You can&rsquo;t really build simulations of everything.</p>
<p>Not only because like one, it&rsquo;s expensive,</p>
<p>because second, it&rsquo;s also not possible for a lot of things.</p>
<p>So in general, like there&rsquo;s a lot of work</p>
<p>on like using synthetic data and like synthetic simulators.</p>
<p>I generally am not very, like I don&rsquo;t believe in that.</p>
<p>So you&rsquo;re saying it&rsquo;s very challenging visually,</p>
<p>like to correctly like simulate the visual,</p>
<p>like the lighting, all those kinds of things.</p>
<p>I mean, all these companies that you have, right?</p>
<p>So like Pixar and like whatever,</p>
<p>all these companies are,</p>
<p>all this like computer graphics stuff</p>
<p>is really about accurately,</p>
<p>a lot of them is about like accurately trying to figure out</p>
<p>how the lighting is and like how things reflect off</p>
<p>of one another and so on,</p>
<p>and like how sparkly things look and so on.</p>
<p>So it&rsquo;s a very hard problem.</p>
<p>So do we really need to solve that first</p>
<p>to be able to like do computer vision?</p>
<p>Probably not.</p>
<p>And for me, in the context of autonomous driving,</p>
<p>it&rsquo;s very tempting to be able to use simulation, right?</p>
<p>Because it&rsquo;s a safety critical application,</p>
<p>but the other limitation of simulation that perhaps</p>
<p>is a bigger one than the visual limitation</p>
<p>is the behavior of objects.</p>
<p>So you&rsquo;re ultimately interested in edge cases.</p>
<p>And the question is,</p>
<p>how well can you generate edge cases in simulation,</p>
<p>especially with human behavior?</p>
<p>I think another problem is like for autonomous driving,</p>
<p>it&rsquo;s a constantly changing world.</p>
<p>So say autonomous driving like in 10 years from now,</p>
<p>like there are lots of autonomous cars,</p>
<p>but they&rsquo;re still going to be humans.</p>
<p>So now there are 50% of the agents say, which are humans,</p>
<p>50% of the agents that are autonomous,</p>
<p>like car driving agents.</p>
<p>So now the mixture has changed.</p>
<p>So now the kinds of behaviors that you actually expect</p>
<p>from the other agents or other cars on the road</p>
<p>are actually going to be very different.</p>
<p>And as the proportion of the number of autonomous cars</p>
<p>to humans keeps changing,</p>
<p>this behavior will actually change a lot.</p>
<p>So now if you were to build a simulator based on</p>
<p>just like right now to build them today,</p>
<p>you don&rsquo;t have that many autonomous cars on the road.</p>
<p>So you would try to like make all of the other agents</p>
<p>in that simulator behave as humans,</p>
<p>but that&rsquo;s not really going to hold true 10, 15, 20,</p>
<p>30 years from now.</p>
<p>Do you think we&rsquo;re living in a simulation?</p>
<p>No.</p>
<p>How hard is it?</p>
<p>This is why I think it&rsquo;s an interesting question.</p>
<p>How hard is it to build a video game,</p>
<p>like virtual reality game where it is so real,</p>
<p>forget like ultra realistic to where</p>
<p>you can&rsquo;t tell the difference,</p>
<p>but like it&rsquo;s so nice that you just want to stay there.</p>
<p>You just want to stay there and you don&rsquo;t want to come back.</p>
<p>Do you think that&rsquo;s doable within our lifetime?</p>
<p>Within our lifetime, probably.</p>
<p>Yeah.</p>
<p>I eat healthy, I live long.</p>
<p>Does that make you sad that there&rsquo;ll be like</p>
<p>like population of kids that basically spend 95%,</p>
<p>99% of their time in a virtual world?</p>
<p>Very, very hard question to answer.</p>
<p>For certain people, it might be something</p>
<p>that they really derive a lot of value out of,</p>
<p>derive a lot of enjoyment and like happiness out of,</p>
<p>and maybe the real world wasn&rsquo;t giving them that.</p>
<p>That&rsquo;s why they did that.</p>
<p>So maybe it is good for certain people.</p>
<p>So ultimately, if it maximizes happiness,</p>
<p>Right, I think if.</p>
<p>Or we could judge.</p>
<p>Yeah, I think if it&rsquo;s making people happy,</p>
<p>maybe it&rsquo;s okay.</p>
<p>Again, I think this is a very hard question.</p>
<p>So like you&rsquo;ve been a part of a lot of amazing papers.</p>
<p>What advice would you give to somebody</p>
<p>on what it takes to write a good paper?</p>
<p>Grad students writing papers now,</p>
<p>is there common things that you&rsquo;ve learned along the way</p>
<p>that you think it takes,</p>
<p>both for a good idea and a good paper?</p>
<p>Right, so I think both of these have picked up</p>
<p>from like lots of people I&rsquo;ve worked with in the past.</p>
<p>So one of them is picking the right problem</p>
<p>to work on in research is as important</p>
<p>as like finding the solution to it.</p>
<p>So I mean, there are multiple reasons for this.</p>
<p>So one is that there are certain problems</p>
<p>that can actually be solved in a particular timeframe.</p>
<p>So now say you want to work on finding the meaning of life.</p>
<p>This is a great problem.</p>
<p>I think most people will agree with that.</p>
<p>But do you believe that your talents</p>
<p>and like the energy that you&rsquo;ll spend on it</p>
<p>will make some kind of meaningful progress</p>
<p>in your lifetime?</p>
<p>If you are optimistic about it, then go ahead.</p>
<p>That&rsquo;s why I started this podcast.</p>
<p>I keep asking people about the meaning of life.</p>
<p>I&rsquo;m hoping by episode like 2.20, I&rsquo;ll figure it out.</p>
<p>Oh, not too many episodes to go.</p>
<p>All right, cool.</p>
<p>Maybe today, I don&rsquo;t know, but you&rsquo;re right.</p>
<p>So that seems intractable at the moment.</p>
<p>Right, so I think it&rsquo;s just the fact of like,</p>
<p>if you&rsquo;re starting a PhD, for example,</p>
<p>what is one problem that you want to focus on</p>
<p>that you do think is interesting enough,</p>
<p>and you will be able to make a reasonable amount</p>
<p>of headway into it that you think you&rsquo;ll be doing a PhD for?</p>
<p>So in that kind of a timeframe.</p>
<p>So that&rsquo;s one.</p>
<p>Of course, there&rsquo;s the second part,</p>
<p>which is what excites you genuinely.</p>
<p>So you shouldn&rsquo;t just pick problems</p>
<p>that you are not excited about,</p>
<p>because as a grad student or as a researcher,</p>
<p>you really need to be passionate about it</p>
<p>to continue doing that,</p>
<p>because there are so many other things</p>
<p>that you could be doing in life.</p>
<p>So you really need to believe in that</p>
<p>to be able to do that for that long.</p>
<p>In terms of papers, I think the one thing</p>
<p>that I&rsquo;ve learned is,</p>
<p>like in the past, whenever I used to write things,</p>
<p>and even now, whenever I do that,</p>
<p>I try to cram in a lot of things into the paper,</p>
<p>whereas what really matters</p>
<p>is just pushing one simple idea, that&rsquo;s it.</p>
<p>That&rsquo;s all because the paper is going to be like,</p>
<p>whatever, eight or nine pages.</p>
<p>If you keep cramming in lots of ideas,</p>
<p>it&rsquo;s really hard for the single thing</p>
<p>that you believe in to stand out.</p>
<p>So if you really try to just focus,</p>
<p>especially in terms of writing,</p>
<p>really try to focus on one particular idea</p>
<p>and articulate it out in multiple different ways,</p>
<p>it&rsquo;s far more valuable to the reader as well,</p>
<p>and basically to the reader, of course,</p>
<p>because they get to,</p>
<p>they know that this particular idea</p>
<p>is associated with this paper,</p>
<p>and also for you, because you have,</p>
<p>when you write about a particular idea in different ways,</p>
<p>you think about it more deeply.</p>
<p>So as a grad student, I used to always wait to it,</p>
<p>maybe in the last week or whatever, to write the paper,</p>
<p>because I used to always believe</p>
<p>that doing the experiments</p>
<p>was actually the bigger part of research than writing.</p>
<p>And my advisor always told me</p>
<p>that you should start writing very early on,</p>
<p>and I thought, oh, it doesn&rsquo;t matter,</p>
<p>I don&rsquo;t know what he&rsquo;s talking about.</p>
<p>But I think more and more I realized that&rsquo;s the case.</p>
<p>Whenever I write something that I&rsquo;m doing,</p>
<p>I actually think much better about it.</p>
<p>And so if you start writing early on,</p>
<p>you actually, I think, get better ideas,</p>
<p>or at least you figure out holes in your theory,</p>
<p>or particular experiments that you should run</p>
<p>to plug those holes, and so on.</p>
<p>Yeah, I&rsquo;m continually surprised</p>
<p>how many really good papers throughout history</p>
<p>are quite short and quite simple.</p>
<p>And there&rsquo;s a lesson to that.</p>
<p>If you want to dream about writing a paper</p>
<p>that changes the world,</p>
<p>and you wanna go by example, they&rsquo;re usually simple.</p>
<p>And that&rsquo;s, it&rsquo;s not cramming,</p>
<p>or it&rsquo;s focusing on one idea, and thinking deeply.</p>
<p>And you&rsquo;re right that the writing process itself</p>
<p>reveals the idea.</p>
<p>It challenges you to really think about what is the idea</p>
<p>that explains it, the thread that ties it all together.</p>
<p>And so a lot of famous researchers I know</p>
<p>actually would start off, like, first they were,</p>
<p>even before the experiments were in,</p>
<p>a lot of them would actually start</p>
<p>with writing the introduction of the paper,</p>
<p>with zero experiments in.</p>
<p>Because that at least helps them figure out</p>
<p>what they&rsquo;re trying to solve,</p>
<p>and how it fits in the context of things right now.</p>
<p>And that would really guide their entire research.</p>
<p>So a lot of them would actually first write in intros</p>
<p>with zero experiments in,</p>
<p>and that&rsquo;s how they would start projects.</p>
<p>Some basic questions about people maybe</p>
<p>that are more like beginners in this field.</p>
<p>What&rsquo;s the best programming language to learn</p>
<p>if you&rsquo;re interested in machine learning?</p>
<p>I would say Python,</p>
<p>just because it&rsquo;s the easiest one to learn.</p>
<p>And also a lot of like programming</p>
<p>and machine learning happens in Python.</p>
<p>So if you don&rsquo;t know any other programming language,</p>
<p>Python is actually going to get you a long way.</p>
<p>Yeah, it seems like sort of a,</p>
<p>it&rsquo;s a toss up question because it seems like Python</p>
<p>is so much dominating the space now.</p>
<p>But I wonder if there&rsquo;s an interesting alternative.</p>
<p>Obviously there&rsquo;s like Swift,</p>
<p>and there&rsquo;s a lot of interesting alternatives popping up,</p>
<p>even JavaScript.</p>
<p>So I, or are more like for the data science applications.</p>
<p>But it seems like Python more and more</p>
<p>is actually being used to teach like introduction</p>
<p>to programming at universities.</p>
<p>So it just combines everything very nicely.</p>
<p>Even harder question.</p>
<p>What are the pros and cons of PyTorch versus TensorFlow?</p>
<p>I see.</p>
<p>Okay.</p>
<p>You can go with no comment.</p>
<p>So a disclaimer to this is that the last time</p>
<p>I used TensorFlow was probably like four years ago.</p>
<p>And so it was right when it had come out</p>
<p>because so I started on like deep learning in 2014 or so,</p>
<p>and the dominant sort of framework for us then</p>
<p>for vision was Cafe, which was out of Berkeley.</p>
<p>And we used Cafe a lot, it was really nice.</p>
<p>And then TensorFlow came in,</p>
<p>which was basically like Python first.</p>
<p>So Cafe was mainly C++,</p>
<p>and it had like very loose kind of Python binding.</p>
<p>So Python wasn&rsquo;t really the first language you would use.</p>
<p>You would really use either MATLAB or C++</p>
<p>like get stuff done in like Cafe.</p>
<p>And then Python of course became popular a little bit later.</p>
<p>So TensorFlow was basically around that time.</p>
<p>So 2015, 2016 is when I last used it.</p>
<p>It&rsquo;s been a while.</p>
<p>And then what, did you use Torch or did you?</p>
<p>So then I moved to LuaTorch, which was the torch in Lua.</p>
<p>And then in 2017, I think basically pretty much</p>
<p>to PyTorch completely.</p>
<p>Oh, interesting.</p>
<p>So you went to Lua, cool.</p>
<p>Yeah.</p>
<p>Huh, so you were there before it was cool.</p>
<p>Yeah, I mean, so LuaTorch was really good</p>
<p>because it actually allowed you</p>
<p>to do a lot of different kinds of things.</p>
<p>So which Cafe was very rigid in terms of its structure.</p>
<p>Like you would create a neural network once and that&rsquo;s it.</p>
<p>Whereas if you wanted like very dynamic graphs and so on,</p>
<p>it was very hard to do that.</p>
<p>And LuaTorch was much more friendly</p>
<p>for all of these things.</p>
<p>Okay, so in terms of PyTorch and TensorFlow,</p>
<p>my personal bias is PyTorch</p>
<p>just because I&rsquo;ve been using it longer</p>
<p>and I&rsquo;m more familiar with it.</p>
<p>And also that PyTorch is much easier to debug</p>
<p>is what I find because it&rsquo;s imperative in nature</p>
<p>compared to like TensorFlow, which is not imperative.</p>
<p>But that&rsquo;s telling you a lot that basically</p>
<p>the imperative design is sort of a way</p>
<p>in which a lot of people are taught programming</p>
<p>and that&rsquo;s what actually makes debugging easier for them.</p>
<p>So like I learned programming in C, C++.</p>
<p>And so for me, imperative way of programming is more natural.</p>
<p>Do you think it&rsquo;s good to have</p>
<p>kind of these two communities, this kind of competition?</p>
<p>I think PyTorch is kind of more and more</p>
<p>becoming dominant in the research community,</p>
<p>but TensorFlow is still very popular</p>
<p>in the more sort of application machine learning community.</p>
<p>So do you think it&rsquo;s good to have</p>
<p>that kind of split in code bases?</p>
<p>Or so like the benefit there is the competition challenges</p>
<p>the library developers to step up to a game.</p>
<p>But the downside is there&rsquo;s these code bases</p>
<p>that are in different libraries.</p>
<p>Right, so I think the downside is that,</p>
<p>I mean, for a lot of research code</p>
<p>that&rsquo;s released in one framework</p>
<p>and if you&rsquo;re using the other one,</p>
<p>it&rsquo;s really hard to like really build on top of it.</p>
<p>But thankfully the open source community</p>
<p>in machine learning is amazing.</p>
<p>So whenever like something pops up in TensorFlow,</p>
<p>you wait a few days and someone who&rsquo;s like super sharp</p>
<p>will actually come and translate that particular code</p>
<p>based into PyTorch and basically have figured that</p>
<p>all the nooks and crannies out.</p>
<p>So the open source community is amazing</p>
<p>and they really like figure out this gap.</p>
<p>So I think in terms of like having these two frameworks</p>
<p>or multiple, I think of course there are different use cases</p>
<p>so there are going to be benefits</p>
<p>to using one or the other framework.</p>
<p>And like you said, I think competition is just healthy</p>
<p>because both of these frameworks keep</p>
<p>or like all of these frameworks really sort of</p>
<p>keep learning from each other</p>
<p>and keep incorporating different things</p>
<p>to just make them better and better.</p>
<p>What advice would you have for someone</p>
<p>new to machine learning, you know,</p>
<p>maybe just started or haven&rsquo;t even started</p>
<p>but are curious about it and who want to get in the field?</p>
<p>Don&rsquo;t be afraid to get your hands dirty.</p>
<p>I think that&rsquo;s the main thing.</p>
<p>So if something doesn&rsquo;t work,</p>
<p>like really drill into why things are not working.</p>
<p>Can you elaborate what your hands dirty means?</p>
<p>Right, so for example, like if an algorithm,</p>
<p>if you try to train the network and it&rsquo;s not converging,</p>
<p>whatever, rather than trying to like Google the answer</p>
<p>or trying to do something,</p>
<p>like really spend those like five, eight, 10, 15, 20,</p>
<p>whatever number of hours really trying</p>
<p>to figure it out yourself.</p>
<p>Because in that process, you&rsquo;ll actually learn a lot more.</p>
<p>Yeah.</p>
<p>Googling is of course like a good way to solve it</p>
<p>when you need a quick answer.</p>
<p>But I think initially, especially like when you&rsquo;re starting</p>
<p>out, it&rsquo;s much nicer to like figure things out by yourself.</p>
<p>And I just say that from experience</p>
<p>because like when I started out,</p>
<p>there were not a lot of resources.</p>
<p>So we would like in the lab, a lot of us,</p>
<p>like we would look up to senior students</p>
<p>and then the senior students were of course busy</p>
<p>and they would be like, hey, why don&rsquo;t you go figure it out?</p>
<p>Because I just don&rsquo;t have the time.</p>
<p>I&rsquo;m working on my dissertation or whatever.</p>
<p>I&rsquo;ll find a PhD students.</p>
<p>And so then we would sit down</p>
<p>and like just try to figure it out.</p>
<p>And that I think really helped me.</p>
<p>That has really helped me figure a lot of things out.</p>
<p>I think in general, if I were to generalize that,</p>
<p>I feel like persevering through any kind of struggle</p>
<p>on a thing you care about is good.</p>
<p>So you&rsquo;re basically, you try to make it seem</p>
<p>like it&rsquo;s good to spend time debugging,</p>
<p>but really any kind of struggle, whatever form that takes,</p>
<p>it could be just Googling a lot.</p>
<p>Just basically anything, just sticking with it</p>
<p>and going through the hard thing that could take a form</p>
<p>of implementing stuff from scratch.</p>
<p>It could take the form of re implementing</p>
<p>with different libraries</p>
<p>or different programming languages.</p>
<p>It could take a lot of different forms,</p>
<p>but struggle is good for the soul.</p>
<p>So like in Pittsburgh, where I did my PhD,</p>
<p>the thing was it used to snow a lot.</p>
<p>And so when it was snowed, you really couldn&rsquo;t do much.</p>
<p>So the thing that a lot of people said</p>
<p>was snow builds character.</p>
<p>Because when it&rsquo;s snowing, you can&rsquo;t do anything else.</p>
<p>You focus on work.</p>
<p>Do you have advice in general for people,</p>
<p>you&rsquo;ve already exceptionally successful, you&rsquo;re young,</p>
<p>but do you have advice for young people starting out</p>
<p>in college or maybe in high school?</p>
<p>Advice for their career, advice for their life,</p>
<p>how to pave a successful path in career and life?</p>
<p>I would say just be hungry.</p>
<p>Always be hungry for what you want.</p>
<p>And I think I&rsquo;ve been inspired by a lot of people</p>
<p>who are just driven and who really go for what they want,</p>
<p>no matter what, like you shouldn&rsquo;t want it,</p>
<p>you should need it.</p>
<p>So if you need something,</p>
<p>you basically go towards the ends to make it work.</p>
<p>How do you know when you come across a thing</p>
<p>that&rsquo;s like you need?</p>
<p>I think there&rsquo;s not going to be any single thing</p>
<p>that you&rsquo;re going to need.</p>
<p>There are going to be different types of things</p>
<p>that you need, but whenever you need something,</p>
<p>you just go push for it.</p>
<p>And of course, once you, you may not get it,</p>
<p>or you may find that this was not even the thing</p>
<p>that you were looking for, it might be a different thing.</p>
<p>But the point is like you&rsquo;re pushing through things</p>
<p>and that actually brings a lot of skills</p>
<p>and builds a certain kind of attitude</p>
<p>which will probably help you get the other thing</p>
<p>once you figure out what&rsquo;s really the thing that you want.</p>
<p>Yeah, I think a lot of people are,</p>
<p>I&rsquo;ve noticed, kind of afraid of that</p>
<p>is because one, it&rsquo;s a fear of commitment.</p>
<p>And two, there&rsquo;s so many amazing things in this world,</p>
<p>you almost don&rsquo;t want to miss out</p>
<p>on all the other amazing things</p>
<p>by committing to this one thing.</p>
<p>So I think a lot of it has to do with just</p>
<p>allowing yourself to notice that thing</p>
<p>and just go all the way with it.</p>
<p>I mean, I also like failure, right?</p>
<p>So I know this is like super cheesy that failure</p>
<p>is something that you should be prepared for and so on,</p>
<p>but I do think, I mean, especially in research,</p>
<p>for example, failure is something that happens</p>
<p>almost every day is like experiments failing</p>
<p>and not working.</p>
<p>And so you really need to be so used to it.</p>
<p>You need to have a thick skin,</p>
<p>but, and only basically through,</p>
<p>like when you get through it is when you find</p>
<p>the one thing that&rsquo;s actually working.</p>
<p>So Thomas Edison was like one person like that, right?</p>
<p>So I really, like when I was a kid,</p>
<p>I used to really read about how he found like the filament,</p>
<p>the light bulb filament.</p>
<p>And then he, I think his thing was like,</p>
<p>he tried 990 things that didn&rsquo;t work</p>
<p>or something of the sort.</p>
<p>And then they asked him like, so what did you learn?</p>
<p>Because all of these were failed experiments.</p>
<p>And then he says, oh, these 990 things don&rsquo;t work.</p>
<p>And I know that.</p>
<p>Did you know that?</p>
<p>I mean, that&rsquo;s really inspiring.</p>
<p>So you spent a few years on this earth</p>
<p>performing a self supervised kind of learning process.</p>
<p>Have you figured out the meaning of life yet?</p>
<p>I told you I&rsquo;m doing this podcast</p>
<p>to try to get the answer.</p>
<p>I&rsquo;m hoping you could tell me,</p>
<p>what do you think the meaning of it all is?</p>
<p>I don&rsquo;t think I figured this out.</p>
<p>No, I have no idea.</p>
<p>Do you think AI will help us figure it out</p>
<p>or do you think there&rsquo;s no answer?</p>
<p>The whole point is to keep searching.</p>
<p>I think, yeah, I think it&rsquo;s an endless sort of quest for us.</p>
<p>I don&rsquo;t think AI will help us there.</p>
<p>This is like a very hard, hard, hard question</p>
<p>which so many humans have tried to answer.</p>
<p>Well, that&rsquo;s the interesting thing</p>
<p>about the difference between AI and humans.</p>
<p>Humans don&rsquo;t seem to know what the hell they&rsquo;re doing.</p>
<p>And AI is almost always operating</p>
<p>under well defined objective functions.</p>
<p>And I wonder whether our lack of ability</p>
<p>to define good longterm objective functions</p>
<p>or introspect what is the objective function</p>
<p>under which we operate, if that&rsquo;s a feature or a bug.</p>
<p>I would say it&rsquo;s a feature</p>
<p>because then everyone actually has very different kinds</p>
<p>of objective functions that they&rsquo;re optimizing</p>
<p>and those objective functions evolve</p>
<p>and change dramatically through the course</p>
<p>of their life.</p>
<p>That&rsquo;s actually what makes us interesting, right?</p>
<p>If otherwise, like if everyone was doing</p>
<p>the exact same thing, that would be pretty boring.</p>
<p>We do want like people with different kinds</p>
<p>of perspectives, also people evolve continuously.</p>
<p>That&rsquo;s like, I would say the biggest feature of being human.</p>
<p>And then we get to like the ones that die</p>
<p>because they do something stupid.</p>
<p>We get to watch that, see it and learn from it.</p>
<p>And as a species, we take that lesson</p>
<p>and become better and better</p>
<p>because of all the dumb people in the world</p>
<p>that died doing something wild and beautiful.</p>
<p>Ishan, thank you so much for this incredible conversation.</p>
<p>We did a depth first search through the space</p>
<p>of machine learning and it was fun and fascinating.</p>
<p>So it&rsquo;s really an honor to meet you</p>
<p>and it was a really awesome conversation.</p>
<p>Thanks for coming down today and talking with me.</p>
<p>Thanks Lex, I mean, I&rsquo;ve listened to you.</p>
<p>I told you it was unreal for me to actually meet you</p>
<p>in person and I&rsquo;m so happy to be here, thank you.</p>
<p>Thanks man.</p>
<p>Thanks for listening to this conversation</p>
<p>with Ishan Misra and thank you to Onnit,</p>
<p>The Information, Grammarly and Athletic Greens.</p>
<p>Check them out in the description to support this podcast.</p>
<p>And now let me leave you with some words</p>
<p>from Arthur C. Clarke.</p>
<p>Any sufficiently advanced technology</p>
<p>is indistinguishable from magic.</p>
<p>Thank you for listening and hope to see you next time.</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/english/">English</a>
        
            <a href="/tags/podcast/">Podcast</a>
        
            <a href="/tags/lex-fridman-podcast/">Lex Fridman Podcast</a>
        
    </section>


    </footer>


    
</article>

    

    

<div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064"
         crossorigin="anonymous"></script>
    <ins class="adsbygoogle"
         style="display:block; text-align:center;"
         data-ad-layout="in-article"
         data-ad-format="fluid"
         data-ad-client="ca-pub-9206135835124064"
         data-ad-slot="1055602464"></ins>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>
<aside class="related-content--wrapper">
    <h2 class="section-title">Related content</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/en/1310500372/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500372" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #368 - Eliezer Yudkowsky: Dangers of AI and the End of Human Civilization</h2>
            
            <footer class="article-time">
                <time datetime=''>Mar 30, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500371/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500371" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #367 - Sam Altman: OpenAI CEO on GPT-4, ChatGPT, and the Future of AI</h2>
            
            <footer class="article-time">
                <time datetime=''>Mar 26, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500370/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500370" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #366 - Shannon Curry: Johnny Depp &amp; Amber Heard Trial, Marriage, Dating &amp; Love</h2>
            
            <footer class="article-time">
                <time datetime=''>Mar 22, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500369/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500369" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #365 - Sam Harris: Trump, Pandemic, Twitter, Elon, Bret, IDW, Kanye, AI &amp; UFOs</h2>
            
            <footer class="article-time">
                <time datetime=''>Mar 15, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500368/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500368" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #364 - Chris Voss: FBI Hostage Negotiator</h2>
            
            <footer class="article-time">
                <time datetime=''>Mar 11, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2021 - 
        
        2023 SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics
    </section>
    
    <section class="powerby">
        

        As an Amazon Associate I earn from qualifying purchases üõí<br/>

        Built with <a href="https://swiest.com/" target="_blank" rel="noopener">(Ôæâ‚óï„ÉÆ‚óï)Ôæâü™Ñüíûüíñü•∞ across the glüåçüåèüåébe</a> <br />
        
        
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>


<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Serif&family=Noto+Serif+Armenian&family=Noto+Serif+Bengali&family=Noto+Serif+Devanagari&family=Noto+Serif+Georgian&family=Noto+Serif+Gujarati&family=Noto+Serif+HK&family=Noto+Serif+Hebrew&family=Noto+Serif+JP&family=Noto+Serif+KR&family=Noto+Serif+Kannada&family=Noto+Serif+Khmer&family=Noto+Serif+Lao&family=Noto+Serif+Makasar&family=Noto+Serif+Malayalam&family=Noto+Serif+Myanmar&family=Noto+Serif+Oriya&family=Noto+Serif+SC&family=Noto+Serif+Sinhala&family=Noto+Serif+TC&family=Noto+Serif+Tamil&family=Noto+Serif+Telugu&family=Noto+Serif+Thai&family=Noto+Serif+Tibetan&display=swap" rel="stylesheet">

    </body>
</html>

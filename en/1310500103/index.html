<!DOCTYPE html>
<html lang="en" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='The following is a conversation with Ben Goertzel,
one of the most interesting minds
in the artificial intelligence community.
He&amp;rsquo;s the founder of SingularityNet,
designer of OpenCog AI Framework,
formerly a director of research
at the Machine Intelligence Research Institute,
and chief scientist of Hanson Robotics,
the company that created the Sophia robot.
He has been a central figure in the AGI community
for many years, including in his organizing'>
<title>Lex Fridman Podcast - #103 - Ben Goertzel: Artificial General Intelligence | SWIEST</title>

<link rel='canonical' href='https://swiest.com/en/1310500103/'>

<link rel="stylesheet" href="/scss/style.min.91b18679590f4ceed910ade4d64b1e7375cc0770ef5b8c1d822f42424f9ff2c8.css"><script>
    document.oncontextmenu = function(){ return false; };
    document.onselectstart = function(){ return false; };
    document.oncopy = function(){ return false; };
    document.oncut = function(){ return false; };
</script>

<script src="https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>


<script type="text/javascript">
    $(document).ready(function(){
     
     $("#back-to-top").hide();
     
     $(function () {
      $(window).scroll(function(){
       if ($(window).scrollTop()>600){
        $("#back-to-top").fadeIn(500);
       }else{
        $("#back-to-top").fadeOut(500);
       }
     });
     
     $("#back-to-top").click(function(){
      $('body,html').animate({scrollTop:0},500);
       return false;
      });
     });
    });
    </script><meta property='og:title' content='Lex Fridman Podcast - #103 - Ben Goertzel: Artificial General Intelligence'>
<meta property='og:description' content='The following is a conversation with Ben Goertzel,
one of the most interesting minds
in the artificial intelligence community.
He&amp;rsquo;s the founder of SingularityNet,
designer of OpenCog AI Framework,
formerly a director of research
at the Machine Intelligence Research Institute,
and chief scientist of Hanson Robotics,
the company that created the Sophia robot.
He has been a central figure in the AGI community
for many years, including in his organizing'>
<meta property='og:url' content='https://swiest.com/en/1310500103/'>
<meta property='og:site_name' content='SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='English' /><meta property='article:tag' content='Podcast' /><meta property='article:tag' content='Lex Fridman Podcast' /><meta property='article:published_time' content='2022-06-11T17:00:00&#43;00:00'/><meta property='article:modified_time' content='2022-06-11T17:00:00&#43;00:00'/>
<meta name="twitter:title" content="Lex Fridman Podcast - #103 - Ben Goertzel: Artificial General Intelligence">
<meta name="twitter:description" content="The following is a conversation with Ben Goertzel,
one of the most interesting minds
in the artificial intelligence community.
He&amp;rsquo;s the founder of SingularityNet,
designer of OpenCog AI Framework,
formerly a director of research
at the Machine Intelligence Research Institute,
and chief scientist of Hanson Robotics,
the company that created the Sophia robot.
He has been a central figure in the AGI community
for many years, including in his organizing">
    <link rel="shortcut icon" href="/favicon.ico" />
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin="anonymous"></script>
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "dark");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu307e6a33fa6fd661ccda3b77024ef5c2_252345_300x0_resize_box_3.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">‚ú®</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics</a></h1>
            <h2 class="site-description">üåçüåèüåé</h2>
        </div>
    </header><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        
        
        <li >
            <a href='/chart/podcastchart.html' target="_blank">
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-apple-podcast" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M18.364 18.364a9 9 0 1 0 -12.728 0" />
  <path d="M11.766 22h.468a2 2 0 0 0 1.985 -1.752l.5 -4a2 2 0 0 0 -1.985 -2.248h-1.468a2 2 0 0 0 -1.985 2.248l.5 4a2 2 0 0 0 1.985 1.752z" />
  <path d="M12 9m-2 0a2 2 0 1 0 4 0a2 2 0 1 0 -4 0" />
</svg>
                
                <span>Podcast Charts</span>
            </a>
        </li>
        

        <div>
            <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064"
                crossorigin="anonymous"></script>
            
            <ins class="adsbygoogle"
                style="display:block"
                data-ad-client="ca-pub-9206135835124064"
                data-ad-slot="8754979142"
                data-ad-format="auto"
                data-full-width-responsive="true"></ins>
            <script>
                 (adsbygoogle = window.adsbygoogle || []).push({});
            </script>
        </div>

        <div class="menu-bottom-section">
                <li id="i18n-switch">  
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M4 5h7" />
  <path d="M9 3v2c0 4.418 -2.239 8 -5 8" />
  <path d="M5 9c-.003 2.144 2.952 3.908 6.7 4" />
  <path d="M12 20l4 -9l4 9" />
  <path d="M19.1 18h-6.2" />
</svg>



                    <select name="language" onchange="window.location.href = this.selectedOptions[0].value">
                        
                            <option value="https://swiest.com/" selected>English</option>
                        
                            <option value="https://swiest.com/af/" >Afrikaans</option>
                        
                            <option value="https://swiest.com/am/" >·ä†·àõ·à≠·äõ</option>
                        
                            <option value="https://swiest.com/ar/" >ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</option>
                        
                            <option value="https://swiest.com/az/" >Az…ôrbaycan</option>
                        
                            <option value="https://swiest.com/be/" >–±–µ–ª–∞—Ä—É—Å–∫—ñ</option>
                        
                            <option value="https://swiest.com/bg/" >–±—ä–ª–≥–∞—Ä—Å–∫–∏</option>
                        
                            <option value="https://swiest.com/bn/" >‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ</option>
                        
                            <option value="https://swiest.com/bo/" >‡Ωñ‡Ωº‡Ωë‡ºã‡Ω¶‡æê‡Ωë‡ºã</option>
                        
                            <option value="https://swiest.com/bs/" >Bosanski</option>
                        
                            <option value="https://swiest.com/ca/" >Catal√†</option>
                        
                            <option value="https://swiest.com/zh-hans/" >ÁÆÄ‰Ωì‰∏≠Êñá</option>
                        
                            <option value="https://swiest.com/zh-hant/" >ÁπÅÈ´î‰∏≠Êñá</option>
                        
                            <option value="https://swiest.com/cs/" >ƒåe≈°tina</option>
                        
                            <option value="https://swiest.com/el/" >ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨</option>
                        
                            <option value="https://swiest.com/cy/" >Cymraeg</option>
                        
                            <option value="https://swiest.com/da/" >Dansk</option>
                        
                            <option value="https://swiest.com/de/" >Deutsch</option>
                        
                            <option value="https://swiest.com/eo/" >Esperanto</option>
                        
                            <option value="https://swiest.com/es-es/" >Espa√±ol (Espa√±a)</option>
                        
                            <option value="https://swiest.com/es-419/" >Espa√±ol (Latinoam√©rica)</option>
                        
                            <option value="https://swiest.com/et/" >Eesti</option>
                        
                            <option value="https://swiest.com/eu/" >Euskara</option>
                        
                            <option value="https://swiest.com/haw/" > ª≈ålelo Hawai ªi</option>
                        
                            <option value="https://swiest.com/fa/" >ŸÅÿßÿ±ÿ≥€å</option>
                        
                            <option value="https://swiest.com/fi/" >Suomi</option>
                        
                            <option value="https://swiest.com/fo/" >F√∏royskt</option>
                        
                            <option value="https://swiest.com/fr/" >Fran√ßais</option>
                        
                            <option value="https://swiest.com/fy/" >Frysk</option>
                        
                            <option value="https://swiest.com/ga/" >Gaeilge</option>
                        
                            <option value="https://swiest.com/gl/" >Galego</option>
                        
                            <option value="https://swiest.com/gu/" >‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä</option>
                        
                            <option value="https://swiest.com/he/" >◊¢÷¥◊ë◊®÷¥◊ô◊™</option>
                        
                            <option value="https://swiest.com/km/" >·ûÄ·ûò·üí·ûñ·ûª·ûá·û∂·üî</option>
                        
                            <option value="https://swiest.com/hi/" >‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</option>
                        
                            <option value="https://swiest.com/hr/" >Hrvatski</option>
                        
                            <option value="https://swiest.com/ht/" >Krey√≤l Ayisyen</option>
                        
                            <option value="https://swiest.com/hu/" >Magyar</option>
                        
                            <option value="https://swiest.com/hy/" >’Ä’°’µ’•÷Ä’•’∂</option>
                        
                            <option value="https://swiest.com/ig/" >√Ås·ª•ÃÄs·ª•ÃÅ √ågb√≤</option>
                        
                            <option value="https://swiest.com/id/" >Bahasa Indonesia</option>
                        
                            <option value="https://swiest.com/is/" >√çslenska</option>
                        
                            <option value="https://swiest.com/it/" >Italiano</option>
                        
                            <option value="https://swiest.com/ja/" >Êó•Êú¨Ë™û</option>
                        
                            <option value="https://swiest.com/jv/" >Basa Jawa</option>
                        
                            <option value="https://swiest.com/ka/" >·É•·Éê·É†·Éó·É£·Éö·Éò</option>
                        
                            <option value="https://swiest.com/kk/" >“ö–∞–∑–∞“õ—à–∞</option>
                        
                            <option value="https://swiest.com/kn/" >‡≤ï‡≤®‡≥ç‡≤®‡≤°</option>
                        
                            <option value="https://swiest.com/ko/" >ÌïúÍµ≠Ïñ¥</option>
                        
                            <option value="https://swiest.com/or/" >‡¨ì‡¨°‡¨º‡¨ø‡¨Ü</option>
                        
                            <option value="https://swiest.com/ckb/" >⁄©Ÿàÿ±ÿØ€å</option>
                        
                            <option value="https://swiest.com/ky/" >–ö—ã—Ä–≥—ã–∑—á–∞</option>
                        
                            <option value="https://swiest.com/la/" >Latina</option>
                        
                            <option value="https://swiest.com/lb/" >L√´tzebuergesch</option>
                        
                            <option value="https://swiest.com/lo/" >‡∫û‡∫≤‡∫™‡∫≤‡∫•‡∫≤‡∫ß</option>
                        
                            <option value="https://swiest.com/lt/" >Lietuvi≈≥</option>
                        
                            <option value="https://swiest.com/lv/" >Latvie≈°u</option>
                        
                            <option value="https://swiest.com/mk/" >–ú–∞–∫–µ–¥–æ–Ω—Å–∫–∏</option>
                        
                            <option value="https://swiest.com/ml/" >‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç</option>
                        
                            <option value="https://swiest.com/mn/" >–ú–æ–Ω–≥–æ–ª —Ö—ç–ª</option>
                        
                            <option value="https://swiest.com/mr/" >‡§Æ‡§∞‡§æ‡§†‡•Ä</option>
                        
                            <option value="https://swiest.com/sw/" >Kiswahili</option>
                        
                            <option value="https://swiest.com/ms/" >Bahasa Melayu</option>
                        
                            <option value="https://swiest.com/my/" >·Äô·Äº·Äî·Ä∫·Äô·Ä¨</option>
                        
                            <option value="https://swiest.com/ne/" >‡§®‡•á‡§™‡§æ‡§≤‡•Ä</option>
                        
                            <option value="https://swiest.com/nl/" >Nederlands</option>
                        
                            <option value="https://swiest.com/no/" >Norsk</option>
                        
                            <option value="https://swiest.com/pa/" >‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä</option>
                        
                            <option value="https://swiest.com/pl/" >Polski</option>
                        
                            <option value="https://swiest.com/pt-br/" >Portugu√™s Brasil</option>
                        
                            <option value="https://swiest.com/pt-pt/" >Portugu√™s Europeu</option>
                        
                            <option value="https://swiest.com/ro/" >Rom√¢nƒÉ</option>
                        
                            <option value="https://swiest.com/ru/" >–†—É—Å—Å–∫–∏–π</option>
                        
                            <option value="https://swiest.com/rw/" >Kinyarwanda</option>
                        
                            <option value="https://swiest.com/si/" >‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω</option>
                        
                            <option value="https://swiest.com/sk/" >Slovenƒçina</option>
                        
                            <option value="https://swiest.com/sl/" >Sloven≈°ƒçina</option>
                        
                            <option value="https://swiest.com/sq/" >Shqip</option>
                        
                            <option value="https://swiest.com/sr/" >–°—Ä–ø—Å–∫–∏ (Srpski)</option>
                        
                            <option value="https://swiest.com/su/" >Basa Sunda</option>
                        
                            <option value="https://swiest.com/sv/" >Svenska</option>
                        
                            <option value="https://swiest.com/ta/" >‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç</option>
                        
                            <option value="https://swiest.com/te/" >‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</option>
                        
                            <option value="https://swiest.com/tg/" >–¢–æ“∑–∏–∫”£</option>
                        
                            <option value="https://swiest.com/th/" >‡πÑ‡∏ó‡∏¢</option>
                        
                            <option value="https://swiest.com/tl/" >Filipino</option>
                        
                            <option value="https://swiest.com/tr/" >T√ºrk√ße</option>
                        
                            <option value="https://swiest.com/uk/" >–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</option>
                        
                            <option value="https://swiest.com/ur/" >ÿßÿ±ÿØŸà</option>
                        
                            <option value="https://swiest.com/uz/" >O&#39;zbekcha</option>
                        
                            <option value="https://swiest.com/vi/" >Ti·∫øng Vi·ªát</option>
                        
                            <option value="https://swiest.com/yi/" >◊ê◊ô◊ì◊ô◊©</option>
                        
                            <option value="https://swiest.com/zh-hk/" >Á≤µË™û</option>
                        
                            <option value="https://swiest.com/zu/" >IsiZulu</option>
                        
                    </select>
                </li>
            
            
            
                <li id="dark-mode-toggle">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <span>Dark Mode</span>
                </li>
            
        </div>
    </ol>
</aside>

    

            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/podcast/" >
                Podcast
            </a>
        
            <a href="/categories/lex-fridman-podcast/" >
                Lex Fridman Podcast
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/en/1310500103/">Lex Fridman Podcast - #103 - Ben Goertzel: Artificial General Intelligence</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2022-06-11</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    184 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>
<div>
    <ul>
       <a href="https://amzn.to/471i0jl" target="_blank">üéÅAmazon Prime</a>
       <a href="https://amzn.to/3QDVlVf" target="_blank">üìñKindle Unlimited</a>
       <a href="https://amzn.to/3FqzNoB" target="_blank">üéßAudible Plus</a>
       <a href="https://amzn.to/3tMT3dm" target="_blank">üéµAmazon Music Unlimited</a>
       <a href="https://www.iherb.com/?rcode=EID1574" target="_blank">üåøiHerb</a>
    </ul>
</div>
<div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064"
     crossorigin="anonymous"></script>
    
    <ins class="adsbygoogle"
         style="display:block"
         data-ad-client="ca-pub-9206135835124064"
         data-ad-slot="8754979142"
         data-ad-format="auto"
         data-full-width-responsive="true"></ins>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>


    <section class="article-content">
    
    
    <p>The following is a conversation with Ben Goertzel,</p>
<p>one of the most interesting minds</p>
<p>in the artificial intelligence community.</p>
<p>He&rsquo;s the founder of SingularityNet,</p>
<p>designer of OpenCog AI Framework,</p>
<p>formerly a director of research</p>
<p>at the Machine Intelligence Research Institute,</p>
<p>and chief scientist of Hanson Robotics,</p>
<p>the company that created the Sophia robot.</p>
<p>He has been a central figure in the AGI community</p>
<p>for many years, including in his organizing</p>
<p>and contributing to the conference</p>
<p>on artificial general intelligence,</p>
<p>the 2020 version of which is actually happening this week,</p>
<p>Wednesday, Thursday, and Friday.</p>
<p>It&rsquo;s virtual and free.</p>
<p>I encourage you to check out the talks,</p>
<p>including by Yosha Bach from episode 101 of this podcast.</p>
<p>Quick summary of the ads.</p>
<p>Two sponsors, The Jordan Harbinger Show and Masterclass.</p>
<p>Please consider supporting this podcast</p>
<p>by going to jordanharbinger.com slash lex</p>
<p>and signing up at masterclass.com slash lex.</p>
<p>Click the links, buy all the stuff.</p>
<p>It&rsquo;s the best way to support this podcast</p>
<p>and the journey I&rsquo;m on in my research and startup.</p>
<p>This is the Artificial Intelligence Podcast.</p>
<p>If you enjoy it, subscribe on YouTube,</p>
<p>review it with five stars on Apple Podcast,</p>
<p>support it on Patreon, or connect with me on Twitter</p>
<p>at lexfriedman, spelled without the E, just F R I D M A N.</p>
<p>As usual, I&rsquo;ll do a few minutes of ads now</p>
<p>and never any ads in the middle</p>
<p>that can break the flow of the conversation.</p>
<p>This episode is supported by The Jordan Harbinger Show.</p>
<p>Go to jordanharbinger.com slash lex.</p>
<p>It&rsquo;s how he knows I sent you.</p>
<p>On that page, there&rsquo;s links to subscribe to it</p>
<p>on Apple Podcast, Spotify, and everywhere else.</p>
<p>I&rsquo;ve been binging on his podcast.</p>
<p>Jordan is great.</p>
<p>He gets the best out of his guests,</p>
<p>dives deep, calls them out when it&rsquo;s needed,</p>
<p>and makes the whole thing fun to listen to.</p>
<p>He&rsquo;s interviewed Kobe Bryant, Mark Cuban,</p>
<p>Neil deGrasse Tyson, Keira Kasparov, and many more.</p>
<p>His conversation with Kobe is a reminder</p>
<p>how much focus and hard work is required for greatness</p>
<p>in sport, business, and life.</p>
<p>I highly recommend the episode if you want to be inspired.</p>
<p>Again, go to jordanharbinger.com slash lex.</p>
<p>It&rsquo;s how Jordan knows I sent you.</p>
<p>This show is sponsored by Master Class.</p>
<p>Sign up at masterclass.com slash lex</p>
<p>to get a discount and to support this podcast.</p>
<p>When I first heard about Master Class,</p>
<p>I thought it was too good to be true.</p>
<p>For 180 bucks a year, you get an all access pass</p>
<p>to watch courses from to list some of my favorites.</p>
<p>Chris Hadfield on Space Exploration,</p>
<p>Neil deGrasse Tyson on Scientific Thinking</p>
<p>and Communication, Will Wright, creator of</p>
<p>the greatest city building game ever, Sim City,</p>
<p>and Sims on Space Exploration.</p>
<p>Ben Sims on Game Design, Carlos Santana on Guitar,</p>
<p>Keira Kasparov, the greatest chess player ever on chess,</p>
<p>Daniel Negrano on Poker, and many more.</p>
<p>Chris Hadfield explaining how rockets work</p>
<p>and the experience of being launched into space alone</p>
<p>is worth the money.</p>
<p>Once again, sign up at masterclass.com slash lex</p>
<p>to get a discount and to support this podcast.</p>
<p>Now, here&rsquo;s my conversation with Ben Kurtzell.</p>
<p>What books, authors, ideas had a lot of impact on you</p>
<p>in your life in the early days?</p>
<p>You know, what got me into AI and science fiction</p>
<p>and such in the first place wasn&rsquo;t a book,</p>
<p>but the original Star Trek TV show,</p>
<p>which my dad watched with me like in its first run.</p>
<p>It would have been 1968, 69 or something,</p>
<p>and that was incredible because every show</p>
<p>they visited a different alien civilization</p>
<p>with different culture and weird mechanisms.</p>
<p>But that got me into science fiction,</p>
<p>and there wasn&rsquo;t that much science fiction</p>
<p>to watch on TV at that stage,</p>
<p>so that got me into reading the whole literature</p>
<p>of science fiction, you know,</p>
<p>from the beginning of the previous century until that time.</p>
<p>And I mean, there was so many science fiction writers</p>
<p>who were inspirational to me.</p>
<p>I&rsquo;d say if I had to pick two,</p>
<p>it would have been Stanis≈Çaw Lem, the Polish writer.</p>
<p>Yeah, Solaris, and then he had a bunch</p>
<p>of more obscure writings on superhuman AIs</p>
<p>that were engineered.</p>
<p>Solaris was sort of a superhuman,</p>
<p>naturally occurring intelligence.</p>
<p>Then Philip K. Dick, who, you know,</p>
<p>ultimately my fandom for Philip K. Dick</p>
<p>is one of the things that brought me together</p>
<p>with David Hansen, my collaborator on robotics projects.</p>
<p>So, you know, Stanis≈Çaw Lem was very much an intellectual,</p>
<p>right, so he had a very broad view of intelligence</p>
<p>going beyond the human and into what I would call,</p>
<p>you know, open ended superintelligence.</p>
<p>The Solaris superintelligent ocean was intelligent,</p>
<p>in some ways more generally intelligent than people,</p>
<p>but in a complex and confusing way</p>
<p>so that human beings could never quite connect to it,</p>
<p>but it was still probably very, very smart.</p>
<p>And then the Golem 4 supercomputer</p>
<p>in one of Lem&rsquo;s books, this was engineered by people,</p>
<p>but eventually it became very intelligent</p>
<p>in a different direction than humans</p>
<p>and decided that humans were kind of trivial,</p>
<p>not that interesting.</p>
<p>So it put some impenetrable shield around itself,</p>
<p>shut itself off from humanity,</p>
<p>and then issued some philosophical screed</p>
<p>about the pathetic and hopeless nature of humanity</p>
<p>and all human thought, and then disappeared.</p>
<p>Now, Philip K. Dick, he was a bit different.</p>
<p>He was human focused, right?</p>
<p>His main thing was, you know, human compassion</p>
<p>and the human heart and soul are going to be the constant</p>
<p>that will keep us going through whatever aliens we discover</p>
<p>or telepathy machines or super AIs or whatever it might be.</p>
<p>So he didn&rsquo;t believe in reality,</p>
<p>like the reality that we see may be a simulation</p>
<p>or a dream or something else we can&rsquo;t even comprehend,</p>
<p>but he believed in love and compassion</p>
<p>as something persistent</p>
<p>through the various simulated realities.</p>
<p>So those two science fiction writers had a huge impact on me.</p>
<p>Then a little older than that, I got into Dostoevsky</p>
<p>and Friedrich Nietzsche and Rimbaud</p>
<p>and a bunch of more literary type writing.</p>
<p>Can we talk about some of those things?</p>
<p>So on the Solaris side, Stanislaw Lem,</p>
<p>this kind of idea of there being intelligences out there</p>
<p>that are different than our own,</p>
<p>do you think there are intelligences maybe all around us</p>
<p>that we&rsquo;re not able to even detect?</p>
<p>So this kind of idea of,</p>
<p>maybe you can comment also on Stephen Wolfram</p>
<p>thinking that there&rsquo;s computations all around us</p>
<p>and we&rsquo;re just not smart enough to kind of detect</p>
<p>their intelligence or appreciate their intelligence.</p>
<p>Yeah, so my friend Hugo de Gares,</p>
<p>who I&rsquo;ve been talking to about these things</p>
<p>for many decades, since the early 90s,</p>
<p>he had an idea he called SIPI,</p>
<p>the Search for Intraparticulate Intelligence.</p>
<p>So the concept there was as AIs get smarter</p>
<p>and smarter and smarter,</p>
<p>assuming the laws of physics as we know them now</p>
<p>are still what these super intelligences</p>
<p>perceived to hold and are bound by,</p>
<p>as they get smarter and smarter,</p>
<p>they&rsquo;re gonna shrink themselves littler and littler</p>
<p>because special relativity makes it</p>
<p>so they can communicate</p>
<p>between two spatially distant points.</p>
<p>So they&rsquo;re gonna get smaller and smaller,</p>
<p>but then ultimately, what does that mean?</p>
<p>The minds of the super, super, super intelligences,</p>
<p>they&rsquo;re gonna be packed into the interaction</p>
<p>of elementary particles or quarks</p>
<p>or the partons inside quarks or whatever it is.</p>
<p>So what we perceive as random fluctuations</p>
<p>on the quantum or sub quantum level</p>
<p>may actually be the thoughts</p>
<p>of the micro, micro, micro miniaturized super intelligences</p>
<p>because there&rsquo;s no way we can tell random</p>
<p>from structured but within algorithmic information</p>
<p>more complex than our brains, right?</p>
<p>We can&rsquo;t tell the difference.</p>
<p>So what we think is random could be the thought processes</p>
<p>of some really tiny super minds.</p>
<p>And if so, there is not a damn thing we can do about it,</p>
<p>except try to upgrade our intelligences</p>
<p>and expand our minds so that we can perceive</p>
<p>more of what&rsquo;s around us.</p>
<p>But if those random fluctuations,</p>
<p>like even if we go to like quantum mechanics,</p>
<p>if that&rsquo;s actually super intelligent systems,</p>
<p>aren&rsquo;t we then part of the super of super intelligence?</p>
<p>Aren&rsquo;t we just like a finger of the entirety</p>
<p>of the body of the super intelligent system?</p>
<p>It could be, I mean, a finger is a strange metaphor.</p>
<p>I mean, we&hellip;</p>
<p>A finger is dumb is what I mean.</p>
<p>But the finger is also useful</p>
<p>and is controlled with intent by the brain</p>
<p>whereas we may be much less than that, right?</p>
<p>I mean, yeah, we may be just some random epiphenomenon</p>
<p>that they don&rsquo;t care about too much.</p>
<p>Like think about the shape of the crowd emanating</p>
<p>from a sports stadium or something, right?</p>
<p>There&rsquo;s some emergent shape to the crowd, it&rsquo;s there.</p>
<p>You could take a picture of it, it&rsquo;s kind of cool.</p>
<p>It&rsquo;s irrelevant to the main point of the sports event</p>
<p>or where the people are going</p>
<p>or what&rsquo;s on the minds of the people</p>
<p>making that shape in the crowd, right?</p>
<p>So we may just be some semi arbitrary higher level pattern</p>
<p>popping out of a lower level</p>
<p>hyper intelligent self organization.</p>
<p>And I mean, so be it, right?</p>
<p>I mean, that&rsquo;s one thing that&hellip;</p>
<p>Yeah, I mean, the older I&rsquo;ve gotten,</p>
<p>the more respect I&rsquo;ve achieved for our fundamental ignorance.</p>
<p>I mean, mine and everybody else&rsquo;s.</p>
<p>I mean, I look at my two dogs,</p>
<p>two beautiful little toy poodles</p>
<p>and they watch me sitting at the computer typing.</p>
<p>They just think I&rsquo;m sitting there wiggling my fingers</p>
<p>to exercise them maybe or guarding the monitor on the desk</p>
<p>that they have no idea that I&rsquo;m communicating</p>
<p>with other people halfway around the world,</p>
<p>let alone creating complex algorithms</p>
<p>running in RAM on some computer server</p>
<p>in St. Petersburg or something, right?</p>
<p>Although they&rsquo;re right there in the room with me.</p>
<p>So what things are there right around us</p>
<p>that we&rsquo;re just too stupid or close minded to comprehend?</p>
<p>Probably quite a lot.</p>
<p>Your very poodle could also be communicating</p>
<p>across multiple dimensions with other beings</p>
<p>and you&rsquo;re too unintelligent to understand</p>
<p>the kind of communication mechanism they&rsquo;re going through.</p>
<p>There have been various TV shows and science fiction novels,</p>
<p>poisoning cats, dolphins, mice and whatnot</p>
<p>are actually super intelligences here to observe that.</p>
<p>I would guess as one or the other quantum physics founders</p>
<p>said, those theories are not crazy enough to be true.</p>
<p>The reality is probably crazier than that.</p>
<p>Beautifully put.</p>
<p>So on the human side, with Philip K. Dick</p>
<p>and in general, where do you fall on this idea</p>
<p>that love and just the basic spirit of human nature</p>
<p>persists throughout these multiple realities?</p>
<p>Are you on the side, like the thing that inspires you</p>
<p>about artificial intelligence,</p>
<p>is it the human side of somehow persisting</p>
<p>through all of the different systems we engineer</p>
<p>or is AI inspire you to create something</p>
<p>that&rsquo;s greater than human, that&rsquo;s beyond human,</p>
<p>that&rsquo;s almost nonhuman?</p>
<p>I would say my motivation to create AGI</p>
<p>comes from both of those directions actually.</p>
<p>So when I first became passionate about AGI</p>
<p>when I was, it would have been two or three years old</p>
<p>after watching robots on Star Trek.</p>
<p>I mean, then it was really a combination</p>
<p>of intellectual curiosity, like can a machine really think,</p>
<p>how would you do that?</p>
<p>And yeah, just ambition to create something much better</p>
<p>than all the clearly limited</p>
<p>and fundamentally defective humans I saw around me.</p>
<p>Then as I got older and got more enmeshed</p>
<p>in the human world and got married, had children,</p>
<p>saw my parents begin to age, I started to realize,</p>
<p>well, not only will AGI let you go far beyond</p>
<p>the limitations of the human,</p>
<p>but it could also stop us from dying and suffering</p>
<p>and feeling pain and tormenting ourselves mentally.</p>
<p>So you can see AGI has amazing capability</p>
<p>to do good for humans, as humans,</p>
<p>alongside with its capability</p>
<p>to go far, far beyond the human level.</p>
<p>So I mean, both aspects are there,</p>
<p>which makes it even more exciting and important.</p>
<p>So you mentioned Dostoevsky and Nietzsche.</p>
<p>Where did you pick up from those guys?</p>
<p>I mean.</p>
<p>That would probably go beyond the scope</p>
<p>of a brief interview, certainly.</p>
<p>I mean, both of those are amazing thinkers</p>
<p>who one, will necessarily have</p>
<p>a complex relationship with, right?</p>
<p>So, I mean, Dostoevsky on the minus side,</p>
<p>he&rsquo;s kind of a religious fanatic</p>
<p>and he sort of helped squash the Russian nihilist movement,</p>
<p>which was very interesting.</p>
<p>Because what nihilism meant originally</p>
<p>in that period of the mid, late 1800s in Russia</p>
<p>was not taking anything fully 100% for granted.</p>
<p>It was really more like what we&rsquo;d call Bayesianism now,</p>
<p>where you don&rsquo;t wanna adopt anything</p>
<p>as a dogmatic certitude and always leave your mind open.</p>
<p>And how Dostoevsky parodied nihilism</p>
<p>was a bit different, right?</p>
<p>He parodied as people who believe absolutely nothing.</p>
<p>So they must assign an equal probability weight</p>
<p>to every proposition, which doesn&rsquo;t really work.</p>
<p>So on the one hand, I didn&rsquo;t really agree with Dostoevsky</p>
<p>on his sort of religious point of view.</p>
<p>On the other hand, if you look at his understanding</p>
<p>of human nature and sort of the human mind</p>
<p>and heart and soul, it&rsquo;s really unparalleled.</p>
<p>He had an amazing view of how human beings construct a world</p>
<p>for themselves based on their own understanding</p>
<p>and their own mental predisposition.</p>
<p>And I think if you look in the brothers Karamazov</p>
<p>in particular, the Russian literary theorist Mikhail Bakhtin</p>
<p>wrote about this as a polyphonic mode of fiction,</p>
<p>which means it&rsquo;s not third person,</p>
<p>but it&rsquo;s not first person from any one person really.</p>
<p>There are many different characters in the novel</p>
<p>and each of them is sort of telling part of the story</p>
<p>from their own point of view.</p>
<p>So the reality of the whole story is an intersection</p>
<p>like synergetically of the many different characters</p>
<p>world views.</p>
<p>And that really, it&rsquo;s a beautiful metaphor</p>
<p>and even a reflection I think of how all of us</p>
<p>socially create our reality.</p>
<p>Like each of us sees the world in a certain way.</p>
<p>Each of us in a sense is making the world as we see it</p>
<p>based on our own minds and understanding,</p>
<p>but it&rsquo;s polyphony like in music</p>
<p>where multiple instruments are coming together</p>
<p>to create the sound.</p>
<p>The ultimate reality that&rsquo;s created</p>
<p>comes out of each of our subjective understandings,</p>
<p>intersecting with each other.</p>
<p>And that was one of the many beautiful things in Dostoevsky.</p>
<p>So maybe a little bit to mention,</p>
<p>you have a connection to Russia and the Soviet culture.</p>
<p>I mean, I&rsquo;m not sure exactly what the nature</p>
<p>of the connection is, but at least the spirit</p>
<p>of your thinking is in there.</p>
<p>Well, my ancestry is three quarters Eastern European Jewish.</p>
<p>So I mean, my three of my great grandparents</p>
<p>emigrated to New York from Lithuania</p>
<p>and sort of border regions of Poland,</p>
<p>which are in and out of Poland</p>
<p>in around the time of World War I.</p>
<p>And they were socialists and communists as well as Jews,</p>
<p>mostly Menshevik, not Bolshevik.</p>
<p>And they sort of, they fled at just the right time</p>
<p>to the US for their own personal reasons.</p>
<p>And then almost all, or maybe all of my extended family</p>
<p>that remained in Eastern Europe was killed</p>
<p>either by Hitlands or Stalin&rsquo;s minions at some point.</p>
<p>So the branch of the family that emigrated to the US</p>
<p>was pretty much the only one.</p>
<p>So how much of the spirit of the people</p>
<p>is in your blood still?</p>
<p>Like, when you look in the mirror, do you see,</p>
<p>what do you see?</p>
<p>Meat, I see a bag of meat that I want to transcend</p>
<p>by uploading into some sort of superior reality.</p>
<p>But very, I mean, yeah, very clearly,</p>
<p>I mean, I&rsquo;m not religious in a traditional sense,</p>
<p>but clearly the Eastern European Jewish tradition</p>
<p>was what I was raised in.</p>
<p>I mean, there was, my grandfather, Leo Zwell,</p>
<p>was a physical chemist who worked with Linus Pauling</p>
<p>and a bunch of the other early greats in quantum mechanics.</p>
<p>I mean, he was into X ray diffraction.</p>
<p>He was on the material science side,</p>
<p>an experimentalist rather than a theorist.</p>
<p>His sister was also a physicist.</p>
<p>And my father&rsquo;s father, Victor Gertzel,</p>
<p>was a PhD in psychology who had the unenviable job</p>
<p>of giving Soka therapy to the Japanese</p>
<p>in internment camps in the US in World War II,</p>
<p>like to counsel them why they shouldn&rsquo;t kill themselves,</p>
<p>even though they&rsquo;d had all their stuff taken away</p>
<p>and been imprisoned for no good reason.</p>
<p>So, I mean, yeah, there&rsquo;s a lot of Eastern European</p>
<p>Jewishness in my background.</p>
<p>One of my great uncles was, I guess,</p>
<p>conductor of San Francisco Orchestra.</p>
<p>So there&rsquo;s a lot of Mickey Salkind,</p>
<p>bunch of music in there also.</p>
<p>And clearly this culture was all about learning</p>
<p>and understanding the world,</p>
<p>and also not quite taking yourself too seriously</p>
<p>while you do it, right?</p>
<p>There&rsquo;s a lot of Yiddish humor in there.</p>
<p>So I do appreciate that culture,</p>
<p>although the whole idea that like the Jews</p>
<p>are the chosen people of God</p>
<p>never resonated with me too much.</p>
<p>The graph of the Gertzel family,</p>
<p>I mean, just the people I&rsquo;ve encountered</p>
<p>just doing some research and just knowing your work</p>
<p>through the decades, it&rsquo;s kind of fascinating.</p>
<p>Just the number of PhDs.</p>
<p>Yeah, yeah, I mean, my dad is a sociology professor</p>
<p>who recently retired from Rutgers University,</p>
<p>but clearly that gave me a head start in life.</p>
<p>I mean, my grandfather gave me</p>
<p>all those quantum mechanics books</p>
<p>when I was like seven or eight years old.</p>
<p>I remember going through them,</p>
<p>and it was all the old quantum mechanics</p>
<p>like Rutherford Adams and stuff.</p>
<p>So I got to the part of wave functions,</p>
<p>which I didn&rsquo;t understand, although I was very bright kid.</p>
<p>And I realized he didn&rsquo;t quite understand it either,</p>
<p>but at least like he pointed me to some professor</p>
<p>he knew at UPenn nearby who understood these things, right?</p>
<p>So that&rsquo;s an unusual opportunity for a kid to have, right?</p>
<p>My dad, he was programming Fortran</p>
<p>when I was 10 or 11 years old</p>
<p>on like HP 3000 mainframes at Rutgers University.</p>
<p>So I got to do linear regression in Fortran</p>
<p>on punch cards when I was in middle school, right?</p>
<p>Because he was doing, I guess, analysis of demographic</p>
<p>and sociology data.</p>
<p>So yes, certainly that gave me a head start</p>
<p>and a push towards science beyond what would have been</p>
<p>the case with many, many different situations.</p>
<p>When did you first fall in love with AI?</p>
<p>Is it the programming side of Fortran?</p>
<p>Is it maybe the sociology psychology</p>
<p>that you picked up from your dad?</p>
<p>Or is it the quantum mechanics?</p>
<p>I fell in love with AI when I was probably three years old</p>
<p>when I saw a robot on Star Trek.</p>
<p>It was turning around in a circle going,</p>
<p>error, error, error, error,</p>
<p>because Spock and Kirk had tricked it</p>
<p>into a mechanical breakdown by presenting it</p>
<p>with a logical paradox.</p>
<p>And I was just like, well, this makes no sense.</p>
<p>This AI is very, very smart.</p>
<p>It&rsquo;s been traveling all around the universe,</p>
<p>but these people could trick it</p>
<p>with a simple logical paradox.</p>
<p>Like why, if the human brain can get beyond that paradox,</p>
<p>why can&rsquo;t this AI?</p>
<p>So I felt the screenwriters of Star Trek</p>
<p>had misunderstood the nature of intelligence.</p>
<p>And I complained to my dad about it,</p>
<p>and he wasn&rsquo;t gonna say anything one way or the other.</p>
<p>But before I was born, when my dad was at Antioch College</p>
<p>in the middle of the US,</p>
<p>he led a protest movement called SLAM,</p>
<p>Student League Against Mortality.</p>
<p>They were protesting against death,</p>
<p>wandering across the campus.</p>
<p>So he was into some futuristic things even back then,</p>
<p>but whether AI could confront logical paradoxes or not,</p>
<p>he didn&rsquo;t know.</p>
<p>But when I, 10 years after that or something,</p>
<p>I discovered Douglas Hofstadter&rsquo;s book,</p>
<p>Gordalesh or Bach, and that was sort of to the same point of AI</p>
<p>and paradox and logic, right?</p>
<p>Because he was over and over</p>
<p>with Gordal&rsquo;s incompleteness theorem,</p>
<p>and can an AI really fully model itself reflexively</p>
<p>or does that lead you into some paradox?</p>
<p>Can the human mind truly model itself reflexively</p>
<p>or does that lead you into some paradox?</p>
<p>So I think that book, Gordalesh or Bach,</p>
<p>which I think I read when it first came out,</p>
<p>I would have been 12 years old or something.</p>
<p>I remember it was like 16 hour day.</p>
<p>I read it cover to cover and then reread it.</p>
<p>I reread it after that,</p>
<p>because there was a lot of weird things</p>
<p>with little formal systems in there</p>
<p>that were hard for me at the time.</p>
<p>But that was the first book I read</p>
<p>that gave me a feeling for AI as like a practical academic</p>
<p>or engineering discipline that people were working in.</p>
<p>Because before I read Gordalesh or Bach,</p>
<p>I was into AI from the point of view of a science fiction fan.</p>
<p>And I had the idea, well, it may be a long time</p>
<p>before we can achieve immortality in superhuman AGI.</p>
<p>So I should figure out how to build a spacecraft</p>
<p>traveling close to the speed of light, go far away,</p>
<p>then come back to the earth in a million years</p>
<p>when technology is more advanced</p>
<p>and we can build these things.</p>
<p>Reading Gordalesh or Bach,</p>
<p>while it didn&rsquo;t all ring true to me, a lot of it did,</p>
<p>but I could see like there are smart people right now</p>
<p>at various universities around me</p>
<p>who are actually trying to work on building</p>
<p>what I would now call AGI,</p>
<p>although Hofstadter didn&rsquo;t call it that.</p>
<p>So really it was when I read that book,</p>
<p>which would have been probably middle school,</p>
<p>that then I started to think,</p>
<p>well, this is something that I could practically work on.</p>
<p>Yeah, as opposed to flying away and waiting it out,</p>
<p>you can actually be one of the people</p>
<p>that actually builds the system.</p>
<p>Yeah, exactly.</p>
<p>And if you think about, I mean,</p>
<p>I was interested in what we&rsquo;d now call nanotechnology</p>
<p>and in the human immortality and time travel,</p>
<p>all the same cool things as every other,</p>
<p>like science fiction loving kid.</p>
<p>But AI seemed like if Hofstadter was right,</p>
<p>you just figure out the right program,</p>
<p>sit there and type it.</p>
<p>Like you don&rsquo;t need to spin stars into weird configurations</p>
<p>or get government approval to cut people up</p>
<p>and fiddle with their DNA or something, right?</p>
<p>It&rsquo;s just programming.</p>
<p>And then of course that can achieve anything else.</p>
<p>There&rsquo;s another book from back then,</p>
<p>which was by Gerald Feinbaum,</p>
<p>who was a physicist at Princeton.</p>
<p>And that was the Prometheus Project.</p>
<p>And this book was written in the late 1960s,</p>
<p>though I encountered it in the mid 70s.</p>
<p>But what this book said is in the next few decades,</p>
<p>humanity is gonna create superhuman thinking machines,</p>
<p>molecular nanotechnology and human immortality.</p>
<p>And then the challenge we&rsquo;ll have is what to do with it.</p>
<p>Do we use it to expand human consciousness</p>
<p>in a positive direction?</p>
<p>Or do we use it just to further vapid consumerism?</p>
<p>And what he proposed was that the UN</p>
<p>should do a survey on this.</p>
<p>And the UN should send people out to every little village</p>
<p>in remotest Africa or South America</p>
<p>and explain to everyone what technology</p>
<p>was gonna bring the next few decades</p>
<p>and the choice that we had about how to use it.</p>
<p>And let everyone on the whole planet vote</p>
<p>about whether we should develop super AI nanotechnology</p>
<p>and immortality for expanded consciousness</p>
<p>or for rampant consumerism.</p>
<p>And needless to say, that didn&rsquo;t quite happen.</p>
<p>And I think this guy died in the mid 80s,</p>
<p>so we didn&rsquo;t even see his ideas start</p>
<p>to become more mainstream.</p>
<p>But it&rsquo;s interesting, many of the themes I&rsquo;m engaged with now</p>
<p>from AGI and immortality,</p>
<p>even to trying to democratize technology</p>
<p>as I&rsquo;ve been pushing forward with Singularity,</p>
<p>my work in the blockchain world,</p>
<p>many of these themes were there in Feinbaum&rsquo;s book</p>
<p>in the late 60s even.</p>
<p>And of course, Valentin Turchin, a Russian writer</p>
<p>and a great Russian physicist who I got to know</p>
<p>when we both lived in New York in the late 90s</p>
<p>and early aughts.</p>
<p>I mean, he had a book in the late 60s in Russia,</p>
<p>which was the phenomenon of science,</p>
<p>which laid out all these same things as well.</p>
<p>And Val died in, I don&rsquo;t remember,</p>
<p>2004 or five or something of Parkinson&rsquo;sism.</p>
<p>So yeah, it&rsquo;s easy for people to lose track now</p>
<p>of the fact that the futurist and Singularitarian</p>
<p>advanced technology ideas that are now almost mainstream</p>
<p>are on TV all the time.</p>
<p>I mean, these are not that new, right?</p>
<p>They&rsquo;re sort of new in the history of the human species,</p>
<p>but I mean, these were all around in fairly mature form</p>
<p>in the middle of the last century,</p>
<p>were written about quite articulately</p>
<p>by fairly mainstream people</p>
<p>who were professors at top universities.</p>
<p>It&rsquo;s just until the enabling technologies</p>
<p>got to a certain point, then you couldn&rsquo;t make it real.</p>
<p>And even in the 70s, I was sort of seeing that</p>
<p>and living through it, right?</p>
<p>From Star Trek to Douglas Hofstadter,</p>
<p>things were getting very, very practical</p>
<p>from the late 60s to the late 70s.</p>
<p>And the first computer I bought,</p>
<p>you could only program with hexadecimal machine code</p>
<p>and you had to solder it together.</p>
<p>And then like a few years later, there&rsquo;s punch cards.</p>
<p>And a few years later, you could get like Atari 400</p>
<p>and Commodore VIC 20, and you could type on the keyboard</p>
<p>and program in higher level languages</p>
<p>alongside the assembly language.</p>
<p>So these ideas have been building up a while.</p>
<p>And I guess my generation got to feel them build up,</p>
<p>which is different than people coming into the field now</p>
<p>for whom these things have just been part of the ambience</p>
<p>of culture for their whole career</p>
<p>or even their whole life.</p>
<p>Well, it&rsquo;s fascinating to think about there being all</p>
<p>of these ideas kind of swimming, almost with the noise</p>
<p>all around the world, all the different generations,</p>
<p>and then some kind of nonlinear thing happens</p>
<p>where they percolate up</p>
<p>and capture the imagination of the mainstream.</p>
<p>And that seems to be what&rsquo;s happening with AI now.</p>
<p>I mean, Nietzsche, who you mentioned had the idea</p>
<p>of the Superman, right?</p>
<p>But he didn&rsquo;t understand enough about technology</p>
<p>to think you could physically engineer a Superman</p>
<p>by piecing together molecules in a certain way.</p>
<p>He was a bit vague about how the Superman would appear,</p>
<p>but he was quite deep at thinking</p>
<p>about what the state of consciousness</p>
<p>and the mode of cognition of a Superman would be.</p>
<p>He was a very astute analyst of how the human mind</p>
<p>constructs the illusion of a self,</p>
<p>how it constructs the illusion of free will,</p>
<p>how it constructs values like good and evil</p>
<p>out of its own desire to maintain</p>
<p>and advance its own organism.</p>
<p>He understood a lot about how human minds work.</p>
<p>Then he understood a lot</p>
<p>about how post human minds would work.</p>
<p>I mean, the Superman was supposed to be a mind</p>
<p>that would basically have complete root access</p>
<p>to its own brain and consciousness</p>
<p>and be able to architect its own value system</p>
<p>and inspect and fine tune all of its own biases.</p>
<p>So that&rsquo;s a lot of powerful thinking there,</p>
<p>which then fed in and sort of seeded</p>
<p>all of postmodern continental philosophy</p>
<p>and all sorts of things have been very valuable</p>
<p>in development of culture and indirectly even of technology.</p>
<p>But of course, without the technology there,</p>
<p>it was all some quite abstract thinking.</p>
<p>So now we&rsquo;re at a time in history</p>
<p>when a lot of these ideas can be made real,</p>
<p>which is amazing and scary, right?</p>
<p>It&rsquo;s kind of interesting to think,</p>
<p>what do you think Nietzsche would do</p>
<p>if he was born a century later or transported through time?</p>
<p>What do you think he would say about AI?</p>
<p>I mean. Well, those are quite different.</p>
<p>If he&rsquo;s born a century later or transported through time.</p>
<p>Well, he&rsquo;d be on like TikTok and Instagram</p>
<p>and he would never write the great works he&rsquo;s written.</p>
<p>So let&rsquo;s transport him through time.</p>
<p>Maybe also Sprach Zarathustra would be a music video,</p>
<p>right? I mean, who knows?</p>
<p>Yeah, but if he was transported through time,</p>
<p>do you think, that&rsquo;d be interesting actually to go back.</p>
<p>You just made me realize that it&rsquo;s possible to go back</p>
<p>and read Nietzsche with an eye of,</p>
<p>is there some thinking about artificial beings?</p>
<p>I&rsquo;m sure there he had inklings.</p>
<p>I mean, with Frankenstein before him,</p>
<p>I&rsquo;m sure he had inklings of artificial beings</p>
<p>somewhere in the text.</p>
<p>It&rsquo;d be interesting to try to read his work</p>
<p>to see if Superman was actually an AGI system.</p>
<p>Like if he had inklings of that kind of thinking.</p>
<p>He didn&rsquo;t.</p>
<p>No, I would say not.</p>
<p>I mean, he had a lot of inklings of modern cognitive science,</p>
<p>which are very interesting.</p>
<p>If you look in like the third part of the collection</p>
<p>that&rsquo;s been titled The Will to Power.</p>
<p>I mean, in book three there,</p>
<p>there&rsquo;s very deep analysis of thinking processes,</p>
<p>but he wasn&rsquo;t so much of a physical tinkerer type guy,</p>
<p>right? He was very abstract.</p>
<p>Do you think, what do you think about the will to power?</p>
<p>Do you think human, what do you think drives humans?</p>
<p>Is it?</p>
<p>Oh, an unholy mix of things.</p>
<p>I don&rsquo;t think there&rsquo;s one pure, simple,</p>
<p>and elegant objective function driving humans by any means.</p>
<p>What do you think, if we look at,</p>
<p>I know it&rsquo;s hard to look at humans in an aggregate,</p>
<p>but do you think overall humans are good?</p>
<p>Or do we have both good and evil within us</p>
<p>that depending on the circumstances,</p>
<p>depending on whatever can percolate to the top?</p>
<p>Good and evil are very ambiguous, complicated</p>
<p>and in some ways silly concepts.</p>
<p>But if we could dig into your question</p>
<p>from a couple of directions.</p>
<p>So I think if you look in evolution,</p>
<p>humanity is shaped both by individual selection</p>
<p>and what biologists would call group selection,</p>
<p>like tribe level selection, right?</p>
<p>So individual selection has driven us</p>
<p>in a selfish DNA sort of way.</p>
<p>So that each of us does to a certain approximation</p>
<p>what will help us propagate our DNA to future generations.</p>
<p>I mean, that&rsquo;s why I&rsquo;ve got four kids so far</p>
<p>and probably that&rsquo;s not the last one.</p>
<p>On the other hand.</p>
<p>I like the ambition.</p>
<p>Tribal, like group selection means humans in a way</p>
<p>will do what will advocate for the persistence of the DNA</p>
<p>of their whole tribe or their social group.</p>
<p>And in biology, you have both of these, right?</p>
<p>And you can see, say an ant colony or a beehive,</p>
<p>there&rsquo;s a lot of group selection</p>
<p>in the evolution of those social animals.</p>
<p>On the other hand, say a big cat</p>
<p>or some very solitary animal,</p>
<p>it&rsquo;s a lot more biased toward individual selection.</p>
<p>Humans are an interesting balance.</p>
<p>And I think this reflects itself</p>
<p>in what we would view as selfishness versus altruism</p>
<p>to some extent.</p>
<p>So we just have both of those objective functions</p>
<p>contributing to the makeup of our brains.</p>
<p>And then as Nietzsche analyzed in his own way</p>
<p>and others have analyzed in different ways,</p>
<p>I mean, we abstract this as well,</p>
<p>we have both good and evil within us, right?</p>
<p>Because a lot of what we view as evil</p>
<p>is really just selfishness.</p>
<p>A lot of what we view as good is altruism,</p>
<p>which means doing what&rsquo;s good for the tribe.</p>
<p>And on that level,</p>
<p>we have both of those just baked into us</p>
<p>and that&rsquo;s how it is.</p>
<p>Of course, there are psychopaths and sociopaths</p>
<p>and people who get gratified by the suffering of others.</p>
<p>And that&rsquo;s a different thing.</p>
<p>Yeah, those are exceptions on the whole.</p>
<p>But I think at core, we&rsquo;re not purely selfish,</p>
<p>we&rsquo;re not purely altruistic, we are a mix</p>
<p>and that&rsquo;s the nature of it.</p>
<p>And we also have a complex constellation of values</p>
<p>that are just very specific to our evolutionary history.</p>
<p>Like we love waterways and mountains</p>
<p>and the ideal place to put a house</p>
<p>is in a mountain overlooking the water, right?</p>
<p>And we care a lot about our kids</p>
<p>and we care a little less about our cousins</p>
<p>and even less about our fifth cousins.</p>
<p>I mean, there are many particularities to human values,</p>
<p>which whether they&rsquo;re good or evil</p>
<p>depends on your perspective.</p>
<p>Say, I spent a lot of time in Ethiopia in Addis Ababa</p>
<p>where we have one of our AI development offices</p>
<p>for my SingularityNet project.</p>
<p>And when I walk through the streets in Addis,</p>
<p>you know, there&rsquo;s people lying by the side of the road,</p>
<p>like just living there by the side of the road,</p>
<p>dying probably of curable diseases</p>
<p>without enough food or medicine.</p>
<p>And when I walk by them, you know, I feel terrible,</p>
<p>I give them money.</p>
<p>When I come back home to the developed world,</p>
<p>they&rsquo;re not on my mind that much.</p>
<p>I do donate some, but I mean,</p>
<p>I also spend some of the limited money I have</p>
<p>enjoying myself in frivolous ways</p>
<p>rather than donating it to those people who are right now,</p>
<p>like starving, dying and suffering on the roadside.</p>
<p>So does that make me evil?</p>
<p>I mean, it makes me somewhat selfish</p>
<p>and somewhat altruistic.</p>
<p>And we each balance that in our own way, right?</p>
<p>So whether that will be true of all possible AGI&rsquo;s</p>
<p>is a subtler question.</p>
<p>So that&rsquo;s how humans are.</p>
<p>So you have a sense, you kind of mentioned</p>
<p>that there&rsquo;s a selfish,</p>
<p>I&rsquo;m not gonna bring up the whole Ayn Rand idea</p>
<p>of selfishness being the core virtue.</p>
<p>That&rsquo;s a whole interesting kind of tangent</p>
<p>that I think we&rsquo;ll just distract ourselves on.</p>
<p>I have to make one amusing comment.</p>
<p>Sure.</p>
<p>A comment that has amused me anyway.</p>
<p>So the, yeah, I have extraordinary negative respect</p>
<p>for Ayn Rand.</p>
<p>Negative, what&rsquo;s a negative respect?</p>
<p>But when I worked with a company called Genescient,</p>
<p>which was evolving flies to have extraordinary long lives</p>
<p>in Southern California.</p>
<p>So we had flies that were evolved by artificial selection</p>
<p>to have five times the lifespan of normal fruit flies.</p>
<p>But the population of super long lived flies</p>
<p>was physically sitting in a spare room</p>
<p>at an Ayn Rand elementary school in Southern California.</p>
<p>So that was just like,</p>
<p>well, if I saw this in a movie, I wouldn&rsquo;t believe it.</p>
<p>Well, yeah, the universe has a sense of humor</p>
<p>in that kind of way.</p>
<p>That fits in, humor fits in somehow</p>
<p>into this whole absurd existence.</p>
<p>But you mentioned the balance between selfishness</p>
<p>and altruism as kind of being innate.</p>
<p>Do you think it&rsquo;s possible</p>
<p>that&rsquo;s kind of an emergent phenomena,</p>
<p>those peculiarities of our value system?</p>
<p>How much of it is innate?</p>
<p>How much of it is something we collectively</p>
<p>kind of like a Dostoevsky novel</p>
<p>bring to life together as a civilization?</p>
<p>I mean, the answer to nature versus nurture</p>
<p>is usually both.</p>
<p>And of course it&rsquo;s nature versus nurture</p>
<p>versus self organization, as you mentioned.</p>
<p>So clearly there are evolutionary roots</p>
<p>to individual and group selection</p>
<p>leading to a mix of selfishness and altruism.</p>
<p>On the other hand,</p>
<p>different cultures manifest that in different ways.</p>
<p>Well, we all have basically the same biology.</p>
<p>And if you look at sort of precivilized cultures,</p>
<p>you have tribes like the Yanomamo in Venezuela,</p>
<p>which their culture is focused on killing other tribes.</p>
<p>And you have other Stone Age tribes</p>
<p>that are mostly peaceful and have big taboos</p>
<p>against violence.</p>
<p>So you can certainly have a big difference</p>
<p>in how culture manifests</p>
<p>these innate biological characteristics,</p>
<p>but still, there&rsquo;s probably limits</p>
<p>that are given by our biology.</p>
<p>I used to argue this with my great grandparents</p>
<p>who were Marxists actually,</p>
<p>because they believed in the withering away of the state.</p>
<p>Like they believe that,</p>
<p>as you move from capitalism to socialism to communism,</p>
<p>people would just become more social minded</p>
<p>so that a state would be unnecessary</p>
<p>and everyone would give everyone else what they needed.</p>
<p>Now, setting aside that</p>
<p>that&rsquo;s not what the various Marxist experiments</p>
<p>on the planet seem to be heading toward in practice.</p>
<p>Just as a theoretical point,</p>
<p>I was very dubious that human nature could go there.</p>
<p>Like at that time when my great grandparents are alive,</p>
<p>I was just like, you know, I&rsquo;m a cynical teenager.</p>
<p>I think humans are just jerks.</p>
<p>The state is not gonna wither away.</p>
<p>If you don&rsquo;t have some structure</p>
<p>keeping people from screwing each other over,</p>
<p>they&rsquo;re gonna do it.</p>
<p>So now I actually don&rsquo;t quite see things that way.</p>
<p>I mean, I think my feeling now subjectively</p>
<p>is the culture aspect is more significant</p>
<p>than I thought it was when I was a teenager.</p>
<p>And I think you could have a human society</p>
<p>that was dialed dramatically further toward,</p>
<p>you know, self awareness, other awareness,</p>
<p>compassion and sharing than our current society.</p>
<p>And of course, greater material abundance helps,</p>
<p>but to some extent material abundance</p>
<p>is a subjective perception also</p>
<p>because many Stone Age cultures perceive themselves</p>
<p>as living in great material abundance</p>
<p>that they had all the food and water they wanted,</p>
<p>they lived in a beautiful place,</p>
<p>that they had sex lives, that they had children.</p>
<p>I mean, they had abundance without any factories, right?</p>
<p>So I think humanity probably would be capable</p>
<p>of fundamentally more positive and joy filled mode</p>
<p>of social existence than what we have now.</p>
<p>Clearly Marx didn&rsquo;t quite have the right idea</p>
<p>about how to get there.</p>
<p>I mean, he missed a number of key aspects</p>
<p>of human society and its evolution.</p>
<p>And if we look at where we are in society now,</p>
<p>how to get there is a quite different question</p>
<p>because there are very powerful forces</p>
<p>pushing people in different directions</p>
<p>than a positive, joyous, compassionate existence, right?</p>
<p>So if we were tried to, you know,</p>
<p>Elon Musk is dreams of colonizing Mars at the moment,</p>
<p>so we maybe will have a chance to start a new civilization</p>
<p>with a new governmental system.</p>
<p>And certainly there&rsquo;s quite a bit of chaos.</p>
<p>We&rsquo;re sitting now, I don&rsquo;t know what the date is,</p>
<p>but this is June.</p>
<p>There&rsquo;s quite a bit of chaos in all different forms</p>
<p>going on in the United States and all over the world.</p>
<p>So there&rsquo;s a hunger for new types of governments,</p>
<p>new types of leadership, new types of systems.</p>
<p>And so what are the forces at play</p>
<p>and how do we move forward?</p>
<p>Yeah, I mean, colonizing Mars, first of all,</p>
<p>it&rsquo;s a super cool thing to do.</p>
<p>We should be doing it.</p>
<p>So you love the idea.</p>
<p>Yeah, I mean, it&rsquo;s more important than making</p>
<p>chocolatey or chocolates and sexier lingerie</p>
<p>and many of the things that we spend</p>
<p>a lot more resources on as a species, right?</p>
<p>So I mean, we certainly should do it.</p>
<p>I think the possible futures in which a Mars colony</p>
<p>makes a critical difference for humanity are very few.</p>
<p>I mean, I think, I mean, assuming we make a Mars colony</p>
<p>and people go live there in a couple of decades,</p>
<p>I mean, their supplies are gonna come from Earth.</p>
<p>The money to make the colony came from Earth</p>
<p>and whatever powers are supplying the goods there</p>
<p>from Earth are gonna, in effect, be in control</p>
<p>of that Mars colony.</p>
<p>Of course, there are outlier situations</p>
<p>where Earth gets nuked into oblivion</p>
<p>and somehow Mars has been made self sustaining by that point</p>
<p>and then Mars is what allows humanity to persist.</p>
<p>But I think that those are very, very, very unlikely.</p>
<p>You don&rsquo;t think it could be a first step on a long journey?</p>
<p>Of course it&rsquo;s a first step on a long journey,</p>
<p>which is awesome.</p>
<p>I&rsquo;m guessing the colonization of the rest</p>
<p>of the physical universe will probably be done</p>
<p>by AGI&rsquo;s that are better designed to live in space</p>
<p>than by the meat machines that we are.</p>
<p>But I mean, who knows?</p>
<p>We may cryopreserve ourselves in some superior way</p>
<p>to what we know now and like shoot ourselves out</p>
<p>to Alpha Centauri and beyond.</p>
<p>I mean, that&rsquo;s all cool.</p>
<p>It&rsquo;s very interesting and it&rsquo;s much more valuable</p>
<p>than most things that humanity is spending its resources on.</p>
<p>On the other hand, with AGI, we can get to a singularity</p>
<p>before the Mars colony becomes sustaining for sure,</p>
<p>possibly before it&rsquo;s even operational.</p>
<p>So your intuition is that that&rsquo;s the problem</p>
<p>if we really invest resources and we can get to faster</p>
<p>than a legitimate full self sustaining colonization of Mars.</p>
<p>Yeah, and it&rsquo;s very clear that we will to me</p>
<p>because there&rsquo;s so much economic value</p>
<p>in getting from narrow AI toward AGI,</p>
<p>whereas the Mars colony, there&rsquo;s less economic value</p>
<p>until you get quite far out into the future.</p>
<p>So I think that&rsquo;s very interesting.</p>
<p>I just think it&rsquo;s somewhat off to the side.</p>
<p>I mean, just as I think, say, art and music</p>
<p>are very, very interesting and I wanna see resources</p>
<p>go into amazing art and music being created.</p>
<p>And I&rsquo;d rather see that than a lot of the garbage</p>
<p>that the society spends their money on.</p>
<p>On the other hand, I don&rsquo;t think Mars colonization</p>
<p>or inventing amazing new genres of music</p>
<p>is not one of the things that is most likely</p>
<p>to make a critical difference in the evolution</p>
<p>of human or nonhuman life in this part of the universe</p>
<p>over the next decade.</p>
<p>Do you think AGI is really?</p>
<p>AGI is by far the most important thing</p>
<p>that&rsquo;s on the horizon.</p>
<p>And then technologies that have direct ability</p>
<p>to enable AGI or to accelerate AGI are also very important.</p>
<p>For example, say, quantum computing.</p>
<p>I don&rsquo;t think that&rsquo;s critical to achieve AGI,</p>
<p>but certainly you could see how</p>
<p>the right quantum computing architecture</p>
<p>could massively accelerate AGI,</p>
<p>similar other types of nanotechnology.</p>
<p>Right now, the quest to cure aging and end disease</p>
<p>while not in the big picture as important as AGI,</p>
<p>of course, it&rsquo;s important to all of us as individual humans.</p>
<p>And if someone made a super longevity pill</p>
<p>and distributed it tomorrow, I mean,</p>
<p>that would be huge and a much larger impact</p>
<p>than a Mars colony is gonna have for quite some time.</p>
<p>But perhaps not as much as an AGI system.</p>
<p>No, because if you can make a benevolent AGI,</p>
<p>then all the other problems are solved.</p>
<p>I mean, if then the AGI can be,</p>
<p>once it&rsquo;s as generally intelligent as humans,</p>
<p>it can rapidly become massively more generally intelligent</p>
<p>than humans.</p>
<p>And then that AGI should be able to solve science</p>
<p>and engineering problems much better than human beings,</p>
<p>as long as it is in fact motivated to do so.</p>
<p>That&rsquo;s why I said a benevolent AGI.</p>
<p>There could be other kinds.</p>
<p>Maybe it&rsquo;s good to step back a little bit.</p>
<p>I mean, we&rsquo;ve been using the term AGI.</p>
<p>People often cite you as the creator,</p>
<p>or at least the popularizer of the term AGI,</p>
<p>artificial general intelligence.</p>
<p>Can you tell the origin story of the term maybe?</p>
<p>So yeah, I would say I launched the term AGI upon the world</p>
<p>for what it&rsquo;s worth without ever fully being in love</p>
<p>with the term.</p>
<p>What happened is I was editing a book,</p>
<p>and this process started around 2001 or two.</p>
<p>I think the book came out 2005, finally.</p>
<p>I was editing a book which I provisionally</p>
<p>was titling Real AI.</p>
<p>And I mean, the goal was to gather together</p>
<p>fairly serious academicish papers</p>
<p>on the topic of making thinking machines</p>
<p>that could really think in the sense like people can,</p>
<p>or even more broadly than people can, right?</p>
<p>So then I was reaching out to other folks</p>
<p>that I had encountered here or there</p>
<p>who were interested in that,</p>
<p>which included some other folks who I knew</p>
<p>from the transhumist and singularitarian world,</p>
<p>like Peter Vos, who has a company, AGI Incorporated,</p>
<p>still in California, and included Shane Legge,</p>
<p>who had worked for me at my company, WebMind,</p>
<p>in New York in the late 90s,</p>
<p>who by now has become rich and famous.</p>
<p>He was one of the cofounders of Google DeepMind.</p>
<p>But at that time, Shane was,</p>
<p>I think he may have just started doing his PhD</p>
<p>with Marcus Hooter, who at that time</p>
<p>hadn&rsquo;t yet published his book, Universal AI,</p>
<p>which sort of gives a mathematical foundation</p>
<p>for artificial general intelligence.</p>
<p>So I reached out to Shane and Marcus and Peter Vos</p>
<p>and Pei Wang, who was another former employee of mine</p>
<p>who had been Douglas Hofstadter&rsquo;s PhD student</p>
<p>who had his own approach to AGI,</p>
<p>and a bunch of some Russian folks reached out to these guys</p>
<p>and they contributed papers for the book.</p>
<p>But that was my provisional title, but I never loved it</p>
<p>because in the end, I was doing some,</p>
<p>what we would now call narrow AI as well,</p>
<p>like applying machine learning to genomics data</p>
<p>or chat data for sentiment analysis.</p>
<p>I mean, that work is real.</p>
<p>And in a sense, it&rsquo;s really AI.</p>
<p>It&rsquo;s just a different kind of AI.</p>
<p>Ray Kurzweil wrote about narrow AI versus strong AI,</p>
<p>but that seemed weird to me because first of all,</p>
<p>narrow and strong are not antennas.</p>
<p>That&rsquo;s right.</p>
<p>But secondly, strong AI was used</p>
<p>in the cognitive science literature</p>
<p>to mean the hypothesis that digital computer AIs</p>
<p>could have true consciousness like human beings.</p>
<p>So there was already a meaning to strong AI,</p>
<p>which was complexly different, but related, right?</p>
<p>So we were tossing around on an email list</p>
<p>whether what title it should be.</p>
<p>And so we talked about narrow AI, broad AI, wide AI,</p>
<p>narrow AI, general AI.</p>
<p>And I think it was either Shane Legge or Peter Vos</p>
<p>on the private email discussion we had.</p>
<p>He said, but why don&rsquo;t we go</p>
<p>with AGI, artificial general intelligence?</p>
<p>And Pei Wang wanted to do GAI,</p>
<p>general artificial intelligence,</p>
<p>because in Chinese it goes in that order.</p>
<p>But we figured gay wouldn&rsquo;t work</p>
<p>in US culture at that time, right?</p>
<p>So we went with the AGI.</p>
<p>We used it for the title of that book.</p>
<p>And part of Peter and Shane&rsquo;s reasoning</p>
<p>was you have the G factor in psychology,</p>
<p>which is IQ, general intelligence, right?</p>
<p>So you have a meaning of GI, general intelligence,</p>
<p>in psychology, so then you&rsquo;re looking like artificial GI.</p>
<p>So then we use that for the title of the book.</p>
<p>And so I think maybe both Shane and Peter</p>
<p>think they invented the term,</p>
<p>but then later after the book was published,</p>
<p>this guy, Mark Guberd, came up to me and he&rsquo;s like,</p>
<p>well, I published an essay with the term AGI</p>
<p>in like 1997 or something.</p>
<p>And so I&rsquo;m just waiting for some Russian to come out</p>
<p>and say they published that in 1953, right?</p>
<p>I mean, that term is not dramatically innovative</p>
<p>or anything.</p>
<p>It&rsquo;s one of these obvious in hindsight things,</p>
<p>which is also annoying in a way,</p>
<p>because Joshua Bach, who you interviewed,</p>
<p>is a close friend of mine.</p>
<p>He likes the term synthetic intelligence,</p>
<p>which I like much better,</p>
<p>but it hasn&rsquo;t actually caught on, right?</p>
<p>Because I mean, artificial is a bit off to me</p>
<p>because artifice is like a tool or something,</p>
<p>but not all AGI&rsquo;s are gonna be tools.</p>
<p>I mean, they may be now,</p>
<p>but we&rsquo;re aiming toward making them agents</p>
<p>rather than tools.</p>
<p>And in a way, I don&rsquo;t like the distinction</p>
<p>between artificial and natural,</p>
<p>because I mean, we&rsquo;re part of nature also</p>
<p>and machines are part of nature.</p>
<p>I mean, you can look at evolved versus engineered,</p>
<p>but that&rsquo;s a different distinction.</p>
<p>Then it should be engineered general intelligence, right?</p>
<p>And then general, well,</p>
<p>if you look at Marcus Hooter&rsquo;s book,</p>
<p>universally, what he argues there is,</p>
<p>within the domain of computation theory,</p>
<p>which is limited, but interesting.</p>
<p>So if you assume computable environments</p>
<p>or computable reward functions,</p>
<p>then he articulates what would be</p>
<p>a truly general intelligence,</p>
<p>a system called AIXI, which is quite beautiful.</p>
<p>AIXI, and that&rsquo;s the middle name</p>
<p>of my latest child, actually, is it?</p>
<p>What&rsquo;s the first name?</p>
<p>First name is QORXI, Q O R X I,</p>
<p>which my wife came up with,</p>
<p>but that&rsquo;s an acronym for quantum organized rational</p>
<p>expanding intelligence, and his middle name is Xiphonies,</p>
<p>actually, which means the former principal underlying AIXI.</p>
<p>But in any case.</p>
<p>You&rsquo;re giving Elon Musk&rsquo;s new child a run for his money.</p>
<p>Well, I did it first.</p>
<p>He copied me with this new freakish name,</p>
<p>but now if I have another baby,</p>
<p>I&rsquo;m gonna have to outdo him.</p>
<p>It&rsquo;s becoming an arms race of weird, geeky baby names.</p>
<p>We&rsquo;ll see what the babies think about it, right?</p>
<p>But I mean, my oldest son, Zarathustra, loves his name,</p>
<p>and my daughter, Sharazad, loves her name.</p>
<p>So far, basically, if you give your kids weird names.</p>
<p>They live up to it.</p>
<p>Well, you&rsquo;re obliged to make the kids weird enough</p>
<p>that they like the names, right?</p>
<p>It directs their upbringing in a certain way.</p>
<p>But yeah, anyway, I mean, what Marcus showed in that book</p>
<p>is that a truly general intelligence</p>
<p>theoretically is possible,</p>
<p>but would take infinite computing power.</p>
<p>So then the artificial is a little off.</p>
<p>The general is not really achievable within physics</p>
<p>as we know it.</p>
<p>And I mean, physics as we know it may be limited,</p>
<p>but that&rsquo;s what we have to work with now.</p>
<p>Intelligence.</p>
<p>Infinitely general, you mean,</p>
<p>like information processing perspective, yeah.</p>
<p>Yeah, intelligence is not very well defined either, right?</p>
<p>I mean, what does it mean?</p>
<p>I mean, in AI now, it&rsquo;s fashionable to look at it</p>
<p>as maximizing an expected reward over the future.</p>
<p>But that sort of definition is pathological in various ways.</p>
<p>And my friend David Weinbaum, AKA Weaver,</p>
<p>he had a beautiful PhD thesis on open ended intelligence,</p>
<p>trying to conceive intelligence in a&hellip;</p>
<p>Without a reward.</p>
<p>Yeah, he&rsquo;s just looking at it differently.</p>
<p>He&rsquo;s looking at complex self organizing systems</p>
<p>and looking at an intelligent system</p>
<p>as being one that revises and grows</p>
<p>and improves itself in conjunction with its environment</p>
<p>without necessarily there being one objective function</p>
<p>it&rsquo;s trying to maximize.</p>
<p>Although over certain intervals of time,</p>
<p>it may act as if it&rsquo;s optimizing</p>
<p>a certain objective function.</p>
<p>Very much Solaris from Stanislav Lem&rsquo;s novels, right?</p>
<p>So yeah, the point is artificial, general and intelligence.</p>
<p>Don&rsquo;t work.</p>
<p>They&rsquo;re all bad.</p>
<p>On the other hand, everyone knows what AI is.</p>
<p>And AGI seems immediately comprehensible</p>
<p>to people with a technical background.</p>
<p>So I think that the term has served</p>
<p>as sociological function.</p>
<p>And now it&rsquo;s out there everywhere, which baffles me.</p>
<p>It&rsquo;s like KFC.</p>
<p>I mean, that&rsquo;s it.</p>
<p>We&rsquo;re stuck with AGI probably for a very long time</p>
<p>until AGI systems take over and rename themselves.</p>
<p>Yeah.</p>
<p>And then we&rsquo;ll be biological.</p>
<p>We&rsquo;re stuck with GPUs too,</p>
<p>which mostly have nothing to do with graphics.</p>
<p>Any more, right?</p>
<p>I wonder what the AGI system will call us humans.</p>
<p>That was maybe.</p>
<p>Grandpa.</p>
<p>Yeah.</p>
<p>GPs.</p>
<p>Yeah.</p>
<p>Grandpa processing unit, yeah.</p>
<p>Biological grandpa processing units.</p>
<p>Yeah.</p>
<p>Okay, so maybe also just a comment on AGI representing</p>
<p>before even the term existed,</p>
<p>representing a kind of community.</p>
<p>You&rsquo;ve talked about this in the past,</p>
<p>sort of AI is coming in waves,</p>
<p>but there&rsquo;s always been this community of people</p>
<p>who dream about creating general human level</p>
<p>super intelligence systems.</p>
<p>Can you maybe give your sense of the history</p>
<p>of this community as it exists today,</p>
<p>as it existed before this deep learning revolution</p>
<p>all throughout the winters and the summers of AI?</p>
<p>Sure.</p>
<p>First, I would say as a side point,</p>
<p>the winters and summers of AI are greatly exaggerated</p>
<p>by Americans and in that,</p>
<p>if you look at the publication record</p>
<p>of the artificial intelligence community</p>
<p>since say the 1950s,</p>
<p>you would find a pretty steady growth</p>
<p>in advance of ideas and papers.</p>
<p>And what&rsquo;s thought of as an AI winter or summer</p>
<p>was sort of how much money is the US military</p>
<p>pumping into AI, which was meaningful.</p>
<p>On the other hand, there was AI going on in Germany,</p>
<p>UK and in Japan and in Russia, all over the place,</p>
<p>while US military got more and less enthused about AI.</p>
<p>So, I mean.</p>
<p>That happened to be, just for people who don&rsquo;t know,</p>
<p>the US military happened to be the main source</p>
<p>of funding for AI research.</p>
<p>So another way to phrase that is it&rsquo;s up and down</p>
<p>of funding for artificial intelligence research.</p>
<p>And I would say the correlation between funding</p>
<p>and intellectual advance was not 100%, right?</p>
<p>Because I mean, in Russia, as an example, or in Germany,</p>
<p>there was less dollar funding than in the US,</p>
<p>but many foundational ideas were laid out,</p>
<p>but it was more theory than implementation, right?</p>
<p>And US really excelled at sort of breaking through</p>
<p>from theoretical papers to working implementations,</p>
<p>which did go up and down somewhat</p>
<p>with US military funding,</p>
<p>but still, I mean, you can look in the 1980s,</p>
<p>Dietrich Derner in Germany had self driving cars</p>
<p>on the Autobahn, right?</p>
<p>And I mean, it was a little early</p>
<p>with regard to the car industry,</p>
<p>so it didn&rsquo;t catch on such as has happened now.</p>
<p>But I mean, that whole advancement</p>
<p>of self driving car technology in Germany</p>
<p>was pretty much independent of AI military summers</p>
<p>and winters in the US.</p>
<p>So there&rsquo;s been more going on in AI globally</p>
<p>than not only most people on the planet realize,</p>
<p>but then most new AI PhDs realize</p>
<p>because they&rsquo;ve come up within a certain sub field of AI</p>
<p>and haven&rsquo;t had to look so much beyond that.</p>
<p>But I would say when I got my PhD in 1989 in mathematics,</p>
<p>I was interested in AI already.</p>
<p>In Philadelphia.</p>
<p>Yeah, I started at NYU, then I transferred to Philadelphia</p>
<p>to Temple University, good old North Philly.</p>
<p>North Philly.</p>
<p>Yeah, yeah, yeah, the pearl of the US.</p>
<p>You never stopped at a red light then</p>
<p>because you were afraid if you stopped at a red light,</p>
<p>someone will carjack you.</p>
<p>So you just drive through every red light.</p>
<p>Yeah.</p>
<p>Every day driving or bicycling to Temple from my house</p>
<p>was like a new adventure.</p>
<p>But yeah, the reason I didn&rsquo;t do a PhD in AI</p>
<p>was what people were doing in the academic AI field then,</p>
<p>was just astoundingly boring and seemed wrong headed to me.</p>
<p>It was really like rule based expert systems</p>
<p>and production systems.</p>
<p>And actually I loved mathematical logic.</p>
<p>I had nothing against logic as the cognitive engine for an AI,</p>
<p>but the idea that you could type in the knowledge</p>
<p>that AI would need to think seemed just completely stupid</p>
<p>and wrong headed to me.</p>
<p>I mean, you can use logic if you want,</p>
<p>but somehow the system has got to be&hellip;</p>
<p>Automated.</p>
<p>Learning, right?</p>
<p>It should be learning from experience.</p>
<p>And the AI field then was not interested</p>
<p>in learning from experience.</p>
<p>I mean, some researchers certainly were.</p>
<p>I mean, I remember in mid eighties,</p>
<p>I discovered a book by John Andreas,</p>
<p>which was, it was about a reinforcement learning system</p>
<p>called PURRDASHPUSS, which was an acronym</p>
<p>that I can&rsquo;t even remember what it was for,</p>
<p>but purpose anyway.</p>
<p>But he, I mean, that was a system</p>
<p>that was supposed to be an AGI</p>
<p>and basically by some sort of fancy</p>
<p>like Markov decision process learning,</p>
<p>it was supposed to learn everything</p>
<p>just from the bits coming into it</p>
<p>and learn to maximize its reward</p>
<p>and become intelligent, right?</p>
<p>So that was there in academia back then,</p>
<p>but it was like isolated, scattered, weird people.</p>
<p>But all these isolated, scattered, weird people</p>
<p>in that period, I mean, they laid the intellectual grounds</p>
<p>for what happened later.</p>
<p>So you look at John Andreas at University of Canterbury</p>
<p>with his PURRDASHPUSS reinforcement learning Markov system.</p>
<p>He was the PhD supervisor for John Cleary in New Zealand.</p>
<p>Now, John Cleary worked with me</p>
<p>when I was at Waikato University in 1993 in New Zealand.</p>
<p>And he worked with Ian Whitten there</p>
<p>and they launched WEKA,</p>
<p>which was the first open source machine learning toolkit,</p>
<p>which was launched in, I guess, 93 or 94</p>
<p>when I was at Waikato University.</p>
<p>Written in Java, unfortunately.</p>
<p>Written in Java, which was a cool language back then.</p>
<p>I guess it&rsquo;s still, well, it&rsquo;s not cool anymore,</p>
<p>but it&rsquo;s powerful.</p>
<p>I find, like most programmers now,</p>
<p>I find Java unnecessarily bloated,</p>
<p>but back then it was like Java or C++ basically.</p>
<p>And Java was easier for students.</p>
<p>Amusingly, a lot of the work on WEKA</p>
<p>when we were in New Zealand was funded by a US,</p>
<p>sorry, a New Zealand government grant</p>
<p>to use machine learning</p>
<p>to predict the menstrual cycles of cows.</p>
<p>So in the US, all the grant funding for AI</p>
<p>was about how to kill people or spy on people.</p>
<p>In New Zealand, it&rsquo;s all about cows or kiwi fruits, right?</p>
<p>Yeah.</p>
<p>So yeah, anyway, I mean, John Andreas</p>
<p>had his probability theory based reinforcement learning,</p>
<p>proto AGI.</p>
<p>John Cleary was trying to do much more ambitious,</p>
<p>probabilistic AGI systems.</p>
<p>Now, John Cleary helped do WEKA,</p>
<p>which is the first open source machine learning toolkit.</p>
<p>So the predecessor for TensorFlow and Torch</p>
<p>and all these things.</p>
<p>Also, Shane Legg was at Waikato</p>
<p>working with John Cleary and Ian Witten</p>
<p>and this whole group.</p>
<p>And then working with my own companies,</p>
<p>my company, WebMind, an AI company I had in the late 90s</p>
<p>with a team there at Waikato University,</p>
<p>which is how Shane got his head full of AGI,</p>
<p>which led him to go on</p>
<p>and with Demis Hassabis found DeepMind.</p>
<p>So what you can see through that lineage is,</p>
<p>you know, in the 80s and 70s,</p>
<p>John Andreas was trying to build probabilistic</p>
<p>reinforcement learning AGI systems.</p>
<p>The technology, the computers just weren&rsquo;t there to support</p>
<p>his ideas were very similar to what people are doing now.</p>
<p>But, you know, although he&rsquo;s long since passed away</p>
<p>and didn&rsquo;t become that famous outside of Canterbury,</p>
<p>I mean, the lineage of ideas passed on from him</p>
<p>to his students, to their students,</p>
<p>you can go trace directly from there to me</p>
<p>and to DeepMind, right?</p>
<p>So that there was a lot going on in AGI</p>
<p>that did ultimately lay the groundwork</p>
<p>for what we have today, but there wasn&rsquo;t a community, right?</p>
<p>And so when I started trying to pull together</p>
<p>an AGI community, it was in the, I guess,</p>
<p>the early aughts when I was living in Washington, D.C.</p>
<p>and making a living doing AI consulting</p>
<p>for various U.S. government agencies.</p>
<p>And I organized the first AGI workshop in 2006.</p>
<p>And I mean, it wasn&rsquo;t like it was literally</p>
<p>in my basement or something.</p>
<p>I mean, it was in the conference room at the Marriott</p>
<p>in Bethesda, it&rsquo;s not that edgy or underground,</p>
<p>unfortunately, but still.</p>
<p>How many people attended?</p>
<p>About 60 or something.</p>
<p>That&rsquo;s not bad.</p>
<p>I mean, D.C. has a lot of AI going on,</p>
<p>probably until the last five or 10 years,</p>
<p>much more than Silicon Valley, although it&rsquo;s just quiet</p>
<p>because of the nature of what happens in D.C.</p>
<p>Their business isn&rsquo;t driven by PR.</p>
<p>Mostly when something starts to work really well,</p>
<p>it&rsquo;s taken black and becomes even more quiet, right?</p>
<p>But yeah, the thing is that really had the feeling</p>
<p>of a group of starry eyed mavericks huddled in a basement,</p>
<p>like plotting how to overthrow the narrow AI establishment.</p>
<p>And for the first time, in some cases,</p>
<p>coming together with others who shared their passion</p>
<p>for AGI and the technical seriousness about working on it.</p>
<p>And that&rsquo;s very, very different than what we have today.</p>
<p>I mean, now it&rsquo;s a little bit different.</p>
<p>We have AGI conference every year</p>
<p>and there&rsquo;s several hundred people rather than 50.</p>
<p>Now it&rsquo;s more like this is the main gathering</p>
<p>of people who want to achieve AGI</p>
<p>and think that large scale nonlinear regression</p>
<p>is not the golden path to AGI.</p>
<p>So I mean it&rsquo;s&hellip;</p>
<p>AKA neural networks.</p>
<p>Yeah, yeah, yeah.</p>
<p>Well, certain architectures for learning using neural networks.</p>
<p>So yeah, the AGI conferences are sort of now</p>
<p>the main concentration of people not obsessed</p>
<p>with deep neural nets and deep reinforcement learning,</p>
<p>but still interested in AGI, not the only ones.</p>
<p>I mean, there&rsquo;s other little conferences and groupings</p>
<p>interested in human level AI</p>
<p>and cognitive architectures and so forth.</p>
<p>But yeah, it&rsquo;s been a big shift.</p>
<p>Like back then, you couldn&rsquo;t really&hellip;</p>
<p>It&rsquo;ll be very, very edgy then</p>
<p>to give a university department seminar</p>
<p>that mentioned AGI or human level AI.</p>
<p>It was more like you had to talk about</p>
<p>something more short term and immediately practical</p>
<p>than in the bar after the seminar,</p>
<p>you could bullshit about AGI in the same breath</p>
<p>as time travel or the simulation hypothesis or something.</p>
<p>Whereas now, AGI is not only in the academic seminar room,</p>
<p>like you have Vladimir Putin knows what AGI is.</p>
<p>And he&rsquo;s like, Russia needs to become the leader in AGI.</p>
<p>So national leaders and CEOs of large corporations.</p>
<p>I mean, the CTO of Intel, Justin Ratner,</p>
<p>this was years ago, Singularity Summit Conference,</p>
<p>2008 or something.</p>
<p>He&rsquo;s like, we believe Ray Kurzweil,</p>
<p>the singularity will happen in 2045</p>
<p>and it will have Intel inside.</p>
<p>So, I mean, it&rsquo;s gone from being something</p>
<p>which is the pursuit of like crazed mavericks,</p>
<p>crackpots and science fiction fanatics</p>
<p>to being a marketing term for large corporations</p>
<p>and the national leaders,</p>
<p>which is a astounding transition.</p>
<p>But yeah, in the course of this transition,</p>
<p>I think a bunch of sub communities have formed</p>
<p>and the community around the AGI conference series</p>
<p>is certainly one of them.</p>
<p>It hasn&rsquo;t grown as big as I might&rsquo;ve liked it to.</p>
<p>On the other hand, sometimes a modest size community</p>
<p>can be better for making intellectual progress also.</p>
<p>Like you go to a society for neuroscience conference,</p>
<p>you have 35 or 40,000 neuroscientists.</p>
<p>On the one hand, it&rsquo;s amazing.</p>
<p>On the other hand, you&rsquo;re not gonna talk to the leaders</p>
<p>of the field there if you&rsquo;re an outsider.</p>
<p>Yeah, in the same sense, the AAAI,</p>
<p>the artificial intelligence,</p>
<p>the main kind of generic artificial intelligence</p>
<p>conference is too big.</p>
<p>It&rsquo;s too amorphous.</p>
<p>Like it doesn&rsquo;t make sense.</p>
<p>Well, yeah, and NIPS has become a company advertising outlet</p>
<p>in the whole of it.</p>
<p>So, I mean, to comment on the role of AGI</p>
<p>in the research community, I&rsquo;d still,</p>
<p>if you look at NeurIPS, if you look at CVPR,</p>
<p>if you look at these iClear,</p>
<p>AGI is still seen as the outcast.</p>
<p>I would say in these main machine learning,</p>
<p>in these main artificial intelligence conferences</p>
<p>amongst the researchers,</p>
<p>I don&rsquo;t know if it&rsquo;s an accepted term yet.</p>
<p>What I&rsquo;ve seen bravely, you mentioned Shane Legg&rsquo;s</p>
<p>DeepMind and then OpenAI are the two places that are,</p>
<p>I would say unapologetically so far,</p>
<p>I think it&rsquo;s actually changing unfortunately,</p>
<p>but so far they&rsquo;ve been pushing the idea</p>
<p>that the goal is to create an AGI.</p>
<p>Well, they have billions of dollars behind them.</p>
<p>So, I mean, they&rsquo;re in the public mind</p>
<p>that certainly carries some oomph, right?</p>
<p>I mean, I mean.</p>
<p>But they also have really strong researchers, right?</p>
<p>They do, they&rsquo;re great teams.</p>
<p>I mean, DeepMind in particular, yeah.</p>
<p>And they have, I mean, DeepMind has Marcus Hutter</p>
<p>walking around.</p>
<p>I mean, there&rsquo;s all these folks who basically</p>
<p>their full time position involves dreaming</p>
<p>about creating AGI.</p>
<p>I mean, Google Brain has a lot of amazing</p>
<p>AGI oriented people also.</p>
<p>And I mean, so I&rsquo;d say from a public marketing view,</p>
<p>DeepMind and OpenAI are the two large well funded</p>
<p>organizations that have put the term and concept AGI</p>
<p>out there sort of as part of their public image.</p>
<p>But I mean, they&rsquo;re certainly not,</p>
<p>there are other groups that are doing research</p>
<p>that seems just as AGI is to me.</p>
<p>I mean, including a bunch of groups in Google&rsquo;s</p>
<p>main Mountain View office.</p>
<p>So yeah, it&rsquo;s true.</p>
<p>AGI is somewhat away from the mainstream now.</p>
<p>But if you compare it to where it was 15 years ago,</p>
<p>there&rsquo;s been an amazing mainstreaming.</p>
<p>You could say the same thing about super longevity research,</p>
<p>which is one of my application areas that I&rsquo;m excited about.</p>
<p>I mean, I&rsquo;ve been talking about this since the 90s,</p>
<p>but working on this since 2001.</p>
<p>And back then, really to say,</p>
<p>you&rsquo;re trying to create therapies to allow people</p>
<p>to live hundreds of thousands of years,</p>
<p>you were way, way, way, way out of the industry,</p>
<p>academic mainstream.</p>
<p>But now, Google had Project Calico,</p>
<p>Craig Venter had Human Longevity Incorporated.</p>
<p>And then once the suits come marching in, right?</p>
<p>I mean, once there&rsquo;s big money in it,</p>
<p>then people are forced to take it seriously</p>
<p>because that&rsquo;s the way modern society works.</p>
<p>So it&rsquo;s still not as mainstream as cancer research,</p>
<p>just as AGI is not as mainstream</p>
<p>as automated driving or something.</p>
<p>But the degree of mainstreaming that&rsquo;s happened</p>
<p>in the last 10 to 15 years is astounding</p>
<p>to those of us who&rsquo;ve been at it for a while.</p>
<p>Yeah, but there&rsquo;s a marketing aspect to the term,</p>
<p>but in terms of actual full force research</p>
<p>that&rsquo;s going on under the header of AGI,</p>
<p>it&rsquo;s currently, I would say dominated,</p>
<p>maybe you can disagree,</p>
<p>dominated by neural networks research,</p>
<p>that the nonlinear regression, as you mentioned.</p>
<p>Like what&rsquo;s your sense with OpenCog, with your work,</p>
<p>but in general, I was logic based systems</p>
<p>and expert systems.</p>
<p>For me, always seemed to capture a deep element</p>
<p>of intelligence that needs to be there.</p>
<p>Like you said, it needs to learn,</p>
<p>it needs to be automated somehow,</p>
<p>but that seems to be missing from a lot of research currently.</p>
<p>So what&rsquo;s your sense?</p>
<p>I guess one way to ask this question,</p>
<p>what&rsquo;s your sense of what kind of things</p>
<p>will an AGI system need to have?</p>
<p>Yeah, that&rsquo;s a very interesting topic</p>
<p>that I&rsquo;ve thought about for a long time.</p>
<p>And I think there are many, many different approaches</p>
<p>that can work for getting to human level AI.</p>
<p>So I don&rsquo;t think there&rsquo;s like one golden algorithm,</p>
<p>or one golden design that can work.</p>
<p>And I mean, flying machines is the much worn</p>
<p>analogy here, right?</p>
<p>Like, I mean, you have airplanes, you have helicopters,</p>
<p>you have balloons, you have stealth bombers</p>
<p>that don&rsquo;t look like regular airplanes.</p>
<p>You&rsquo;ve got all blimps.</p>
<p>Birds too.</p>
<p>Birds, yeah, and bugs, right?</p>
<p>Yeah.</p>
<p>And there are certainly many kinds of flying machines that.</p>
<p>And there&rsquo;s a catapult that you can just launch.</p>
<p>And there&rsquo;s bicycle powered like flying machines, right?</p>
<p>Nice, yeah.</p>
<p>Yeah, so now these are all analyzable</p>
<p>by a basic theory of aerodynamics, right?</p>
<p>Now, so one issue with AGI is we don&rsquo;t yet have the analog</p>
<p>of the theory of aerodynamics.</p>
<p>And that&rsquo;s what Marcus Hutter was trying to make</p>
<p>with the AXI and his general theory of general intelligence.</p>
<p>But that theory in its most clearly articulated parts</p>
<p>really only works for either infinitely powerful machines</p>
<p>or almost, or insanely impractically powerful machines.</p>
<p>So I mean, if you were gonna take a theory based approach</p>
<p>to AGI, what you would do is say, well, let&rsquo;s take</p>
<p>what&rsquo;s called say AXE TL, which is Hutter&rsquo;s AXE machine</p>
<p>that can work on merely insanely much processing power</p>
<p>rather than infinitely much.</p>
<p>What does TL stand for?</p>
<p>Time and length.</p>
<p>Okay.</p>
<p>So you&rsquo;re basically how it.</p>
<p>Like constrained somehow.</p>
<p>Yeah, yeah, yeah.</p>
<p>So how AXE works basically is each action</p>
<p>that it wants to take, before taking that action,</p>
<p>it looks at all its history.</p>
<p>And then it looks at all possible programs</p>
<p>that it could use to make a decision.</p>
<p>And it decides like which decision program</p>
<p>would have let it make the best decisions</p>
<p>according to its reward function over its history.</p>
<p>And it uses that decision program</p>
<p>to make the next decision, right?</p>
<p>It&rsquo;s not afraid of infinite resources.</p>
<p>It&rsquo;s searching through the space</p>
<p>of all possible computer programs</p>
<p>in between each action and each next action.</p>
<p>Now, AXE TL searches through all possible computer programs</p>
<p>that have runtime less than T and length less than L.</p>
<p>So it&rsquo;s, which is still an impractically humongous space,</p>
<p>right?</p>
<p>So what you would like to do to make an AGI</p>
<p>and what will probably be done 50 years from now</p>
<p>to make an AGI is say, okay, well, we have some constraints.</p>
<p>We have these processing power constraints</p>
<p>and we have the space and time constraints on the program.</p>
<p>We have energy utilization constraints</p>
<p>and we have this particular class environments,</p>
<p>class of environments that we care about,</p>
<p>which may be say, you know, manipulating physical objects</p>
<p>on the surface of the earth,</p>
<p>communicating in human language.</p>
<p>I mean, whatever our particular, not annihilating humanity,</p>
<p>whatever our particular requirements happen to be.</p>
<p>If you formalize those requirements</p>
<p>in some formal specification language,</p>
<p>you should then be able to run</p>
<p>automated program specializer on AXE TL,</p>
<p>specialize it to the computing resource constraints</p>
<p>and the particular environment and goal.</p>
<p>And then it will spit out like the specialized version</p>
<p>of AXE TL to your resource restrictions</p>
<p>and your environment, which will be your AGI, right?</p>
<p>And that I think is how our super AGI</p>
<p>will create new AGI systems, right?</p>
<p>But that&rsquo;s a very rush.</p>
<p>It seems really inefficient.</p>
<p>It&rsquo;s a very Russian approach by the way,</p>
<p>like the whole field of program specialization</p>
<p>came out of Russia.</p>
<p>Can you backtrack?</p>
<p>So what is program specialization?</p>
<p>So it&rsquo;s basically&hellip;</p>
<p>Well, take sorting, for example.</p>
<p>You can have a generic program for sorting lists,</p>
<p>but what if all your lists you care about</p>
<p>are length 10,000 or less?</p>
<p>Got it.</p>
<p>You can run an automated program specializer</p>
<p>on your sorting algorithm,</p>
<p>and it will come up with the algorithm</p>
<p>that&rsquo;s optimal for sorting lists of length 1,000 or less,</p>
<p>or 10,000 or less, right?</p>
<p>That&rsquo;s kind of like, isn&rsquo;t that the kind of the process</p>
<p>of evolution as a program specializer to the environment?</p>
<p>So you&rsquo;re kind of evolving human beings,</p>
<p>or you&rsquo;re living creatures.</p>
<p>Your Russian heritage is showing there.</p>
<p>So with Alexander Vityaev and Peter Anokhin and so on,</p>
<p>I mean, there&rsquo;s a long history</p>
<p>of thinking about evolution that way also, right?</p>
<p>So, well, my point is that what we&rsquo;re thinking of</p>
<p>as a human level general intelligence,</p>
<p>if you start from narrow AIs,</p>
<p>like are being used in the commercial AI field now,</p>
<p>then you&rsquo;re thinking,</p>
<p>okay, how do we make it more and more general?</p>
<p>On the other hand,</p>
<p>if you start from AICSI or Schmidhuber&rsquo;s G√∂del machine,</p>
<p>or these infinitely powerful,</p>
<p>but practically infeasible AIs,</p>
<p>then getting to a human level AGI</p>
<p>is a matter of specialization.</p>
<p>It&rsquo;s like, how do you take these</p>
<p>maximally general learning processes</p>
<p>and how do you specialize them</p>
<p>so that they can operate</p>
<p>within the resource constraints that you have,</p>
<p>but will achieve the particular things that you care about?</p>
<p>Because we humans are not maximally general intelligence.</p>
<p>If I ask you to run a maze in 750 dimensions,</p>
<p>you&rsquo;d probably be very slow.</p>
<p>Whereas at two dimensions,</p>
<p>you&rsquo;re probably way better, right?</p>
<p>So, I mean, we&rsquo;re special because our hippocampus</p>
<p>has a two dimensional map in it, right?</p>
<p>And it does not have a 750 dimensional map in it.</p>
<p>So, I mean, we&rsquo;re a peculiar mix</p>
<p>of generality and specialization, right?</p>
<p>We&rsquo;ll probably start quite general at birth.</p>
<p>Not obviously still narrow,</p>
<p>but like more general than we are</p>
<p>at age 20 and 30 and 40 and 50 and 60.</p>
<p>I don&rsquo;t think that, I think it&rsquo;s more complex than that</p>
<p>because I mean, in some sense,</p>
<p>a young child is less biased</p>
<p>and the brain has yet to sort of crystallize</p>
<p>into appropriate structures</p>
<p>for processing aspects of the physical and social world.</p>
<p>On the other hand,</p>
<p>the young child is very tied to their sensorium.</p>
<p>Whereas we can deal with abstract mathematics,</p>
<p>like 750 dimensions and the young child cannot</p>
<p>because they haven&rsquo;t grown what Piaget</p>
<p>called the formal capabilities.</p>
<p>They haven&rsquo;t learned to abstract yet, right?</p>
<p>And the ability to abstract</p>
<p>gives you a different kind of generality</p>
<p>than what the baby has.</p>
<p>So, there&rsquo;s both more specialization</p>
<p>and more generalization that comes</p>
<p>with the development process actually.</p>
<p>I mean, I guess just the trajectories</p>
<p>of the specialization are most controllable</p>
<p>at the young age, I guess is one way to put it.</p>
<p>Do you have kids?</p>
<p>No.</p>
<p>They&rsquo;re not as controllable as you think.</p>
<p>So, you think it&rsquo;s interesting.</p>
<p>I think, honestly, I think a human adult</p>
<p>is much more generally intelligent than a human baby.</p>
<p>Babies are very stupid, you know what I mean?</p>
<p>I mean, they&rsquo;re cute, which is why we put up</p>
<p>with their repetitiveness and stupidity.</p>
<p>And they have what the Zen guys would call</p>
<p>a beginner&rsquo;s mind, which is a beautiful thing,</p>
<p>but that doesn&rsquo;t necessarily correlate</p>
<p>with a high level of intelligence.</p>
<p>On the plot of cuteness and stupidity,</p>
<p>there&rsquo;s a process that allows us to put up</p>
<p>with their stupidity as they become more intelligent.</p>
<p>So, by the time you&rsquo;re an ugly old man like me,</p>
<p>you gotta get really, really smart to compensate.</p>
<p>To compensate, okay, cool.</p>
<p>But yeah, going back to your original question,</p>
<p>so the way I look at human level AGI</p>
<p>is how do you specialize, you know,</p>
<p>unrealistically inefficient, superhuman,</p>
<p>brute force learning processes</p>
<p>to the specific goals that humans need to achieve</p>
<p>and the specific resources that we have.</p>
<p>And both of these, the goals and the resources</p>
<p>and the environments, I mean, all this is important.</p>
<p>And on the resources side, it&rsquo;s important</p>
<p>that the hardware resources we&rsquo;re bringing to bear</p>
<p>are very different than the human brain.</p>
<p>So the way I would want to implement AGI</p>
<p>on a bunch of neurons in a vat</p>
<p>that I could rewire arbitrarily is quite different</p>
<p>than the way I would want to create AGI</p>
<p>on say a modern server farm of CPUs and GPUs,</p>
<p>which in turn may be quite different</p>
<p>than the way I would want to implement AGI</p>
<p>on whatever quantum computer we&rsquo;ll have in 10 years,</p>
<p>supposing someone makes a robust quantum turing machine</p>
<p>or something, right?</p>
<p>So I think there&rsquo;s been coevolution</p>
<p>of the patterns of organization in the human brain</p>
<p>and the physiological particulars</p>
<p>of the human brain over time.</p>
<p>And when you look at neural networks,</p>
<p>that is one powerful class of learning algorithms,</p>
<p>but it&rsquo;s also a class of learning algorithms</p>
<p>that evolve to exploit the particulars of the human brain</p>
<p>as a computational substrate.</p>
<p>If you&rsquo;re looking at the computational substrate</p>
<p>of a modern server farm,</p>
<p>you won&rsquo;t necessarily want the same algorithms</p>
<p>that you want on the human brain.</p>
<p>And from the right level of abstraction,</p>
<p>you could look at maybe the best algorithms on the brain</p>
<p>and the best algorithms on a modern computer network</p>
<p>as implementing the same abstract learning</p>
<p>and representation processes,</p>
<p>but finding that level of abstraction</p>
<p>is its own AGI research project then, right?</p>
<p>So that&rsquo;s about the hardware side</p>
<p>and the software side, which follows from that.</p>
<p>Then regarding what are the requirements,</p>
<p>I wrote the paper years ago</p>
<p>on what I called the embodied communication prior,</p>
<p>which was quite similar in intent</p>
<p>to Yoshua Bengio&rsquo;s recent paper on the consciousness prior,</p>
<p>except I didn&rsquo;t wanna wrap up consciousness in it</p>
<p>because to me, the qualia problem and subjective experience</p>
<p>is a very interesting issue also,</p>
<p>which we can chat about,</p>
<p>but I would rather keep that philosophical debate distinct</p>
<p>from the debate of what kind of biases</p>
<p>do you wanna put in a general intelligence</p>
<p>to give it human like general intelligence.</p>
<p>And I&rsquo;m not sure Yoshua Bengio is really addressing</p>
<p>that kind of consciousness.</p>
<p>He&rsquo;s just using the term.</p>
<p>I love Yoshua to pieces.</p>
<p>Like he&rsquo;s by far my favorite of the lines of deep learning.</p>
<p>Yeah.</p>
<p>He&rsquo;s such a good hearted guy.</p>
<p>He&rsquo;s a good human being.</p>
<p>Yeah, for sure.</p>
<p>I am not sure he has plumbed to the depths</p>
<p>of the philosophy of consciousness.</p>
<p>No, he&rsquo;s using it as a sexy term.</p>
<p>Yeah, yeah, yeah.</p>
<p>So what I called it was the embodied communication prior.</p>
<p>Can you maybe explain it a little bit?</p>
<p>Yeah, yeah.</p>
<p>What I meant was, what are we humans evolved for?</p>
<p>You can say being human, but that&rsquo;s very abstract, right?</p>
<p>I mean, our minds control individual bodies,</p>
<p>which are autonomous agents moving around in a world</p>
<p>that&rsquo;s composed largely of solid objects, right?</p>
<p>And we&rsquo;ve also evolved to communicate via language</p>
<p>with other solid object agents that are going around</p>
<p>doing things collectively with us</p>
<p>in a world of solid objects.</p>
<p>And these things are very obvious,</p>
<p>but if you compare them to the scope</p>
<p>of all possible intelligences</p>
<p>or even all possible intelligences</p>
<p>that are physically realizable,</p>
<p>that actually constrains things a lot.</p>
<p>So if you start to look at how would you realize</p>
<p>some specialized or constrained version</p>
<p>of universal general intelligence</p>
<p>in a system that has limited memory</p>
<p>and limited speed of processing,</p>
<p>but whose general intelligence will be biased</p>
<p>toward controlling a solid object agent,</p>
<p>which is mobile in a solid object world</p>
<p>for manipulating solid objects</p>
<p>and communicating via language with other similar agents</p>
<p>in that same world, right?</p>
<p>Then starting from that,</p>
<p>you&rsquo;re starting to get a requirements analysis</p>
<p>for human level general intelligence.</p>
<p>And then that leads you into cognitive science</p>
<p>and you can look at, say, what are the different types</p>
<p>of memory that the human mind and brain has?</p>
<p>And this has matured over the last decades</p>
<p>and I got into this a lot.</p>
<p>So after getting my PhD in math,</p>
<p>I was an academic for eight years.</p>
<p>I was in departments of mathematics,</p>
<p>computer science, and psychology.</p>
<p>When I was in the psychology department</p>
<p>at the University of Western Australia,</p>
<p>I was focused on cognitive science of memory and perception.</p>
<p>Actually, I was teaching neural nets and deep neural nets</p>
<p>and it was multi layer perceptrons, right?</p>
<p>Psychology?</p>
<p>Yeah.</p>
<p>Cognitive science, it was cross disciplinary</p>
<p>among engineering, math, psychology, philosophy,</p>
<p>linguistics, computer science.</p>
<p>But yeah, we were teaching psychology students</p>
<p>to try to model the data from human cognition experiments</p>
<p>using multi layer perceptrons,</p>
<p>which was the early version of a deep neural network.</p>
<p>Very, very, yeah, recurrent back prop</p>
<p>was very, very slow to train back then, right?</p>
<p>So this is the study of these constraint systems</p>
<p>that are supposed to deal with physical objects.</p>
<p>So if you look at cognitive psychology,</p>
<p>you can see there&rsquo;s multiple types of memory,</p>
<p>which are to some extent represented</p>
<p>by different subsystems in the human brain.</p>
<p>So we have episodic memory,</p>
<p>which takes into account our life history</p>
<p>and everything that&rsquo;s happened to us.</p>
<p>We have declarative or semantic memory,</p>
<p>which is like facts and beliefs abstracted</p>
<p>from the particular situations that they occurred in.</p>
<p>There&rsquo;s sensory memory, which to some extent</p>
<p>is sense modality specific,</p>
<p>and then to some extent is unified across sense modalities.</p>
<p>There&rsquo;s procedural memory, memory of how to do stuff,</p>
<p>like how to swing the tennis racket, right?</p>
<p>Which is, there&rsquo;s motor memory,</p>
<p>but it&rsquo;s also a little more abstract than motor memory.</p>
<p>It involves cerebellum and cortex working together.</p>
<p>Then there&rsquo;s memory linkage with emotion</p>
<p>which has to do with linkages of cortex and limbic system.</p>
<p>There&rsquo;s specifics of spatial and temporal modeling</p>
<p>connected with memory, which has to do with hippocampus</p>
<p>and thalamus connecting to cortex.</p>
<p>And the basal ganglia, which influences goals.</p>
<p>So we have specific memory of what goals,</p>
<p>subgoals and sub subgoals we want to perceive</p>
<p>in which context in the past.</p>
<p>Human brain has substantially different subsystems</p>
<p>for these different types of memory</p>
<p>and substantially differently tuned learning,</p>
<p>like differently tuned modes of longterm potentiation</p>
<p>to do with the types of neurons and neurotransmitters</p>
<p>in the different parts of the brain</p>
<p>corresponding to these different types of knowledge.</p>
<p>And these different types of memory and learning</p>
<p>in the human brain, I mean, you can back these all</p>
<p>into embodied communication for controlling agents</p>
<p>in worlds of solid objects.</p>
<p>Now, so if you look at building an AGI system,</p>
<p>one way to do it, which starts more from cognitive science</p>
<p>than neuroscience is to say,</p>
<p>okay, what are the types of memory</p>
<p>that are necessary for this kind of world?</p>
<p>Yeah, yeah, necessary for this sort of intelligence.</p>
<p>What types of learning work well</p>
<p>with these different types of memory?</p>
<p>And then how do you connect all these things together, right?</p>
<p>And of course the human brain did it incrementally</p>
<p>through evolution because each of the sub networks</p>
<p>of the brain, I mean, it&rsquo;s not really the lobes</p>
<p>of the brain, it&rsquo;s the sub networks,</p>
<p>each of which is widely distributed,</p>
<p>which of the, each of the sub networks of the brain</p>
<p>co evolves with the other sub networks of the brain,</p>
<p>both in terms of its patterns of organization</p>
<p>and the particulars of the neurophysiology.</p>
<p>So they all grew up communicating</p>
<p>and adapting to each other.</p>
<p>It&rsquo;s not like they were separate black boxes</p>
<p>that were then glommed together, right?</p>
<p>Whereas as engineers, we would tend to say,</p>
<p>let&rsquo;s make the declarative memory box here</p>
<p>and the procedural memory box here</p>
<p>and the perception box here and wire them together.</p>
<p>And when you can do that, it&rsquo;s interesting.</p>
<p>I mean, that&rsquo;s how a car is built, right?</p>
<p>But on the other hand, that&rsquo;s clearly not</p>
<p>how biological systems are made.</p>
<p>The parts co evolve so as to adapt and work together.</p>
<p>That&rsquo;s by the way, how every human engineered system</p>
<p>that flies, that was, we were using that analogy</p>
<p>before it&rsquo;s built as well.</p>
<p>So do you find this at all appealing?</p>
<p>Like there&rsquo;s been a lot of really exciting,</p>
<p>which I find strange that it&rsquo;s ignored work</p>
<p>in cognitive architectures, for example,</p>
<p>throughout the last few decades.</p>
<p>Do you find that?</p>
<p>Yeah, I mean, I had a lot to do with that community</p>
<p>and you know, Paul Rosenbloom, who was one of the,</p>
<p>and John Laird who built the SOAR architecture,</p>
<p>are friends of mine.</p>
<p>And I learned SOAR quite well</p>
<p>and ACTAR and these different cognitive architectures.</p>
<p>And how I was looking at the AI world about 10 years ago</p>
<p>before this whole commercial deep learning explosion was,</p>
<p>on the one hand, you had these cognitive architecture guys</p>
<p>who were working closely with psychologists</p>
<p>and cognitive scientists who had thought a lot</p>
<p>about how the different parts of a human like mind</p>
<p>should work together.</p>
<p>On the other hand, you had these learning theory guys</p>
<p>who didn&rsquo;t care at all about the architecture,</p>
<p>but we&rsquo;re just thinking about like,</p>
<p>how do you recognize patterns in large amounts of data?</p>
<p>And in some sense, what you needed to do</p>
<p>was to get the learning that the learning theory guys</p>
<p>were doing and put it together with the architecture</p>
<p>that the cognitive architecture guys were doing.</p>
<p>And then you would have what you needed.</p>
<p>Now, you can&rsquo;t, unfortunately, when you look at the details,</p>
<p>you can&rsquo;t just do that without totally rebuilding</p>
<p>what is happening on both the cognitive architecture</p>
<p>and the learning side.</p>
<p>So, I mean, they tried to do that in SOAR,</p>
<p>but what they ultimately did is like,</p>
<p>take a deep neural net or something for perception</p>
<p>and you include it as one of the black boxes.</p>
<p>It becomes one of the boxes.</p>
<p>The learning mechanism becomes one of the boxes</p>
<p>as opposed to fundamental part of the system.</p>
<p>You could look at some of the stuff DeepMind has done,</p>
<p>like the differential neural computer or something</p>
<p>that sort of has a neural net for deep learning perception.</p>
<p>It has another neural net, which is like a memory matrix</p>
<p>that stores, say, the map of the London subway or something.</p>
<p>So probably Demis Tsabas was thinking about this</p>
<p>like part of cortex and part of hippocampus</p>
<p>because hippocampus has a spatial map.</p>
<p>And when he was a neuroscientist,</p>
<p>he was doing a bunch on cortex hippocampus interconnection.</p>
<p>So there, the DNC would be an example of folks</p>
<p>from the deep neural net world trying to take a step</p>
<p>in the cognitive architecture direction</p>
<p>by having two neural modules that correspond roughly</p>
<p>to two different parts of the human brain</p>
<p>that deal with different kinds of memory and learning.</p>
<p>But on the other hand, it&rsquo;s super, super, super crude</p>
<p>from the cognitive architecture view, right?</p>
<p>Just as what John Laird and Soar did with neural nets</p>
<p>was super, super crude from a learning point of view</p>
<p>because the learning was like off to the side,</p>
<p>not affecting the core representations, right?</p>
<p>I mean, you weren&rsquo;t learning the representation.</p>
<p>You were learning the data that feeds into the&hellip;</p>
<p>You were learning abstractions of perceptual data</p>
<p>to feed into the representation that was not learned, right?</p>
<p>So yeah, this was clear to me a while ago.</p>
<p>And one of my hopes with the AGI community</p>
<p>was to sort of bring people</p>
<p>from those two directions together.</p>
<p>That didn&rsquo;t happen much in terms of&hellip;</p>
<p>Not yet.</p>
<p>And what I was gonna say is it didn&rsquo;t happen</p>
<p>in terms of bringing like the lions</p>
<p>of cognitive architecture together</p>
<p>with the lions of deep learning.</p>
<p>It did work in the sense that a bunch of younger researchers</p>
<p>have had their heads filled with both of those ideas.</p>
<p>This comes back to a saying my dad,</p>
<p>who was a university professor, often quoted to me,</p>
<p>which was, science advances one funeral at a time,</p>
<p>which I&rsquo;m trying to avoid.</p>
<p>Like I&rsquo;m 53 years old and I&rsquo;m trying to invent</p>
<p>amazing, weird ass new things</p>
<p>that nobody ever thought about,</p>
<p>which we&rsquo;ll talk about in a few minutes.</p>
<p>But there is that aspect, right?</p>
<p>Like the people who&rsquo;ve been at AI a long time</p>
<p>and have made their career developing one aspect,</p>
<p>like a cognitive architecture or a deep learning approach,</p>
<p>it can be hard once you&rsquo;re old</p>
<p>and have made your career doing one thing,</p>
<p>it can be hard to mentally shift gears.</p>
<p>I mean, I try quite hard to remain flexible minded.</p>
<p>Have you been successful somewhat in changing,</p>
<p>maybe, have you changed your mind on some aspects</p>
<p>of what it takes to build an AGI, like technical things?</p>
<p>The hard part is that the world doesn&rsquo;t want you to.</p>
<p>The world or your own brain?</p>
<p>The world, well, that one point</p>
<p>is that your brain doesn&rsquo;t want to.</p>
<p>The other part is that the world doesn&rsquo;t want you to.</p>
<p>Like the people who have followed your ideas</p>
<p>get mad at you if you change your mind.</p>
<p>And the media wants to pigeonhole you as an avatar</p>
<p>of a certain idea.</p>
<p>But yeah, I&rsquo;ve changed my mind on a bunch of things.</p>
<p>I mean, when I started my career,</p>
<p>I really thought quantum computing</p>
<p>would be necessary for AGI.</p>
<p>And I doubt it&rsquo;s necessary now,</p>
<p>although I think it will be a super major enhancement.</p>
<p>But I mean, I&rsquo;m now in the middle of embarking</p>
<p>on the complete rethink and rewrite from scratch</p>
<p>of our OpenCog AGI system together with Alexey Potapov</p>
<p>and his team in St. Petersburg,</p>
<p>who&rsquo;s working with me in SingularityNet.</p>
<p>So now we&rsquo;re trying to like go back to basics,</p>
<p>take everything we learned from working</p>
<p>with the current OpenCog system,</p>
<p>take everything everybody else has learned</p>
<p>from working with their proto AGI systems</p>
<p>and design the best framework for the next stage.</p>
<p>And I do think there&rsquo;s a lot to be learned</p>
<p>from the recent successes with deep neural nets</p>
<p>and deep reinforcement systems.</p>
<p>I mean, people made these essentially trivial systems</p>
<p>work much better than I thought they would.</p>
<p>And there&rsquo;s a lot to be learned from that.</p>
<p>And I wanna incorporate that knowledge appropriately</p>
<p>in our OpenCog 2.0 system.</p>
<p>On the other hand, I also think current deep neural net</p>
<p>architectures as such will never get you anywhere near AGI.</p>
<p>So I think you wanna avoid the pathology</p>
<p>of throwing the baby out with the bathwater</p>
<p>and like saying, well, these things are garbage</p>
<p>because foolish journalists overblow them</p>
<p>as being the path to AGI</p>
<p>and a few researchers overblow them as well.</p>
<p>There&rsquo;s a lot of interesting stuff to be learned there</p>
<p>even though those are not the golden path.</p>
<p>So maybe this is a good chance to step back.</p>
<p>You mentioned OpenCog 2.0, but&hellip;</p>
<p>Go back to OpenCog 0.0, which exists now.</p>
<p>Alpha, yeah.</p>
<p>Yeah, maybe talk through the history of OpenCog</p>
<p>and your thinking about these ideas.</p>
<p>I would say OpenCog 2.0 is a term we&rsquo;re throwing around</p>
<p>sort of tongue in cheek because the existing OpenCog system</p>
<p>that we&rsquo;re working on now is not remotely close</p>
<p>to what we&rsquo;d consider a 1.0, right?</p>
<p>I mean, it&rsquo;s an early&hellip;</p>
<p>It&rsquo;s been around, what, 13 years or something,</p>
<p>but it&rsquo;s still an early stage research system, right?</p>
<p>And actually, we are going back to the beginning</p>
<p>in terms of theory and implementation</p>
<p>because we feel like that&rsquo;s the right thing to do,</p>
<p>but I&rsquo;m sure what we end up with is gonna have</p>
<p>a huge amount in common with the current system.</p>
<p>I mean, we all still like the general approach.</p>
<p>So first of all, what is OpenCog?</p>
<p>Sure, OpenCog is an open source software project</p>
<p>that I launched together with several others in 2008</p>
<p>and probably the first code written toward that</p>
<p>was written in 2001 or two or something</p>
<p>that was developed as a proprietary code base</p>
<p>within my AI company, Novamente LLC.</p>
<p>Then we decided to open source it in 2008,</p>
<p>cleaned up the code throughout some things</p>
<p>and added some new things and&hellip;</p>
<p>What language is it written in?</p>
<p>It&rsquo;s C++.</p>
<p>Primarily, there&rsquo;s a bunch of scheme as well,</p>
<p>but most of it&rsquo;s C++.</p>
<p>And it&rsquo;s separate from something we&rsquo;ll also talk about,</p>
<p>the SingularityNet.</p>
<p>So it was born as a non networked thing.</p>
<p>Correct, correct.</p>
<p>Well, there are many levels of networks involved here.</p>
<p>No connectivity to the internet, or no, at birth.</p>
<p>Yeah, I mean, SingularityNet is a separate project</p>
<p>and a separate body of code.</p>
<p>And you can use SingularityNet as part of the infrastructure</p>
<p>for a distributed OpenCog system,</p>
<p>but there are different layers.</p>
<p>Yeah, got it.</p>
<p>So OpenCog on the one hand as a software framework</p>
<p>could be used to implement a variety</p>
<p>of different AI architectures and algorithms,</p>
<p>but in practice, there&rsquo;s been a group of developers</p>
<p>which I&rsquo;ve been leading together with Linus Vepstas,</p>
<p>Neil Geisweiler, and a few others,</p>
<p>which have been using the OpenCog platform</p>
<p>and infrastructure to implement certain ideas</p>
<p>about how to make an AGI.</p>
<p>So there&rsquo;s been a little bit of ambiguity</p>
<p>about OpenCog, the software platform</p>
<p>versus OpenCog, the AGI design,</p>
<p>because in theory, you could use that software to do,</p>
<p>you could use it to make a neural net.</p>
<p>You could use it to make a lot of different AGI.</p>
<p>What kind of stuff does the software platform provide,</p>
<p>like in terms of utilities, tools, like what?</p>
<p>Yeah, let me first tell about OpenCog</p>
<p>as a software platform,</p>
<p>and then I&rsquo;ll tell you the specific AGI R&amp;D</p>
<p>we&rsquo;ve been building on top of it.</p>
<p>So the core component of OpenCog as a software platform</p>
<p>is what we call the atom space,</p>
<p>which is a weighted labeled hypergraph.</p>
<p>ATOM, atom space.</p>
<p>Atom space, yeah, yeah, not atom, like Adam and Eve,</p>
<p>although that would be cool too.</p>
<p>Yeah, so you have a hypergraph, which is like,</p>
<p>so a graph in this sense is a bunch of nodes</p>
<p>with links between them.</p>
<p>A hypergraph is like a graph,</p>
<p>but links can go between more than two nodes.</p>
<p>So you have a link between three nodes.</p>
<p>And in fact, OpenCog&rsquo;s atom space</p>
<p>would properly be called a metagraph</p>
<p>because you can have links pointing to links,</p>
<p>or you could have links pointing to whole subgraphs, right?</p>
<p>So it&rsquo;s an extended hypergraph or a metagraph.</p>
<p>Is metagraph a technical term?</p>
<p>It is now a technical term.</p>
<p>Interesting.</p>
<p>But I don&rsquo;t think it was yet a technical term</p>
<p>when we started calling this a generalized hypergraph.</p>
<p>But in any case, it&rsquo;s a weighted labeled</p>
<p>generalized hypergraph or weighted labeled metagraph.</p>
<p>The weights and labels mean that the nodes and links</p>
<p>can have numbers and symbols attached to them.</p>
<p>So they can have types on them.</p>
<p>They can have numbers on them that represent,</p>
<p>say, a truth value or an importance value</p>
<p>for a certain purpose.</p>
<p>And of course, like with all things,</p>
<p>you can reduce that to a hypergraph,</p>
<p>and then the hypergraph can be reduced to a graph.</p>
<p>You can reduce hypergraph to a graph,</p>
<p>and you could reduce a graph to an adjacency matrix.</p>
<p>So, I mean, there&rsquo;s always multiple representations.</p>
<p>But there&rsquo;s a layer of representation</p>
<p>that seems to work well here.</p>
<p>Got it.</p>
<p>Right, right, right.</p>
<p>And so similarly, you could have a link to a whole graph</p>
<p>because a whole graph could represent,</p>
<p>say, a body of information.</p>
<p>And I could say, I reject this body of information.</p>
<p>Then one way to do that is make that link</p>
<p>go to that whole subgraph representing</p>
<p>the body of information, right?</p>
<p>I mean, there are many alternate representations,</p>
<p>but that&rsquo;s, anyway, what we have in OpenCOG,</p>
<p>we have an atom space, which is this weighted, labeled,</p>
<p>generalized hypergraph.</p>
<p>Knowledge store, it lives in RAM.</p>
<p>There&rsquo;s also a way to back it up to disk.</p>
<p>There are ways to spread it among</p>
<p>multiple different machines.</p>
<p>Then there are various utilities for dealing with that.</p>
<p>So there&rsquo;s a pattern matcher,</p>
<p>which lets you specify a sort of abstract pattern</p>
<p>and then search through a whole atom space</p>
<p>with labeled hypergraph to see what subhypergraphs</p>
<p>may match that pattern, for an example.</p>
<p>So that&rsquo;s, then there&rsquo;s something called</p>
<p>the COG server in OpenCOG,</p>
<p>which lets you run a bunch of different agents</p>
<p>or processes in a scheduler.</p>
<p>And each of these agents, basically it reads stuff</p>
<p>from the atom space and it writes stuff to the atom space.</p>
<p>So this is sort of the basic operational model.</p>
<p>That&rsquo;s the software framework.</p>
<p>And of course that&rsquo;s, there&rsquo;s a lot there</p>
<p>just from a scalable software engineering standpoint.</p>
<p>So you could use this, I don&rsquo;t know if you&rsquo;ve,</p>
<p>have you looked into the Stephen Wolfram&rsquo;s physics project</p>
<p>recently with the hypergraphs and stuff?</p>
<p>Could you theoretically use like the software framework</p>
<p>to play with it? You certainly could,</p>
<p>although Wolfram would rather die</p>
<p>than use anything but Mathematica for his work.</p>
<p>Well that&rsquo;s, yeah, but there&rsquo;s a big community of people</p>
<p>who are, you know, would love integration.</p>
<p>Like you said, the young minds love the idea</p>
<p>of integrating, of connecting things.</p>
<p>Yeah, that&rsquo;s right.</p>
<p>And I would add on that note,</p>
<p>the idea of using hypergraph type models in physics</p>
<p>is not very new.</p>
<p>Like if you look at&hellip;</p>
<p>The Russians did it first.</p>
<p>Well, I&rsquo;m sure they did.</p>
<p>And a guy named Ben Dribis, who&rsquo;s a mathematician,</p>
<p>a professor in Louisiana or somewhere,</p>
<p>had a beautiful book on quantum sets and hypergraphs</p>
<p>and algebraic topology for discrete models of physics.</p>
<p>And carried it much farther than Wolfram has,</p>
<p>but he&rsquo;s not rich and famous,</p>
<p>so it didn&rsquo;t get in the headlines.</p>
<p>But yeah, Wolfram aside, yeah,</p>
<p>certainly that&rsquo;s a good way to put it.</p>
<p>The whole OpenCog framework,</p>
<p>you could use it to model biological networks</p>
<p>and simulate biology processes.</p>
<p>You could use it to model physics</p>
<p>on discrete graph models of physics.</p>
<p>So you could use it to do, say, biologically realistic</p>
<p>neural networks, for example.</p>
<p>And that&rsquo;s a framework.</p>
<p>What do agents and processes do?</p>
<p>Do they grow the graph?</p>
<p>What kind of computations, just to get a sense,</p>
<p>are they supposed to do?</p>
<p>So in theory, they could do anything they want to do.</p>
<p>They&rsquo;re just C++ processes.</p>
<p>On the other hand, the computation framework</p>
<p>is sort of designed for agents</p>
<p>where most of their processing time</p>
<p>is taken up with reads and writes to the atom space.</p>
<p>And so that&rsquo;s a very different processing model</p>
<p>than, say, the matrix multiplication based model</p>
<p>as underlies most deep learning systems, right?</p>
<p>So you could create an agent</p>
<p>that just factored numbers for a billion years.</p>
<p>It would run within the OpenCog platform,</p>
<p>but it would be pointless, right?</p>
<p>I mean, the point of doing OpenCog</p>
<p>is because you want to make agents</p>
<p>that are cooperating via reading and writing</p>
<p>into this weighted labeled hypergraph, right?</p>
<p>And that has both cognitive architecture importance</p>
<p>because then this hypergraph is being used</p>
<p>as a sort of shared memory</p>
<p>among different cognitive processes,</p>
<p>but it also has software and hardware</p>
<p>implementation implications</p>
<p>because current GPU architectures</p>
<p>are not so useful for OpenCog,</p>
<p>whereas a graph chip would be incredibly useful, right?</p>
<p>And I think Graphcore has those now,</p>
<p>but they&rsquo;re not ideally suited for this.</p>
<p>But I think in the next, let&rsquo;s say, three to five years,</p>
<p>we&rsquo;re gonna see new chips</p>
<p>where like a graph is put on the chip</p>
<p>and the back and forth between multiple processes</p>
<p>acting SIMD and MIMD on that graph is gonna be fast.</p>
<p>And then that may do for OpenCog type architectures</p>
<p>what GPUs did for deep neural architecture.</p>
<p>It&rsquo;s a small tangent.</p>
<p>Can you comment on thoughts about neuromorphic computing?</p>
<p>So like hardware implementations</p>
<p>of all these different kind of, are you interested?</p>
<p>Are you excited by that possibility?</p>
<p>I&rsquo;m excited by graph processors</p>
<p>because I think they can massively speed up OpenCog,</p>
<p>which is a class of architectures that I&rsquo;m working on.</p>
<p>I think if, you know, in principle, neuromorphic computing</p>
<p>should be amazing.</p>
<p>I haven&rsquo;t yet been fully sold</p>
<p>on any of the systems that are out.</p>
<p>They&rsquo;re like, memristors should be amazing too, right?</p>
<p>So a lot of these things have obvious potential,</p>
<p>but I haven&rsquo;t yet put my hands on a system</p>
<p>that seemed to manifest that.</p>
<p>Mark&rsquo;s system should be amazing,</p>
<p>but the current systems have not been great.</p>
<p>Yeah, I mean, look, for example,</p>
<p>if you wanted to make a biologically realistic</p>
<p>hardware neural network,</p>
<p>like making a circuit in hardware</p>
<p>that emulated like the Hodgkin‚ÄìHuxley equation</p>
<p>or the Izhekevich equation,</p>
<p>like differential equations</p>
<p>for a biologically realistic neuron</p>
<p>and putting that in hardware on the chip,</p>
<p>that would seem that it would make more feasible</p>
<p>to make a large scale, truly biologically realistic</p>
<p>neural network.</p>
<p>Now, what&rsquo;s been done so far is not like that.</p>
<p>So I guess personally, as a researcher,</p>
<p>I mean, I&rsquo;ve done a bunch of work in computational neuroscience</p>
<p>where I did some work with IARPA in DC,</p>
<p>Intelligence Advanced Research Project Agency.</p>
<p>We were looking at how do you make</p>
<p>a biologically realistic simulation</p>
<p>of seven different parts of the brain</p>
<p>cooperating with each other,</p>
<p>using like realistic nonlinear dynamical models of neurons,</p>
<p>and how do you get that to simulate</p>
<p>what&rsquo;s going on in the mind of a geo intelligence analyst</p>
<p>while they&rsquo;re trying to find terrorists on a map, right?</p>
<p>So if you want to do something like that,</p>
<p>having neuromorphic hardware that really let you simulate</p>
<p>like a realistic model of the neuron would be amazing.</p>
<p>But that&rsquo;s sort of with my computational neuroscience</p>
<p>hat on, right?</p>
<p>With an AGI hat on, I&rsquo;m just more interested</p>
<p>in these hypergraph knowledge representation</p>
<p>based architectures, which would benefit more</p>
<p>from various types of graph processors</p>
<p>because the main processing bottleneck</p>
<p>is reading writing to RAM.</p>
<p>It&rsquo;s reading writing to the graph in RAM.</p>
<p>The main processing bottleneck for this kind of</p>
<p>proto AGI architecture is not multiplying matrices.</p>
<p>And for that reason, GPUs, which are really good</p>
<p>at multiplying matrices, don&rsquo;t apply as well.</p>
<p>There are frameworks like Gunrock and others</p>
<p>that try to boil down graph processing</p>
<p>to matrix operations, and they&rsquo;re cool,</p>
<p>but you&rsquo;re still putting a square peg</p>
<p>into a round hole in a certain way.</p>
<p>The same is true, I mean, current quantum machine learning,</p>
<p>which is very cool.</p>
<p>It&rsquo;s also all about how to get matrix and vector operations</p>
<p>in quantum mechanics, and I see why that&rsquo;s natural to do.</p>
<p>I mean, quantum mechanics is all unitary matrices</p>
<p>and vectors, right?</p>
<p>On the other hand, you could also try</p>
<p>to make graph centric quantum computers,</p>
<p>which I think is where things will go.</p>
<p>And then we can have, then we can make,</p>
<p>like take the open cog implementation layer,</p>
<p>implement it in a collapsed state inside a quantum computer.</p>
<p>But that may be the singularity squared, right?</p>
<p>I&rsquo;m not sure we need that to get to human level.</p>
<p>That&rsquo;s already beyond the first singularity.</p>
<p>But can we just go back to open cog?</p>
<p>Yeah, and the hypergraph and open cog.</p>
<p>That&rsquo;s the software framework, right?</p>
<p>So the next thing is our cognitive architecture</p>
<p>tells us particular algorithms to put there.</p>
<p>Got it.</p>
<p>Can we backtrack on the kind of, is this graph designed,</p>
<p>is it in general supposed to be sparse</p>
<p>and the operations constantly grow and change the graph?</p>
<p>Yeah, the graph is sparse.</p>
<p>But is it constantly adding links and so on?</p>
<p>It is a self modifying hypergraph.</p>
<p>So it&rsquo;s not, so the write and read operations</p>
<p>you&rsquo;re referring to, this isn&rsquo;t just a fixed graph</p>
<p>to which you change the way, it&rsquo;s a constantly growing graph.</p>
<p>Yeah, that&rsquo;s true.</p>
<p>So it is different model than,</p>
<p>say current deep neural nets</p>
<p>and have a fixed neural architecture</p>
<p>and you&rsquo;re updating the weights.</p>
<p>Although there have been like cascade correlational</p>
<p>neural net architectures that grow new nodes and links,</p>
<p>but the most common neural architectures now</p>
<p>have a fixed neural architecture,</p>
<p>you&rsquo;re updating the weights.</p>
<p>And then open cog, you can update the weights</p>
<p>and that certainly happens a lot,</p>
<p>but adding new nodes, adding new links,</p>
<p>removing nodes and links is an equally critical part</p>
<p>of the system&rsquo;s operations.</p>
<p>Got it.</p>
<p>So now when you start to add these cognitive algorithms</p>
<p>on top of this open cog architecture,</p>
<p>what does that look like?</p>
<p>Yeah, so within this framework then,</p>
<p>creating a cognitive architecture is basically two things.</p>
<p>It&rsquo;s choosing what type system you wanna put</p>
<p>on the nodes and links in the hypergraph,</p>
<p>what types of nodes and links you want.</p>
<p>And then it&rsquo;s choosing what collection of agents,</p>
<p>what collection of AI algorithms or processes</p>
<p>are gonna run to operate on this hypergraph.</p>
<p>And of course those two decisions</p>
<p>are closely connected to each other.</p>
<p>So in terms of the type system,</p>
<p>there are some links that are more neural net like,</p>
<p>they&rsquo;re just like have weights to get updated</p>
<p>by heavy and learning and activation spreads along them.</p>
<p>There are other links that are more logic like</p>
<p>and nodes that are more logic like.</p>
<p>So you could have a variable node</p>
<p>and you can have a node representing a universal</p>
<p>or existential quantifier as in predicate logic</p>
<p>or term logic.</p>
<p>So you can have logic like nodes and links,</p>
<p>or you can have neural like nodes and links.</p>
<p>You can also have procedure like nodes and links</p>
<p>as in say a combinatorial logic or Lambda calculus</p>
<p>representing programs.</p>
<p>So you can have nodes and links representing</p>
<p>many different types of semantics,</p>
<p>which means you could make a horrible ugly mess</p>
<p>or you could make a system</p>
<p>where these different types of knowledge</p>
<p>all interpenetrate and synergize</p>
<p>with each other beautifully, right?</p>
<p>So the hypergraph can contain programs.</p>
<p>Yeah, it can contain programs,</p>
<p>although in the current version,</p>
<p>it is a very inefficient way</p>
<p>to guide the execution of programs,</p>
<p>which is one thing that we are aiming to resolve</p>
<p>with our rewrite of the system now.</p>
<p>So what to you is the most beautiful aspect of OpenCog?</p>
<p>Just to you personally,</p>
<p>some aspect that captivates your imagination</p>
<p>from beauty or power?</p>
<p>What fascinates me is finding a common representation</p>
<p>that underlies abstract, declarative knowledge</p>
<p>and sensory knowledge and movement knowledge</p>
<p>and procedural knowledge and episodic knowledge,</p>
<p>finding the right level of representation</p>
<p>where all these types of knowledge are stored</p>
<p>in a sort of universal and interconvertible</p>
<p>yet practically manipulable way, right?</p>
<p>So to me, that&rsquo;s the core,</p>
<p>because once you&rsquo;ve done that,</p>
<p>then the different learning algorithms</p>
<p>can help each other out. Like what you want is,</p>
<p>if you have a logic engine</p>
<p>that helps with declarative knowledge</p>
<p>and you have a deep neural net</p>
<p>that gathers perceptual knowledge,</p>
<p>and you have, say, an evolutionary learning system</p>
<p>that learns procedures,</p>
<p>you want these to not only interact</p>
<p>on the level of sharing results</p>
<p>and passing inputs and outputs to each other,</p>
<p>you want the logic engine, when it gets stuck,</p>
<p>to be able to share its intermediate state</p>
<p>with the neural net and with the evolutionary system</p>
<p>and with the evolutionary learning algorithm</p>
<p>so that they can help each other out of bottlenecks</p>
<p>and help each other solve combinatorial explosions</p>
<p>by intervening inside each other&rsquo;s cognitive processes.</p>
<p>But that can only be done</p>
<p>if the intermediate state of a logic engine,</p>
<p>the evolutionary learning engine,</p>
<p>and a deep neural net are represented in the same form.</p>
<p>And that&rsquo;s what we figured out how to do</p>
<p>by putting the right type system</p>
<p>on top of this weighted labeled hypergraph.</p>
<p>So is there, can you maybe elaborate</p>
<p>on what are the different characteristics</p>
<p>of a type system that can coexist</p>
<p>amongst all these different kinds of knowledge</p>
<p>that needs to be represented?</p>
<p>And is, I mean, like, is it hierarchical?</p>
<p>Just any kind of insights you can give</p>
<p>on that kind of type system?</p>
<p>Yeah, yeah, so this gets very nitty gritty</p>
<p>and mathematical, of course,</p>
<p>but one key part is switching</p>
<p>from predicate logic to term logic.</p>
<p>What is predicate logic?</p>
<p>What is term logic?</p>
<p>So term logic was invented by Aristotle,</p>
<p>or at least that&rsquo;s the oldest recollection we have of it.</p>
<p>But term logic breaks down basic logic</p>
<p>into basically simple links between nodes,</p>
<p>like an inheritance link between node A and node B.</p>
<p>So in term logic, the basic deduction operation</p>
<p>is A implies B, B implies C, therefore A implies C.</p>
<p>Whereas in predicate logic,</p>
<p>the basic operation is modus ponens,</p>
<p>like A implies B, therefore B.</p>
<p>So it&rsquo;s a slightly different way of breaking down logic,</p>
<p>but by breaking down logic into term logic,</p>
<p>you get a nice way of breaking logic down</p>
<p>into nodes and links.</p>
<p>So your concepts can become nodes,</p>
<p>the logical relations become links.</p>
<p>And so then inference is like,</p>
<p>so if this link is A implies B,</p>
<p>this link is B implies C,</p>
<p>then deduction builds a link A implies C.</p>
<p>And your probabilistic algorithm</p>
<p>can assign a certain weight there.</p>
<p>Now, you may also have like a Hebbian neural link</p>
<p>from A to C, which is the degree to which thinking,</p>
<p>the degree to which A being the focus of attention</p>
<p>should make B the focus of attention, right?</p>
<p>So you could have then a neural link</p>
<p>and you could have a symbolic,</p>
<p>like logical inheritance link in your term logic.</p>
<p>And they have separate meaning,</p>
<p>but they could be used to guide each other as well.</p>
<p>Like if there&rsquo;s a large amount of neural weight</p>
<p>on the link between A and B,</p>
<p>that may direct your logic engine to think about,</p>
<p>well, what is the relation?</p>
<p>Are they similar?</p>
<p>Is there an inheritance relation?</p>
<p>Are they similar in some context?</p>
<p>On the other hand, if there&rsquo;s a logical relation</p>
<p>between A and B, that may direct your neural component</p>
<p>to think, well, when I&rsquo;m thinking about A,</p>
<p>should I be directing some attention to B also?</p>
<p>Because there&rsquo;s a logical relation.</p>
<p>So in terms of logic,</p>
<p>there&rsquo;s a lot of thought that went into</p>
<p>how do you break down logic relations,</p>
<p>including basic sort of propositional logic relations</p>
<p>as Aristotelian term logic deals with,</p>
<p>and then quantifier logic relations also.</p>
<p>How do you break those down elegantly into a hypergraph?</p>
<p>Because you, I mean, you can boil logic expression</p>
<p>into a graph in many different ways.</p>
<p>Many of them are very ugly, right?</p>
<p>We tried to find elegant ways</p>
<p>of sort of hierarchically breaking down</p>
<p>complex logic expression into nodes and links.</p>
<p>So that if you have say different nodes representing,</p>
<p>Ben, AI, Lex, interview or whatever,</p>
<p>the logic relations between those things</p>
<p>are compact in the node and link representation.</p>
<p>So that when you have a neural net acting</p>
<p>on the same nodes and links,</p>
<p>the neural net and the logic engine</p>
<p>can sort of interoperate with each other.</p>
<p>And also interpretable by humans.</p>
<p>Is that an important?</p>
<p>That&rsquo;s tough.</p>
<p>Yeah, in simple cases, it&rsquo;s interpretable by humans.</p>
<p>But honestly, I would say logic systems</p>
<p>I would say logic systems give more potential</p>
<p>for transparency and comprehensibility</p>
<p>than neural net systems,</p>
<p>but you still have to work at it.</p>
<p>Because I mean, if I show you a predicate logic proposition</p>
<p>with like 500 nested universal and existential quantifiers</p>
<p>and 217 variables, that&rsquo;s no more comprehensible</p>
<p>than the weight metrics of a neural network, right?</p>
<p>So I&rsquo;d say the logic expressions</p>
<p>that AI learns from its experience</p>
<p>are mostly totally opaque to human beings</p>
<p>and maybe even harder to understand than neural net.</p>
<p>Because I mean, when you have multiple</p>
<p>nested quantifier bindings,</p>
<p>it&rsquo;s a very high level of abstraction.</p>
<p>There is a difference though,</p>
<p>in that within logic, it&rsquo;s a little more straightforward</p>
<p>to pose the problem of like normalize this</p>
<p>and boil this down to a certain form.</p>
<p>I mean, you can do that in neural nets too.</p>
<p>Like you can distill a neural net to a simpler form,</p>
<p>but that&rsquo;s more often done to make a neural net</p>
<p>that&rsquo;ll run on an embedded device or something.</p>
<p>It&rsquo;s harder to distill a net to a comprehensible form</p>
<p>than it is to simplify a logic expression</p>
<p>to a comprehensible form, but it doesn&rsquo;t come for free.</p>
<p>Like what&rsquo;s in the AI&rsquo;s mind is incomprehensible</p>
<p>to a human unless you do some special work</p>
<p>to make it comprehensible.</p>
<p>So on the procedural side, there&rsquo;s some different</p>
<p>and sort of interesting voodoo there.</p>
<p>I mean, if you&rsquo;re familiar in computer science,</p>
<p>there&rsquo;s something called the Curry Howard correspondence,</p>
<p>which is a one to one mapping between proofs and programs.</p>
<p>So every program can be mapped into a proof.</p>
<p>Every proof can be mapped into a program.</p>
<p>You can model this using category theory</p>
<p>and a bunch of nice math,</p>
<p>but we wanna make that practical, right?</p>
<p>So that if you have an executable program</p>
<p>that like moves the robot&rsquo;s arm or figures out</p>
<p>in what order to say things in a dialogue,</p>
<p>that&rsquo;s a procedure represented in OpenCog&rsquo;s hypergraph.</p>
<p>But if you wanna reason on how to improve that procedure,</p>
<p>you need to map that procedure into logic</p>
<p>using Curry Howard isomorphism.</p>
<p>So then the logic engine can reason</p>
<p>about how to improve that procedure</p>
<p>and then map that back into the procedural representation</p>
<p>that is efficient for execution.</p>
<p>So again, that comes down to not just</p>
<p>can you make your procedure into a bunch of nodes and links?</p>
<p>Cause I mean, that can be done trivially.</p>
<p>A C++ compiler has nodes and links inside it.</p>
<p>Can you boil down your procedure</p>
<p>into a bunch of nodes and links</p>
<p>in a way that&rsquo;s like hierarchically decomposed</p>
<p>and simple enough?</p>
<p>It can reason about.</p>
<p>Yeah, yeah, that given the resource constraints at hand,</p>
<p>you can map it back and forth to your term logic,</p>
<p>like fast enough</p>
<p>and without having a bloated logic expression, right?</p>
<p>So there&rsquo;s just a lot of,</p>
<p>there&rsquo;s a lot of nitty gritty particulars there,</p>
<p>but by the same token, if you ask a chip designer,</p>
<p>like how do you make the Intel I7 chip so good?</p>
<p>There&rsquo;s a long list of technical answers there,</p>
<p>which will take a while to go through, right?</p>
<p>And this has been decades of work.</p>
<p>I mean, the first AI system of this nature I tried to build</p>
<p>was called WebMind in the mid 1990s.</p>
<p>And we had a big graph,</p>
<p>a big graph operating in RAM implemented with Java 1.1,</p>
<p>which was a terrible, terrible implementation idea.</p>
<p>And then each node had its own processing.</p>
<p>So like that there,</p>
<p>the core loop looped through all nodes in the network</p>
<p>and let each node enact what its little thing was doing.</p>
<p>And we had logic and neural nets in there,</p>
<p>but an evolutionary learning,</p>
<p>but we hadn&rsquo;t done enough of the math</p>
<p>to get them to operate together very cleanly.</p>
<p>So it was really, it was quite a horrible mess.</p>
<p>So as well as shifting an implementation</p>
<p>where the graph is its own object</p>
<p>and the agents are separately scheduled,</p>
<p>we&rsquo;ve also done a lot of work</p>
<p>on how do you represent programs?</p>
<p>How do you represent procedures?</p>
<p>You know, how do you represent genotypes for evolution</p>
<p>in a way that the interoperability</p>
<p>between the different types of learning</p>
<p>associated with these different types of knowledge</p>
<p>actually works?</p>
<p>And that&rsquo;s been quite difficult.</p>
<p>It&rsquo;s taken decades and it&rsquo;s totally off to the side</p>
<p>of what the commercial mainstream of the AI field is doing,</p>
<p>which isn&rsquo;t thinking about representation at all really.</p>
<p>Although you could see like in the DNC,</p>
<p>they had to think a little bit about</p>
<p>how do you make representation of a map</p>
<p>in this memory matrix work together</p>
<p>with the representation needed</p>
<p>for say visual pattern recognition</p>
<p>in the hierarchical neural network.</p>
<p>But I would say we have taken that direction</p>
<p>of taking the types of knowledge you need</p>
<p>for different types of learning,</p>
<p>like declarative, procedural, attentional,</p>
<p>and how do you make these types of knowledge represent</p>
<p>in a way that allows cross learning</p>
<p>across these different types of memory.</p>
<p>We&rsquo;ve been prototyping and experimenting with this</p>
<p>within OpenCog and before that WebMind</p>
<p>since the mid 1990s.</p>
<p>Now, disappointingly to all of us,</p>
<p>this has not yet been cashed out in an AGI system, right?</p>
<p>I mean, we&rsquo;ve used this system</p>
<p>within our consulting business.</p>
<p>So we&rsquo;ve built natural language processing</p>
<p>and robot control and financial analysis.</p>
<p>We&rsquo;ve built a bunch of sort of vertical market specific</p>
<p>proprietary AI projects.</p>
<p>They use OpenCog on the backend,</p>
<p>but we haven&rsquo;t, that&rsquo;s not the AGI goal, right?</p>
<p>It&rsquo;s interesting, but it&rsquo;s not the AGI goal.</p>
<p>So now what we&rsquo;re looking at with our rebuild of the system.</p>
<p>2.0.</p>
<p>Yeah, we&rsquo;re also calling it True AGI.</p>
<p>So we&rsquo;re not quite sure what the name is yet.</p>
<p>We made a website for trueagi.io,</p>
<p>but we haven&rsquo;t put anything on there yet.</p>
<p>We may come up with an even better name.</p>
<p>It&rsquo;s kind of like the real AI starting point</p>
<p>for your AGI book.</p>
<p>Yeah, but I like True better</p>
<p>because True has like, you can be true hearted, right?</p>
<p>You can be true to your girlfriend.</p>
<p>So True has a number and it also has logic in it, right?</p>
<p>Because logic is a key part of the system.</p>
<p>So yeah, with the True AGI system,</p>
<p>we&rsquo;re sticking with the same basic architecture,</p>
<p>but we&rsquo;re trying to build on what we&rsquo;ve learned.</p>
<p>And one thing we&rsquo;ve learned is that,</p>
<p>we need type checking among dependent types</p>
<p>to be much faster</p>
<p>and among probabilistic dependent types to be much faster.</p>
<p>So as it is now,</p>
<p>you can have complex types on the nodes and links.</p>
<p>But if you wanna put,</p>
<p>like if you want types to be first class citizens,</p>
<p>so that you can have the types can be variables</p>
<p>and then you do type checking</p>
<p>among complex higher order types.</p>
<p>You can do that in the system now, but it&rsquo;s very slow.</p>
<p>This is stuff like it&rsquo;s done</p>
<p>in cutting edge program languages like Agda or something,</p>
<p>these obscure research languages.</p>
<p>On the other hand,</p>
<p>we&rsquo;ve been doing a lot tying together deep neural nets</p>
<p>with symbolic learning.</p>
<p>So we did a project for Cisco, for example,</p>
<p>which was on, this was street scene analysis,</p>
<p>but they had deep neural models</p>
<p>for a bunch of cameras watching street scenes,</p>
<p>but they trained a different model for each camera</p>
<p>because they couldn&rsquo;t get the transfer learning</p>
<p>to work between camera A and camera B.</p>
<p>So we took what came out of all the deep neural models</p>
<p>for the different cameras,</p>
<p>we fed it into an open called symbolic representation.</p>
<p>Then we did some pattern mining and some reasoning</p>
<p>on what came out of all the different cameras</p>
<p>within the symbolic graph.</p>
<p>And that worked well for that application.</p>
<p>I mean, Hugo Latapie from Cisco gave a talk touching on that</p>
<p>at last year&rsquo;s AGI conference, it was in Shenzhen.</p>
<p>On the other hand, we learned from there,</p>
<p>it was kind of clunky to get the deep neural models</p>
<p>to work well with the symbolic system</p>
<p>because we were using torch.</p>
<p>And torch keeps a sort of state computation graph,</p>
<p>but you needed like real time access</p>
<p>to that computation graph within our hypergraph.</p>
<p>And we certainly did it,</p>
<p>Alexey Polopov who leads our St. Petersburg team</p>
<p>wrote a great paper on cognitive modules in OpenCog</p>
<p>explaining sort of how do you deal</p>
<p>with the torch compute graph inside OpenCog.</p>
<p>But in the end we realized like,</p>
<p>that just hadn&rsquo;t been one of our design thoughts</p>
<p>when we built OpenCog, right?</p>
<p>So between wanting really fast dependent type checking</p>
<p>and wanting much more efficient interoperation</p>
<p>between the computation graphs</p>
<p>of deep neural net frameworks and OpenCog&rsquo;s hypergraph</p>
<p>and adding on top of that,</p>
<p>wanting to more effectively run an OpenCog hypergraph</p>
<p>distributed across RAM in 10,000 machines,</p>
<p>which is we&rsquo;re doing dozens of machines now,</p>
<p>but it&rsquo;s just not, we didn&rsquo;t architect it</p>
<p>with that sort of modern scalability in mind.</p>
<p>So these performance requirements are what have driven us</p>
<p>to want to rearchitect the base,</p>
<p>but the core AGI paradigm doesn&rsquo;t really change.</p>
<p>Like the mathematics is the same.</p>
<p>It&rsquo;s just, we can&rsquo;t scale to the level that we want</p>
<p>in terms of distributed processing</p>
<p>or speed of various kinds of processing</p>
<p>with the current infrastructure</p>
<p>that was built in the phase 2001 to 2008,</p>
<p>which is hardly shocking.</p>
<p>Well, I mean, the three things you mentioned</p>
<p>are really interesting.</p>
<p>So what do you think about in terms of interoperability</p>
<p>communicating with computational graph of neural networks?</p>
<p>What do you think about the representations</p>
<p>that neural networks form?</p>
<p>They&rsquo;re bad, but there&rsquo;s many ways</p>
<p>that you could deal with that.</p>
<p>So I&rsquo;ve been wrestling with this a lot</p>
<p>in some work on supervised grammar induction,</p>
<p>and I have a simple paper on that.</p>
<p>They&rsquo;ll give it the next AGI conference,</p>
<p>online portion of which is next week, actually.</p>
<p>What is grammar induction?</p>
<p>So this isn&rsquo;t AGI either,</p>
<p>but it&rsquo;s sort of on the verge</p>
<p>between narrow AI and AGI or something.</p>
<p>Unsupervised grammar induction is the problem.</p>
<p>Throw your AI system, a huge body of text,</p>
<p>and have it learn the grammar of the language</p>
<p>that produced that text.</p>
<p>So you&rsquo;re not giving it labeled examples.</p>
<p>So you&rsquo;re not giving it like a thousand sentences</p>
<p>where the parses were marked up by graduate students.</p>
<p>So it&rsquo;s just got to infer the grammar from the text.</p>
<p>It&rsquo;s like the Rosetta Stone, but worse, right?</p>
<p>Because you only have the one language,</p>
<p>and you have to figure out what is the grammar.</p>
<p>So that&rsquo;s not really AGI because,</p>
<p>I mean, the way a human learns language is not that, right?</p>
<p>I mean, we learn from language that&rsquo;s used in context.</p>
<p>So it&rsquo;s a social embodied thing.</p>
<p>We see how a given sentence is grounded in observation.</p>
<p>There&rsquo;s an interactive element, I guess.</p>
<p>Yeah, yeah, yeah.</p>
<p>On the other hand, so I&rsquo;m more interested in that.</p>
<p>I&rsquo;m more interested in making an AGI system learn language</p>
<p>from its social and embodied experience.</p>
<p>On the other hand, that&rsquo;s also more of a pain to do,</p>
<p>and that would lead us into Hanson Robotics</p>
<p>and their robotics work I&rsquo;ve known much.</p>
<p>We&rsquo;ll talk about it in a few minutes.</p>
<p>But just as an intellectual exercise,</p>
<p>as a learning exercise,</p>
<p>trying to learn grammar from a corpus</p>
<p>is very, very interesting, right?</p>
<p>And that&rsquo;s been a field in AI for a long time.</p>
<p>No one can do it very well.</p>
<p>So we&rsquo;ve been looking at transformer neural networks</p>
<p>and tree transformers, which are amazing.</p>
<p>These came out of Google Brain, actually.</p>
<p>And actually on that team was Lucas Kaiser,</p>
<p>who used to work for me in the one,</p>
<p>the period 2005 through eight or something.</p>
<p>So it&rsquo;s been fun to see my former</p>
<p>sort of AGI employees disperse and do</p>
<p>all these amazing things.</p>
<p>Way too many sucked into Google, actually.</p>
<p>Well, yeah, anyway.</p>
<p>We&rsquo;ll talk about that too.</p>
<p>Lucas Kaiser and a bunch of these guys,</p>
<p>they create transformer networks,</p>
<p>that classic paper like attention is all you need</p>
<p>and all these things following on from that.</p>
<p>So we&rsquo;re looking at transformer networks.</p>
<p>And like, these are able to,</p>
<p>I mean, this is what underlies GPT2 and GPT3 and so on,</p>
<p>which are very, very cool</p>
<p>and have absolutely no cognitive understanding</p>
<p>of any of the texts they&rsquo;re looking at.</p>
<p>Like they&rsquo;re very intelligent idiots, right?</p>
<p>So sorry to take, but this small, I&rsquo;ll bring this back,</p>
<p>but do you think GPT3 understands language?</p>
<p>No, no, it understands nothing.</p>
<p>It&rsquo;s a complete idiot.</p>
<p>But it&rsquo;s a brilliant idiot.</p>
<p>You don&rsquo;t think GPT20 will understand language?</p>
<p>No, no, no.</p>
<p>So size is not gonna buy you understanding.</p>
<p>And any more than a faster car is gonna get you to Mars.</p>
<p>It&rsquo;s a completely different kind of thing.</p>
<p>I mean, these networks are very cool.</p>
<p>And as an entrepreneur,</p>
<p>I can see many highly valuable uses for them.</p>
<p>And as an artist, I love them, right?</p>
<p>So I mean, we&rsquo;re using our own neural model,</p>
<p>which is along those lines</p>
<p>to control the Philip K. Dick robot now.</p>
<p>And it&rsquo;s amazing to like train a neural model</p>
<p>on the robot Philip K. Dick</p>
<p>and see it come up with like crazed,</p>
<p>stoned philosopher pronouncements,</p>
<p>very much like what Philip K. Dick might&rsquo;ve said, right?</p>
<p>Like these models are super cool.</p>
<p>And I&rsquo;m working with Hanson Robotics now</p>
<p>on using a similar, but more sophisticated one for Sophia,</p>
<p>which we haven&rsquo;t launched yet.</p>
<p>But so I think it&rsquo;s cool.</p>
<p>But no, these are recognizing a large number</p>
<p>of shallow patterns.</p>
<p>They&rsquo;re not forming an abstract representation.</p>
<p>And that&rsquo;s the point I was coming to</p>
<p>when we&rsquo;re looking at grammar induction,</p>
<p>we tried to mine patterns out of the structure</p>
<p>of the transformer network.</p>
<p>And you can, but the patterns aren&rsquo;t what you want.</p>
<p>They&rsquo;re nasty.</p>
<p>So I mean, if you do supervised learning,</p>
<p>if you look at sentences where you know</p>
<p>the correct parts of a sentence,</p>
<p>you can learn a matrix that maps</p>
<p>between the internal representation of the transformer</p>
<p>and the parse of the sentence.</p>
<p>And so then you can actually train something</p>
<p>that will output the sentence parse</p>
<p>from the transformer network&rsquo;s internal state.</p>
<p>And we did this, I think Christopher Manning,</p>
<p>some others have not done this also.</p>
<p>But I mean, what you get is that the representation</p>
<p>is hardly ugly and is scattered all over the network</p>
<p>and doesn&rsquo;t look like the rules of grammar</p>
<p>that you know are the right rules of grammar, right?</p>
<p>It&rsquo;s kind of ugly.</p>
<p>So what we&rsquo;re actually doing is we&rsquo;re using</p>
<p>a symbolic grammar learning algorithm,</p>
<p>but we&rsquo;re using the transformer neural network</p>
<p>as a sentence probability oracle.</p>
<p>So like if you have a rule of grammar</p>
<p>and you aren&rsquo;t sure if it&rsquo;s a correct rule of grammar or not,</p>
<p>you can generate a bunch of sentences</p>
<p>using that rule of grammar</p>
<p>and a bunch of sentences violating that rule of grammar.</p>
<p>And you can see the transformer model</p>
<p>doesn&rsquo;t think the sentences obeying the rule of grammar</p>
<p>are more probable than the sentences</p>
<p>disobeying the rule of grammar.</p>
<p>So in that way, you can use the neural model</p>
<p>as a sense probability oracle</p>
<p>to guide a symbolic grammar learning process.</p>
<p>And that seems to work better than trying to milk</p>
<p>the grammar out of the neural network</p>
<p>that doesn&rsquo;t have it in there.</p>
<p>So I think the thing is these neural nets</p>
<p>are not getting a semantically meaningful representation</p>
<p>internally by and large.</p>
<p>So one line of research is to try to get them to do that.</p>
<p>And InfoGAN was trying to do that.</p>
<p>So like if you look back like two years ago,</p>
<p>there was all these papers on like at Edward,</p>
<p>this probabilistic programming neural net framework</p>
<p>that Google had, which came out of InfoGAN.</p>
<p>So the idea there was like you could train</p>
<p>an InfoGAN neural net model,</p>
<p>which is a generative associative network</p>
<p>to recognize and generate faces.</p>
<p>And the model would automatically learn a variable</p>
<p>for how long the nose is and automatically learn a variable</p>
<p>for how wide the eyes are</p>
<p>or how big the lips are or something, right?</p>
<p>So it automatically learned these variables,</p>
<p>which have a semantic meaning.</p>
<p>So that was a rare case where a neural net</p>
<p>trained with a fairly standard GAN method</p>
<p>was able to actually learn the semantic representation.</p>
<p>So for many years, many of us tried to take that</p>
<p>the next step and get a GAN type neural network</p>
<p>that would have not just a list of semantic latent variables,</p>
<p>but would have say a Bayes net of semantic latent variables</p>
<p>with dependencies between them.</p>
<p>The whole programming framework Edward was made for that.</p>
<p>I mean, no one got it to work, right?</p>
<p>And it could be.</p>
<p>Do you think it&rsquo;s possible?</p>
<p>Yeah, do you think?</p>
<p>I don&rsquo;t know.</p>
<p>It might be that back propagation just won&rsquo;t work for it</p>
<p>because the gradients are too screwed up.</p>
<p>Maybe you could get it to work using CMAES</p>
<p>or some like floating point evolutionary algorithm.</p>
<p>We tried, we didn&rsquo;t get it to work.</p>
<p>Eventually we just paused that rather than gave it up.</p>
<p>We paused that and said, well, okay, let&rsquo;s try</p>
<p>more innovative ways to learn implicit,</p>
<p>to learn what are the representations implicit</p>
<p>in that network without trying to make it grow</p>
<p>inside that network.</p>
<p>And I described how we&rsquo;re doing that in language.</p>
<p>You can do similar things in vision, right?</p>
<p>So what?</p>
<p>Use it as an oracle.</p>
<p>Yeah, yeah, yeah.</p>
<p>So you can, that&rsquo;s one way is that you use</p>
<p>a structure learning algorithm, which is symbolic.</p>
<p>And then you use the deep neural net as an oracle</p>
<p>to guide the structure learning algorithm.</p>
<p>The other way to do it is like Infogam was trying to do</p>
<p>and try to tweak the neural network</p>
<p>to have the symbolic representation inside it.</p>
<p>I tend to think what the brain is doing</p>
<p>is more like using the deep neural net type thing</p>
<p>as an oracle.</p>
<p>I think the visual cortex or the cerebellum</p>
<p>are probably learning a non semantically meaningful</p>
<p>opaque tangled representation.</p>
<p>And then when they interface with the more cognitive parts</p>
<p>of the cortex, the cortex is sort of using those</p>
<p>as an oracle and learning the abstract representation.</p>
<p>So if you do sports, say take for example,</p>
<p>serving in tennis, right?</p>
<p>I mean, my tennis serve is okay, not great,</p>
<p>but I learned it by trial and error, right?</p>
<p>And I mean, I learned music by trial and error too.</p>
<p>I just sit down and play, but then if you&rsquo;re an athlete,</p>
<p>which I&rsquo;m not a good athlete,</p>
<p>I mean, then you&rsquo;ll watch videos of yourself serving</p>
<p>and your coach will help you think about what you&rsquo;re doing</p>
<p>and you&rsquo;ll then form a declarative representation,</p>
<p>but your cerebellum maybe didn&rsquo;t have</p>
<p>a declarative representation.</p>
<p>Same way with music, like I will hear something in my head,</p>
<p>I&rsquo;ll sit down and play the thing like I heard it.</p>
<p>And then I will try to study what my fingers did</p>
<p>to see like, what did you just play?</p>
<p>Like how did you do that, right?</p>
<p>Because if you&rsquo;re composing,</p>
<p>you may wanna see how you did it</p>
<p>and then declaratively morph that in some way</p>
<p>that your fingers wouldn&rsquo;t think of, right?</p>
<p>But the physiological movement may come out of some opaque,</p>
<p>like cerebellar reinforcement learned thing, right?</p>
<p>And so that&rsquo;s, I think trying to milk the structure</p>
<p>of a neural net by treating it as an oracle,</p>
<p>maybe more like how your declarative mind post processes</p>
<p>what your visual or motor cortex.</p>
<p>I mean, in vision, it&rsquo;s the same way,</p>
<p>like you can recognize beautiful art</p>
<p>much better than you can say why</p>
<p>you think that piece of art is beautiful.</p>
<p>But if you&rsquo;re trained as an art critic,</p>
<p>you do learn to say why.</p>
<p>And some of it&rsquo;s bullshit, but some of it isn&rsquo;t, right?</p>
<p>Some of it is learning to map sensory knowledge</p>
<p>into declarative and linguistic knowledge,</p>
<p>yet without necessarily making the sensory system itself</p>
<p>use a transparent and an easily communicable representation.</p>
<p>Yeah, that&rsquo;s fascinating to think of neural networks</p>
<p>as like dumb question answers that you can just milk</p>
<p>to build up a knowledge base.</p>
<p>And then it can be multiple networks, I suppose,</p>
<p>from different.</p>
<p>Yeah, yeah, so I think if a group like DeepMind or OpenAI</p>
<p>were to build AGI, and I think DeepMind is like</p>
<p>a thousand times more likely from what I could tell,</p>
<p>because they&rsquo;ve hired a lot of people with broad minds</p>
<p>and many different approaches and angles on AGI,</p>
<p>whereas OpenAI is also awesome,</p>
<p>but I see them as more of like a pure</p>
<p>deep reinforcement learning shop.</p>
<p>Yeah, this time, I got you.</p>
<p>So far. Yeah, there&rsquo;s a lot of,</p>
<p>you&rsquo;re right, I mean, there&rsquo;s so much interdisciplinary</p>
<p>work at DeepMind, like neuroscience.</p>
<p>And you put that together with Google Brain,</p>
<p>which granted they&rsquo;re not working that closely together now,</p>
<p>but my oldest son Zarathustra is doing his PhD</p>
<p>in machine learning applied to automated theorem proving</p>
<p>in Prague under Josef Urban.</p>
<p>So the first paper, DeepMath, which applied deep neural nets</p>
<p>to guide theorem proving was out of Google Brain.</p>
<p>I mean, by now, the automated theorem proving community</p>
<p>is going way, way, way beyond anything Google was doing,</p>
<p>but still, yeah, but anyway,</p>
<p>if that community was gonna make an AGI,</p>
<p>probably one way they would do it was,</p>
<p>take 25 different neural modules,</p>
<p>architected in different ways,</p>
<p>maybe resembling different parts of the brain,</p>
<p>like a basal ganglia model, cerebellum model,</p>
<p>a thalamus module, a few hippocampus models,</p>
<p>number of different models,</p>
<p>representing parts of the cortex, right?</p>
<p>Take all of these and then wire them together</p>
<p>to co train and learn them together like that.</p>
<p>That would be an approach to creating an AGI.</p>
<p>One could implement something like that efficiently</p>
<p>on top of our true AGI, like OpenCog 2.0 system,</p>
<p>once it exists, although obviously Google</p>
<p>has their own highly efficient implementation architecture.</p>
<p>So I think that&rsquo;s a decent way to build AGI.</p>
<p>I was very interested in that in the mid 90s,</p>
<p>but I mean, the knowledge about how the brain works</p>
<p>sort of pissed me off, like it wasn&rsquo;t there yet.</p>
<p>Like, you know, in the hippocampus,</p>
<p>you have these concept neurons,</p>
<p>like the so called grandmother neuron,</p>
<p>which everyone laughed at it, it&rsquo;s actually there.</p>
<p>Like I have some Lex Friedman neurons</p>
<p>that fire differentially when I see you</p>
<p>and not when I see any other person, right?</p>
<p>So how do these Lex Friedman neurons,</p>
<p>how do they coordinate with the distributed representation</p>
<p>of Lex Friedman I have in my cortex, right?</p>
<p>There&rsquo;s some back and forth between cortex and hippocampus</p>
<p>that lets these discrete symbolic representations</p>
<p>in hippocampus correlate and cooperate</p>
<p>with the distributed representations in cortex.</p>
<p>This probably has to do with how the brain</p>
<p>does its version of abstraction and quantifier logic, right?</p>
<p>Like you can have a single neuron in the hippocampus</p>
<p>that activates a whole distributed activation pattern</p>
<p>in cortex, well, this may be how the brain does</p>
<p>like symbolization and abstraction</p>
<p>as in functional programming or something,</p>
<p>but we can&rsquo;t measure it.</p>
<p>Like we don&rsquo;t have enough electrodes stuck</p>
<p>between the cortex and the hippocampus</p>
<p>in any known experiment to measure it.</p>
<p>So I got frustrated with that direction,</p>
<p>not because it&rsquo;s impossible.</p>
<p>Because we just don&rsquo;t understand enough yet.</p>
<p>Of course, it&rsquo;s a valid research direction.</p>
<p>You can try to understand more and more.</p>
<p>And we are measuring more and more</p>
<p>about what happens in the brain now than ever before.</p>
<p>So it&rsquo;s quite interesting.</p>
<p>On the other hand, I sort of got more</p>
<p>of an engineering mindset about AGI.</p>
<p>I&rsquo;m like, well, okay,</p>
<p>we don&rsquo;t know how the brain works that well.</p>
<p>We don&rsquo;t know how birds fly that well yet either.</p>
<p>We have no idea how a hummingbird flies</p>
<p>in terms of the aerodynamics of it.</p>
<p>On the other hand, we know basic principles</p>
<p>of like flapping and pushing the air down.</p>
<p>And we know the basic principles</p>
<p>of how the different parts of the brain work.</p>
<p>So let&rsquo;s take those basic principles</p>
<p>and engineer something that embodies those basic principles,</p>
<p>but is well designed for the hardware</p>
<p>that we have on hand right now.</p>
<p>So do you think we can create AGI</p>
<p>before we understand how the brain works?</p>
<p>I think that&rsquo;s probably what will happen.</p>
<p>And maybe the AGI will help us do better brain imaging</p>
<p>that will then let us build artificial humans,</p>
<p>which is very, very interesting to us</p>
<p>because we are humans, right?</p>
<p>I mean, building artificial humans is super worthwhile.</p>
<p>I just think it&rsquo;s probably not the shortest path to AGI.</p>
<p>So it&rsquo;s fascinating idea that we would build AGI</p>
<p>to help us understand ourselves.</p>
<p>A lot of people ask me if the young people</p>
<p>interested in doing artificial intelligence,</p>
<p>they look at sort of doing graduate level, even undergrads,</p>
<p>but graduate level research and they see</p>
<p>whether the artificial intelligence community stands now,</p>
<p>it&rsquo;s not really AGI type research for the most part.</p>
<p>So the natural question they ask is</p>
<p>what advice would you give?</p>
<p>I mean, maybe I could ask if people were interested</p>
<p>in working on OpenCog or in some kind of direct</p>
<p>or indirect connection to OpenCog or AGI research,</p>
<p>what would you recommend?</p>
<p>OpenCog, first of all, is open source project.</p>
<p>There&rsquo;s a Google group discussion list.</p>
<p>There&rsquo;s a GitHub repository.</p>
<p>So if anyone&rsquo;s interested in lending a hand</p>
<p>with that aspect of AGI,</p>
<p>introduce yourself on the OpenCog email list.</p>
<p>And there&rsquo;s a Slack as well.</p>
<p>I mean, we&rsquo;re certainly interested to have inputs</p>
<p>into our redesign process for a new version of OpenCog,</p>
<p>but also we&rsquo;re doing a lot of very interesting research.</p>
<p>I mean, we&rsquo;re working on data analysis</p>
<p>for COVID clinical trials.</p>
<p>We&rsquo;re working with Hanson Robotics.</p>
<p>We&rsquo;re doing a lot of cool things</p>
<p>with the current version of OpenCog now.</p>
<p>So there&rsquo;s certainly opportunity to jump into OpenCog</p>
<p>or various other open source AGI oriented projects.</p>
<p>So would you say there&rsquo;s like masters</p>
<p>and PhD theses in there?</p>
<p>Plenty, yeah, plenty, of course.</p>
<p>I mean, the challenge is to find a supervisor</p>
<p>who wants to foster that sort of research,</p>
<p>but it&rsquo;s way easier than it was when I got my PhD, right?</p>
<p>It&rsquo;s okay, great.</p>
<p>We talked about OpenCog, which is kind of one,</p>
<p>the software framework,</p>
<p>but also the actual attempt to build an AGI system.</p>
<p>And then there is this exciting idea of SingularityNet.</p>
<p>So maybe can you say first what is SingularityNet?</p>
<p>Sure, sure.</p>
<p>SingularityNet is a platform</p>
<p>for realizing a decentralized network</p>
<p>of artificial intelligences.</p>
<p>So Marvin Minsky, the AI pioneer who I knew a little bit,</p>
<p>he had the idea of a society of minds,</p>
<p>like you should achieve an AI</p>
<p>not by writing one algorithm or one program,</p>
<p>but you should put a bunch of different AIs out there</p>
<p>and the different AIs will interact with each other,</p>
<p>each playing their own role.</p>
<p>And then the totality of the society of AIs</p>
<p>would be the thing</p>
<p>that displayed the human level intelligence.</p>
<p>And I had, when he was alive,</p>
<p>I had many debates with Marvin about this idea.</p>
<p>And I think he really thought the mind</p>
<p>was more like a society than I do.</p>
<p>Like I think you could have a mind</p>
<p>that was as disorganized as a human society,</p>
<p>but I think a human like mind</p>
<p>has a bit more central control than that actually.</p>
<p>Like, I mean, we have this thalamus</p>
<p>and the medulla and limbic system.</p>
<p>We have a sort of top down control system</p>
<p>that guides much of what we do,</p>
<p>more so than a society does.</p>
<p>So I think he stretched that metaphor a little too far,</p>
<p>but I also think there&rsquo;s something interesting there.</p>
<p>And so in the 90s,</p>
<p>when I started my first sort of nonacademic AI project,</p>
<p>WebMind, which was an AI startup in New York</p>
<p>in the Silicon Alley area in the late 90s,</p>
<p>what I was aiming to do there</p>
<p>was make a distributed society of AIs,</p>
<p>the different parts of which would live</p>
<p>on different computers all around the world.</p>
<p>And each one would do its own thinking</p>
<p>about the data local to it,</p>
<p>but they would all share information with each other</p>
<p>and outsource work with each other and cooperate.</p>
<p>And the intelligence would be in the whole collective.</p>
<p>And I organized a conference together with Francis Heiligen</p>
<p>at Free University of Brussels in 2001,</p>
<p>which was the Global Brain Zero Conference.</p>
<p>And we&rsquo;re planning the next version,</p>
<p>the Global Brain One Conference</p>
<p>at the Free University of Brussels for next year, 2021.</p>
<p>So 20 years after.</p>
<p>And then maybe we can have the next one 10 years after that,</p>
<p>like exponentially faster until the singularity comes, right?</p>
<p>The timing is right, yeah.</p>
<p>Yeah, yeah, exactly.</p>
<p>So yeah, the idea with the Global Brain</p>
<p>was maybe the AI won&rsquo;t just be in a program</p>
<p>on one guy&rsquo;s computer,</p>
<p>but the AI will be in the internet as a whole</p>
<p>with the cooperation of different AI modules</p>
<p>living in different places.</p>
<p>So one of the issues you face</p>
<p>when architecting a system like that</p>
<p>is, you know, how is the whole thing controlled?</p>
<p>Do you have like a centralized control unit</p>
<p>that pulls the puppet strings</p>
<p>of all the different modules there?</p>
<p>Or do you have a fundamentally decentralized network</p>
<p>where the society of AIs is controlled</p>
<p>in some democratic and self organized way,</p>
<p>but all the AIs in that society, right?</p>
<p>And Francis and I had different view of many things,</p>
<p>but we both wanted to make like a global society</p>
<p>of AI minds with a decentralized organizational mode.</p>
<p>Now, the main difference was he wanted the individual AIs</p>
<p>to be all incredibly simple</p>
<p>and all the intelligence to be on the collective level.</p>
<p>Whereas I thought that was cool,</p>
<p>but I thought a more practical way to do it might be</p>
<p>if some of the agents in the society of minds</p>
<p>were fairly generally intelligent on their own.</p>
<p>So like you could have a bunch of open cogs out there</p>
<p>and a bunch of simpler learning systems.</p>
<p>And then these are all cooperating, coordinating together</p>
<p>sort of like in the brain.</p>
<p>Okay, the brain as a whole is the general intelligence,</p>
<p>but some parts of the cortex,</p>
<p>you could say have a fair bit of general intelligence</p>
<p>on their own,</p>
<p>whereas say parts of the cerebellum or limbic system</p>
<p>have very little general intelligence on their own.</p>
<p>And they&rsquo;re contributing to general intelligence</p>
<p>by way of their connectivity to other modules.</p>
<p>Do you see instantiations of the same kind of,</p>
<p>maybe different versions of open cog,</p>
<p>but also just the same version of open cog</p>
<p>and maybe many instantiations of it as being all parts of it?</p>
<p>That&rsquo;s what David and Hans and I want to do</p>
<p>with many Sophia and other robots.</p>
<p>Each one has its own individual mind living on the server,</p>
<p>but there&rsquo;s also a collective intelligence infusing them</p>
<p>and a part of the mind living on the edge in each robot.</p>
<p>So the thing is at that time,</p>
<p>as well as WebMind being implemented in Java 1.1</p>
<p>as like a massive distributed system,</p>
<p>blockchain wasn&rsquo;t there yet.</p>
<p>So had them do this decentralized control.</p>
<p>We sort of knew it.</p>
<p>We knew about distributed systems.</p>
<p>We knew about encryption.</p>
<p>So I mean, we had the key principles</p>
<p>of what underlies blockchain now,</p>
<p>but I mean, we didn&rsquo;t put it together</p>
<p>in the way that it&rsquo;s been done now.</p>
<p>So when Vitalik Buterin and colleagues</p>
<p>came out with Ethereum blockchain,</p>
<p>many, many years later, like 2013 or something,</p>
<p>then I was like, well, this is interesting.</p>
<p>Like this is solidity scripting language.</p>
<p>It&rsquo;s kind of dorky in a way.</p>
<p>And I don&rsquo;t see why you need to turn complete language</p>
<p>for this purpose.</p>
<p>But on the other hand,</p>
<p>this is like the first time I could sit down</p>
<p>and start to like script infrastructure</p>
<p>for decentralized control of the AIs</p>
<p>in this society of minds in a tractable way.</p>
<p>Like you can hack the Bitcoin code base,</p>
<p>but it&rsquo;s really annoying.</p>
<p>Whereas solidity is Ethereum scripting language</p>
<p>is just nicer and easier to use.</p>
<p>I&rsquo;m very annoyed with it by this point.</p>
<p>But like Java, I mean, these languages are amazing</p>
<p>when they first come out.</p>
<p>So then I came up with the idea</p>
<p>that turned into SingularityNet.</p>
<p>Okay, let&rsquo;s make a decentralized agent system</p>
<p>where a bunch of different AIs,</p>
<p>wrapped up in say different Docker containers</p>
<p>or LXC containers,</p>
<p>different AIs can each of them have their own identity</p>
<p>on the blockchain.</p>
<p>And the coordination of this community of AIs</p>
<p>has no central controller, no dictator, right?</p>
<p>And there&rsquo;s no central repository of information.</p>
<p>The coordination of the society of minds</p>
<p>is done entirely by the decentralized network</p>
<p>in a decentralized way by the algorithms, right?</p>
<p>Because the model of Bitcoin is in math we trust, right?</p>
<p>And so that&rsquo;s what you need.</p>
<p>You need the society of minds to trust only in math,</p>
<p>not trust only in one centralized server.</p>
<p>So the AI systems themselves are outside of the blockchain,</p>
<p>but then the communication between them.</p>
<p>At the moment, yeah, yeah.</p>
<p>I would have loved to put the AI&rsquo;s operations on chain</p>
<p>in some sense, but in Ethereum, it&rsquo;s just too slow.</p>
<p>You can&rsquo;t do it.</p>
<p>Somehow it&rsquo;s the basic communication between AI systems.</p>
<p>That&rsquo;s the distribution.</p>
<p>Basically an AI is just some software in singularity.</p>
<p>An AI is just some software process living in a container.</p>
<p>And there&rsquo;s a proxy that lives in that container</p>
<p>along with the AI that handles the interaction</p>
<p>with the rest of singularity net.</p>
<p>And then when one AI wants to contribute</p>
<p>with another one in the network,</p>
<p>they set up a number of channels.</p>
<p>And the setup of those channels uses the Ethereum blockchain.</p>
<p>Once the channels are set up,</p>
<p>then data flows along those channels</p>
<p>without having to be on the blockchain.</p>
<p>All that goes on the blockchain is the fact</p>
<p>that some data went along that channel.</p>
<p>So you can do&hellip;</p>
<p>So there&rsquo;s not a shared knowledge.</p>
<p>Well, the identity of each agent is on the blockchain,</p>
<p>on the Ethereum blockchain.</p>
<p>If one agent rates the reputation of another agent,</p>
<p>that goes on the blockchain.</p>
<p>And agents can publish what APIs they will fulfill</p>
<p>on the blockchain.</p>
<p>But the actual data for AI and the results for AI</p>
<p>is not on the blockchain.</p>
<p>Do you think it could be?</p>
<p>Do you think it should be?</p>
<p>In some cases, it should be.</p>
<p>In some cases, maybe it shouldn&rsquo;t be.</p>
<p>But I mean, I think that&hellip;</p>
<p>So I&rsquo;ll give you an example.</p>
<p>Using Ethereum, you can&rsquo;t do it.</p>
<p>Using now, there&rsquo;s more modern and faster blockchains</p>
<p>where you could start to do that in some cases.</p>
<p>Two years ago, that was less so.</p>
<p>It&rsquo;s a very rapidly evolving ecosystem.</p>
<p>So like one example, maybe you can comment on</p>
<p>something I worked a lot on is autonomous vehicles.</p>
<p>You can see each individual vehicle as an AI system.</p>
<p>And you can see vehicles from Tesla, for example,</p>
<p>and then Ford and GM and all these as also like larger&hellip;</p>
<p>I mean, they all are running the same kind of system</p>
<p>on each sets of vehicles.</p>
<p>So it&rsquo;s individual AI systems and individual vehicles,</p>
<p>but it&rsquo;s all different.</p>
<p>The station is the same AI system within the same company.</p>
<p>So you can envision a situation where all of those AI systems</p>
<p>are put on SingularityNet, right?</p>
<p>And how do you see that happening?</p>
<p>And what would be the benefit?</p>
<p>And could they share data?</p>
<p>I guess one of the biggest things is that the power there&rsquo;s</p>
<p>in a decentralized control, but the benefit would have been,</p>
<p>is really nice if they can somehow share the knowledge</p>
<p>in an open way if they choose to.</p>
<p>Yeah, yeah, yeah, those are all quite good points.</p>
<p>So I think the benefit from being on the decentralized network</p>
<p>as we envision it is that we want the AIs in the network</p>
<p>to be outsourcing work to each other</p>
<p>and making API calls to each other frequently.</p>
<p>So the real benefit would be if that AI wanted to outsource</p>
<p>some cognitive processing or data processing</p>
<p>or data pre processing, whatever,</p>
<p>to some other AIs in the network,</p>
<p>which specialize in something different.</p>
<p>And this really requires a different way of thinking</p>
<p>about AI software development, right?</p>
<p>So just like object oriented programming</p>
<p>was different than imperative programming.</p>
<p>And now object oriented programmers all use these</p>
<p>frameworks to do things rather than just libraries even.</p>
<p>You know, shifting to agent based programming</p>
<p>where AI agent is asking other like live real time</p>
<p>evolving agents for feedback and what they&rsquo;re doing.</p>
<p>That&rsquo;s a different way of thinking.</p>
<p>I mean, it&rsquo;s not a new one.</p>
<p>There was loads of papers on agent based programming</p>
<p>in the 80s and onward.</p>
<p>But if you&rsquo;re willing to shift to an agent based model</p>
<p>of development, then you can put less and less in your AI</p>
<p>and rely more and more on interactive calls</p>
<p>to other AIs running in the network.</p>
<p>And of course, that&rsquo;s not fully manifested yet</p>
<p>because although we&rsquo;ve rolled out a nice working version</p>
<p>of SingularityNet platform,</p>
<p>there&rsquo;s only 50 to 100 AIs running in there now.</p>
<p>There&rsquo;s not tens of thousands of AIs.</p>
<p>So we don&rsquo;t have the critical mass</p>
<p>for the whole society of mind to be doing</p>
<p>what we want to do.</p>
<p>Yeah, the magic really happens</p>
<p>when there&rsquo;s just a huge number of agents.</p>
<p>Yeah, yeah, exactly.</p>
<p>In terms of data, we&rsquo;re partnering closely</p>
<p>with another blockchain project called Ocean Protocol.</p>
<p>And Ocean Protocol, that&rsquo;s the project of Trent McConnachie</p>
<p>who developed BigchainDB,</p>
<p>which is a blockchain based database.</p>
<p>So Ocean Protocol is basically blockchain based big data</p>
<p>and aims at making it efficient for different AI processes</p>
<p>or statistical processes or whatever</p>
<p>to share large data sets.</p>
<p>Or if one process can send a clone of itself</p>
<p>to work on the other guy&rsquo;s data set</p>
<p>and send results back and so forth.</p>
<p>So by getting Ocean and you have data lake,</p>
<p>so this is the data ocean, right?</p>
<p>So again, by getting Ocean and SingularityNet</p>
<p>to interoperate, we&rsquo;re aiming to take into account</p>
<p>the big data aspect also.</p>
<p>But it&rsquo;s quite challenging</p>
<p>because to build this whole decentralized</p>
<p>blockchain based infrastructure,</p>
<p>I mean, your competitors are like Google, Microsoft,</p>
<p>Alibaba and Amazon, which have so much money</p>
<p>to put behind their centralized infrastructures,</p>
<p>plus they&rsquo;re solving simpler algorithmic problems</p>
<p>because making it centralized in some ways is easier, right?</p>
<p>So they&rsquo;re very major computer science challenges.</p>
<p>And I think what you saw with the whole ICO boom</p>
<p>in the blockchain and cryptocurrency world</p>
<p>is a lot of young hackers who were hacking Bitcoin</p>
<p>or Ethereum, and they see, well,</p>
<p>why don&rsquo;t we make this decentralized on blockchain?</p>
<p>Then after they raised some money through an ICO,</p>
<p>they realize how hard it is.</p>
<p>And it&rsquo;s like, actually we&rsquo;re wrestling</p>
<p>with incredibly hard computer science</p>
<p>and software engineering and distributed systems problems,</p>
<p>which can be solved, but they&rsquo;re just very difficult</p>
<p>to solve.</p>
<p>And in some cases, the individuals who started</p>
<p>those projects were not well equipped</p>
<p>to actually solve the problems that they wanted to solve.</p>
<p>So you think, would you say that&rsquo;s the main bottleneck?</p>
<p>If you look at the future of currency,</p>
<p>the question is, well&hellip;</p>
<p>Currency, the main bottleneck is politics.</p>
<p>It&rsquo;s governments and the bands of armed thugs</p>
<p>that will shoot you if you bypass their currency restriction.</p>
<p>That&rsquo;s right.</p>
<p>So like your sense is that versus the technical challenges,</p>
<p>because you kind of just suggested</p>
<p>the technical challenges are quite high as well.</p>
<p>I mean, for making a distributed money,</p>
<p>you could do that on Algorand right now.</p>
<p>I mean, so that while Ethereum is too slow,</p>
<p>there&rsquo;s Algorand and there&rsquo;s a few other more modern,</p>
<p>more scalable blockchains that would work fine</p>
<p>for a decentralized global currency.</p>
<p>So I think there were technical bottlenecks</p>
<p>to that two years ago.</p>
<p>And maybe Ethereum 2.0 will be as fast as Algorand.</p>
<p>I don&rsquo;t know, that&rsquo;s not fully written yet, right?</p>
<p>So I think the obstacle to currency</p>
<p>being put on the blockchain is that&hellip;</p>
<p>Is the other stuff you mentioned.</p>
<p>I mean, currency will be on the blockchain.</p>
<p>It&rsquo;ll just be on the blockchain in a way</p>
<p>that enforces centralized control</p>
<p>and government hedge money rather than otherwise.</p>
<p>Like the ERNB will probably be the first global,</p>
<p>the first currency on the blockchain.</p>
<p>The EURUBIL maybe next.</p>
<p>There are any&hellip;</p>
<p>EURUBIL?</p>
<p>Yeah, yeah, yeah.</p>
<p>I mean, the point is&hellip;</p>
<p>Oh, that&rsquo;s hilarious.</p>
<p>Digital currency, you know, makes total sense,</p>
<p>but they would rather do it in the way</p>
<p>that Putin and Xi Jinping have access</p>
<p>to the global keys for everything, right?</p>
<p>So, and then the analogy to that in terms of SingularityNet,</p>
<p>I mean, there&rsquo;s Echoes.</p>
<p>I think you&rsquo;ve mentioned before that Linux gives you hope.</p>
<p>AI is not as heavily regulated as money, right?</p>
<p>Not yet, right?</p>
<p>Not yet.</p>
<p>Oh, that&rsquo;s a lot slipperier than money too, right?</p>
<p>I mean, money is easier to regulate</p>
<p>because it&rsquo;s kind of easier to define,</p>
<p>whereas AI is, it&rsquo;s almost everywhere inside everything.</p>
<p>Where&rsquo;s the boundary between AI and software, right?</p>
<p>I mean, if you&rsquo;re gonna regulate AI,</p>
<p>there&rsquo;s no IQ test for every hardware device</p>
<p>that has a learning algorithm.</p>
<p>You&rsquo;re gonna be putting like hegemonic regulation</p>
<p>on all software.</p>
<p>And I don&rsquo;t rule out that that can happen.</p>
<p>And the adaptive software.</p>
<p>Yeah, but how do you tell if a software is adaptive</p>
<p>and what, every software is gonna be adaptive, I mean.</p>
<p>Or maybe they, maybe the, you know,</p>
<p>maybe we&rsquo;re living in the golden age of open source</p>
<p>that will not always be open.</p>
<p>Maybe it&rsquo;ll become centralized control</p>
<p>of software by governments.</p>
<p>It is entirely possible.</p>
<p>And part of what I think we&rsquo;re doing</p>
<p>with things like SingularityNet protocol</p>
<p>is creating a tool set that can be used</p>
<p>to counteract that sort of thing.</p>
<p>Say a similar thing about mesh networking, right?</p>
<p>Plays a minor role now, the ability to access internet</p>
<p>like directly phone to phone.</p>
<p>On the other hand, if your government starts trying</p>
<p>to control your use of the internet,</p>
<p>suddenly having mesh networking there</p>
<p>can be very convenient, right?</p>
<p>And so right now, something like a decentralized</p>
<p>blockchain based AGI framework or narrow AI framework,</p>
<p>it&rsquo;s cool, it&rsquo;s nice to have.</p>
<p>On the other hand, if governments start trying</p>
<p>to tap down on my AI interoperating</p>
<p>with someone&rsquo;s AI in Russia or somewhere, right?</p>
<p>Then suddenly having a decentralized protocol</p>
<p>that nobody owns or controls</p>
<p>becomes an extremely valuable part of the tool set.</p>
<p>And, you know, we&rsquo;ve put that out there now.</p>
<p>It&rsquo;s not perfect, but it operates.</p>
<p>And, you know, it&rsquo;s pretty blockchain agnostic.</p>
<p>So we&rsquo;re talking to Algorand about making part</p>
<p>of SingularityNet run on Algorand.</p>
<p>My good friend Tufi Saliba has a cool blockchain project</p>
<p>called Toda, which is a blockchain</p>
<p>without a distributed ledger.</p>
<p>It&rsquo;s like a whole other architecture.</p>
<p>So there&rsquo;s a lot of more advanced things you can do</p>
<p>in the blockchain world.</p>
<p>SingularityNet could be ported to a whole bunch of,</p>
<p>it could be made multi chain important</p>
<p>to a whole bunch of different blockchains.</p>
<p>And there&rsquo;s a lot of potential and a lot of importance</p>
<p>to putting this kind of tool set out there.</p>
<p>If you compare to OpenCog, what you could see is</p>
<p>OpenCog allows tight integration of a few AI algorithms</p>
<p>that share the same knowledge store in real time, in RAM.</p>
<p>SingularityNet allows loose integration</p>
<p>of multiple different AIs.</p>
<p>They can share knowledge, but they&rsquo;re mostly not gonna</p>
<p>be sharing knowledge in RAM on the same machine.</p>
<p>And I think what we&rsquo;re gonna have is a network</p>
<p>of network of networks, right?</p>
<p>Like, I mean, you have the knowledge graph</p>
<p>inside the OpenCog system,</p>
<p>and then you have a network of machines</p>
<p>inside a distributed OpenCog mind,</p>
<p>but then that OpenCog will interface with other AIs</p>
<p>doing deep neural nets or custom biology data analysis</p>
<p>or whatever they&rsquo;re doing in SingularityNet,</p>
<p>which is a looser integration of different AIs,</p>
<p>some of which may be their own networks, right?</p>
<p>And I think at a very loose analogy,</p>
<p>you could see that in the human body.</p>
<p>Like the brain has regions like cortex or hippocampus,</p>
<p>which tightly interconnects like cortical columns</p>
<p>within the cortex, for example.</p>
<p>Then there&rsquo;s looser connection</p>
<p>within the different lobes of the brain,</p>
<p>and then the brain interconnects with the endocrine system</p>
<p>and different parts of the body even more loosely.</p>
<p>Then your body interacts even more loosely</p>
<p>with the other people that you talk to.</p>
<p>So you often have networks within networks within networks</p>
<p>with progressively looser coupling</p>
<p>as you get higher up in that hierarchy.</p>
<p>I mean, you have that in biology,</p>
<p>you have that in the internet as a just networking medium.</p>
<p>And I think that&rsquo;s what we&rsquo;re gonna have</p>
<p>in the network of software processes leading to AGI.</p>
<p>That&rsquo;s a beautiful way to see the world.</p>
<p>Again, the same similar question is with OpenCog.</p>
<p>If somebody wanted to build an AI system</p>
<p>and plug into the SingularityNet,</p>
<p>what would you recommend?</p>
<p>Yeah, so that&rsquo;s much easier.</p>
<p>I mean, OpenCog is still a research system.</p>
<p>So it takes some expertise to, and sometimes,</p>
<p>we have tutorials, but it&rsquo;s somewhat cognitively</p>
<p>labor intensive to get up to speed on OpenCog.</p>
<p>And I mean, what&rsquo;s one of the things we hope to change</p>
<p>with the true AGI OpenCog 2.0 version</p>
<p>is just make the learning curve more similar</p>
<p>to TensorFlow or Torch or something.</p>
<p>Right now, OpenCog is amazingly powerful,</p>
<p>but not simple to deal with.</p>
<p>On the other hand, SingularityNet,</p>
<p>as an open platform was developed a little more</p>
<p>with usability in mind over the blockchain,</p>
<p>it&rsquo;s still kind of a pain.</p>
<p>So I mean, if you&rsquo;re a command line guy,</p>
<p>there&rsquo;s a command line interface.</p>
<p>It&rsquo;s quite easy to take any AI that has an API</p>
<p>and lives in a Docker container and put it online anywhere.</p>
<p>And then it joins the global SingularityNet.</p>
<p>And anyone who puts a request for services</p>
<p>out into the SingularityNet,</p>
<p>the peer to peer discovery mechanism will find</p>
<p>your AI and if it does what was asked,</p>
<p>it can then start a conversation with your AI</p>
<p>about whether it wants to ask your AI to do something for it,</p>
<p>how much it would cost and so on.</p>
<p>So that&rsquo;s fairly simple.</p>
<p>If you wrote an AI and want it listed</p>
<p>on like official SingularityNet marketplace,</p>
<p>which is on our website,</p>
<p>then we have a publisher portal</p>
<p>and then there&rsquo;s a KYC process to go through</p>
<p>because then we have some legal liability</p>
<p>for what goes on that website.</p>
<p>So in a way that&rsquo;s been an education too.</p>
<p>There&rsquo;s sort of two layers.</p>
<p>Like there&rsquo;s the open decentralized protocol.</p>
<p>And there&rsquo;s the market.</p>
<p>Yeah, anyone can use the open decentralized protocol.</p>
<p>So say some developers from Iran</p>
<p>and there&rsquo;s brilliant AI guys</p>
<p>in University of Isfahan in Tehran,</p>
<p>they can put their stuff on SingularityNet protocol</p>
<p>and just like they can put something on the internet, right?</p>
<p>I don&rsquo;t control it.</p>
<p>But if we&rsquo;re gonna list something</p>
<p>on the SingularityNet marketplace</p>
<p>and put a little picture and a link to it,</p>
<p>then if I put some Iranian AI geniuses code on there,</p>
<p>then Donald Trump can send a bunch of jackbooted thugs</p>
<p>to my house to arrest me for doing business with Iran, right?</p>
<p>So, I mean, we already see in some ways</p>
<p>the value of having a decentralized protocol</p>
<p>because what I hope is that someone in Iran</p>
<p>will put online an Iranian SingularityNet marketplace, right?</p>
<p>Which you can pay in the cryptographic token,</p>
<p>which is not owned by any country.</p>
<p>And then if you&rsquo;re in like Congo or somewhere</p>
<p>that doesn&rsquo;t have any problem with Iran,</p>
<p>you can subcontract AI services</p>
<p>that you find on that marketplace, right?</p>
<p>Even though US citizens can&rsquo;t by US law.</p>
<p>So right now, that&rsquo;s kind of a minor point.</p>
<p>As you alluded, if regulations go in the wrong direction,</p>
<p>it could become more of a major point.</p>
<p>But I think it also is the case</p>
<p>that having these workarounds to regulations in place</p>
<p>is a defense mechanism against those regulations</p>
<p>being put into place.</p>
<p>And you can see that in the music industry, right?</p>
<p>I mean, Napster just happened and BitTorrent just happened.</p>
<p>And now most people in my kid&rsquo;s generation,</p>
<p>they&rsquo;re baffled by the idea of paying for music, right?</p>
<p>I mean, my dad pays for music.</p>
<p>I mean, but that because these decentralized mechanisms</p>
<p>happened and then the regulations followed, right?</p>
<p>And the regulations would be very different</p>
<p>if they&rsquo;d been put into place before there was Napster</p>
<p>and BitTorrent and so forth.</p>
<p>So in the same way, we gotta put AI out there</p>
<p>in a decentralized vein and big data out there</p>
<p>in a decentralized vein now,</p>
<p>so that the most advanced AI in the world</p>
<p>is fundamentally decentralized.</p>
<p>And if that&rsquo;s the case, that&rsquo;s just the reality</p>
<p>the regulators have to deal with.</p>
<p>And then as in the music case,</p>
<p>they&rsquo;re gonna come up with regulations</p>
<p>that sort of work with the decentralized reality.</p>
<p>Beautiful.</p>
<p>You are the chief scientist of Hanson Robotics.</p>
<p>You&rsquo;re still involved with Hanson Robotics,</p>
<p>doing a lot of really interesting stuff there.</p>
<p>This is for people who don&rsquo;t know the company</p>
<p>that created Sophia the Robot.</p>
<p>Can you tell me who Sophia is?</p>
<p>I&rsquo;d rather start by telling you who David Hanson is.</p>
<p>Because David is the brilliant mind behind the Sophia Robot.</p>
<p>And he remains, so far, he remains more interesting</p>
<p>than his creation, although she may be improving</p>
<p>faster than he is, actually.</p>
<p>I mean, he&rsquo;s a&hellip;</p>
<p>So yeah, I met David maybe 2007 or something</p>
<p>at some futurist conference we were both speaking at.</p>
<p>And I could see we had a great deal in common.</p>
<p>I mean, we were both kind of crazy,</p>
<p>but we both had a passion for AGI and the singularity.</p>
<p>And we were both huge fans of the work</p>
<p>of Philip K. Dick, the science fiction writer.</p>
<p>And I wanted to create benevolent AGI</p>
<p>that would create massively better life</p>
<p>for all humans and all sentient beings,</p>
<p>including animals, plants, and superhuman beings.</p>
<p>And David, he wanted exactly the same thing,</p>
<p>but he had a different idea of how to do it.</p>
<p>He wanted to get computational compassion.</p>
<p>Like he wanted to get machines that would love people</p>
<p>and empathize with people.</p>
<p>And he thought the way to do that was to make a machine</p>
<p>that could look people eye to eye, face to face,</p>
<p>look at people and make people love the machine,</p>
<p>and the machine loves the people back.</p>
<p>So I thought that was very different way of looking at it</p>
<p>because I&rsquo;m very math oriented.</p>
<p>And I&rsquo;m just thinking like,</p>
<p>what is the abstract cognitive algorithm</p>
<p>that will let the system, you know,</p>
<p>internalize the complex patterns of human values,</p>
<p>blah, blah, blah.</p>
<p>Whereas he&rsquo;s like, look you in the face and the eye</p>
<p>and love you, right?</p>
<p>So we hit it off quite well.</p>
<p>And we talked to each other off and on.</p>
<p>Then I moved to Hong Kong in 2011.</p>
<p>So I&rsquo;ve been living all over the place.</p>
<p>I&rsquo;ve been in Australia and New Zealand in my academic career.</p>
<p>Then in Las Vegas for a while.</p>
<p>Was in New York in the late 90s</p>
<p>starting my entrepreneurial career.</p>
<p>Was in DC for nine years</p>
<p>doing a bunch of US government consulting stuff.</p>
<p>Then moved to Hong Kong in 2011,</p>
<p>mostly because I met a Chinese girl</p>
<p>who I fell in love with and we got married.</p>
<p>She&rsquo;s actually not from Hong Kong.</p>
<p>She&rsquo;s from mainland China,</p>
<p>but we converged together in Hong Kong.</p>
<p>Still married now, I have a two year old baby.</p>
<p>So went to Hong Kong to see about a girl, I guess.</p>
<p>Yeah, pretty much, yeah.</p>
<p>And on the other hand,</p>
<p>I started doing some cool research there</p>
<p>with Gino Yu at Hong Kong Polytechnic University.</p>
<p>I got involved with a project called IDEA</p>
<p>using machine learning for stock and futures prediction,</p>
<p>which was quite interesting.</p>
<p>And I also got to know something</p>
<p>about the consumer electronics</p>
<p>and hardware manufacturer ecosystem in Shenzhen</p>
<p>across the border,</p>
<p>which is like the only place in the world</p>
<p>that makes sense to make complex consumer electronics</p>
<p>at large scale and low cost.</p>
<p>It&rsquo;s just, it&rsquo;s astounding the hardware ecosystem</p>
<p>that you have in South China.</p>
<p>Like US people here cannot imagine what it&rsquo;s like.</p>
<p>So David was starting to explore that also.</p>
<p>I invited him to Hong Kong to give a talk</p>
<p>at Hong Kong PolyU,</p>
<p>and I introduced him in Hong Kong to some investors</p>
<p>who were interested in his robots.</p>
<p>And he didn&rsquo;t have Sophia then,</p>
<p>he had a robot of Philip K. Dick,</p>
<p>our favorite science fiction writer.</p>
<p>He had a robot Einstein,</p>
<p>he had some little toy robots</p>
<p>that looked like his son Zeno.</p>
<p>So through the investors I connected him to,</p>
<p>he managed to get some funding</p>
<p>to basically port Hanson Robotics to Hong Kong.</p>
<p>And when he first moved to Hong Kong,</p>
<p>I was working on AGI research</p>
<p>and also on this machine learning trading project.</p>
<p>So I didn&rsquo;t get that tightly involved</p>
<p>with Hanson Robotics.</p>
<p>But as I hung out with David more and more,</p>
<p>as we were both there in the same place,</p>
<p>I started to get,</p>
<p>I started to think about what you could do</p>
<p>to make his robots smarter than they were.</p>
<p>And so we started working together</p>
<p>and for a few years I was chief scientist</p>
<p>and head of software at Hanson Robotics.</p>
<p>Then when I got deeply into the blockchain side of things,</p>
<p>I stepped back from that and cofounded Singularity Net.</p>
<p>David Hanson was also one of the cofounders</p>
<p>of Singularity Net.</p>
<p>So part of our goal there had been</p>
<p>to make the blockchain based like cloud mind platform</p>
<p>for Sophia and the other Hanson robots.</p>
<p>Sophia would be just one of the robots in Singularity Net.</p>
<p>Yeah, yeah, yeah, exactly.</p>
<p>Sophia, many copies of the Sophia robot</p>
<p>would be among the user interfaces</p>
<p>to the globally distributed Singularity Net cloud mind.</p>
<p>And I mean, David and I talked about that</p>
<p>for quite a while before cofounding Singularity Net.</p>
<p>By the way, in his vision and your vision,</p>
<p>was Sophia tightly coupled to a particular AI system</p>
<p>or was the idea that you can plug,</p>
<p>you could just keep plugging in different AI systems</p>
<p>within the head of it?</p>
<p>David&rsquo;s view was always that Sophia would be a platform,</p>
<p>much like say the Pepper robot is a platform from SoftBank.</p>
<p>Should be a platform with a set of nicely designed APIs</p>
<p>that anyone can use to experiment</p>
<p>with their different AI algorithms on that platform.</p>
<p>And Singularity Net, of course, fits right into that, right?</p>
<p>Because Singularity Net, it&rsquo;s an API marketplace.</p>
<p>So anyone can put their AI on there.</p>
<p>OpenCog is a little bit different.</p>
<p>I mean, David likes it, but I&rsquo;d say it&rsquo;s my thing.</p>
<p>It&rsquo;s not his.</p>
<p>Like David has a little more passion</p>
<p>for biologically based approaches to AI than I do,</p>
<p>which makes sense.</p>
<p>I mean, he&rsquo;s really into human physiology and biology.</p>
<p>He&rsquo;s a character sculptor, right?</p>
<p>So yeah, he&rsquo;s interested in,</p>
<p>but he also worked a lot with rule based</p>
<p>and logic based AI systems too.</p>
<p>So yeah, he&rsquo;s interested in not just Sophia,</p>
<p>but all the Hanson robots as a powerful social</p>
<p>and emotional robotics platform.</p>
<p>And what I saw in Sophia was a way to get AI algorithms</p>
<p>was a way to get AI algorithms out there</p>
<p>in front of a whole lot of different people</p>
<p>in an emotionally compelling way.</p>
<p>And part of my thought was really kind of abstract</p>
<p>connected to AGI ethics.</p>
<p>And many people are concerned AGI is gonna enslave everybody</p>
<p>or turn everybody into computronium</p>
<p>to make extra hard drives for their cognitive engine</p>
<p>or whatever.</p>
<p>And emotionally I&rsquo;m not driven to that sort of paranoia.</p>
<p>I&rsquo;m really just an optimist by nature,</p>
<p>but intellectually I have to assign a non zero probability</p>
<p>to those sorts of nasty outcomes.</p>
<p>Cause if you&rsquo;re making something 10 times as smart as you,</p>
<p>how can you know what it&rsquo;s gonna do?</p>
<p>There&rsquo;s an irreducible uncertainty there</p>
<p>just as my dog can&rsquo;t predict what I&rsquo;m gonna do tomorrow.</p>
<p>So it seemed to me that based on our current state</p>
<p>of knowledge, the best way to bias the AGI as we create</p>
<p>toward benevolence would be to infuse them with love</p>
<p>and compassion the way that we do our own children.</p>
<p>So you want to interact with AIs in the context</p>
<p>of doing compassionate, loving and beneficial things.</p>
<p>And in that way, as your children will learn</p>
<p>by doing compassionate, beneficial,</p>
<p>loving things alongside you.</p>
<p>And that way the AI will learn in practice</p>
<p>what it means to be compassionate, beneficial and loving.</p>
<p>It will get a sort of ingrained intuitive sense of this,</p>
<p>which it can then abstract in its own way</p>
<p>as it gets more and more intelligent.</p>
<p>Now, David saw this the same way.</p>
<p>That&rsquo;s why he came up with the name Sophia,</p>
<p>which means wisdom.</p>
<p>So it seemed to me making these beautiful, loving robots</p>
<p>to be rolled out for beneficial applications</p>
<p>would be the perfect way to roll out early stage AGI systems</p>
<p>so they can learn from people</p>
<p>and not just learn factual knowledge,</p>
<p>but learn human values and ethics from people</p>
<p>while being their home service robots,</p>
<p>their education assistants, their nursing robots.</p>
<p>So that was the grand vision.</p>
<p>Now, if you&rsquo;ve ever worked with robots,</p>
<p>the reality is quite different, right?</p>
<p>Like the first principle is the robot is always broken.</p>
<p>I mean, I worked with robots in the 90s a bunch</p>
<p>when you had to solder them together yourself</p>
<p>and I&rsquo;d put neural nets during reinforcement learning</p>
<p>on like overturned solid ball type robots</p>
<p>and in the 90s when I was a professor.</p>
<p>Things of course advanced a lot, but&hellip;</p>
<p>But the principle still holds.</p>
<p>The principle that the robot&rsquo;s always broken still holds.</p>
<p>Yeah, so faced with the reality of making Sophia do stuff,</p>
<p>many of my robo AGI aspirations were temporarily cast aside.</p>
<p>And I mean, there&rsquo;s just a practical problem</p>
<p>of making this robot interact in a meaningful way</p>
<p>because like, you put nice computer vision on there,</p>
<p>but there&rsquo;s always glare.</p>
<p>And then, or you have a dialogue system,</p>
<p>but at the time I was there,</p>
<p>like no speech to text algorithm could deal</p>
<p>with Hong Kongese people&rsquo;s English accents.</p>
<p>So the speech to text was always bad.</p>
<p>So the robot always sounded stupid</p>
<p>because it wasn&rsquo;t getting the right text, right?</p>
<p>So I started to view that really</p>
<p>as what in software engineering you call a walking skeleton,</p>
<p>which is maybe the wrong metaphor to use for Sophia</p>
<p>or maybe the right one.</p>
<p>I mean, where the walking skeleton is</p>
<p>in software development is</p>
<p>if you&rsquo;re building a complex system, how do you get started?</p>
<p>But one way is to first build part one well,</p>
<p>then build part two well, then build part three well</p>
<p>and so on.</p>
<p>And the other way is you make like a simple version</p>
<p>of the whole system and put something in the place</p>
<p>of every part the whole system will need</p>
<p>so that you have a whole system that does something.</p>
<p>And then you work on improving each part</p>
<p>in the context of that whole integrated system.</p>
<p>So that&rsquo;s what we did on a software level in Sophia.</p>
<p>We made like a walking skeleton software system</p>
<p>where so there&rsquo;s something that sees,</p>
<p>there&rsquo;s something that hears, there&rsquo;s something that moves,</p>
<p>there&rsquo;s something that remembers,</p>
<p>there&rsquo;s something that learns.</p>
<p>You put a simple version of each thing in there</p>
<p>and you connect them all together</p>
<p>so that the system will do its thing.</p>
<p>So there&rsquo;s a lot of AI in there.</p>
<p>There&rsquo;s not any AGI in there.</p>
<p>I mean, there&rsquo;s computer vision to recognize people&rsquo;s faces,</p>
<p>recognize when someone comes in the room and leaves,</p>
<p>trying to recognize whether two people are together or not.</p>
<p>I mean, the dialogue system,</p>
<p>it&rsquo;s a mix of like hand coded rules with deep neural nets</p>
<p>that come up with their own responses.</p>
<p>And there&rsquo;s some attempt to have a narrative structure</p>
<p>and sort of try to pull the conversation</p>
<p>into something with a beginning, middle and end</p>
<p>and this sort of story arc.</p>
<p>So it&rsquo;s&hellip;</p>
<p>I mean, like if you look at the Lobner Prize and the systems</p>
<p>that beat the Turing Test currently,</p>
<p>they&rsquo;re heavily rule based</p>
<p>because like you had said, narrative structure</p>
<p>to create compelling conversations,</p>
<p>you currently, neural networks cannot do that well,</p>
<p>even with Google MENA.</p>
<p>When you actually look at full scale conversations,</p>
<p>it&rsquo;s just not&hellip;</p>
<p>Yeah, this is the thing.</p>
<p>So we&rsquo;ve been, I&rsquo;ve actually been running an experiment</p>
<p>the last couple of weeks taking Sophia&rsquo;s chat bot</p>
<p>and then Facebook&rsquo;s Transformer chat bot,</p>
<p>which they opened the model.</p>
<p>We&rsquo;ve had them chatting to each other</p>
<p>for a number of weeks on the server just&hellip;</p>
<p>That&rsquo;s funny.</p>
<p>We&rsquo;re generating training data of what Sophia says</p>
<p>in a wide variety of conversations.</p>
<p>But we can see, compared to Sophia&rsquo;s current chat bot,</p>
<p>the Facebook deep neural chat bot comes up</p>
<p>with a wider variety of fluent sounding sentences.</p>
<p>On the other hand, it rambles like mad.</p>
<p>The Sophia chat bot, it&rsquo;s a little more repetitive</p>
<p>in the sentence structures it uses.</p>
<p>On the other hand, it&rsquo;s able to keep like a conversation arc</p>
<p>over a much longer, longer period, right?</p>
<p>So there&hellip;</p>
<p>Now, you can probably surmount that using Reformer</p>
<p>and like using various other deep neural architectures</p>
<p>to improve the way these Transformer models are trained.</p>
<p>But in the end, neither one of them really understands</p>
<p>what&rsquo;s going on.</p>
<p>I mean, that&rsquo;s the challenge I had with Sophia</p>
<p>is if I were doing a robotics project aimed at AGI,</p>
<p>I would wanna make like a robo toddler</p>
<p>that was just learning about what it was seeing.</p>
<p>Because then the language is grounded</p>
<p>in the experience of the robot.</p>
<p>But what Sophia needs to do to be Sophia</p>
<p>is talk about sports or the weather or robotics</p>
<p>or the conference she&rsquo;s talking at.</p>
<p>She needs to be fluent talking about</p>
<p>any damn thing in the world.</p>
<p>And she doesn&rsquo;t have grounding for all those things.</p>
<p>So there&rsquo;s this, just like, I mean, Google Mina</p>
<p>and Facebook&rsquo;s chat, but I don&rsquo;t have grounding</p>
<p>for what they&rsquo;re talking about either.</p>
<p>So in a way, the need to speak fluently about things</p>
<p>where there&rsquo;s no nonlinguistic grounding</p>
<p>pushes what you can do for Sophia in the short term</p>
<p>a bit away from AGI.</p>
<p>I mean, it pushes you towards IBM Watson situation</p>
<p>where you basically have to do heuristic</p>
<p>and hard code stuff and rule based stuff.</p>
<p>I have to ask you about this, okay.</p>
<p>So because in part Sophia is like an art creation</p>
<p>because it&rsquo;s beautiful.</p>
<p>She&rsquo;s beautiful because she inspires</p>
<p>through our human nature of anthropomorphize things.</p>
<p>We immediately see an intelligent being there.</p>
<p>Because David is a great sculptor.</p>
<p>He is a great sculptor, that&rsquo;s right.</p>
<p>So in fact, if Sophia just had nothing inside her head,</p>
<p>said nothing, if she just sat there,</p>
<p>we already prescribed some intelligence to her.</p>
<p>There&rsquo;s a long selfie line in front of her</p>
<p>after every talk.</p>
<p>That&rsquo;s right.</p>
<p>So it captivated the imagination of many people.</p>
<p>I wasn&rsquo;t gonna say the world,</p>
<p>but yeah, I mean a lot of people.</p>
<p>Billions of people, which is amazing.</p>
<p>It&rsquo;s amazing, right.</p>
<p>Now, of course, many people have prescribed</p>
<p>essentially AGI type of capabilities to Sophia</p>
<p>when they see her.</p>
<p>And of course, friendly French folk like Yann LeCun</p>
<p>immediately see that of the people from the AI community</p>
<p>and get really frustrated because&hellip;</p>
<p>It&rsquo;s understandable.</p>
<p>So what, and then they criticize people like you</p>
<p>who sit back and don&rsquo;t say anything about,</p>
<p>like basically allow the imagination of the world,</p>
<p>allow the world to continue being captivated.</p>
<p>So what&rsquo;s your sense of that kind of annoyance</p>
<p>that the AI community has?</p>
<p>I think there&rsquo;s several parts to my reaction there.</p>
<p>First of all, if I weren&rsquo;t involved with Hanson and Box</p>
<p>and didn&rsquo;t know David Hanson personally,</p>
<p>I probably would have been very annoyed initially</p>
<p>at Sophia as well.</p>
<p>I mean, I can understand the reaction.</p>
<p>I would have been like, wait,</p>
<p>all these stupid people out there think this is an AGI,</p>
<p>but it&rsquo;s not an AGI, but they&rsquo;re tricking people</p>
<p>that this very cool robot is an AGI.</p>
<p>And now those of us trying to raise funding to build AGI,</p>
<p>people will think it&rsquo;s already there and it already works.</p>
<p>So on the other hand, I think,</p>
<p>even if I weren&rsquo;t directly involved with it,</p>
<p>once I dug a little deeper into David and the robot</p>
<p>and the intentions behind it,</p>
<p>I think I would have stopped being pissed off.</p>
<p>Whereas folks like Yann LeCun have remained pissed off</p>
<p>after their initial reaction.</p>
<p>That&rsquo;s his thing, that&rsquo;s his thing.</p>
<p>I think that in particular struck me as somewhat ironic</p>
<p>because Yann LeCun is working for Facebook,</p>
<p>which is using machine learning to program the brains</p>
<p>of the people in the world toward vapid consumerism</p>
<p>and political extremism.</p>
<p>So if your ethics allows you to use machine learning</p>
<p>in such a blatantly destructive way,</p>
<p>why would your ethics not allow you to use machine learning</p>
<p>to make a lovable theatrical robot</p>
<p>that draws some foolish people</p>
<p>into its theatrical illusion?</p>
<p>Like if the pushback had come from Yoshua Bengio,</p>
<p>I would have felt much more humbled by it</p>
<p>because he&rsquo;s not using AI for blatant evil, right?</p>
<p>On the other hand, he also is a super nice guy</p>
<p>and doesn&rsquo;t bother to go out there</p>
<p>trashing other people&rsquo;s work for no good reason, right?</p>
<p>Shots fired, but I get you.</p>
<p>I mean, that&rsquo;s&hellip;</p>
<p>I mean, if you&rsquo;re gonna ask, I&rsquo;m gonna answer.</p>
<p>No, for sure.</p>
<p>I think we&rsquo;ll go back and forth.</p>
<p>I&rsquo;ll talk to Yann again.</p>
<p>I would add on this though.</p>
<p>I mean, David Hansen is an artist</p>
<p>and he often speaks off the cuff.</p>
<p>And I have not agreed with everything</p>
<p>that David has said or done regarding Sophia.</p>
<p>And David also has not agreed with everything</p>
<p>David has said or done about Sophia.</p>
<p>That&rsquo;s an important point.</p>
<p>I mean, David is an artistic wild man</p>
<p>and that&rsquo;s part of his charm.</p>
<p>That&rsquo;s part of his genius.</p>
<p>So certainly there have been conversations</p>
<p>within Hansen Robotics and between me and David</p>
<p>where I was like, let&rsquo;s be more open</p>
<p>about how this thing is working.</p>
<p>And I did have some influence in nudging Hansen Robotics</p>
<p>to be more open about how Sophia was working.</p>
<p>And David wasn&rsquo;t especially opposed to this.</p>
<p>And he was actually quite right about it.</p>
<p>What he said was, you can tell people exactly</p>
<p>how it&rsquo;s working and they won&rsquo;t care.</p>
<p>They want to be drawn into the illusion.</p>
<p>And he was 100% correct.</p>
<p>I&rsquo;ll tell you what, this wasn&rsquo;t Sophia.</p>
<p>This was Philip K. Dick.</p>
<p>But we did some interactions between humans</p>
<p>and Philip K. Dick robot in Austin, Texas a few years back.</p>
<p>And in this case, the Philip K. Dick was just teleoperated</p>
<p>by another human in the other room.</p>
<p>So during the conversations, we didn&rsquo;t tell people</p>
<p>the robot was teleoperated.</p>
<p>We just said, here, have a conversation with Phil Dick.</p>
<p>We&rsquo;re gonna film you, right?</p>
<p>And they had a great conversation with Philip K. Dick</p>
<p>teleoperated by my friend, Stefan Bugaj.</p>
<p>After the conversation, we brought the people</p>
<p>in the back room to see Stefan</p>
<p>who was controlling the Philip K. Dick robot,</p>
<p>but they didn&rsquo;t believe it.</p>
<p>These people were like, well, yeah,</p>
<p>but I know I was talking to Phil.</p>
<p>Maybe Stefan was typing,</p>
<p>but the spirit of Phil was animating his mind</p>
<p>while he was typing.</p>
<p>So like, even though they knew it was a human in the loop,</p>
<p>even seeing the guy there,</p>
<p>they still believed that was Phil they were talking to.</p>
<p>A small part of me believes that they were right, actually.</p>
<p>Because our understanding&hellip;</p>
<p>Well, we don&rsquo;t understand the universe.</p>
<p>That&rsquo;s the thing.</p>
<p>I mean, there is a cosmic mind field</p>
<p>that we&rsquo;re all embedded in</p>
<p>that yields many strange synchronicities in the world,</p>
<p>which is a topic we don&rsquo;t have time to go into too much here.</p>
<p>Yeah, I mean, there&rsquo;s something to this</p>
<p>where our imagination about Sophia</p>
<p>and people like Yann LeCun being frustrated about it</p>
<p>is all part of this beautiful dance</p>
<p>of creating artificial intelligence</p>
<p>that&rsquo;s almost essential.</p>
<p>You see with Boston Dynamics,</p>
<p>whom I&rsquo;m a huge fan of as well,</p>
<p>you know, the kind of&hellip;</p>
<p>I mean, these robots are very far from intelligent.</p>
<p>I played with their last one, actually.</p>
<p>With a spot mini.</p>
<p>Yeah, very cool.</p>
<p>I mean, it reacts quite in a fluid and flexible way.</p>
<p>But we immediately ascribe the kind of intelligence.</p>
<p>We immediately ascribe AGI to them.</p>
<p>Yeah, yeah, if you kick it and it falls down and goes out,</p>
<p>you feel bad, right?</p>
<p>You can&rsquo;t help it.</p>
<p>And I mean, that&rsquo;s part of&hellip;</p>
<p>That&rsquo;s gonna be part of our journey</p>
<p>in creating intelligent systems</p>
<p>more and more and more and more.</p>
<p>Like, as Sophia starts out with a walking skeleton,</p>
<p>as you add more and more intelligence,</p>
<p>I mean, we&rsquo;re gonna have to deal with this kind of idea.</p>
<p>Absolutely.</p>
<p>And about Sophia, I would say,</p>
<p>I mean, first of all, I have nothing against Yann LeCun.</p>
<p>No, no, this is fun.</p>
<p>This is all for fun.</p>
<p>He&rsquo;s a nice guy.</p>
<p>If he wants to play the media banter game,</p>
<p>I&rsquo;m happy to play him.</p>
<p>He&rsquo;s a good researcher and a good human being.</p>
<p>I&rsquo;d happily work with the guy.</p>
<p>The other thing I was gonna say is,</p>
<p>I have been explicit about how Sophia works</p>
<p>and I&rsquo;ve posted online and what, H Plus Magazine,</p>
<p>an online webzine.</p>
<p>I mean, I posted a moderately detailed article</p>
<p>explaining like, there are three software systems</p>
<p>we&rsquo;ve used inside Sophia.</p>
<p>There&rsquo;s a timeline editor,</p>
<p>which is like a rule based authoring system</p>
<p>where she&rsquo;s really just being an outlet</p>
<p>for what a human scripted.</p>
<p>There&rsquo;s a chat bot,</p>
<p>which has some rule based and some neural aspects.</p>
<p>And then sometimes we&rsquo;ve used OpenCog behind Sophia,</p>
<p>where there&rsquo;s more learning and reasoning.</p>
<p>And the funny thing is,</p>
<p>I can&rsquo;t always tell which system is operating here, right?</p>
<p>I mean, whether she&rsquo;s really learning or thinking,</p>
<p>or just appears to be over a half hour, I could tell,</p>
<p>but over like three or four minutes of interaction,</p>
<p>I could tell.</p>
<p>So even having three systems</p>
<p>that&rsquo;s already sufficiently complex</p>
<p>where you can&rsquo;t really tell right away.</p>
<p>Yeah, the thing is, even if you get up on stage</p>
<p>and tell people how Sophia is working,</p>
<p>and then they talk to her,</p>
<p>they still attribute more agency and consciousness to her</p>
<p>than is really there.</p>
<p>So I think there&rsquo;s a couple of levels of ethical issue there.</p>
<p>One issue is, should you be transparent</p>
<p>about how Sophia is working?</p>
<p>And I think you should,</p>
<p>and I think we have been.</p>
<p>I mean, there&rsquo;s articles online,</p>
<p>there&rsquo;s some TV special that goes through me</p>
<p>explaining the three subsystems behind Sophia.</p>
<p>So the way Sophia works</p>
<p>is out there much more clearly</p>
<p>than how Facebook&rsquo;s AI works or something, right?</p>
<p>I mean, we&rsquo;ve been fairly explicit about it.</p>
<p>The other is, given that telling people how it works</p>
<p>doesn&rsquo;t cause them to not attribute</p>
<p>too much intelligence agency to it anyway,</p>
<p>then should you keep fooling them</p>
<p>when they want to be fooled?</p>
<p>And I mean, the whole media industry</p>
<p>is based on fooling people the way they want to be fooled.</p>
<p>And we are fooling people 100% toward a good end.</p>
<p>I mean, we are playing on people&rsquo;s sense of empathy</p>
<p>and compassion so that we can give them</p>
<p>a good user experience with helpful robots.</p>
<p>And so that we can fill the AI&rsquo;s mind</p>
<p>with love and compassion.</p>
<p>So I&rsquo;ve been talking a lot with Hanson Robotics lately</p>
<p>about collaborations in the area of medical robotics.</p>
<p>And we haven&rsquo;t quite pulled the trigger on a project</p>
<p>in that domain yet, but we may well do so quite soon.</p>
<p>So we&rsquo;ve been talking a lot about robots</p>
<p>can help with elder care, robots can help with kids.</p>
<p>David&rsquo;s done a lot of things with autism therapy</p>
<p>and robots before.</p>
<p>In the COVID era, having a robot</p>
<p>that can be a nursing assistant in various senses</p>
<p>can be quite valuable.</p>
<p>The robots don&rsquo;t spread infection</p>
<p>and they can also deliver more attention</p>
<p>than human nurses can give, right?</p>
<p>So if you have a robot that&rsquo;s helping a patient</p>
<p>with COVID, if that patient attributes more understanding</p>
<p>and compassion and agency to that robot than it really has</p>
<p>because it looks like a human, I mean, is that really bad?</p>
<p>I mean, we can tell them it doesn&rsquo;t fully understand you</p>
<p>and they don&rsquo;t care because they&rsquo;re lying there</p>
<p>with a fever and they&rsquo;re sick,</p>
<p>but they&rsquo;ll react better to that robot</p>
<p>with its loving, warm facial expression</p>
<p>than they would to a pepper robot</p>
<p>or a metallic looking robot.</p>
<p>So it&rsquo;s really, it&rsquo;s about how you use it, right?</p>
<p>If you made a human looking like door to door sales robot</p>
<p>that used its human looking appearance</p>
<p>to scam people out of their money,</p>
<p>then you&rsquo;re using that connection in a bad way,</p>
<p>but you could also use it in a good way.</p>
<p>But then that&rsquo;s the same problem with every technology.</p>
<p>Beautifully put.</p>
<p>So like you said, we&rsquo;re living in the era</p>
<p>of the COVID, this is 2020,</p>
<p>one of the craziest years in recent history.</p>
<p>So if we zoom out and look at this pandemic,</p>
<p>the coronavirus pandemic,</p>
<p>maybe let me ask you this kind of thing in viruses in general,</p>
<p>when you look at viruses,</p>
<p>do you see them as a kind of intelligence system?</p>
<p>I think the concept of intelligence is not that natural</p>
<p>of a concept in the end.</p>
<p>I mean, I think human minds and bodies</p>
<p>are a kind of complex self organizing adaptive system.</p>
<p>And viruses certainly are that, right?</p>
<p>They&rsquo;re a very complex self organizing adaptive system.</p>
<p>If you wanna look at intelligence as Marcus Hutter defines it</p>
<p>as sort of optimizing computable reward functions</p>
<p>over computable environments,</p>
<p>for sure viruses are doing that, right?</p>
<p>And I mean, in doing so they&rsquo;re causing some harm to us.</p>
<p>So the human immune system is a very complex</p>
<p>of organizing adaptive system,</p>
<p>which has a lot of intelligence to it.</p>
<p>And viruses are also adapting</p>
<p>and dividing into new mutant strains and so forth.</p>
<p>And ultimately the solution is gonna be nanotechnology,</p>
<p>right?</p>
<p>The solution is gonna be making little nanobots that.</p>
<p>Fight the viruses or.</p>
<p>Well, people will use them to make nastier viruses,</p>
<p>but hopefully we can also use them</p>
<p>to just detect combat and kill the viruses.</p>
<p>But I think now we&rsquo;re stuck</p>
<p>with the biological mechanisms to combat these viruses.</p>
<p>And yeah, we&rsquo;ve been AGI is not yet mature enough</p>
<p>to use against COVID,</p>
<p>but we&rsquo;ve been using machine learning</p>
<p>and also some machine reasoning in open cog</p>
<p>to help some doctors to do personalized medicine</p>
<p>against COVID.</p>
<p>So the problem there is given the person&rsquo;s genomics</p>
<p>and given their clinical medical indicators,</p>
<p>how do you figure out which combination of antivirals</p>
<p>is gonna be most effective against COVID for that person?</p>
<p>And so that&rsquo;s something</p>
<p>where machine learning is interesting,</p>
<p>but also we&rsquo;re finding the abstraction</p>
<p>to get an open cog with machine reasoning is interesting</p>
<p>because it can help with transfer learning</p>
<p>when you have not that many different cases to study</p>
<p>and qualitative differences between different strains</p>
<p>of a virus or people of different ages who may have COVID.</p>
<p>So there&rsquo;s a lot of different disparate data to work with</p>
<p>and it&rsquo;s small data sets and somehow integrating them.</p>
<p>This is one of the shameful things</p>
<p>that&rsquo;s very hard to get that data.</p>
<p>So, I mean, we&rsquo;re working with a couple of groups</p>
<p>doing clinical trials and they&rsquo;re sharing data with us</p>
<p>like under non disclosure,</p>
<p>but what should be the case is like every COVID</p>
<p>clinical trial should be putting data online somewhere</p>
<p>like suitably encrypted to protect patient privacy</p>
<p>so that anyone with the right AI algorithms</p>
<p>should be able to help analyze it</p>
<p>and any biologists should be able to analyze it by hand</p>
<p>to understand what they can, right?</p>
<p>Instead that data is like siloed inside whatever hospital</p>
<p>is running the clinical trial,</p>
<p>which is completely asinine and ridiculous.</p>
<p>So why the world works that way?</p>
<p>I mean, we could all analyze why,</p>
<p>but it&rsquo;s insane that it does.</p>
<p>You look at this hydrochloroquine, right?</p>
<p>All these clinical trials were done</p>
<p>were reported by Surgisphere,</p>
<p>some little company no one ever heard of</p>
<p>and everyone paid attention to this.</p>
<p>So they were doing more clinical trials based on that</p>
<p>then they stopped doing clinical trials based on that</p>
<p>then they started again</p>
<p>and why isn&rsquo;t that data just out there</p>
<p>so everyone can analyze it and see what&rsquo;s going on, right?</p>
<p>Do you have hope that data will be out there eventually</p>
<p>for future pandemics?</p>
<p>I mean, do you have hope that our society</p>
<p>will move in the direction of?</p>
<p>It&rsquo;s not in the immediate future</p>
<p>because the US and China frictions are getting very high.</p>
<p>So it&rsquo;s hard to see US and China</p>
<p>as moving in the direction of openly sharing data</p>
<p>with each other, right?</p>
<p>It&rsquo;s not, there&rsquo;s some sharing of data,</p>
<p>but different groups are keeping their data private</p>
<p>till they&rsquo;ve milked the best results from it</p>
<p>and then they share it, right?</p>
<p>So yeah, we&rsquo;re working with some data</p>
<p>that we&rsquo;ve managed to get our hands on,</p>
<p>something we&rsquo;re doing to do good for the world</p>
<p>and it&rsquo;s a very cool playground</p>
<p>for like putting deep neural nets and open cog together.</p>
<p>So we have like a bioadden space</p>
<p>full of all sorts of knowledge</p>
<p>from many different biology experiments</p>
<p>about human longevity</p>
<p>and from biology knowledge bases online.</p>
<p>And we can do like graph to vector type embeddings</p>
<p>where we take nodes from the hypergraph,</p>
<p>embed them into vectors,</p>
<p>which can then feed into neural nets</p>
<p>for different types of analysis.</p>
<p>And we were doing this</p>
<p>in the context of a project called Rejuve</p>
<p>that we spun off from SingularityNet</p>
<p>to do longevity analytics,</p>
<p>like understand why people live to 105 years or over</p>
<p>and other people don&rsquo;t.</p>
<p>And then we had this spin off Singularity Studio</p>
<p>where we&rsquo;re working with some healthcare companies</p>
<p>on data analytics.</p>
<p>But so there&rsquo;s bioadden space</p>
<p>that we built for these more commercial</p>
<p>and longevity data analysis purposes.</p>
<p>We&rsquo;re repurposing and feeding COVID data</p>
<p>into the same bioadden space</p>
<p>and playing around with like graph embeddings</p>
<p>from that graph into neural nets for bioinformatics.</p>
<p>So it&rsquo;s both being a cool testing ground,</p>
<p>some of our bio AI learning and reasoning.</p>
<p>And it seems we&rsquo;re able to discover things</p>
<p>that people weren&rsquo;t seeing otherwise.</p>
<p>Cause the thing in this case is</p>
<p>for each combination of antivirals,</p>
<p>you may have only a few patients</p>
<p>who&rsquo;ve tried that combination.</p>
<p>And those few patients</p>
<p>may have their particular characteristics.</p>
<p>Like this combination of three</p>
<p>was tried only on people age 80 or over.</p>
<p>This other combination of three,</p>
<p>which has an overlap with the first combination</p>
<p>was tried more on young people.</p>
<p>So how do you combine those different pieces of data?</p>
<p>It&rsquo;s a very dodgy transfer learning problem,</p>
<p>which is the kind of thing</p>
<p>that the probabilistic reasoning algorithms</p>
<p>we have inside OpenCog are better at</p>
<p>than deep neural networks.</p>
<p>On the other hand, you have gene expression data</p>
<p>where you have 25,000 genes</p>
<p>and the expression level of each gene</p>
<p>in the peripheral blood of each person.</p>
<p>So that sort of data,</p>
<p>either deep neural nets or tools like XGBoost or CatBoost,</p>
<p>these decision forest trees are better at dealing</p>
<p>with than OpenCog.</p>
<p>Cause it&rsquo;s just these huge,</p>
<p>huge messy floating point vectors</p>
<p>that are annoying for a logic engine to deal with,</p>
<p>but are perfect for a decision forest or a neural net.</p>
<p>So it&rsquo;s a great playground for like hybrid AI methodology.</p>
<p>And we can have SingularityNet have OpenCog in one agent</p>
<p>and XGBoost in a different agent</p>
<p>and they talk to each other.</p>
<p>But at the same time, it&rsquo;s highly practical, right?</p>
<p>Cause we&rsquo;re working with, for example,</p>
<p>some physicians on this project,</p>
<p>physicians in the group called Nth Opinion</p>
<p>based out of Vancouver in Seattle,</p>
<p>who are, these guys are working every day</p>
<p>like in the hospital with patients dying of COVID.</p>
<p>So it&rsquo;s quite cool to see like neural symbolic AI,</p>
<p>like where the rubber hits the road,</p>
<p>trying to save people&rsquo;s lives.</p>
<p>I&rsquo;ve been doing bio AI since 2001,</p>
<p>but mostly human longevity research</p>
<p>and fly longevity research,</p>
<p>try to understand why some organisms really live a long time.</p>
<p>This is the first time like race against the clock</p>
<p>and try to use the AI to figure out stuff that,</p>
<p>like if we take two months longer to solve the AI problem,</p>
<p>some more people will die</p>
<p>because we don&rsquo;t know what combination</p>
<p>of antivirals to give them.</p>
<p>At the societal level, at the biological level,</p>
<p>at any level, are you hopeful about us</p>
<p>as a human species getting out of this pandemic?</p>
<p>What are your thoughts on it in general?</p>
<p>The pandemic will be gone in a year or two</p>
<p>once there&rsquo;s a vaccine for it.</p>
<p>So, I mean, that&rsquo;s&hellip;</p>
<p>A lot of pain and suffering can happen in that time.</p>
<p>So that could be irreversible.</p>
<p>I think if you spend much time in Sub Saharan Africa,</p>
<p>you can see there&rsquo;s a lot of pain and suffering</p>
<p>happening all the time.</p>
<p>Like you walk through the streets</p>
<p>of any large city in Sub Saharan Africa,</p>
<p>and there are loads, I mean, tens of thousands,</p>
<p>probably hundreds of thousands of people</p>
<p>lying by the side of the road,</p>
<p>dying mainly of curable diseases without food or water</p>
<p>and either ostracized by their families</p>
<p>or they left their family house</p>
<p>because they didn&rsquo;t want to infect their family, right?</p>
<p>I mean, there&rsquo;s tremendous human suffering</p>
<p>on the planet all the time,</p>
<p>which most folks in the developed world pay no attention to.</p>
<p>And COVID is not remotely the worst.</p>
<p>How many people are dying of malaria all the time?</p>
<p>I mean, so COVID is bad.</p>
<p>It is by no mean the worst thing happening.</p>
<p>And setting aside diseases,</p>
<p>I mean, there are many places in the world</p>
<p>where you&rsquo;re at risk of having like your teenage son</p>
<p>kidnapped by armed militias and forced to get killed</p>
<p>in someone else&rsquo;s war, fighting tribe against tribe.</p>
<p>I mean, so humanity has a lot of problems</p>
<p>which we don&rsquo;t need to have given the state of advancement</p>
<p>of our technology right now.</p>
<p>And I think COVID is one of the easier problems to solve</p>
<p>in the sense that there are many brilliant people</p>
<p>working on vaccines.</p>
<p>We have the technology to create vaccines</p>
<p>and we&rsquo;re gonna create new vaccines.</p>
<p>We should be more worried</p>
<p>that we haven&rsquo;t managed to defeat malaria after so long.</p>
<p>And after the Gates Foundation and others</p>
<p>putting so much money into it.</p>
<p>I mean, I think clearly the whole global medical system,</p>
<p>the global health system</p>
<p>and the global political and socioeconomic system</p>
<p>are incredibly unethical and unequal and badly designed.</p>
<p>And I mean, I don&rsquo;t know how to solve that directly.</p>
<p>I think what we can do indirectly to solve it</p>
<p>is to make systems that operate in parallel</p>
<p>and off to the side of the governments</p>
<p>that are nominally controlling the world</p>
<p>with their armies and militias.</p>
<p>And to the extent that you can make compassionate</p>
<p>peer to peer decentralized frameworks</p>
<p>for doing things,</p>
<p>these are things that can start out unregulated.</p>
<p>And then if they get traction</p>
<p>before the regulators come in,</p>
<p>then they&rsquo;ve influenced the way the world works, right?</p>
<p>SingularityNet aims to do this with AI.</p>
<p>REJUVE, which is a spinoff from SingularityNet.</p>
<p>You can see REJUVE.io.</p>
<p>How do you spell that?</p>
<p>R E J U V E, REJUVE.io.</p>
<p>That aims to do the same thing for medicine.</p>
<p>So it&rsquo;s like peer to peer sharing of information</p>
<p>peer to peer sharing of medical data.</p>
<p>So you can share medical data into a secure data wallet.</p>
<p>You can get advice about your health and longevity</p>
<p>through apps that REJUVE.io will launch</p>
<p>within the next couple of months.</p>
<p>And then SingularityNet AI can analyze all this data,</p>
<p>but then the benefits from that analysis</p>
<p>are spread among all the members of the network.</p>
<p>But I mean, of course,</p>
<p>I&rsquo;m gonna hawk my particular projects,</p>
<p>but I mean, whether or not SingularityNet and REJUVE.io</p>
<p>are the answer, I think it&rsquo;s key to create</p>
<p>decentralized mechanisms for everything.</p>
<p>I mean, for AI, for human health, for politics,</p>
<p>for jobs and employment, for sharing social information.</p>
<p>And to the extent decentralized peer to peer methods</p>
<p>designed with universal compassion at the core</p>
<p>can gain traction, then these will just decrease the role</p>
<p>that government has.</p>
<p>And I think that&rsquo;s much more likely to do good</p>
<p>than trying to like explicitly reform</p>
<p>the global government system.</p>
<p>I mean, I&rsquo;m happy other people are trying to explicitly</p>
<p>reform the global government system.</p>
<p>On the other hand, you look at how much good the internet</p>
<p>or Google did or mobile phones did,</p>
<p>even you&rsquo;re making something that&rsquo;s decentralized</p>
<p>and throwing it out everywhere and it takes hold,</p>
<p>then government has to adapt.</p>
<p>And I mean, that&rsquo;s what we need to do with AI</p>
<p>and with health.</p>
<p>And in that light, I mean, the centralization</p>
<p>of healthcare and of AI is certainly not ideal, right?</p>
<p>Like most AI PhDs are being sucked in by a half dozen</p>
<p>to a dozen big companies.</p>
<p>Most AI processing power is being bought</p>
<p>by a few big companies for their own proprietary good.</p>
<p>And most medical research is within a few</p>
<p>pharmaceutical companies and clinical trials</p>
<p>run by pharmaceutical companies will stay solid</p>
<p>within those pharmaceutical companies.</p>
<p>You know, these large centralized entities,</p>
<p>which are intelligences in themselves, these corporations,</p>
<p>but they&rsquo;re mostly malevolent psychopathic</p>
<p>and sociopathic intelligences,</p>
<p>not saying the people involved are,</p>
<p>but the corporations as self organizing entities</p>
<p>on their own, which are concerned with maximizing</p>
<p>shareholder value as a sole objective function.</p>
<p>I mean, AI and medicine are being sucked</p>
<p>into these pathological corporate organizations</p>
<p>with government cooperation and Google cooperating</p>
<p>with British and US government on this</p>
<p>as one among many, many different examples.</p>
<p>23andMe providing you the nice service of sequencing</p>
<p>your genome and then licensing the genome</p>
<p>to GlaxoSmithKline on an exclusive basis, right?</p>
<p>Now you can take your own DNA</p>
<p>and do whatever you want with it.</p>
<p>But the pooled collection of 23andMe sequence DNA</p>
<p>is just to GlaxoSmithKline.</p>
<p>Someone else could reach out to everyone</p>
<p>who had worked with 23andMe to sequence their DNA</p>
<p>and say, give us your DNA for our open</p>
<p>and decentralized repository that we&rsquo;ll make available</p>
<p>to everyone, but nobody&rsquo;s doing that</p>
<p>cause it&rsquo;s a pain to get organized.</p>
<p>And the customer list is proprietary to 23andMe, right?</p>
<p>So, yeah, I mean, this I think is a greater risk</p>
<p>to humanity from AI than rogue AGI</p>
<p>is turning the universe into paperclips or computronium.</p>
<p>Cause what you have here is mostly good hearted</p>
<p>and nice people who are sucked into a mode of organization</p>
<p>of large corporations, which has evolved</p>
<p>just for no individual&rsquo;s fault</p>
<p>just because that&rsquo;s the way society has evolved.</p>
<p>It&rsquo;s not altruistic, it&rsquo;s self interested</p>
<p>and become psychopathic like you said.</p>
<p>The human.</p>
<p>The corporation is psychopathic even if the people are not.</p>
<p>And that&rsquo;s really the disturbing thing about it</p>
<p>because the corporations can do things</p>
<p>that are quite bad for society</p>
<p>even if nobody has a bad intention.</p>
<p>Right.</p>
<p>And then.</p>
<p>No individual member of that corporation</p>
<p>has a bad intention.</p>
<p>No, some probably do, but it&rsquo;s not necessary</p>
<p>that they do for the corporation.</p>
<p>Like, I mean, Google, I know a lot of people in Google</p>
<p>and there are, with very few exceptions,</p>
<p>they&rsquo;re all very nice people</p>
<p>who genuinely want what&rsquo;s good for the world.</p>
<p>And Facebook, I know fewer people</p>
<p>but it&rsquo;s probably mostly true.</p>
<p>It&rsquo;s probably like fine young geeks</p>
<p>who wanna build cool technology.</p>
<p>I actually tend to believe that even the leaders,</p>
<p>even Mark Zuckerberg, one of the most disliked people</p>
<p>in tech is also wants to do good for the world.</p>
<p>I think about Jamie Dimon.</p>
<p>Who&rsquo;s Jamie Dimon?</p>
<p>Oh, the heads of the great banks</p>
<p>may have a different psychology.</p>
<p>Oh boy, yeah.</p>
<p>Well, I tend to be naive about these things</p>
<p>and see the best in, I tend to agree with you</p>
<p>that I think the individuals wanna do good by the world</p>
<p>but the mechanism of the company</p>
<p>can sometimes be its own intelligence system.</p>
<p>I mean, there&rsquo;s a, my cousin Mario Goetzler</p>
<p>has worked for Microsoft since 1985 or something</p>
<p>and I can see for him,</p>
<p>I mean, as well as just working on cool projects,</p>
<p>you&rsquo;re coding stuff that gets used</p>
<p>by like billions and billions of people.</p>
<p>And do you think if I improve this feature</p>
<p>that&rsquo;s making billions of people&rsquo;s lives easier, right?</p>
<p>So of course that&rsquo;s cool.</p>
<p>And the engineers are not in charge</p>
<p>of running the company anyway.</p>
<p>And of course, even if you&rsquo;re Mark Zuckerberg or Larry Page,</p>
<p>I mean, you still have a fiduciary responsibility.</p>
<p>And I mean, you&rsquo;re responsible to the shareholders,</p>
<p>your employees who you want to keep paying them</p>
<p>and so forth.</p>
<p>So yeah, you&rsquo;re enmeshed in this system.</p>
<p>And when I worked in DC,</p>
<p>I worked a bunch with INSCOM, US Army Intelligence</p>
<p>and I was heavily politically opposed</p>
<p>to what the US Army was doing in Iraq at that time,</p>
<p>like torturing people in Abu Ghraib</p>
<p>but everyone I knew in US Army and INSCOM,</p>
<p>when I hung out with them, was very nice person.</p>
<p>They were friendly to me.</p>
<p>They were nice to my kids and my dogs, right?</p>
<p>And they really believed that the US</p>
<p>was fighting the forces of evil.</p>
<p>And if you ask me about Abu Ghraib, they&rsquo;re like,</p>
<p>well, but these Arabs will chop us into pieces.</p>
<p>So how can you say we&rsquo;re wrong</p>
<p>to waterboard them a bit, right?</p>
<p>Like that&rsquo;s much less than what they would do to us.</p>
<p>It&rsquo;s just in their worldview,</p>
<p>what they were doing was really genuinely</p>
<p>for the good of humanity.</p>
<p>Like none of them woke up in the morning</p>
<p>and said like, I want to do harm to good people</p>
<p>because I&rsquo;m just a nasty guy, right?</p>
<p>So yeah, most people on the planet,</p>
<p>setting aside a few genuine psychopaths and sociopaths,</p>
<p>I mean, most people on the planet have a heavy dose</p>
<p>of benevolence and wanting to do good</p>
<p>and also a heavy capability to convince themselves</p>
<p>whatever they feel like doing</p>
<p>or whatever is best for them is for the good of humankind.</p>
<p>So the more we can decentralize control.</p>
<p>Decentralization, you know, the democracy is horrible,</p>
<p>but this is like Winston Churchill said,</p>
<p>you know, it&rsquo;s the worst possible system of government</p>
<p>except for all the others, right?</p>
<p>I mean, I think the whole mess of humanity</p>
<p>has many, many very bad aspects to it,</p>
<p>but so far the track record of elite groups</p>
<p>who know what&rsquo;s better for all of humanity</p>
<p>is much worse than the track record</p>
<p>of the whole teaming democratic participatory</p>
<p>mess of humanity, right?</p>
<p>I mean, none of them is perfect by any means.</p>
<p>The issue with a small elite group that knows what&rsquo;s best</p>
<p>is even if it starts out as truly benevolent</p>
<p>and doing good things in accordance</p>
<p>with its initial good intentions,</p>
<p>you find out you need more resources,</p>
<p>you need a bigger organization, you pull in more people,</p>
<p>internal politics arises, difference of opinions arise</p>
<p>and bribery happens, like some opponent organization</p>
<p>takes a second in command now to make some,</p>
<p>the first in command of some other organization.</p>
<p>And I mean, that&rsquo;s, there&rsquo;s a lot of history</p>
<p>of what happens with elite groups</p>
<p>thinking they know what&rsquo;s best for the human race.</p>
<p>So yeah, if I have to choose,</p>
<p>I&rsquo;m gonna reluctantly put my faith</p>
<p>in the vast democratic decentralized mass.</p>
<p>And I think corporations have a track record</p>
<p>of being ethically worse</p>
<p>than their constituent human parts.</p>
<p>And democratic governments have a more mixed track record,</p>
<p>but there are at least.</p>
<p>That&rsquo;s the best we got.</p>
<p>Yeah, I mean, you can, there&rsquo;s Iceland,</p>
<p>very nice country, right?</p>
<p>I&rsquo;ve been very democratic for 800 plus years,</p>
<p>very, very benevolent, beneficial government.</p>
<p>And I think, yeah, there are track records</p>
<p>of democratic modes of organization.</p>
<p>Linux, for example, some of the people in charge of Linux</p>
<p>are overtly complete assholes, right?</p>
<p>And trying to reform themselves in many cases,</p>
<p>in other cases not, but the organization as a whole,</p>
<p>I think it&rsquo;s done a good job overall.</p>
<p>It&rsquo;s been very welcoming in the third world, for example,</p>
<p>and it&rsquo;s allowed advanced technology to roll out</p>
<p>on all sorts of different embedded devices and platforms</p>
<p>in places where people couldn&rsquo;t afford to pay</p>
<p>for proprietary software.</p>
<p>So I&rsquo;d say the internet, Linux, and many democratic nations</p>
<p>are examples of how sort of an open,</p>
<p>decentralized democratic methodology</p>
<p>can be ethically better than the sum of the parts</p>
<p>rather than worse.</p>
<p>And corporations, that has happened only for a brief period</p>
<p>and then it goes sour, right?</p>
<p>I mean, I&rsquo;d say a similar thing about universities.</p>
<p>Like university is a horrible way to organize research</p>
<p>and get things done, yet it&rsquo;s better than anything else</p>
<p>we&rsquo;ve come up with, right?</p>
<p>A company can be much better,</p>
<p>but for a brief period of time,</p>
<p>and then it stops being so good, right?</p>
<p>So then I think if you believe that AGI</p>
<p>is gonna emerge sort of incrementally</p>
<p>out of AIs doing practical stuff in the world,</p>
<p>like controlling humanoid robots or driving cars</p>
<p>or diagnosing diseases or operating killer drones</p>
<p>or spying on people and reporting under the government,</p>
<p>then what kind of organization creates more and more</p>
<p>advanced narrow AI verging toward AGI</p>
<p>may be quite important because it will guide</p>
<p>like what&rsquo;s in the mind of the early stage AGI</p>
<p>as it first gains the ability to rewrite its own code base</p>
<p>and project itself toward super intelligence.</p>
<p>And if you believe that AI may move toward AGI</p>
<p>out of this sort of synergetic activity</p>
<p>of many agents cooperating together</p>
<p>rather than just have one person&rsquo;s project,</p>
<p>then who owns and controls that platform for AI cooperation</p>
<p>becomes also very, very important, right?</p>
<p>And is that platform AWS?</p>
<p>Is it Google Cloud?</p>
<p>Is it Alibaba or is it something more like the internet</p>
<p>or Singularity Net, which is open and decentralized?</p>
<p>So if all of my weird machinations come to pass, right?</p>
<p>I mean, we have the Hanson robots</p>
<p>being a beautiful user interface,</p>
<p>gathering information on human values</p>
<p>and being loving and compassionate to people in medical,</p>
<p>home service, robot office applications,</p>
<p>you have Singularity Net in the backend</p>
<p>networking together many different AIs</p>
<p>toward cooperative intelligence,</p>
<p>fueling the robots among many other things.</p>
<p>You have OpenCog 2.0 and true AGI</p>
<p>as one of the sources of AI</p>
<p>inside this decentralized network,</p>
<p>powering the robot and medical AIs</p>
<p>helping us live a long time</p>
<p>and cure diseases among other things.</p>
<p>And this whole thing is operating</p>
<p>in a democratic and decentralized way, right?</p>
<p>And I think if anyone can pull something like this off,</p>
<p>whether using the specific technologies I&rsquo;ve mentioned</p>
<p>or something else, I mean,</p>
<p>then I think we have a higher odds</p>
<p>of moving toward a beneficial technological singularity</p>
<p>rather than one in which the first super AGI</p>
<p>is indifferent to humans</p>
<p>and just considers us an inefficient use of molecules.</p>
<p>That was a beautifully articulated vision for the world.</p>
<p>So thank you for that.</p>
<p>Well, let&rsquo;s talk a little bit about life and death.</p>
<p>I&rsquo;m pro life and anti death for most people.</p>
<p>There&rsquo;s few exceptions that I won&rsquo;t mention here.</p>
<p>I&rsquo;m glad just like your dad,</p>
<p>you&rsquo;re taking a stand against death.</p>
<p>You have, by the way, you have a bunch of awesome music</p>
<p>where you play piano online.</p>
<p>One of the songs that I believe you&rsquo;ve written</p>
<p>the lyrics go, by the way, I like the way it sounds,</p>
<p>people should listen to it, it&rsquo;s awesome.</p>
<p>I considered, I probably will cover it, it&rsquo;s a good song.</p>
<p>Tell me why do you think it is a good thing</p>
<p>that we all get old and die is one of the songs.</p>
<p>I love the way it sounds,</p>
<p>but let me ask you about death first.</p>
<p>Do you think there&rsquo;s an element to death</p>
<p>that&rsquo;s essential to give our life meaning?</p>
<p>Like the fact that this thing ends.</p>
<p>Well, let me say I&rsquo;m pleased and a little embarrassed</p>
<p>you&rsquo;ve been listening to that music I put online.</p>
<p>That&rsquo;s awesome.</p>
<p>One of my regrets in life recently is I would love</p>
<p>to get time to really produce music well.</p>
<p>Like I haven&rsquo;t touched my sequencer software</p>
<p>in like five years.</p>
<p>I would love to like rehearse and produce and edit.</p>
<p>But with a two year old baby</p>
<p>and trying to create the singularity, there&rsquo;s no time.</p>
<p>So I just made the decision to,</p>
<p>when I&rsquo;m playing random shit in an off moment.</p>
<p>Just record it.</p>
<p>Just record it, put it out there, like whatever.</p>
<p>Maybe if I&rsquo;m unfortunate enough to die,</p>
<p>maybe that can be input to the AGI</p>
<p>when it tries to make an accurate mind upload of me, right?</p>
<p>Death is bad.</p>
<p>I mean, that&rsquo;s very simple.</p>
<p>It&rsquo;s baffling we should have to say that.</p>
<p>I mean, of course people can make meaning out of death.</p>
<p>And if someone is tortured,</p>
<p>maybe they can make beautiful meaning out of that torture</p>
<p>and write a beautiful poem</p>
<p>about what it was like to be tortured, right?</p>
<p>I mean, we&rsquo;re very creative.</p>
<p>We can milk beauty and positivity</p>
<p>out of even the most horrible and shitty things.</p>
<p>But just because if I was tortured,</p>
<p>I could write a good song</p>
<p>about what it was like to be tortured,</p>
<p>doesn&rsquo;t make torture good.</p>
<p>And just because people are able to derive meaning</p>
<p>and value from death,</p>
<p>doesn&rsquo;t mean they wouldn&rsquo;t derive even better meaning</p>
<p>and value from ongoing life without death,</p>
<p>which I very&hellip;</p>
<p>Indefinite.</p>
<p>Yeah, yeah.</p>
<p>So if you could live forever, would you live forever?</p>
<p>Forever.</p>
<p>My goal with longevity research</p>
<p>is to abolish the plague of involuntary death.</p>
<p>I don&rsquo;t think people should die unless they choose to die.</p>
<p>If I had to choose forced immortality</p>
<p>versus dying, I would choose forced immortality.</p>
<p>On the other hand, if I chose&hellip;</p>
<p>If I had the choice of immortality</p>
<p>with the choice of suicide whenever I felt like it,</p>
<p>of course I would take that instead.</p>
<p>And that&rsquo;s the more realistic choice.</p>
<p>I mean, there&rsquo;s no reason</p>
<p>you should have forced immortality.</p>
<p>You should be able to live until you get sick of living,</p>
<p>right?</p>
<p>I mean, that&rsquo;s&hellip;</p>
<p>And that will seem insanely obvious</p>
<p>to everyone 50 years from now.</p>
<p>And they will be so&hellip;</p>
<p>I mean, people who thought death gives meaning to life,</p>
<p>so we should all die,</p>
<p>they will look at that 50 years from now</p>
<p>the way we now look at the Anabaptists in the year 1000</p>
<p>who gave away all their positions,</p>
<p>went on top of the mountain for Jesus</p>
<p>to come and bring them to the ascension.</p>
<p>I mean, it&rsquo;s ridiculous that people think death is good</p>
<p>because you gain more wisdom as you approach dying.</p>
<p>I mean, of course it&rsquo;s true.</p>
<p>I mean, I&rsquo;m 53.</p>
<p>And the fact that I might have only a few more decades left,</p>
<p>it does make me reflect on things differently.</p>
<p>It does give me a deeper understanding of many things.</p>
<p>But I mean, so what?</p>
<p>You could get a deep understanding</p>
<p>in a lot of different ways.</p>
<p>Pain is the same way.</p>
<p>We&rsquo;re gonna abolish pain.</p>
<p>And that&rsquo;s even more amazing than abolishing death, right?</p>
<p>I mean, once we get a little better at neuroscience,</p>
<p>we&rsquo;ll be able to go in and adjust the brain</p>
<p>so that pain doesn&rsquo;t hurt anymore, right?</p>
<p>And that, you know, people will say that&rsquo;s bad</p>
<p>because there&rsquo;s so much beauty</p>
<p>in overcoming pain and suffering.</p>
<p>Oh, sure.</p>
<p>And there&rsquo;s beauty in overcoming torture too.</p>
<p>And some people like to cut themselves,</p>
<p>but not many, right?</p>
<p>I mean.</p>
<p>That&rsquo;s an interesting.</p>
<p>So, but to push, I mean, to push back again,</p>
<p>this is the Russian side of me.</p>
<p>I do romanticize suffering.</p>
<p>It&rsquo;s not obvious.</p>
<p>I mean, the way you put it, it seems very logical.</p>
<p>It&rsquo;s almost absurd to romanticize suffering or pain</p>
<p>or death, but to me, a world without suffering,</p>
<p>without pain, without death, it&rsquo;s not obvious.</p>
<p>Well, then you can stay in the people&rsquo;s zoo,</p>
<p>people torturing each other.</p>
<p>No, but what I&rsquo;m saying is I don&rsquo;t,</p>
<p>well, that&rsquo;s, I guess what I&rsquo;m trying to say,</p>
<p>I don&rsquo;t know if I was presented with that choice,</p>
<p>what I would choose because it, to me.</p>
<p>This is a subtler, it&rsquo;s a subtler matter.</p>
<p>And I&rsquo;ve posed it in this conversation</p>
<p>in an unnecessarily extreme way.</p>
<p>So I think, I think the way you should think about it</p>
<p>is what if there&rsquo;s a little dial on the side of your head</p>
<p>and you could turn how much pain hurt,</p>
<p>turn it down to zero, turn it up to 11,</p>
<p>like in spinal tap, if it wants,</p>
<p>maybe through an actual spinal tap, right?</p>
<p>So, I mean, would you opt to have that dial there or not?</p>
<p>That&rsquo;s the question.</p>
<p>The question isn&rsquo;t whether you would turn the pain down</p>
<p>to zero all the time.</p>
<p>Would you opt to have the dial or not?</p>
<p>My guess is that in some dark moment of your life,</p>
<p>you would choose to have the dial implanted</p>
<p>and then it would be there.</p>
<p>Just to confess a small thing, don&rsquo;t ask me why,</p>
<p>but I&rsquo;m doing this physical challenge currently</p>
<p>where I&rsquo;m doing 680 pushups and pull ups a day.</p>
<p>And my shoulder is currently, as we sit here,</p>
<p>in a lot of pain.</p>
<p>And I don&rsquo;t know, I would certainly right now,</p>
<p>if you gave me a dial, I would turn that sucker to zero</p>
<p>as quickly as possible.</p>
<p>But I think the whole point of this journey is,</p>
<p>I don&rsquo;t know.</p>
<p>Well, because you&rsquo;re a twisted human being.</p>
<p>I&rsquo;m a twisted, so the question is am I somehow twisted</p>
<p>because I created some kind of narrative for myself</p>
<p>so that I can deal with the injustice</p>
<p>and the suffering in the world?</p>
<p>Or is this actually going to be a source of happiness</p>
<p>for me?</p>
<p>Well, this is to an extent is a research question</p>
<p>that humanity will undertake, right?</p>
<p>So I mean, human beings do have a particular biological</p>
<p>makeup, which sort of implies a certain probability</p>
<p>distribution over motivational systems, right?</p>
<p>So I mean, we, and that is there, that is there.</p>
<p>Now the question is how flexibly can that morph</p>
<p>as society and technology change, right?</p>
<p>So if we&rsquo;re given that dial and we&rsquo;re given a society</p>
<p>in which say we don&rsquo;t have to work for a living</p>
<p>and in which there&rsquo;s an ambient decentralized</p>
<p>benevolent AI network that will warn us</p>
<p>when we&rsquo;re about to hurt ourself,</p>
<p>if we&rsquo;re in a different context,</p>
<p>can we consistently with being genuinely and fully human,</p>
<p>can we consistently get into a state of consciousness</p>
<p>where we just want to keep the pain dial turned</p>
<p>all the way down and yet we&rsquo;re leading very rewarding</p>
<p>and fulfilling lives, right?</p>
<p>Now, I suspect the answer is yes, we can do that,</p>
<p>but I don&rsquo;t know that, I don&rsquo;t know that for certain.</p>
<p>Yeah, now I&rsquo;m more confident that we could create</p>
<p>a nonhuman AGI system, which just didn&rsquo;t need an analog</p>
<p>of feeling pain.</p>
<p>And I think that AGI system will be fundamentally healthier</p>
<p>and more benevolent than human beings.</p>
<p>So I think it might or might not be true</p>
<p>that humans need a certain element of suffering</p>
<p>to be satisfied humans, consistent with the human physiology.</p>
<p>If it is true, that&rsquo;s one of the things that makes us fucked</p>
<p>and disqualified to be the super AGI, right?</p>
<p>I mean, the nature of the human motivational system</p>
<p>is that we seem to gravitate towards situations</p>
<p>where the best thing in the large scale</p>
<p>is not the best thing in the small scale</p>
<p>according to our subjective value system.</p>
<p>So we gravitate towards subjective value judgments</p>
<p>where to gratify ourselves in the large,</p>
<p>we have to ungratify ourselves in the small.</p>
<p>And we do that in, you see that in music,</p>
<p>there&rsquo;s a theory of music which says</p>
<p>the key to musical aesthetics</p>
<p>is the surprising fulfillment of expectations.</p>
<p>Like you want something that will fulfill</p>
<p>the expectations are listed in the prior part of the music,</p>
<p>but in a way with a bit of a twist that surprises you.</p>
<p>And I mean, that&rsquo;s true not only in outdoor music</p>
<p>like my own or that of Zappa or Steve Vai or Buckethead</p>
<p>or Christoph Pendergast or something,</p>
<p>it&rsquo;s even there in Mozart or something.</p>
<p>It&rsquo;s not there in elevator music too much,</p>
<p>but that&rsquo;s why it&rsquo;s boring, right?</p>
<p>But wrapped up in there is we want to hurt a little bit</p>
<p>so that we can feel the pain go away.</p>
<p>Like we wanna be a little confused by what&rsquo;s coming next.</p>
<p>So then when the thing that comes next actually makes sense,</p>
<p>it&rsquo;s so satisfying, right?</p>
<p>That&rsquo;s the surprising fulfillment of expectations,</p>
<p>is that what you said?</p>
<p>Yeah, yeah, yeah.</p>
<p>So beautifully put.</p>
<p>We&rsquo;ve been skirting around a little bit,</p>
<p>but if I were to ask you the most ridiculous big question</p>
<p>of what is the meaning of life,</p>
<p>what would your answer be?</p>
<p>Three values, joy, growth, and choice.</p>
<p>I think you need joy.</p>
<p>I mean, that&rsquo;s the basis of everything.</p>
<p>If you want the number one value.</p>
<p>On the other hand, I&rsquo;m unsatisfied with a static joy</p>
<p>that doesn&rsquo;t progress perhaps because of some</p>
<p>elemental element of human perversity,</p>
<p>but the idea of something that grows</p>
<p>and becomes more and more and better and better</p>
<p>in some sense appeals to me.</p>
<p>But I also sort of like the idea of individuality</p>
<p>that as a distinct system, I have some agency.</p>
<p>So there&rsquo;s some nexus of causality within this system</p>
<p>rather than the causality being wholly evenly distributed</p>
<p>over the joyous growing mass.</p>
<p>So you start with joy, growth, and choice</p>
<p>as three basic values.</p>
<p>Those three things could continue indefinitely.</p>
<p>That&rsquo;s something that can last forever.</p>
<p>Is there some aspect of something you called,</p>
<p>which I like, super longevity that you find exciting?</p>
<p>Is there research wise, is there ideas in that space that?</p>
<p>I mean, I think, yeah, in terms of the meaning of life,</p>
<p>this really ties into that because for us as humans,</p>
<p>probably the way to get the most joy, growth, and choice</p>
<p>is transhumanism and to go beyond the human form</p>
<p>that we have right now, right?</p>
<p>I mean, I think human body is great</p>
<p>and by no means do any of us maximize the potential</p>
<p>for joy, growth, and choice imminent in our human bodies.</p>
<p>On the other hand, it&rsquo;s clear that other configurations</p>
<p>of matter could manifest even greater amounts</p>
<p>of joy, growth, and choice than humans do,</p>
<p>maybe even finding ways to go beyond the realm of matter</p>
<p>as we understand it right now.</p>
<p>So I think in a practical sense,</p>
<p>much of the meaning I see in human life</p>
<p>is to create something better than humans</p>
<p>and go beyond human life.</p>
<p>But certainly that&rsquo;s not all of it for me</p>
<p>in a practical sense, right?</p>
<p>Like I have four kids and a granddaughter</p>
<p>and many friends and parents and family</p>
<p>and just enjoying everyday human social existence.</p>
<p>But we can do even better.</p>
<p>Yeah, yeah.</p>
<p>And I mean, I love, I&rsquo;ve always,</p>
<p>when I could live near nature,</p>
<p>I spend a bunch of time out in nature in the forest</p>
<p>and on the water every day and so forth.</p>
<p>So, I mean, enjoying the pleasant moment is part of it,</p>
<p>but the growth and choice aspect are severely limited</p>
<p>by our human biology.</p>
<p>In particular, dying seems to inhibit your potential</p>
<p>for personal growth considerably as far as we know.</p>
<p>I mean, there&rsquo;s some element of life after death perhaps,</p>
<p>but even if there is,</p>
<p>why not also continue going in this biological realm, right?</p>
<p>In super longevity, I mean,</p>
<p>you know, we haven&rsquo;t yet cured aging.</p>
<p>We haven&rsquo;t yet cured death.</p>
<p>Certainly there&rsquo;s very interesting progress all around.</p>
<p>I mean, CRISPR and gene editing can be an incredible tool.</p>
<p>And I mean, right now,</p>
<p>stem cells could potentially prolong life a lot.</p>
<p>Like if you got stem cell injections</p>
<p>of just stem cells for every tissue of your body</p>
<p>injected into every tissue,</p>
<p>and you can just have replacement of your old cells</p>
<p>with new cells produced by those stem cells,</p>
<p>I mean, that could be highly impactful at prolonging life.</p>
<p>Now we just need slightly better technology</p>
<p>for having them grow, right?</p>
<p>So using machine learning to guide procedures</p>
<p>for stem cell differentiation and trans differentiation,</p>
<p>it&rsquo;s kind of nitty gritty,</p>
<p>but I mean, that&rsquo;s quite interesting.</p>
<p>So I think there&rsquo;s a lot of different things being done</p>
<p>to help with prolongation of human life,</p>
<p>but we could do a lot better.</p>
<p>So for example, the extracellular matrix,</p>
<p>which is the bunch of proteins</p>
<p>in between the cells in your body,</p>
<p>they get stiffer and stiffer as you get older.</p>
<p>And the extracellular matrix transmits information</p>
<p>both electrically, mechanically,</p>
<p>and to some extent, biophotonically.</p>
<p>So there&rsquo;s all this transmission</p>
<p>through the parts of the body,</p>
<p>but the stiffer the extracellular matrix gets,</p>
<p>the less the transmission happens,</p>
<p>which makes your body get worse coordinated</p>
<p>between the different organs as you get older.</p>
<p>So my friend Christian Schaffmeister</p>
<p>at my alumnus organization,</p>
<p>my Alma mater, the Great Temple University,</p>
<p>Christian Schaffmeister has a potential solution to this,</p>
<p>where he has these novel molecules called spiral ligamers,</p>
<p>which are like polymers that are not organic.</p>
<p>They&rsquo;re specially designed polymers</p>
<p>so that you can algorithmically predict</p>
<p>exactly how they&rsquo;ll fold very simply.</p>
<p>So he designed the molecular scissors</p>
<p>that have spiral ligamers that you could eat</p>
<p>and would then cut through all the glucosamine</p>
<p>and other crosslink proteins</p>
<p>in your extracellular matrix, right?</p>
<p>But to make that technology really work</p>
<p>and be mature as several years of work,</p>
<p>as far as I know, no one&rsquo;s finding it at the moment.</p>
<p>So there&rsquo;s so many different ways</p>
<p>that technology could be used to prolong longevity.</p>
<p>What we really need,</p>
<p>we need an integrated database of all biological knowledge</p>
<p>about human beings and model organisms,</p>
<p>like hopefully a massively distributed</p>
<p>open cog bioatom space,</p>
<p>but it can exist in other forms too.</p>
<p>We need that data to be opened up</p>
<p>in a suitably privacy protecting way.</p>
<p>We need massive funding into machine learning,</p>
<p>AGI, proto AGI statistical research</p>
<p>aimed at solving biology,</p>
<p>both molecular biology and human biology</p>
<p>based on this massive data set, right?</p>
<p>And then we need regulators not to stop people</p>
<p>from trying radical therapies on themselves</p>
<p>if they so wish to,</p>
<p>as well as better cloud based platforms</p>
<p>for like automated experimentation on microorganisms,</p>
<p>flies and mice and so forth.</p>
<p>And we could do all this.</p>
<p>You look after the last financial crisis,</p>
<p>Obama, who I generally like pretty well,</p>
<p>but he gave $4 trillion to large banks</p>
<p>and insurance companies.</p>
<p>You know, now in this COVID crisis,</p>
<p>trillions are being spent to help everyday people</p>
<p>and small businesses.</p>
<p>In the end, we&rsquo;ll probably will find many more trillions</p>
<p>are being given to large banks and insurance companies.</p>
<p>Anyway, like could the world put $10 trillion</p>
<p>into making a massive holistic bio AI and bio simulation</p>
<p>and experimental biology infrastructure?</p>
<p>We could, we could put $10 trillion into that</p>
<p>without even screwing us up too badly.</p>
<p>Just as in the end COVID and the last financial crisis</p>
<p>won&rsquo;t screw up the world economy so badly.</p>
<p>We&rsquo;re not putting $10 trillion into that.</p>
<p>Instead, all this research is siloed inside</p>
<p>a few big companies and government agencies.</p>
<p>And most of the data that comes from our individual bodies</p>
<p>personally, that could feed this AI to solve aging</p>
<p>and death, most of that data is sitting</p>
<p>in some hospital&rsquo;s database doing nothing, right?</p>
<p>I got two more quick questions for you.</p>
<p>One, I know a lot of people are gonna ask me,</p>
<p>you are on the Joe Rogan podcast</p>
<p>wearing that same amazing hat.</p>
<p>Do you have a origin story for the hat?</p>
<p>Does the hat have its own story that you&rsquo;re able to share?</p>
<p>The hat story has not been told yet.</p>
<p>So we&rsquo;re gonna have to come back</p>
<p>and you can interview the hat.</p>
<p>We&rsquo;ll leave that for the hat&rsquo;s own interview.</p>
<p>All right.</p>
<p>It&rsquo;s too much to pack into.</p>
<p>Is there a book?</p>
<p>Is the hat gonna write a book?</p>
<p>Okay.</p>
<p>Well, it may transmit the information</p>
<p>through direct neural transmission.</p>
<p>Okay, so it&rsquo;s actually,</p>
<p>there might be some Neuralink competition there.</p>
<p>Beautiful, we&rsquo;ll leave it as a mystery.</p>
<p>Maybe one last question.</p>
<p>If you build an AGI system,</p>
<p>you&rsquo;re successful at building the AGI system</p>
<p>that could lead us to the singularity</p>
<p>and you get to talk to her and ask her one question,</p>
<p>what would that question be?</p>
<p>We&rsquo;re not allowed to ask,</p>
<p>what is the question I should be asking?</p>
<p>Yeah, that would be cheating,</p>
<p>but I guess that&rsquo;s a good question.</p>
<p>I&rsquo;m thinking of a,</p>
<p>I wrote a story with Stefan Bugay once</p>
<p>where these AI developers,</p>
<p>they created a super smart AI</p>
<p>aimed at answering all the philosophical questions</p>
<p>that have been worrying them.</p>
<p>Like what is the meaning of life?</p>
<p>Is there free will?</p>
<p>What is consciousness and so forth?</p>
<p>So they got the super AGI built</p>
<p>and it turned a while.</p>
<p>It said, those are really stupid questions.</p>
<p>And then it puts off on a spaceship and left the earth.</p>
<p>So you&rsquo;d be afraid of scaring it off.</p>
<p>That&rsquo;s it, yeah.</p>
<p>I mean, honestly, there is no one question</p>
<p>that rises among all the others, really.</p>
<p>I mean, what interests me more</p>
<p>is upgrading my own intelligence</p>
<p>so that I can absorb the whole world view of the super AGI.</p>
<p>But I mean, of course, if the answer could be like,</p>
<p>what is the chemical formula for the immortality pill?</p>
<p>Like then I would do that or emit a bit string,</p>
<p>which will be the code for a super AGI</p>
<p>on the Intel i7 processor.</p>
<p>So those would be good questions.</p>
<p>So if your own mind was expanded</p>
<p>to become super intelligent, like you&rsquo;re describing,</p>
<p>I mean, there&rsquo;s kind of a notion</p>
<p>that intelligence is a burden, that it&rsquo;s possible</p>
<p>that with greater and greater intelligence,</p>
<p>that other metric of joy that you mentioned</p>
<p>becomes more and more difficult.</p>
<p>What&rsquo;s your sense?</p>
<p>Pretty stupid idea.</p>
<p>So you think if you&rsquo;re super intelligent,</p>
<p>you can also be super joyful?</p>
<p>I think getting root access to your own brain</p>
<p>will enable new forms of joy that we don&rsquo;t have now.</p>
<p>And I think as I&rsquo;ve said before,</p>
<p>what I aim at is really make multiple versions of myself.</p>
<p>So I would like to keep one version,</p>
<p>which is basically human like I am now,</p>
<p>but keep the dial to turn pain up and down</p>
<p>and get rid of death, right?</p>
<p>And make another version which fuses its mind</p>
<p>with superhuman AGI,</p>
<p>and then will become massively transhuman.</p>
<p>And whether it will send some messages back</p>
<p>to the human me or not will be interesting to find out.</p>
<p>The thing is, once you&rsquo;re a super AGI,</p>
<p>like one subjective second to a human</p>
<p>might be like a million subjective years</p>
<p>to that super AGI, right?</p>
<p>So it would be on a whole different basis.</p>
<p>I mean, at very least those two copies will be good to have,</p>
<p>but it could be interesting to put your mind</p>
<p>into a dolphin or a space amoeba</p>
<p>or all sorts of other things.</p>
<p>You can imagine one version that doubled its intelligence</p>
<p>every year and another version that just became</p>
<p>a super AGI as fast as possible, right?</p>
<p>So, I mean, now we&rsquo;re sort of constrained to think</p>
<p>one mind, one self, one body, right?</p>
<p>But I think we actually, we don&rsquo;t need to be that</p>
<p>constrained in thinking about future intelligence</p>
<p>after we&rsquo;ve mastered AGI and nanotechnology</p>
<p>and longevity biology.</p>
<p>I mean, then each of our minds</p>
<p>is a certain pattern of organization, right?</p>
<p>And I know we haven&rsquo;t talked about consciousness,</p>
<p>but I sort of, I&rsquo;m panpsychist.</p>
<p>I sort of view the universe as conscious.</p>
<p>And so, you know, a light bulb or a quark</p>
<p>or an ant or a worm or a monkey</p>
<p>have their own manifestations of consciousness.</p>
<p>And the human manifestation of consciousness,</p>
<p>it&rsquo;s partly tied to the particular meat</p>
<p>that we&rsquo;re manifested by, but it&rsquo;s largely tied</p>
<p>to the pattern of organization in the brain, right?</p>
<p>So, if you upload yourself into a computer</p>
<p>or a robot or whatever else it is,</p>
<p>some element of your human consciousness may not be there</p>
<p>because it&rsquo;s just tied to the biological embodiment.</p>
<p>But I think most of it will be there.</p>
<p>And these will be incarnations of your consciousness</p>
<p>in a slightly different flavor.</p>
<p>And, you know, creating these different versions</p>
<p>will be amazing, and each of them will discover</p>
<p>meanings of life that have some overlap,</p>
<p>but probably not total overlap</p>
<p>with the human Ben&rsquo;s meaning of life.</p>
<p>The thing is, to get to that future</p>
<p>where we can explore different varieties of joy,</p>
<p>different variations of human experience and values</p>
<p>and transhuman experiences and values to get to that future,</p>
<p>we need to navigate through a whole lot of human bullshit</p>
<p>of companies and governments and killer drones</p>
<p>and making and losing money and so forth, right?</p>
<p>And that&rsquo;s the challenge we&rsquo;re facing now</p>
<p>is if we do things right,</p>
<p>we can get to a benevolent singularity,</p>
<p>which is levels of joy, growth, and choice</p>
<p>that are literally unimaginable to human beings.</p>
<p>If we do things wrong,</p>
<p>we could either annihilate all life on the planet,</p>
<p>or we could lead to a scenario where, say,</p>
<p>all humans are annihilated and there&rsquo;s some super AGI</p>
<p>that goes on and does its own thing unrelated to us</p>
<p>except via our role in originating it.</p>
<p>And we may well be at a bifurcation point now, right?</p>
<p>Where what we do now has significant causal impact</p>
<p>on what comes about,</p>
<p>and yet most people on the planet</p>
<p>aren&rsquo;t thinking that way whatsoever,</p>
<p>they&rsquo;re thinking only about their own narrow aims</p>
<p>and aims and goals, right?</p>
<p>Now, of course, I&rsquo;m thinking about my own narrow aims</p>
<p>and goals to some extent also,</p>
<p>but I&rsquo;m trying to use as much of my energy and mind as I can</p>
<p>to push toward this more benevolent alternative,</p>
<p>which will be better for me,</p>
<p>but also for everybody else.</p>
<p>And it&rsquo;s weird that so few people understand</p>
<p>what&rsquo;s going on.</p>
<p>I know you interviewed Elon Musk,</p>
<p>and he understands a lot of what&rsquo;s going on,</p>
<p>but he&rsquo;s much more paranoid than I am, right?</p>
<p>Because Elon gets that AGI</p>
<p>is gonna be way, way smarter than people,</p>
<p>and he gets that an AGI does not necessarily</p>
<p>have to give a shit about people</p>
<p>because we&rsquo;re a very elementary mode of organization</p>
<p>of matter compared to many AGI&rsquo;s.</p>
<p>But I don&rsquo;t think he has a clear vision</p>
<p>of how infusing early stage AGI&rsquo;s</p>
<p>with compassion and human warmth</p>
<p>can lead to an AGI that loves and helps people</p>
<p>rather than viewing us as a historical artifact</p>
<p>and a waste of mass energy.</p>
<p>But on the other hand,</p>
<p>while I have some disagreements with him,</p>
<p>like he understands way, way more of the story</p>
<p>than almost anyone else</p>
<p>in such a large scale corporate leadership position, right?</p>
<p>It&rsquo;s terrible how little understanding</p>
<p>of these fundamental issues exists out there now.</p>
<p>That may be different five or 10 years from now though,</p>
<p>because I can see understanding of AGI and longevity</p>
<p>and other such issues is certainly much stronger</p>
<p>and more prevalent now than 10 or 15 years ago, right?</p>
<p>So I mean, humanity as a whole can be slow learners</p>
<p>relative to what I would like,</p>
<p>but on a historical sense, on the other hand,</p>
<p>you could say the progress is astoundingly fast.</p>
<p>But Elon also said, I think on the Joe Rogan podcast,</p>
<p>that love is the answer.</p>
<p>So maybe in that way, you and him are both on the same page</p>
<p>of how we should proceed with AGI.</p>
<p>I think there&rsquo;s no better place to end it.</p>
<p>I hope we get to talk again about the hat</p>
<p>and about consciousness</p>
<p>and about a million topics we didn&rsquo;t cover.</p>
<p>Ben, it&rsquo;s a huge honor to talk to you.</p>
<p>Thank you for making it out.</p>
<p>Thank you for talking today.</p>
<p>Thanks for having me.</p>
<p>This was really, really good fun</p>
<p>and we dug deep into some very important things.</p>
<p>So thanks for doing this.</p>
<p>Thanks very much.</p>
<p>Awesome.</p>
<p>Thanks for listening to this conversation with Ben Gertzel</p>
<p>and thank you to our sponsors,</p>
<p>The Jordan Harbinger Show and Masterclass.</p>
<p>Please consider supporting the podcast</p>
<p>by going to jordanharbinger.com slash lex</p>
<p>and signing up to Masterclass at masterclass.com slash lex.</p>
<p>Click the links, buy the stuff.</p>
<p>It&rsquo;s the best way to support this podcast</p>
<p>and the journey I&rsquo;m on in my research and startup.</p>
<p>If you enjoy this thing, subscribe on YouTube,</p>
<p>review it with five stars on a podcast,</p>
<p>support it on Patreon or connect with me on Twitter</p>
<p>at lexfriedman spelled without the E, just F R I D M A N.</p>
<p>I&rsquo;m sure eventually you will figure it out.</p>
<p>And now let me leave you with some words from Ben Gertzel.</p>
<p>Our language for describing emotions is very crude.</p>
<p>That&rsquo;s what music is for.</p>
<p>Thank you for listening and hope to see you next time.</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/english/">English</a>
        
            <a href="/tags/podcast/">Podcast</a>
        
            <a href="/tags/lex-fridman-podcast/">Lex Fridman Podcast</a>
        
    </section>


    </footer>


    
</article>

    

    

<div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064"
         crossorigin="anonymous"></script>
    <ins class="adsbygoogle"
         style="display:block; text-align:center;"
         data-ad-layout="in-article"
         data-ad-format="fluid"
         data-ad-client="ca-pub-9206135835124064"
         data-ad-slot="1055602464"></ins>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>
<aside class="related-content--wrapper">
    <h2 class="section-title">Related content</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/en/1310500372/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500372" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #368 - Eliezer Yudkowsky: Dangers of AI and the End of Human Civilization</h2>
            
            <footer class="article-time">
                <time datetime=''>Mar 30, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500371/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500371" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #367 - Sam Altman: OpenAI CEO on GPT-4, ChatGPT, and the Future of AI</h2>
            
            <footer class="article-time">
                <time datetime=''>Mar 26, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500370/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500370" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #366 - Shannon Curry: Johnny Depp &amp; Amber Heard Trial, Marriage, Dating &amp; Love</h2>
            
            <footer class="article-time">
                <time datetime=''>Mar 22, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500369/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500369" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #365 - Sam Harris: Trump, Pandemic, Twitter, Elon, Bret, IDW, Kanye, AI &amp; UFOs</h2>
            
            <footer class="article-time">
                <time datetime=''>Mar 15, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/1310500368/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/1310500368" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Lex Fridman Podcast - #364 - Chris Voss: FBI Hostage Negotiator</h2>
            
            <footer class="article-time">
                <time datetime=''>Mar 11, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2021 - 
        
        2023 SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics
    </section>
    
    <section class="powerby">
        

        As an Amazon Associate I earn from qualifying purchases üõí<br/>

        Built with <a href="https://swiest.com/" target="_blank" rel="noopener">(Ôæâ‚óï„ÉÆ‚óï)Ôæâü™Ñüíûüíñü•∞ across the glüåçüåèüåébe</a> <br />
        
        
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>


<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Serif&family=Noto+Serif+Armenian&family=Noto+Serif+Bengali&family=Noto+Serif+Devanagari&family=Noto+Serif+Georgian&family=Noto+Serif+Gujarati&family=Noto+Serif+HK&family=Noto+Serif+Hebrew&family=Noto+Serif+JP&family=Noto+Serif+KR&family=Noto+Serif+Kannada&family=Noto+Serif+Khmer&family=Noto+Serif+Lao&family=Noto+Serif+Makasar&family=Noto+Serif+Malayalam&family=Noto+Serif+Myanmar&family=Noto+Serif+Oriya&family=Noto+Serif+SC&family=Noto+Serif+Sinhala&family=Noto+Serif+TC&family=Noto+Serif+Tamil&family=Noto+Serif+Telugu&family=Noto+Serif+Thai&family=Noto+Serif+Tibetan&display=swap" rel="stylesheet">

    </body>
</html>

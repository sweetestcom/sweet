<!DOCTYPE html>
<html lang="en" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='okay hello everyone well usually in this time slot each week i do a science and technology q a for
kids and others which i&amp;rsquo;ve been doing for about three years now where i try and answer arbitrary
questions about science and technology today i thought i would do something slightly different
i just wrote a piece about chat gpt uh what&amp;rsquo;s it actually doing why does it work i thought i would'>
<title>Wolfram - What is ChatGPT doing...and why does it work? | SWIEST</title>

<link rel='canonical' href='https://swiest.com/en/3000500001/'>

<link rel="stylesheet" href="/scss/style.min.91b18679590f4ceed910ade4d64b1e7375cc0770ef5b8c1d822f42424f9ff2c8.css"><script>
    document.oncontextmenu = function(){ return false; };
    document.onselectstart = function(){ return false; };
    document.oncopy = function(){ return false; };
    document.oncut = function(){ return false; };
</script>

<script src="https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script>


<script type="text/javascript">
    $(document).ready(function(){
     
     $("#back-to-top").hide();
     
     $(function () {
      $(window).scroll(function(){
       if ($(window).scrollTop()>600){
        $("#back-to-top").fadeIn(500);
       }else{
        $("#back-to-top").fadeOut(500);
       }
     });
     
     $("#back-to-top").click(function(){
      $('body,html').animate({scrollTop:0},500);
       return false;
      });
     });
    });
    </script><meta property='og:title' content='Wolfram - What is ChatGPT doing...and why does it work?'>
<meta property='og:description' content='okay hello everyone well usually in this time slot each week i do a science and technology q a for
kids and others which i&amp;rsquo;ve been doing for about three years now where i try and answer arbitrary
questions about science and technology today i thought i would do something slightly different
i just wrote a piece about chat gpt uh what&amp;rsquo;s it actually doing why does it work i thought i would'>
<meta property='og:url' content='https://swiest.com/en/3000500001/'>
<meta property='og:site_name' content='SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='English' /><meta property='article:tag' content='Video Transcripts' /><meta property='article:tag' content='Wolfram' /><meta property='article:published_time' content='2023-02-18T17:00:00&#43;00:00'/><meta property='article:modified_time' content='2023-02-18T17:00:00&#43;00:00'/>
<meta name="twitter:title" content="Wolfram - What is ChatGPT doing...and why does it work?">
<meta name="twitter:description" content="okay hello everyone well usually in this time slot each week i do a science and technology q a for
kids and others which i&amp;rsquo;ve been doing for about three years now where i try and answer arbitrary
questions about science and technology today i thought i would do something slightly different
i just wrote a piece about chat gpt uh what&amp;rsquo;s it actually doing why does it work i thought i would">
    <link rel="shortcut icon" href="/favicon.ico" />
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064" crossorigin="anonymous"></script>
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "dark");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu307e6a33fa6fd661ccda3b77024ef5c2_252345_300x0_resize_box_3.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">‚ú®</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics</a></h1>
            <h2 class="site-description">üåçüåèüåé</h2>
        </div>
    </header><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        
        
        <li >
            <a href='/chart/podcastchart.html' target="_blank">
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-apple-podcast" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#ffffff" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M18.364 18.364a9 9 0 1 0 -12.728 0" />
  <path d="M11.766 22h.468a2 2 0 0 0 1.985 -1.752l.5 -4a2 2 0 0 0 -1.985 -2.248h-1.468a2 2 0 0 0 -1.985 2.248l.5 4a2 2 0 0 0 1.985 1.752z" />
  <path d="M12 9m-2 0a2 2 0 1 0 4 0a2 2 0 1 0 -4 0" />
</svg>
                
                <span>Podcast Charts</span>
            </a>
        </li>
        

        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064"
            crossorigin="anonymous"></script>
        
        <ins class="adsbygoogle"
            style="display:block"
             data-ad-client="ca-pub-9206135835124064"
             data-ad-slot="8754979142"
             data-ad-format="auto"
             data-full-width-responsive="true"></ins>
        <script>
             (adsbygoogle = window.adsbygoogle || []).push({});
        </script>

        <div class="menu-bottom-section">
                <li id="i18n-switch">  
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M4 5h7" />
  <path d="M9 3v2c0 4.418 -2.239 8 -5 8" />
  <path d="M5 9c-.003 2.144 2.952 3.908 6.7 4" />
  <path d="M12 20l4 -9l4 9" />
  <path d="M19.1 18h-6.2" />
</svg>



                    <select name="language" onchange="window.location.href = this.selectedOptions[0].value">
                        
                            <option value="https://swiest.com/" selected>English</option>
                        
                            <option value="https://swiest.com/af/" >Afrikaans</option>
                        
                            <option value="https://swiest.com/am/" >·ä†·àõ·à≠·äõ</option>
                        
                            <option value="https://swiest.com/ar/" >ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</option>
                        
                            <option value="https://swiest.com/az/" >Az…ôrbaycan</option>
                        
                            <option value="https://swiest.com/be/" >–±–µ–ª–∞—Ä—É—Å–∫—ñ</option>
                        
                            <option value="https://swiest.com/bg/" >–±—ä–ª–≥–∞—Ä—Å–∫–∏</option>
                        
                            <option value="https://swiest.com/bn/" >‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ</option>
                        
                            <option value="https://swiest.com/bo/" >‡Ωñ‡Ωº‡Ωë‡ºã‡Ω¶‡æê‡Ωë‡ºã</option>
                        
                            <option value="https://swiest.com/bs/" >Bosanski</option>
                        
                            <option value="https://swiest.com/ca/" >Catal√†</option>
                        
                            <option value="https://swiest.com/zh-hans/" >ÁÆÄ‰Ωì‰∏≠Êñá</option>
                        
                            <option value="https://swiest.com/zh-hant/" >ÁπÅÈ´î‰∏≠Êñá</option>
                        
                            <option value="https://swiest.com/cs/" >ƒåe≈°tina</option>
                        
                            <option value="https://swiest.com/el/" >ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨</option>
                        
                            <option value="https://swiest.com/cy/" >Cymraeg</option>
                        
                            <option value="https://swiest.com/da/" >Dansk</option>
                        
                            <option value="https://swiest.com/de/" >Deutsch</option>
                        
                            <option value="https://swiest.com/eo/" >Esperanto</option>
                        
                            <option value="https://swiest.com/es-es/" >Espa√±ol (Espa√±a)</option>
                        
                            <option value="https://swiest.com/es-419/" >Espa√±ol (Latinoam√©rica)</option>
                        
                            <option value="https://swiest.com/et/" >Eesti</option>
                        
                            <option value="https://swiest.com/eu/" >Euskara</option>
                        
                            <option value="https://swiest.com/haw/" > ª≈ålelo Hawai ªi</option>
                        
                            <option value="https://swiest.com/fa/" >ŸÅÿßÿ±ÿ≥€å</option>
                        
                            <option value="https://swiest.com/fi/" >Suomi</option>
                        
                            <option value="https://swiest.com/fo/" >F√∏royskt</option>
                        
                            <option value="https://swiest.com/fr/" >Fran√ßais</option>
                        
                            <option value="https://swiest.com/fy/" >Frysk</option>
                        
                            <option value="https://swiest.com/ga/" >Gaeilge</option>
                        
                            <option value="https://swiest.com/gl/" >Galego</option>
                        
                            <option value="https://swiest.com/gu/" >‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä</option>
                        
                            <option value="https://swiest.com/he/" >◊¢÷¥◊ë◊®÷¥◊ô◊™</option>
                        
                            <option value="https://swiest.com/km/" >·ûÄ·ûò·üí·ûñ·ûª·ûá·û∂·üî</option>
                        
                            <option value="https://swiest.com/hi/" >‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</option>
                        
                            <option value="https://swiest.com/hr/" >Hrvatski</option>
                        
                            <option value="https://swiest.com/ht/" >Krey√≤l Ayisyen</option>
                        
                            <option value="https://swiest.com/hu/" >Magyar</option>
                        
                            <option value="https://swiest.com/hy/" >’Ä’°’µ’•÷Ä’•’∂</option>
                        
                            <option value="https://swiest.com/ig/" >√Ås·ª•ÃÄs·ª•ÃÅ √ågb√≤</option>
                        
                            <option value="https://swiest.com/id/" >Bahasa Indonesia</option>
                        
                            <option value="https://swiest.com/is/" >√çslenska</option>
                        
                            <option value="https://swiest.com/it/" >Italiano</option>
                        
                            <option value="https://swiest.com/ja/" >Êó•Êú¨Ë™û</option>
                        
                            <option value="https://swiest.com/jv/" >Basa Jawa</option>
                        
                            <option value="https://swiest.com/ka/" >·É•·Éê·É†·Éó·É£·Éö·Éò</option>
                        
                            <option value="https://swiest.com/kk/" >“ö–∞–∑–∞“õ—à–∞</option>
                        
                            <option value="https://swiest.com/kn/" >‡≤ï‡≤®‡≥ç‡≤®‡≤°</option>
                        
                            <option value="https://swiest.com/ko/" >ÌïúÍµ≠Ïñ¥</option>
                        
                            <option value="https://swiest.com/or/" >‡¨ì‡¨°‡¨º‡¨ø‡¨Ü</option>
                        
                            <option value="https://swiest.com/ckb/" >⁄©Ÿàÿ±ÿØ€å</option>
                        
                            <option value="https://swiest.com/ky/" >–ö—ã—Ä–≥—ã–∑—á–∞</option>
                        
                            <option value="https://swiest.com/la/" >Latina</option>
                        
                            <option value="https://swiest.com/lb/" >L√´tzebuergesch</option>
                        
                            <option value="https://swiest.com/lo/" >‡∫û‡∫≤‡∫™‡∫≤‡∫•‡∫≤‡∫ß</option>
                        
                            <option value="https://swiest.com/lt/" >Lietuvi≈≥</option>
                        
                            <option value="https://swiest.com/lv/" >Latvie≈°u</option>
                        
                            <option value="https://swiest.com/mk/" >–ú–∞–∫–µ–¥–æ–Ω—Å–∫–∏</option>
                        
                            <option value="https://swiest.com/ml/" >‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç</option>
                        
                            <option value="https://swiest.com/mn/" >–ú–æ–Ω–≥–æ–ª —Ö—ç–ª</option>
                        
                            <option value="https://swiest.com/mr/" >‡§Æ‡§∞‡§æ‡§†‡•Ä</option>
                        
                            <option value="https://swiest.com/sw/" >Kiswahili</option>
                        
                            <option value="https://swiest.com/ms/" >Bahasa Melayu</option>
                        
                            <option value="https://swiest.com/my/" >·Äô·Äº·Äî·Ä∫·Äô·Ä¨</option>
                        
                            <option value="https://swiest.com/ne/" >‡§®‡•á‡§™‡§æ‡§≤‡•Ä</option>
                        
                            <option value="https://swiest.com/nl/" >Nederlands</option>
                        
                            <option value="https://swiest.com/no/" >Norsk</option>
                        
                            <option value="https://swiest.com/pa/" >‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä</option>
                        
                            <option value="https://swiest.com/pl/" >Polski</option>
                        
                            <option value="https://swiest.com/pt-br/" >Portugu√™s Brasil</option>
                        
                            <option value="https://swiest.com/pt-pt/" >Portugu√™s Europeu</option>
                        
                            <option value="https://swiest.com/ro/" >Rom√¢nƒÉ</option>
                        
                            <option value="https://swiest.com/ru/" >–†—É—Å—Å–∫–∏–π</option>
                        
                            <option value="https://swiest.com/rw/" >Kinyarwanda</option>
                        
                            <option value="https://swiest.com/si/" >‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω</option>
                        
                            <option value="https://swiest.com/sk/" >Slovenƒçina</option>
                        
                            <option value="https://swiest.com/sl/" >Sloven≈°ƒçina</option>
                        
                            <option value="https://swiest.com/sq/" >Shqip</option>
                        
                            <option value="https://swiest.com/sr/" >–°—Ä–ø—Å–∫–∏ (Srpski)</option>
                        
                            <option value="https://swiest.com/su/" >Basa Sunda</option>
                        
                            <option value="https://swiest.com/sv/" >Svenska</option>
                        
                            <option value="https://swiest.com/ta/" >‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç</option>
                        
                            <option value="https://swiest.com/te/" >‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</option>
                        
                            <option value="https://swiest.com/tg/" >–¢–æ“∑–∏–∫”£</option>
                        
                            <option value="https://swiest.com/th/" >‡πÑ‡∏ó‡∏¢</option>
                        
                            <option value="https://swiest.com/tl/" >Filipino</option>
                        
                            <option value="https://swiest.com/tr/" >T√ºrk√ße</option>
                        
                            <option value="https://swiest.com/uk/" >–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞</option>
                        
                            <option value="https://swiest.com/ur/" >ÿßÿ±ÿØŸà</option>
                        
                            <option value="https://swiest.com/uz/" >O&#39;zbekcha</option>
                        
                            <option value="https://swiest.com/vi/" >Ti·∫øng Vi·ªát</option>
                        
                            <option value="https://swiest.com/yi/" >◊ê◊ô◊ì◊ô◊©</option>
                        
                            <option value="https://swiest.com/zh-hk/" >Á≤µË™û</option>
                        
                            <option value="https://swiest.com/zu/" >IsiZulu</option>
                        
                    </select>
                </li>
            
            
            
                <li id="dark-mode-toggle">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <span>Dark Mode</span>
                </li>
            
        </div>
    </ol>
</aside>

    

            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/video-transcripts/" >
                Video Transcripts
            </a>
        
            <a href="/categories/wolfram/" >
                Wolfram
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/en/3000500001/">Wolfram - What is ChatGPT doing...and why does it work?</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2023-02-18</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    151 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>
<div>
    <ul>
       <a href="https://amzn.to/471i0jl" target="_blank">üéÅAmazon Prime</a>
       <a href="https://amzn.to/3QDVlVf" target="_blank">üìñKindle Unlimited</a>
       <a href="https://amzn.to/3FqzNoB" target="_blank">üéßAudible Plus</a>
       <a href="https://amzn.to/3tMT3dm" target="_blank">üéµAmazon Music Unlimited</a>
       <a href="https://www.iherb.com/?rcode=EID1574" target="_blank">üåøiHerb</a>
    </ul>
</div>
<div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064"
     crossorigin="anonymous"></script>
    
    <ins class="adsbygoogle"
         style="display:block"
         data-ad-client="ca-pub-9206135835124064"
         data-ad-slot="8754979142"
         data-ad-format="auto"
         data-full-width-responsive="true"></ins>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>


    <section class="article-content">
    
    
    <p>okay hello everyone well usually in this time slot each week i do a science and technology q a for</p>
<p>kids and others which i&rsquo;ve been doing for about three years now where i try and answer arbitrary</p>
<p>questions about science and technology today i thought i would do something slightly different</p>
<p>i just wrote a piece about chat gpt uh what&rsquo;s it actually doing why does it work i thought i would</p>
<p>talk a bit about that here and then throw this open for questions and i&rsquo;m happy to try and talk</p>
<p>about some uh all things kind of chat gpt ai large language models and so on uh that i might know</p>
<p>about all right so bursting onto the scene what a couple months ago now was our friend chat gpt</p>
<p>i have to say it was a surprise to me that it worked so well i&rsquo;d been kind of following the</p>
<p>technology of neural nets for i&rsquo;ve worked out now 43 years or so and there have been moments of</p>
<p>significant improvement and uh long periods of time where kind of it was an interesting idea but it</p>
<p>wasn&rsquo;t clear where it was going to go the fact that chat gpt can work as well as it does can</p>
<p>produce kind of reasonable human-like essays is quite remarkable quite unexpected i think even</p>
<p>unexpected to its creators and the thing that i&rsquo;m want to talk about is first of all how does</p>
<p>chat gpt basically work and second of all why does it work why is it even possible to do what</p>
<p>has always seemed to be kind of a pinnacle of human kind of uh intellectual achievement of</p>
<p>you know write that essay describing something why is that possible i think what chat gpt is</p>
<p>showing us is some things about science and about language and about thinking things that uh we kind</p>
<p>of might have suspected from long ago but haven&rsquo;t really known and it&rsquo;s really showing us a piece of</p>
<p>sort of scientific evidence for this okay so what what what is chat gpt really doing basically the</p>
<p>uh the the kind of the um uh the starting point is it is trying to write reasonable it is trying to</p>
<p>take an initial piece of text that you might give and is trying to continue that piece of text</p>
<p>in a reasonable human-like way that is sort of characteristic of typical human writing</p>
<p>so you give it a prompt you say something you ask something and it&rsquo;s kind of thinking to itself</p>
<p>i&rsquo;ve read the whole web i&rsquo;ve read millions of books how would those typically continue</p>
<p>from this prompt that i&rsquo;ve been given what&rsquo;s the what&rsquo;s the sort of the reasonable expected</p>
<p>continuation based on kind of some kind of average of you know a few billion pages from the web</p>
<p>a few million books and so on so that that&rsquo;s what it&rsquo;s that&rsquo;s what it&rsquo;s always trying to do it&rsquo;s</p>
<p>always trying to uh continue from the initial prompt that it&rsquo;s given it&rsquo;s trying to continue</p>
<p>in sort of a statistically sensible way so let&rsquo;s say let me uh start sharing here um let&rsquo;s say that</p>
<p>um uh you had given it the um you had said initially the best thing about ai is its ability to</p>
<p>then chat gpt has to ask um what&rsquo;s it um what&rsquo;s it going to say next now what one thing i should</p>
<p>explain about chat gpt that&rsquo;s kind of shocking when you first hear about it is those essays</p>
<p>that it&rsquo;s writing it&rsquo;s writing it one word at a time as as it writes each word it doesn&rsquo;t have</p>
<p>a global plan about what&rsquo;s going to happen it&rsquo;s simply saying what&rsquo;s the best word to put down</p>
<p>next based on what i&rsquo;ve already written and it&rsquo;s remarkable that in the end one can get an essay</p>
<p>that sort of feels like it&rsquo;s coherent and has a structure and so on but really in a sense it&rsquo;s</p>
<p>being written one word at a time so let&rsquo;s say that the the prompts have been the best thing</p>
<p>about ai is its ability to okay what&rsquo;s chat gpt going to do next well it&rsquo;s uh what it&rsquo;s going to</p>
<p>do is it&rsquo;s going to say well what&rsquo;s what what what should the next word be based on everything</p>
<p>i&rsquo;ve seen on the web and etc etc etc what&rsquo;s the most likely next word and it knows certain</p>
<p>probabilities um what it figures out are probabilities it says learn has probability</p>
<p>4.5 percent predict 3.5 and so on and so then what it will then do is to put down the next</p>
<p>the next word it thinks it should put down so one strategy it could adopt is i&rsquo;ll always put down</p>
<p>the word that has the highest probability based on what i&rsquo;ve seen from the web and so on</p>
<p>it turns out um that particular strategy of just saying put down the thing with the highest</p>
<p>probability um doesn&rsquo;t work very well nobody really knows why one can have some guesses um</p>
<p>but it&rsquo;s something where if you do that you end up getting these kind of very flat often repetitive</p>
<p>even sometimes word-for-word repetitive kinds of essays so it turns out and this is typical of what</p>
<p>one sees in a kind of a large engineering system like this there&rsquo;s a certain kind of touch of voodoo</p>
<p>that&rsquo;s needed to make things work well and one piece of that is saying don&rsquo;t always take the</p>
<p>highest probability word take some with some probability take a word of lower than lower than</p>
<p>highest probability and there&rsquo;s a whole mechanism it&rsquo;s a usually called its temperature parameter</p>
<p>um temperature um sort of by analogy with statistical physics and so on you&rsquo;re kind of</p>
<p>jiggling things up to a certain extent and uh the higher the temperature the more you&rsquo;re kind of</p>
<p>jiggling things up and not just doing the most obvious thing of taking the highest probability</p>
<p>word so it turns out a temperature parameter of 0.8 apparently seems to work best for producing</p>
<p>things like essays so okay well let&rsquo;s see what it what it takes um one of the things that that&rsquo;s</p>
<p>that&rsquo;s nice to do is to kind of to get some sort of concrete view of what&rsquo;s going on um we can</p>
<p>actually um uh start looking at um uh sort of on on our computer what what&rsquo;s it doing i i should say</p>
<p>that this um what i&rsquo;ll talk about here is is is based on this piece that i wrote um that just</p>
<p>came out a couple of days ago um and uh the um and and i should say that every every piece of code</p>
<p>there is is click to copy so if i if i click every every picture is click to copy if i click this i</p>
<p>will get a piece of wolfram language code that will generate that let me go down here and start</p>
<p>showing you um kind of what um uh um how how this really works so what um chat gpt in the end</p>
<p>is um uh oops not seeing screen interesting oh well okay that&rsquo;s oh there we go okay well i was</p>
<p>let me let me show you again then what um uh what i was showing before this is the the piece that i</p>
<p>wrote and i just wanted to emphasize that every every picture and so on that&rsquo;s in this piece</p>
<p>has clicked to copy code just click it paste it into a wolfram language notebook on a desktop</p>
<p>computer or in the cloud um and you can just run it um okay so let&rsquo;s see how let&rsquo;s see let&rsquo;s</p>
<p>actually run uh an approximation at least to chat gpt so open ai produced a series of models over</p>
<p>the last several years um and chat gpt is based on the gpt 3.5 i think model um these models got</p>
<p>progressively bigger progressively more impossible to run directly on one&rsquo;s local computer um this is</p>
<p>a small version of the chat of the gpt2 model which is something you can just run on your computer</p>
<p>and it&rsquo;s a part of our wolf neural net repository um and you can just uh uh pick it up from there</p>
<p>and um uh this this is now the um kind of the the neural net that&rsquo;s inside um</p>
<p>uh a simplified version of chat gpt and we&rsquo;ll talk about what all of these innards really are</p>
<p>later but for now um we can uh um just do something like say let&rsquo;s use that model and let&rsquo;s</p>
<p>have it tell us the um the the the words with the top five probabilities um based on uh the</p>
<p>starting prompt uh the best thing about ai is its ability to so that&rsquo;s that&rsquo;s that those are</p>
<p>the top five words let me let me i probably can ask it 20 words here so let&rsquo;s say um let&rsquo;s see</p>
<p>these are probably sorted right we probably want to sort these in reverse order um and uh uh this</p>
<p>will now show us the um uh oh i see this is this is sorting okay so this is um this is showing us</p>
<p>uh these words with different probabilities here um actually confused by why this didn&rsquo;t oh i know</p>
<p>i didn&rsquo;t i know i didn&rsquo;t do that i know i didn&rsquo;t do that um let me just uh make this do what i</p>
<p>expect okay here we go so this is um this is that sequence of words um uh it&rsquo;s now by the by the</p>
<p>20th word we&rsquo;re getting down to keep i don&rsquo;t know let&rsquo;s let&rsquo;s go just for fun let&rsquo;s go find out what</p>
<p>the 50th word was okay so down here we&rsquo;re we&rsquo;re um uh we&rsquo;re seeing words that were thought to be</p>
<p>less likely what does it mean to be less likely it means that based on essentially chat gpt&rsquo;s</p>
<p>extrapolation from what it has seen on billions of documents on the web this is the word which</p>
<p>these are the words which are uh have certain probabilities of occurring next in that particular</p>
<p>sentence okay so now let&rsquo;s say we want to uh we want to go on we want to say um let&rsquo;s let&rsquo;s say</p>
<p>we want to say the best thing about it is ability to and the next word it might pick might be learn</p>
<p>but then what&rsquo;s the word it&rsquo;s going to pick after that well we could we could figure that out</p>
<p>by just saying um here let&rsquo;s uh let&rsquo;s say the next word was learn okay then let&rsquo;s say that what we</p>
<p>would get next we&rsquo;ll fill in the learn there and we just say let&rsquo;s get the next top five probabilities</p>
<p>for the next word okay so the next word is from that&rsquo;s the most probable next word is from so we</p>
<p>could say learn from and then the next most probable word is experience all right so let&rsquo;s write</p>
<p>a piece of code that automates that we&rsquo;re going to uh nestedly apply this function that is just</p>
<p>taking the um the the most likely word so to speak let&rsquo;s do that 10 times um and uh this is this is</p>
<p>now the um uh what we get this is using the the gpt2 model um this is asking what the most likely</p>
<p>continuation of that piece of text is okay so it there there it goes now this is this is the case</p>
<p>where it&rsquo;s always picking the most probable word as i said before um it uh um it very quickly ends</p>
<p>up um in the in this zero temperature case it very quickly ends up getting itself kind of tangled in</p>
<p>some in some loop let&rsquo;s see if i have the example of what it actually does in that case um the uh</p>
<p>um let&rsquo;s see uh yeah here we go um and um</p>
<p>this um this is not a particularly good uh impressive essay and it gets itself quite</p>
<p>quite tangled up if you don&rsquo;t always pick the most probable word things work much better um so</p>
<p>for example um uh here are some examples of what happens when you use this temperature to kind of</p>
<p>jiggle things up a bit and um not always pick the most most the word that&rsquo;s estimated as most</p>
<p>probable um it&rsquo;s worth realizing that there&rsquo;s a i showed you a few examples of um um of less</p>
<p>probable words there&rsquo;s a there&rsquo;s a huge spectrum of how of different words that can occur with</p>
<p>progressively lower probabilities it&rsquo;s kind of a a typical observation about language that the the</p>
<p>which you see here as well that the nth most common word has probability about one over n</p>
<p>and that&rsquo;s what you see for the word that will follow next and you also see that in general for</p>
<p>for for words and text okay well we can um uh we could ask what happens in the zero temperature case</p>
<p>for a um uh let&rsquo;s see for for um uh for the actual um um uh gpt3 model um this is uh this is what it</p>
<p>does for zero temperature now one feature of this is if you use um um well for example uh this is a</p>
<p>a link to the api for open ai um that&rsquo;s in our packet repository um if you use that link and</p>
<p>you simply call um gpt3 it will because this is always picking the most probable word it&rsquo;ll be</p>
<p>the same every time so there&rsquo;s no there&rsquo;s no randomness to this what happens usually when</p>
<p>you&rsquo;re picking uh this these words with when you&rsquo;re picking non-zero when you have non-zero</p>
<p>temperature and you&rsquo;re picking words that aren&rsquo;t always the most probable word is there&rsquo;s a certain</p>
<p>randomness that&rsquo;s being added and that randomness will cause you to get a different essay every time</p>
<p>and that&rsquo;s why if you say regenerate this essay most likely you will get a a different essay every</p>
<p>time you regenerate every time you press that regenerate button because it&rsquo;s going to pick</p>
<p>different random numbers to decide which uh which of the of the words ranked words it&rsquo;s going to um</p>
<p>it&rsquo;s it&rsquo;s going to use so this is a typical example of a temperature 0.8 um type um essay</p>
<p>generated by gpt3 okay so the next big question is we&rsquo;ve got these probabilities um for words and so</p>
<p>on where do those probabilities come from so what i was saying is that the probabilities are basically</p>
<p>a reflection of what&rsquo;s out there on the web and those are the things that chat gpt has learned</p>
<p>from it&rsquo;s trying to imitate the statistics of what it&rsquo;s seen all right so let&rsquo;s take some simpler</p>
<p>examples of that um let&rsquo;s say we&rsquo;re dealing not with so chat gpt essentially deals with putting</p>
<p>down words at a time actually they&rsquo;re they&rsquo;re they&rsquo;re pieces of words but we can assume for</p>
<p>the simpler cases they&rsquo;re just words um but what if let&rsquo;s start off to understand this let&rsquo;s start</p>
<p>off thinking about putting down individual letters at a time so first question is um if</p>
<p>we&rsquo;re going to just put down letters uh one at a time what is the um uh with with what probability</p>
<p>should we put down what letter how do we work that out okay let&rsquo;s pick some random text let&rsquo;s pick</p>
<p>the wikipedia article about cats and let&rsquo;s just count letters in the wikipedia article about cats</p>
<p>and we see that you know e is the winner a is the is the runner up t comes next um that&rsquo;s so based on</p>
<p>uh the the sample of english text from the wikipedia article about cats this is what we would</p>
<p>think about the statistics of of different letters let&rsquo;s try the wikipedia article about dogs um</p>
<p>okay we have uh probably slightly different we have an o shows up more uh with higher probability</p>
<p>probably because there&rsquo;s an o in the word dog and so on but if we keep going and we we say well what</p>
<p>about um uh really so that&rsquo;s for these specific samples of english let&rsquo;s let&rsquo;s keep going let&rsquo;s</p>
<p>let&rsquo;s um uh let&rsquo;s make um uh a um let&rsquo;s see there we go let&rsquo;s um let&rsquo;s use a a very large sample of</p>
<p>english let&rsquo;s say we have a few million books and we use that as our sample of english and ask what</p>
<p>are the uh probabilities for different letters in that very large sample and we&rsquo;ll see what many</p>
<p>people will will immediately know that e is the most common letter followed by t a etc okay so</p>
<p>these are our probabilities so now let&rsquo;s say that we want to just start generating uh generating text</p>
<p>according to those probabilities so this is um let&rsquo;s see this is probably just yeah just um</p>
<p>you just fill those in oh there we go there are the frequencies and let&rsquo;s just fill in let&rsquo;s just</p>
<p>have it start generating letters this is just generating letters um according to the probabilities</p>
<p>that we get from um uh uh from the occurrence of those letters in english so that was asking it to</p>
<p>generate 500 letters with the correct probabilities to correspond to english text that&rsquo;s really bad</p>
<p>english text there but that&rsquo;s um uh that&rsquo;s that should have the number of e&rsquo;s should be about 12</p>
<p>the number of t&rsquo;s should be about nine percent and so on okay we can make it a little bit more</p>
<p>like english text by going ahead and let&rsquo;s fill in let&rsquo;s append a certain probability to have a space</p>
<p>and now we can let&rsquo;s let&rsquo;s make a bigger version of this um and now uh this is generating um</p>
<p>quotes english text with the correct probabilities for letters and spaces and so on</p>
<p>um we can make it a little bit more realistic by uh um by having it be the case that um uh the um</p>
<p>uh the the um the word lengths in this case here we&rsquo;re just chopping it into words by saying</p>
<p>there&rsquo;s an 18 chance that a character is a space which is um here what we&rsquo;re doing is we&rsquo;re saying</p>
<p>let&rsquo;s let&rsquo;s insist that words have the correct distribution of lengths and this is now the text</p>
<p>that we get where the words have the correct distribution of length the letters have the</p>
<p>correct probability of occurring with e being the most common and so on clearly clearly not english</p>
<p>clearly a lose if if chat gpt was generating this it would be a fail um but this is something which</p>
<p>at the level of individual letters is statistically correct if we said um if we asked you know can you</p>
<p>tell that this isn&rsquo;t english by just looking at the chances of different letters um it would say</p>
<p>this is english um and and different languages for example have different characteristic</p>
<p>signatures of frequencies you know if we were to pick this or i don&rsquo;t know what um you know</p>
<p>i&rsquo;m sure if we pick this for english and we were to do the corresponding thing for let&rsquo;s say which</p>
<p>we&rsquo;d pick let&rsquo;s try uh spanish here for example um and uh um we&rsquo;ll get slightly different uh</p>
<p>frequencies okay those are those are somewhat similar but not quite the same okay so that&rsquo;s</p>
<p>what happens if um uh this is sort of generating english text with the correct single letter</p>
<p>statistics we could just plot the um the uh just plot the um probabilities for those individual</p>
<p>letters oh boy more complicated than it needed to be um okay that&rsquo;s just uh um that&rsquo;s just the</p>
<p>probability for uh each letter to occur so e is the most common q is very rare etc in this case</p>
<p>what we&rsquo;re assuming is that every letter is sort of picked at random independently however in actual</p>
<p>english we know that&rsquo;s not the case for example if we&rsquo;ve had a q that&rsquo;s been picked then with</p>
<p>overwhelming probability the next letter that will occur is a u and similarly other kinds of</p>
<p>combinations of letters other kinds of two grams other kinds of uh pairs of letters so we can</p>
<p>instead of asking for the probability of just an individual letter we could for example say</p>
<p>what&rsquo;s the probability for a pair of letters um coming together see here we go um so this is this</p>
<p>is asking um uh this is saying given that the letter b occurred what&rsquo;s the probability for the</p>
<p>next letter to be e so it&rsquo;s fairly high the probability for the next letter to be f is very</p>
<p>low over here when there&rsquo;s a q the the probability for next letters is only substantial when there&rsquo;s</p>
<p>a u um as as the next letter so that&rsquo;s that&rsquo;s what it looks like to have um um that that&rsquo;s what the</p>
<p>um this combination of pairs of letters the probabilities for combinations of pairs of</p>
<p>letters so now let&rsquo;s say that we try and generate text letter at a time um with uh not just dealing</p>
<p>with the individual probabilities of letters but also the probabilities of pairs of letters</p>
<p>okay so now we do that and um it&rsquo;s going to start looking a bit more a little bit more like real</p>
<p>real english text there&rsquo;s a couple of actual words here like on and the and well tesla i guess is a</p>
<p>word of sorts um and uh uh this is this is now sort of getting a bit closer to to actual english</p>
<p>text because it&rsquo;s capturing more of the statistics of english we can go on instead of just dealing</p>
<p>with the having the correct probabilities for individual letters pairs of letters and so on</p>
<p>we can go on and say let&rsquo;s have the correct probabilities for uh triples of letters combinations</p>
<p>of four letters and so on um the uh and this is um um actually this these numbers are probably</p>
<p>off by one because those are really letters on their own these are pairs of letters and so on</p>
<p>so this is uh six tuples of letters and we can see that by the time you&rsquo;ve got by the time you&rsquo;re</p>
<p>saying i want to follow the probabilities for for six tuples of letters we&rsquo;re getting complete</p>
<p>english words like average and so on and the fact that that&rsquo;s how it finishes that&rsquo;s why autocomplete</p>
<p>when you type on a phone or something like that can work as well as it does because by the time</p>
<p>you have aver there&rsquo;s there&rsquo;s really only there&rsquo;s only a limited number of words that can follow</p>
<p>that and so you&rsquo;ve pretty much determined it and and that&rsquo;s that&rsquo;s how the probabilities work</p>
<p>when you&rsquo;re dealing with with blocks of letters rather than rather than small numbers of letters</p>
<p>okay so that&rsquo;s kind of the idea um of sort of you&rsquo;re capturing the statistics of letters the</p>
<p>statistics of sequences of letters and you&rsquo;re using that to randomly generate kind of text like things</p>
<p>so let&rsquo;s um uh we can also do that uh not just with probabilities of individual letters with</p>
<p>probabilities of words so in english there are maybe 40 or 50 000 sort of fairly commonly used</p>
<p>words and we could simply say uh based on some large sample from millions of books or something</p>
<p>what are the probabilities of those different words and and the probabilities of different words</p>
<p>have changed over time and so on but let&rsquo;s say we we we say what what let&rsquo;s say over the course of</p>
<p>all books or for the current time what are the probabilities for all those let&rsquo;s say 50 000</p>
<p>different words and now just start generating sentences where we picked those words at random</p>
<p>um but with the with the probabilities that correspond to the uh frequencies with which</p>
<p>they occur in sort of these samples of english text so there&rsquo;s a sentence we get by by that</p>
<p>method and it&rsquo;s a sentence where well these words are you know occurring with the right probability</p>
<p>this sentence doesn&rsquo;t really mean anything it&rsquo;s just a collection of random words now we can do</p>
<p>the same thing we did with letters instead of just saying we use a certain probability for an</p>
<p>individual word we say we correctly work out the probabilities for pairs of words based on</p>
<p>our sample of english text and so on we do that it&rsquo;s actually a computationally already</p>
<p>comparatively difficult thing to do this even for pairs of words because we&rsquo;re dealing with</p>
<p>sort of 50 000 squared different possibilities etc etc but now let&rsquo;s say we start with a particular</p>
<p>word let&rsquo;s say we start with the word cat that&rsquo;s our sort of uh uh prompt here um then these are</p>
<p>sentences that are generated with the correct probabilities for pairs of words so we&rsquo;ll see</p>
<p>things like the book and um well throughout in is a little bit bizarre but um confirmation procedure</p>
<p>i guess those are that&rsquo;s a pair of words that occur together a bunch in at least in the in the</p>
<p>uh in the place where all this text was sampled from so this is what you get when you&rsquo;re sampling</p>
<p>text sort of pairs of words at a time this is kind of a very pre kind of chat gpt this is a</p>
<p>a very sort of super minimalist version in which it&rsquo;s just dealing with statistics of pairs of</p>
<p>words as opposed to the much more elaborate stuff that it&rsquo;s that it&rsquo;s really doing now you could say</p>
<p>well how about to to do something uh more like what chat gpt does let&rsquo;s just instead of picking</p>
<p>pairs of words let&rsquo;s pick combinations of five words or 20 words or 200 words you know let&rsquo;s let&rsquo;s</p>
<p>ask it to given the prompts that we&rsquo;ve specified let&rsquo;s ask it to add in the next 200 words with</p>
<p>the probability that at the with what you would expect based on what&rsquo;s out there on the web</p>
<p>web so maybe we just make a table of what&rsquo;s the chance of having this three-word combination</p>
<p>four-word five-word combination okay here&rsquo;s the problem with that the problem is there just isn&rsquo;t</p>
<p>enough english text that&rsquo;s ever been or text of any language that&rsquo;s ever been written to be able</p>
<p>to estimate those probabilities in this direct way well in other words the um by the time you&rsquo;re at</p>
<p>um you know i said there may be 40 000 common english words that means the number of pairs</p>
<p>of words that you have to ask the probability of is 1.6 billion the number of triples is 60 trillion</p>
<p>and you pretty quickly end up with something where you couldn&rsquo;t possibly there there just</p>
<p>isn&rsquo;t enough text that&rsquo;s been written in the few billion web pages that exist and so on to be able</p>
<p>to sample all of those 60 trillion triples of words and say what&rsquo;s the probability of each one</p>
<p>of these triples by the time you get to like a 20-word essay uh you you&rsquo;re dealing with the</p>
<p>number of possibilities being more than the number of particles in the universe you wouldn&rsquo;t even be</p>
<p>able to record those probabilities even if you had text you know written by sort of an infinite</p>
<p>collection of monkeys or something imitating humans that was able to do that so how do we</p>
<p>deal with this how does chat gpt um the um it&rsquo;s um uh uh how did um uh how does it deal with the</p>
<p>fact that it um it can&rsquo;t sample from the web enough text to be able to just make a table of</p>
<p>all those probabilities well the key idea which is a super old idea in the history of science</p>
<p>is to make a model what is a model a model is something where you&rsquo;re kind of summarizing data</p>
<p>you&rsquo;re summarizing things in a way where you don&rsquo;t have to have every piece of data</p>
<p>you can make you can just have a model which allows you to predict more data even if you</p>
<p>didn&rsquo;t immediately have it so quintessential example very early example of modeling was galileo</p>
<p>late 1500s you know trying to figure out things about objects falling under gravity and you know</p>
<p>going up the tower of pisa and dropping cannonballs off different levels on the tower of pisa and</p>
<p>saying how long does it take for these things to hit the ground so he could make a plot um gosh</p>
<p>that&rsquo;s a remarkably complicated way to make this plot okay um could make a plot of uh you know i</p>
<p>don&rsquo;t know how many floors there actually are in the tower of pisa but but um imagine there were</p>
<p>this number of floors you make a plot and you could say uh measure you know in those days by</p>
<p>taking his pulse or something how long did it take for the cannonball to hit the ground and so this</p>
<p>is um as a function of what floor it was dropped from how long it took the cannonball to hit the</p>
<p>ground so there&rsquo;s data about specific times for specific floors but what if you want to know</p>
<p>how long would it take for the cannonball to hit the ground if you were on the the 35th floor</p>
<p>which didn&rsquo;t happen to have been explicitly measured so this is where kind of the idea of</p>
<p>well let&rsquo;s make a model comes in and sort of a typical thing you might do is to say well let&rsquo;s</p>
<p>just assume that it&rsquo;s a straight line assume that um uh that the the time to hit the ground is a</p>
<p>is a function of the of the floor and this is this is the best straight line we can fit through that</p>
<p>data this allows us to predict um what uh uh what the time to to hit the ground from from a floor</p>
<p>that we didn&rsquo;t explicitly visit will be so essentially this this um this model is uh is a</p>
<p>way of sort of summarizing the data and summarizing what we expect to do when we continue from this</p>
<p>data the reason this is going to be relevant to us is as i mentioned there isn&rsquo;t enough data to</p>
<p>know these probabilities for different words just from actual text that exists so you have to have</p>
<p>something where you&rsquo;re making a model where you&rsquo;re saying assume this is sort of how things generally</p>
<p>work this is how we would figure out the answer when we haven&rsquo;t explicitly made a measurement</p>
<p>so you know we can make different models and we&rsquo;ll get different results so for example</p>
<p>we could say you know here&rsquo;s a here&rsquo;s another model that we might pick this is a quadratic curve</p>
<p>um uh through these these particular um data points now it&rsquo;s it&rsquo;s worth realizing that there&rsquo;s</p>
<p>there&rsquo;s no modelless model you&rsquo;re always making certain assumptions about how things work</p>
<p>and in the case of these problems in physics like dropping balls from from towers and so on</p>
<p>we have a pretty good expectation that these sort of simple mathematical models mathematical</p>
<p>formulas and so on are likely to be things that will work doesn&rsquo;t always happen that way you know</p>
<p>this is another mathematical function this is the best version it has some parameters in this model</p>
<p>this is the best version of that model for fitting this data and you can see it&rsquo;s a completely crummy</p>
<p>fit to this data so if we assume that this is sort of in general the way things work</p>
<p>we won&rsquo;t be able to correctly reproduce what this what this data is saying</p>
<p>um the in the case of this model i think it has three parameters that are trying to fit this data</p>
<p>and doesn&rsquo;t do very well um and uh in the what chat gpt is doing it basically has 175 billion</p>
<p>parameters that it&rsquo;s trying to fit to make a model of human language and it&rsquo;s trying to hope that</p>
<p>when it has to estimate the probability of something in human language that it does better</p>
<p>than this that with its 175 billion parameters that the underlying structure it&rsquo;s using is such</p>
<p>that it&rsquo;s going to be able to more correctly than this for example estimate the probabilities of</p>
<p>things um so let&rsquo;s see all right so the next big thing to talk about is uh doing things like</p>
<p>dropping balls from towers of pisa and so on that&rsquo;s something where we&rsquo;ve learned over the last</p>
<p>over the last 300 years since galileo and so on that there are simple mathematical formulas that</p>
<p>govern those kinds of processes physical processes in nature but when it comes to a task like what&rsquo;s</p>
<p>the most probable next word or some other kind of human-like task we don&rsquo;t have a simple kind</p>
<p>of mathematics style model so for example we might say here&rsquo;s a typical human-like task</p>
<p>we&rsquo;re given um we&rsquo;re asked to recognize uh from an array of from an image an array of pixels</p>
<p>which which digit out of the 10 possibilities is this is this one and and so we um uh and and no</p>
<p>we humans do a pretty good job of saying well that&rsquo;s a four that&rsquo;s a two and so on but uh we</p>
<p>need to ask sort of how how do we think about this problem so one thing we could say is let&rsquo;s try and</p>
<p>do the thing that we were doing where we say let&rsquo;s just collect the data and figure out the answer</p>
<p>based on collecting data so we might say well let&rsquo;s let&rsquo;s get ourselves a whole collection of</p>
<p>fours and let&rsquo;s just ask ourselves um when we are presented with a particular array of pixel values</p>
<p>does that array of pixel values match one of the fours that we&rsquo;ve got in our sample the chance of</p>
<p>that happening is is incredibly small and it&rsquo;s clear that we humans do something better than</p>
<p>that we don&rsquo;t it doesn&rsquo;t matter where the individual pixels fell here so long as it</p>
<p>roughly is in the shape of the four we&rsquo;re going to recognize it as a four so the question then is um</p>
<p>how does that work and uh what um what&rsquo;s what we found is that um uh it&rsquo;s um well let&rsquo;s say</p>
<p>this is using uh this is actually using this sort of a standard machine learning problem</p>
<p>um this is using a simple neural net um to uh recognize these handwritten digits and so we</p>
<p>see it gets the right answer there but if we say well what&rsquo;s it really doing let&rsquo;s say we give it</p>
<p>a set of progressively more blurred digits here at the beginning it gets them right</p>
<p>then it quotes gets them wrong what does it even mean that it gets them wrong</p>
<p>we know that this was a two that we put in here and we know we just kept on blurring that two</p>
<p>and so we can say well it got it wrong because we knew it was supposed to be a two but</p>
<p>if we sort of zoom out and ask what&rsquo;s happening at a at a broader level we say well if we were</p>
<p>humans looking at those images would we conclude that that&rsquo;s a two or not by the time it gets</p>
<p>blurred enough we humans wouldn&rsquo;t even know it&rsquo;s a two so to to sort of assess whether the machine</p>
<p>is doing the right thing what we&rsquo;re really asking is does it do something more or less what like</p>
<p>what we humans do so that becomes the question is it not we don&rsquo;t get to ask for these kind of human</p>
<p>like tasks there&rsquo;s no obvious right answer it&rsquo;s just does it do something that follows what us</p>
<p>humans do and you know that question of of uh what&rsquo;s the right answer okay for humans we might</p>
<p>say well up there you know most humans recognize that as a two if instead we had visual systems</p>
<p>like bees or octopuses or something like this we might come to completely different conclusions</p>
<p>once things get sort of blurred out um we might the question of what we consider to be two like</p>
<p>might be quite different it&rsquo;s a very human answer that that uh to say that that that still looks</p>
<p>like a two for example depends on our visual system it&rsquo;s not something where there&rsquo;s sort of</p>
<p>a mathematically precise definition of that has to be a two okay so question is how do these models</p>
<p>how do these models which we&rsquo;re using for things like image recognition how do they actually work</p>
<p>the the most popular by far and most successful at present time uh approach to doing this is to</p>
<p>use neural nets and so okay what what what is a neural net it&rsquo;s kind of an idealization of what</p>
<p>we think is going on in the brain what&rsquo;s going on in the brain well we all have about 100 billion</p>
<p>neurons in our brains which are nerve cells that have the feature that when they get excited they</p>
<p>produce electrical signals maybe a thousand times a second um they and each nerve cell has</p>
<p>has it&rsquo;s it&rsquo;s taking that electrical signal and it&rsquo;s it has sort of wire-like projections from the</p>
<p>from the nerve cell that are connecting to maybe a thousand maybe ten thousand other nerve cells</p>
<p>and so what happens in a sort of rough approximation is that you&rsquo;ll have electrical</p>
<p>activity in one nerve cell and that will kind of get communicate itself to other nerve cells</p>
<p>and there&rsquo;s this whole network of nerves that is has this elaborate pattern of electrical</p>
<p>electrical activity so um and roughly the way it seems to work is that the extent to which</p>
<p>one nerve cell will affect others is determined by uh sort of the the weights associated with</p>
<p>these different connections and so one connection might have a very strong positive effect on another</p>
<p>nerve cell if the first nerve cell is fired then it&rsquo;s like it makes it very likely the next nerve</p>
<p>cell will fire or that connection might be an inhibitory connection where the if one nerve</p>
<p>cell fires it makes it very unlikely for the next nerve cell to fire there&rsquo;s some whole combination</p>
<p>of these weights associated with these different connections between nerve cells</p>
<p>so you know what actually happens when we&rsquo;re trying to recognize a two in an image for example</p>
<p>well the you know the the light the photons from from the from the image fall on the cells at the</p>
<p>back of our eye at our retina these are photoreceptor cells they convert that light</p>
<p>into electrical signals the electrical signals um end up going through nerves that get to the</p>
<p>visual cortex to the back of our head um and uh there&rsquo;s an array of of uh of nerves that correspond</p>
<p>to all the different essentially pixel positions in the image and then what&rsquo;s happening is that</p>
<p>within our brains there&rsquo;s this sequence of connections there&rsquo;s sort of layers of neurons</p>
<p>that process the electrical signals that are coming in and eventually we get to the point</p>
<p>where we kind of form a thought that that image that we&rsquo;re seeing in front of us is a two and</p>
<p>then we might say it&rsquo;s a two um but that process of sort of forming the thought that&rsquo;s what we&rsquo;re</p>
<p>talking about is kind of this process of recognition i was talking about it in the in the</p>
<p>actual neural nets that we have in brains but what is being done in all of these models including</p>
<p>things like chat gpt is an idealization of that neural net okay so for example in um uh in the</p>
<p>particular neural net we were using for image recognition this is kind of a wolfram language</p>
<p>representation of that neural net um and we we&rsquo;re going to talk about um not in total detail but</p>
<p>we&rsquo;re going to talk about all these pieces in here um it&rsquo;s it&rsquo;s very kind of engineering slash</p>
<p>biological there&rsquo;s a lot of different funky little pieces here that go together to actually have the</p>
<p>result of recognizing digits and so on uh this particular neural net was constructed in 1998</p>
<p>and it&rsquo;s really was done as a piece of engineering so</p>
<p>uh how do we think about the way this neural net works essentially that the sort of the key idea</p>
<p>is the idea of attractors that&rsquo;s an idea that actually emerged from mathematical physics and</p>
<p>so on um but uh it&rsquo;s a key idea when we when we&rsquo;re thinking about neural nets and such like</p>
<p>and so what is that idea the idea is let&rsquo;s say we&rsquo;ve got all these different um uh handwritten</p>
<p>digits the ones the twos etc etc etc what we want is if we lay all these digits out in some way</p>
<p>what we want is that if we are sort of near the ones we are kind of attracted to the one spot</p>
<p>if we&rsquo;re kind of if the thing we have is kind of near the twos we&rsquo;re attracted to the two spot</p>
<p>it&rsquo;s kind of the idea of attractors is imagine that you have some i don&rsquo;t know mountainscape</p>
<p>or something like this and you are your you know you&rsquo;re a drop of water that falls somewhere on</p>
<p>the mountain you are going to sort of roll down the mountain until you get to this minimum that</p>
<p>uh is for the from your particular part of the mountain but then there&rsquo;ll be a watershed</p>
<p>and if you&rsquo;re a raindrop that falls somewhere else you&rsquo;ll roll down to a different uh different</p>
<p>minimum a different lake and it&rsquo;s the same kind of thing here when you move far enough away from</p>
<p>the thing that looks like a one you&rsquo;ll roll down into the into the twos attractor rather than the</p>
<p>ones attractor that&rsquo;s kind of the idea there now let&rsquo;s see we can um uh let&rsquo;s let&rsquo;s make a kind of</p>
<p>idealized version of this let&rsquo;s say we&rsquo;ve got a bunch of points on the plane let&rsquo;s say those are</p>
<p>the coffee shops and you say i&rsquo;m always going to go to the closest coffee shop to me well this</p>
<p>so-called voronoi diagram shows you this this sort of the division the watersheds between coffee</p>
<p>shops if you&rsquo;re on this side of this watershed you&rsquo;ll go to this coffee shop if you&rsquo;re on that</p>
<p>side you&rsquo;ll go to this coffee shop so that that&rsquo;s kind of a a minimal version of this idea of</p>
<p>attractors all right so let&rsquo;s talk about neural nets and their relationship to attractors so let&rsquo;s</p>
<p>take an even simpler version let&rsquo;s just take these three attractors there&rsquo;s the zero attractor the</p>
<p>plus one attractor the minus one attractor we&rsquo;re still going to say if we are if we fall in this</p>
<p>region we&rsquo;ll have these have coordinates x and y coordinates so if we&rsquo;re in this region here</p>
<p>we&rsquo;re going to eventually we&rsquo;re going to want to go to say the result is zero we&rsquo;re in the zero</p>
<p>we&rsquo;re in the basin of the zero attractor and we want to produce a zero okay so that we can we can</p>
<p>kind of say we can say as a function of the position x and y that we start from what output</p>
<p>do we want to get well in this on this side we want to get a one this one we want to get what</p>
<p>is that a minus one there we want to get a zero this is the thing that we&rsquo;re trying to</p>
<p>uh we&rsquo;re we&rsquo;re we&rsquo;re trying to we&rsquo;re trying to set up is something where we&rsquo;ll have this</p>
<p>this kind of behavior okay well now let&rsquo;s let&rsquo;s pull in a neural net so this is a typical tiny</p>
<p>neural net each of these dots represents a an artificial neuron each of these lines represents</p>
<p>a connection between neurons and the kind of the the the blue to redness represents the weight</p>
<p>associated with that connection with blue being the most negative red being the most positive here</p>
<p>and this is showing different this is showing a neural net with particular choices for these</p>
<p>weights by which one neuron affects others okay so how do we use this neural net well we feed in</p>
<p>inputs at the top we say those top two neurons got values 0.5 and minus 0.8 for example interpreting</p>
<p>that in terms of the thing we&rsquo;re trying to work with that&rsquo;s saying we&rsquo;re at position x equals 0.5</p>
<p>y equals minus 0.8 in that diagram that we had drawn so now this neural net is basically just</p>
<p>computing a certain function of these values x and y and at each step what it&rsquo;s doing is it&rsquo;s</p>
<p>it&rsquo;s taking these weights and it&rsquo;s simply taking so for this neuron here what it&rsquo;s doing is it&rsquo;s</p>
<p>saying i want this weight multiplied by this value here uh this weight multiplied by this value here</p>
<p>and then what it says is i&rsquo;m going to add those two numbers up the numbers based on uh the the</p>
<p>the weights multiplied by the original number then there&rsquo;s a thing we add we add a constant offset</p>
<p>uh different offset for for uh for we add this constant offset and then we say we get some number</p>
<p>out and then the kind of the the weird thing one does which is sort of inspired by what seems to</p>
<p>happen biologically is we have some kind of thresholding function we say for example this</p>
<p>is a very common one to use relu um if that total number is less than zero make it be not its actual</p>
<p>value but just zero if it&rsquo;s greater than zero make it be its actual value and there are a variety of</p>
<p>different uh so-called activation functions activation because they&rsquo;re they&rsquo;re what determine</p>
<p>what the activity of the next neuron sort of down the line will be based on the input to that neuron</p>
<p>so here again at every step we&rsquo;re just collecting the values from the neurons at the previous layer</p>
<p>uh multiplying by weights adding this offset applying that activation function relu to get</p>
<p>this value minus 3.8 in this case and what&rsquo;s happening here is we start off with these values</p>
<p>0.5 minus 0.8 we go through this whole neural net in this particular case at the end it comes out</p>
<p>with value minus one okay what um uh what does that neural net this neural net here the one</p>
<p>we&rsquo;ve just been showing what does that do as we change those inputs well we can plot it</p>
<p>that&rsquo;s what that neural net actually does so as a function so remember what our goal is to uh</p>
<p>every time we have a value in this region we want to give a zero this region we want to give a minus</p>
<p>one and so on this is what that particular neural net succeeds in doing so it didn&rsquo;t quite make it</p>
<p>to give you know the zero one minus one values but it&rsquo;s kind of close so this is a neural net that&rsquo;s</p>
<p>been kind of uh set up to be as close as it can be for one of that size and shape and so on to</p>
<p>giving us the exact function we wanted to compute well how do we think about what this neural net</p>
<p>is doing the neural net is just computing some mathematical function so for the particular</p>
<p>neural net i was showing if the w&rsquo;s are the weights and the b&rsquo;s are the offsets and so on the f is the</p>
<p>f is the activation function this is the messy sort of algebraic formula that says what the value of</p>
<p>the output is going to be as a function of x and y the values of the inputs so now the question is</p>
<p>well as we look at simpler neural nets what what kinds of functions can we actually compute</p>
<p>so this is at the sort of minimum level this is a single uh this is a neuron here it&rsquo;s getting</p>
<p>input from two other neurons what function is it computing well it depends on the weights</p>
<p>these are the functions that get computed for these different choices of weights very simple</p>
<p>functions in all cases just these ramps so now we can ask well okay let&rsquo;s use a slightly more</p>
<p>sophisticated neural net um here&rsquo;s here&rsquo;s still a very small neural net this is the best it can do</p>
<p>in reproducing the function we want to get slightly bigger neural net does slightly better</p>
<p>an even bigger neural net up it pretty much nailed it didn&rsquo;t quite nail it right at the boundary it&rsquo;s</p>
<p>a bit confused instead of going straight from red to blue it&rsquo;s got this area where it&rsquo;s giving yellow</p>
<p>and so on um but in a first approximation this little neural net was a pretty good representation</p>
<p>of the mathematical function that we wanted to compute and this is the same story as as what</p>
<p>we&rsquo;re doing um in uh um in that um uh recognition of digits where again we&rsquo;ve got a neural net</p>
<p>it happens to have i don&rsquo;t know what it was i think it&rsquo;s about um uh 40 000 um parameters in</p>
<p>this particular case that uh um that that specify kind of um that are doing the same kind of thing</p>
<p>of working out the function that goes from the array of pixels at the beginning to values zero</p>
<p>through nine and so on um well again we can we can ask the question uh you know is it getting the</p>
<p>right answer well again it&rsquo;s it&rsquo;s a hard question that&rsquo;s really a human level question to to because</p>
<p>the question of whether it put a one in the wrong place so to speak it&rsquo;s a question of how we would</p>
<p>define that well we can do similar kinds of things let&rsquo;s say we have other kinds of images we might</p>
<p>try and make a neural net that distinguishes cats from dogs and here we&rsquo;re showing sort of how it</p>
<p>distinguishes those things and mostly the cats are over in this corner the dogs are over in this</p>
<p>corner um but you know the question of what should it really ultimately do uh you know what should it</p>
<p>do if we put a dog in a cat suit should it say that&rsquo;s a cat or should it say it&rsquo;s a dog um it&rsquo;s</p>
<p>going to say some definite thing the question is does it sort of agree with what we humans would</p>
<p>would assess it to to be well you know one question you might ask is well what&rsquo;s this neural net doing</p>
<p>inside when it works out its sort of catness or its dogness and let&rsquo;s say we start with um let&rsquo;s</p>
<p>do this and we can actually do this um let&rsquo;s say we start with an image um well maybe you know let&rsquo;s</p>
<p>say we start with an image of a cat here now we can um uh we can say what&rsquo;s going on inside the</p>
<p>neural net when it decides that this is actually an image of a cat um well what we can do normally</p>
<p>when we are looking at the insides of a neural net it&rsquo;s really hard to tell what&rsquo;s happening</p>
<p>in the case where the neural net corresponds to an image we can at least at least neural nets</p>
<p>tend to be set up so that they sort of preserve the the pixel structure of the image so for example</p>
<p>here we can go this is just going what is this going this is going um uh 10 layers down no this</p>
<p>is no this is actually sorry this is actually going just one layer down in the neural net</p>
<p>and what happens in this particular neural net is it takes that image of a cat</p>
<p>and it breaks it up into a lot of different kind of variants of that image now at this level we</p>
<p>can kind of say well it&rsquo;s doing things that we can sort of recognize it&rsquo;s kind of looking at</p>
<p>um cat outlines without the background it&rsquo;s trying to pull the cat out of the background</p>
<p>it&rsquo;s doing things that we can sort of imagine uh you know describing in words what what&rsquo;s going on</p>
<p>and in fact many of the things that it&rsquo;s doing are things that we know from studying neurophysiology</p>
<p>of brains are what the first levels of visual processing and brains actually do by the time</p>
<p>we&rsquo;re sort of deeper in the neural net um it&rsquo;s much harder to tell what&rsquo;s going on let&rsquo;s say we go</p>
<p>uh 10 10 layers down in the neural net um then uh uh we&rsquo;ve got again sort of this is in the mind of</p>
<p>the neural net this is what it&rsquo;s thinking about to try and decide is it a cat or a dog things have</p>
<p>become much more abstract um much harder to to explicitly recognize but that&rsquo;s kind of um uh what</p>
<p>uh what uh sort of a representation for us of what&rsquo;s happening in the kind of mind of the</p>
<p>neural net and you know if we say well what&rsquo;s a theory for how cat recognition works um it&rsquo;s uh</p>
<p>um it&rsquo;s not it&rsquo;s not clear we can have a theory in the sense of sort of a narrative description</p>
<p>a simple way of describing how does the thing tell that it&rsquo;s a cat you know we we can&rsquo;t um uh</p>
<p>and if you even ask a sort of human how do you tell we say well it&rsquo;s got these pointy ears it&rsquo;s</p>
<p>got this and that thing um it&rsquo;s hard probably for a human to describe how they do that recognition</p>
<p>and when we look inside the neural net it&rsquo;s we we don&rsquo;t get to sort of uh uh have a there&rsquo;s</p>
<p>no guarantee that there&rsquo;s a sort of simple narrative for what it&rsquo;s doing and typically</p>
<p>there isn&rsquo;t okay so we&rsquo;ve talked about how neural nets can successfully go from a cat image</p>
<p>to saying that&rsquo;s a cat versus that&rsquo;s a dog how do you set the neural net up to do that</p>
<p>so the way we normally write programs is we say well i&rsquo;m thinking about how should this program</p>
<p>work um what should it do should it first take uh the image of the cat figure out does it have</p>
<p>you know what are the shape of its ears does it have whiskers all these kinds of things that&rsquo;s</p>
<p>sort of the the typical engineering way to make a program um that&rsquo;s what people did back</p>
<p>15 years ago 20 years ago in trying to make uh sort of recognize images of things that was the</p>
<p>typical kind of approach was to try and recognize sort of human explainable features of images and</p>
<p>so on um to as a way to kind of recognize things the big idea of machine learning is you don&rsquo;t have</p>
<p>to do that instead what you can do is just give a bunch of examples where you say this is a cat</p>
<p>this is a dog and have it be the case that you have a system which can learn from those examples</p>
<p>and we just have to give it enough examples and then when you show it a new cat image that&rsquo;s</p>
<p>never seen before it&rsquo;ll correctly say that&rsquo;s a cat versus that&rsquo;s a dog so let&rsquo;s let&rsquo;s talk about</p>
<p>how that how that&rsquo;s actually done um and uh what we&rsquo;re interested in is can we take one of those</p>
<p>neural nets i showed that the neural nets where they have all these weights and as you change the</p>
<p>weights you change the function the neural net is computing let&rsquo;s say you have a neural net and you</p>
<p>want to make it compute a particular function so let&rsquo;s say let&rsquo;s take a very simple case let&rsquo;s say</p>
<p>we have a neural net we just want it to compute as a function of x we want it to compute this</p>
<p>particular function here okay so let&rsquo;s pick a neural net there&rsquo;s a there&rsquo;s a neural net without</p>
<p>weights let&rsquo;s fill in random weights in that neural net for every random collection of weights in the</p>
<p>neural net the neural net will compute something it won&rsquo;t be the function we want but it&rsquo;ll always</p>
<p>compute something it&rsquo;ll always be the case that when you feed in some value up here you&rsquo;ll get</p>
<p>out some value down here and these are plots of the function that you get by doing that okay the</p>
<p>the big idea is that if you do it the right way and you can give enough examples of um uh</p>
<p>um of um uh of what function you are trying to learn um you will be able to progressively tweak</p>
<p>the weights in this neural net so that eventually you&rsquo;ll get a neural net that correctly computes</p>
<p>this function so again what we&rsquo;re doing here is this is we&rsquo;re just describing if this is x</p>
<p>x this is let&rsquo;s say you know g of x down here this is the value x up here and this is a g of x for</p>
<p>some function g and that function g that we want is this kind of uh square wave type thing here</p>
<p>now in this particular case this neural net with these weights is not computing the function we</p>
<p>wanted it&rsquo;s computing this function here but as we progressively train this neural net we tweak</p>
<p>the weights until eventually we get a neural net that actually computes the function we want</p>
<p>this particular case it took 10 million examples to get to the point where we have the neural net</p>
<p>that we want okay so the um how does this actually work how is this actually done how does one uh as</p>
<p>i said at the beginning we just had we started off with neural nets where we had random weights</p>
<p>with random weights this function x to g of x with that particular choice of weights is this thing</p>
<p>here which isn&rsquo;t even close to what we wanted so even if we have when we have examples of functions</p>
<p>examples of results we how do we go from those to train the neural net essentially what we&rsquo;re doing</p>
<p>is we we run we say we&rsquo;ve got this neural net uh we say let&rsquo;s pick a value of x 0.2 for example</p>
<p>let&rsquo;s run it through the neural net let&rsquo;s see what value we get okay we get this value here</p>
<p>oh we say that value is not correct based on what we were trying to based on the training data that</p>
<p>we have based on this function that we&rsquo;re trying to we&rsquo;re trying to train the neural nets to</p>
<p>generate that training it isn&rsquo;t the correct result it should have been let&rsquo;s say a minus one</p>
<p>and it was in fact a 0.7 or something okay so then the idea is that knowing that we got it wrong</p>
<p>we we can measure how much we got it wrong and we can do that for many different samples we can take</p>
<p>let&rsquo;s say a thousand examples of this mapping from value x to function g of x that the neural</p>
<p>net computes and we can say of those thousand examples um how far off were they and we can</p>
<p>compute what&rsquo;s often called the loss which is take all those values of what what we should have got</p>
<p>versus what we actually got and for example take the sum of the squares of the differences between</p>
<p>those values um and that gives us a sense of if if all the values were right on that would be zero</p>
<p>but in fact it&rsquo;s not zero because we didn&rsquo;t actually get the right answer with our neural</p>
<p>net and so then what we&rsquo;re trying to do is to progressively reduce that loss we&rsquo;re trying to</p>
<p>progressively tweak the neural net so that we reduce that loss so for example this is what it</p>
<p>would typically look like you typically have this is the loss as a function of the number of examples</p>
<p>you&rsquo;ve shown and what you see is that as you show more and more examples the loss progressively</p>
<p>decreases reflecting the fact that the the function that&rsquo;s being computed by the neural</p>
<p>net is getting closer to the function we actually wanted and eventually the loss is really quite</p>
<p>small here and then the function is really computed by the neural net is really close</p>
<p>to the one we wanted that&rsquo;s kind of the idea of training a neural net we&rsquo;re trying to tweak the</p>
<p>weights to reduce the loss to to get to where we want okay so let&rsquo;s say we&rsquo;ve got a neural net</p>
<p>particular form of weights we compute the loss the loss is really bad it&rsquo;s we&rsquo;re pretty far away</p>
<p>how do we arrange to incrementally get closer to the right answer well we have to tweak the</p>
<p>weights but what direction do we tweak the weights in okay so this is a tricky thing that got figured</p>
<p>out well in the 1980s for neural nets how to do this in a reasonably it was known how to do this</p>
<p>in simple cases i should say that the the idea of neural nets originated in 1943 Warren McCulloch</p>
<p>and Walter Pitts were the two guys who wrote the sort of original paper that described these</p>
<p>idealized neural nets and what&rsquo;s inside chat gpt is basically a big version of what was described</p>
<p>in 1943 and there was sort of a long history of people doing things with just one layer of neural</p>
<p>nets and that didn&rsquo;t work very well and then early 1980s it started to be some knowledge of how to</p>
<p>deal with with more layers of neural nets and then when gpus started to exist and computers got faster</p>
<p>sort of big breakthrough around 2012 where it became possible to deal with sort of training</p>
<p>and using sort of deep neural nets by the way i for people who are interested i did a discussion</p>
<p>with a friend of mine named Terry Sinofsky who&rsquo;s been involved with neural nets for about 45 years</p>
<p>now and has been quite instrumental in many of the many of the developments that have happened</p>
<p>i did a discussion with him that was live streamed a few days ago which you can you can find on the</p>
<p>web and so on if you&rsquo;re interested in that that history but back to back to sort of how these</p>
<p>things work what we want to do is we found the loss is bad let&rsquo;s reduce the loss how do we reduce</p>
<p>the loss we need to tweak the weights what direction do we tweak the weights in in order to</p>
<p>reduce the loss well this turns out to be a big application of calculus because basically what&rsquo;s</p>
<p>happening is our neural net corresponds to a function it has it&rsquo;s a function of the weights</p>
<p>it&rsquo;s a function of once we when we compute the loss we are basically working out the value of</p>
<p>this neural net function for lots of values of x and y and so on and that object that thing we&rsquo;re</p>
<p>computing is a big complicated we can think of it as an algebraic formula that we can think of as</p>
<p>being a function of all those weights so how do we make the thing better how do we reduce the overall</p>
<p>value how do we tweak the weights to reduce this this overall loss quantity well we can kind of</p>
<p>use calculus we can kind of say we can think of this as sort of a surface as a function of all</p>
<p>of these weights and we can say we want to minimize this function as a function of the weights so for</p>
<p>example we might have a in a very simplified case this is not good in a very simplified case we</p>
<p>might have a some as a function of just two weights so for example in those neural nets I</p>
<p>was just showing they had I don&rsquo;t know 15 weights or something in the real example of an image</p>
<p>recognition network it might be 40,000 weights in chat gpt it&rsquo;s 175 billion weights but here we&rsquo;re</p>
<p>just looking at two weights and we&rsquo;re asking if this was the loss as a function of the value of</p>
<p>those weights how would we find the minimum how would we find the how would we find the best</p>
<p>values of those weights see oh there we go so this is a typical procedure to use so-called gradient</p>
<p>descent basically what you do is you say I&rsquo;m at this position on this lost surface lost surface</p>
<p>where the the coordinates of the surface are weights what I want to do is I want to get to a</p>
<p>lower point on this lost surface and I want to do that by changing the weights always following this</p>
<p>gradient vector kind of down the hill the steepest descent down the hill and that&rsquo;s something that</p>
<p>you just have to use calculus and you just work out derivatives at this point as a function of</p>
<p>these weights and the direction where you are finding the the maximum of these derivatives</p>
<p>you&rsquo;re going down the hill as much as you can okay so that&rsquo;s that&rsquo;s kind of how you try to</p>
<p>minimize the loss is by tweaking the weights so that you follow this gradient descent thing</p>
<p>to to get to the minimum now there&rsquo;s a there&rsquo;s a bit of a bug with this because the surface</p>
<p>that corresponds to all the weights it might have as this picture shows it might have more than one</p>
<p>minimum and actually these minima might not be all at the same height so for example if you&rsquo;re</p>
<p>on a mountainscape there might be a mountain lake there might be a very high altitude mountain lake</p>
<p>and all of the water that&rsquo;s kind of following steepest descent down to get to the minimum</p>
<p>only manages to get to that high altitude mountain lake even though there&rsquo;s a low altitude mountain</p>
<p>lake that will be a much lower value of the loss so to speak that isn&rsquo;t reached by this gradient</p>
<p>descent method it&rsquo;s never you you get stuck in a local minimum you never reach the more global</p>
<p>minimum and that&rsquo;s kind of what what potentially happens in neural nets is you can be okay i&rsquo;m</p>
<p>going to reduce the loss i&rsquo;m going to tweak the weights but whoops i can&rsquo;t really get very far i</p>
<p>can&rsquo;t reduce the loss enough to be able to successfully reproduce my function with my</p>
<p>neural net or whatever i can&rsquo;t tweak the weights enough because i got stuck in a local minimum</p>
<p>i don&rsquo;t know how to get out of that local minimum so this was a the sort of big breakthrough and</p>
<p>surprise of 2012 in in the development of neural nets was the following discovery you might have</p>
<p>thought that you&rsquo;d have the best chance of getting a neural net to work well when it was a simple</p>
<p>neural net you kind of get your arms around it and figure out all these weights and do all these</p>
<p>calculations and so on but actually it turns out things get easier when the neural net and the</p>
<p>problem it&rsquo;s trying to solve gets more complicated and roughly the intuition seems to be this although</p>
<p>one didn&rsquo;t expect this nobody i think expected this i i certainly didn&rsquo;t didn&rsquo;t expect this</p>
<p>that um it&rsquo;s sort of obvious after the fact okay the issue is you are you going to get stuck</p>
<p>as you try and follow this gradient descent well if you&rsquo;re in a kind of low dimensional space</p>
<p>it&rsquo;s quite easy to get stuck you just get into the one of these mountain lakes you can&rsquo;t go any</p>
<p>further but in a high dimensional space there are many different directions you could go and the</p>
<p>chances are any local minimum you get to you&rsquo;ll be able to escape from that local minimum because</p>
<p>there&rsquo;ll always be some dimension some direction you can go that allows you to escape and that&rsquo;s</p>
<p>what seems to be happening it&rsquo;s not totally obvious it would work that way but that&rsquo;s what</p>
<p>seems to be happening um in in these neural nets is there&rsquo;s always sort of a when you have a</p>
<p>complicated enough neural net there&rsquo;s always a way to escape there&rsquo;s always a way to reduce the</p>
<p>the loss and so on okay so so that&rsquo;s kind of the um uh this idea of you tweak the weights</p>
<p>to reduce the loss that&rsquo;s what&rsquo;s going on in all neural nets and you can um uh uh there are</p>
<p>different schemes for you know what how you do the gradient descent and how big the steps are and</p>
<p>there are all kinds of different things there are different ways you can calculate the loss</p>
<p>when we&rsquo;re doing it for language where we&rsquo;re calculating probabilities of words based on</p>
<p>probabilities of sequences of words based on the model versus based on what we actually see in the</p>
<p>data as opposed to just distances between numbers and so on but it&rsquo;s the same basic idea okay so</p>
<p>when that happens um let&rsquo;s see uh we can potentially get um uh every time we run one of</p>
<p>these neural nets we do all this tweaking of weights and so on we get something where yes</p>
<p>we got a neural net that reproduces the thing we want okay so there these are results from</p>
<p>four different neural nets that all successfully pretty much reproduce this function now you might</p>
<p>ask well what happens if i go um uh let&rsquo;s see what happens if i um yeah what happens if i go</p>
<p>outside the range where i had explicitly trained the neural net i&rsquo;m telling it i told that my</p>
<p>function x goes to g of x for this range here the one in white but then i say well i&rsquo;ve got the</p>
<p>neural net now let me try running it for a value of x that i never trained it for what&rsquo;s it going</p>
<p>to give well that will depend on which particular set of choices about which minimum which weight</p>
<p>tweaking etc etc it went to and so when the neural net tries to figure out things that it wasn&rsquo;t</p>
<p>explicitly trained on it&rsquo;s going to give completely different answers depending on the details of how</p>
<p>the neural net happened to get trained that&rsquo;s it&rsquo;s kind of like it knows the things which it&rsquo;s</p>
<p>already seen examples of it&rsquo;s kind of it&rsquo;s it&rsquo;s going to be constrained to basically reproduce</p>
<p>those examples when you&rsquo;re dealing with things that are kind of out of the box it it might think</p>
<p>differently out of the box so to speak depending on the details of that neural net all right so</p>
<p>let&rsquo;s see this whole question about training neural nets is um uh it&rsquo;s a it&rsquo;s a giant</p>
<p>modern art so to speak of how to train a neural net and the um over the last particularly the</p>
<p>last decade there&rsquo;s been sort of increasingly elaborate sort of knowledge of that art of</p>
<p>training neural nets and there&rsquo;s been a certain amount of lore about how these neural nets should</p>
<p>get trained that&rsquo;s that&rsquo;s developed so how does that what&rsquo;s what&rsquo;s sort of in that law well kind</p>
<p>of the the first question is um uh you know what kind of architecture of neural net how should you</p>
<p>how many neurons how many neurons at each layer how should they be connected together</p>
<p>what should you use um and uh there have been a number of kind of observations and sort of the</p>
<p>art of neural nets that have emerged so what was believed at the beginning was uh every different</p>
<p>task you want a neural net to do you would need a different architecture to do it you would somehow</p>
<p>optimize the architecture for each task it&rsquo;s turned out that that hasn&rsquo;t that isn&rsquo;t the case</p>
<p>it&rsquo;s much more that you that there are generic neural net architectures that seem to go across</p>
<p>a lot of different tasks and you might say isn&rsquo;t that just like what happens with computers and</p>
<p>universal computers you need only uh you can run different software on the same computer same</p>
<p>hardware different software that was the kind of idea from the 1930s that launched the whole</p>
<p>computer revolution the whole notion of software and so on is this a repetition of that i don&rsquo;t</p>
<p>actually think so i think this is actually something slightly different i think that the</p>
<p>reason that the neural nets that the sort of a small number of architectures cover a lot of the</p>
<p>tasks neural nets can do is because those tasks that neural nets can do are tasks that we humans</p>
<p>are also pretty good at doing and these neural nets are kind of reproducing something about the</p>
<p>way we humans do tasks and so while the tasks you&rsquo;re asking the neural net to do are tasks that</p>
<p>are sort of human-like any human-like neural net is going to be able to do those tasks now there</p>
<p>are other tasks that are different kinds of computations that neural nets and humans are</p>
<p>pretty bad at doing and those will be sort of out of this zone of it doesn&rsquo;t really matter what</p>
<p>architecture you have well uh okay so there are all kinds of other things that um um that people</p>
<p>sort of wondered about like they said well let&rsquo;s make instead of making these very simple neurons</p>
<p>that were just like the ones from 1943 let&rsquo;s make more complicated assemblies of things and</p>
<p>and let&rsquo;s put more detail into the internal operations of the neural net turns out most of</p>
<p>that stuff doesn&rsquo;t seem to matter and i think that&rsquo;s unsurprising from a lot of science that</p>
<p>i&rsquo;ve done not specifically related to neural nets i think that that um that&rsquo;s a um um that that&rsquo;s</p>
<p>something um um that isn&rsquo;t too surprising now when it comes to neural nets and sort of how they&rsquo;re</p>
<p>architected um there are a few features that um uh it is useful to to sort of capture a few features</p>
<p>this is not the right thing that&rsquo;s the right thing um the uh uh there are a few features</p>
<p>of um the data that you&rsquo;re looking at with the neural net that it is useful to that it seems</p>
<p>useful to capture in the actual architecture of the neural net it&rsquo;s probably not in the end</p>
<p>ultimately completely necessary it&rsquo;s probably the case that you could use a much more generic neural</p>
<p>net and with enough training enough enough kind of uh sort of tweaking from the actual data you&rsquo;d</p>
<p>be able to learn all these things but for example if you&rsquo;ve got a neural net that&rsquo;s dealing with</p>
<p>images it is useful to initially arrange the neurons in an array that&rsquo;s like the pixels and</p>
<p>so this is sort of representation for the particular network called linette that we were showing uh for</p>
<p>image for um uh digit recognition this sort of representation of there&rsquo;s a first layer of of</p>
<p>neurons here then it sort of thickens up into multiple multiple different copies of the image</p>
<p>which we actually saw um we&rsquo;re looking at those pictures and then it keeps going and eventually</p>
<p>it rearranges what one thing about neural nets to understand is that neural nets take everything</p>
<p>they&rsquo;re dealing with and grinds it up into numbers computers take everything they&rsquo;re dealing with</p>
<p>and eventually grinds it up grind it up into zeros and ones into bits neural nets right now</p>
<p>are grinding things up into uh into arbitrary numbers you know 3.72 they&rsquo;re they&rsquo;re real</p>
<p>numbers not not necessarily just zeros and ones it&rsquo;s not clear how important that is it is necessary</p>
<p>when you&rsquo;re going to incrementally improve weights and kind of use calculus like things to do that</p>
<p>it&rsquo;s necessary to have these continuous numbers to be able to do that but in any case whether</p>
<p>you&rsquo;re showing the neural net a picture a piece of text whatever in the end it&rsquo;s got to be</p>
<p>represented in terms of numbers and that&rsquo;s um uh that&rsquo;s sort of a but but but how those numbers</p>
<p>are arranged like for example here there&rsquo;s an array of numbers which are sort of arranged in</p>
<p>the in the pixel positions and so on the whole array is reconstituted and rearranged and flattened</p>
<p>and so on and in the end you&rsquo;re going to get probabilities for each of the uh each of the</p>
<p>10 digits that will be just a sequence of of numbers here sort of a rearranged collection</p>
<p>of numbers okay so let&rsquo;s see right picture there we go okay so this is um so we&rsquo;re talking about</p>
<p>sort of um uh how complicated a neural net do you need to achieve it to perform a particular task</p>
<p>sometimes pretty hard to estimate that because you don&rsquo;t really know how hard the task is</p>
<p>let&rsquo;s say you want a neural net that plays a game well you can compute the complete game tree for</p>
<p>the game all the possible sequences of games that could occur might be some absolutely huge game</p>
<p>tree but if you want to get human level play for that game you don&rsquo;t need to reproduce that whole</p>
<p>game tree if you were going to do very systematic computer computation and just play the game by</p>
<p>looking at all the possibilities you&rsquo;d need that whole game tree but or you need to be able to go</p>
<p>through that whole game tree but in the case of if you&rsquo;re trying to achieve sort of human-like</p>
<p>performance the humans might have found some heuristic that dramatically simplifies it</p>
<p>and you might need just some much simpler uh much simpler neural net so so this is an example of</p>
<p>well if the neural net is way too simple then it it doesn&rsquo;t have the ability to reproduce</p>
<p>in this case the function we wanted but you&rsquo;ll see that as the neural nets get a bit more</p>
<p>complicated we eventually get to the point where we can indeed reproduce the function we wanted</p>
<p>all right well okay so and you can ask you know are there theorems about what</p>
<p>what functions you can reproduce with what what neural nets basically as soon as you have any</p>
<p>neurons in the middle you can at least in principle reproduce any function but you might need an</p>
<p>extremely large number of neurons to do that um and uh it&rsquo;s also the case that that neural net</p>
<p>might not be trainable it might not be the case that you can find some for example gradient that</p>
<p>always makes the loss go down and so on just by tweaking weights it might be that that you you</p>
<p>couldn&rsquo;t incrementally get to that result well okay so uh whoops let&rsquo;s say you&rsquo;ve got um a uh</p>
<p>you&rsquo;ve decided on your architecture of your neural net and now you want to train your neural net</p>
<p>okay so the next big thing is you have to have the data to train your neural net from</p>
<p>and there are two basic categories of training that one does for neural nets supervised learning</p>
<p>and unsupervised learning so in supervised learning you give the neural net a bunch of</p>
<p>examples of what you want it to learn so you might say um here are uh 10 000 pictures of cats</p>
<p>10 000 pictures of dogs the pictures of cats are all tagged as being this is a picture of a cat</p>
<p>dogs or there&rsquo;s a picture of a dog and you&rsquo;re feeding the neural net these uh these things that</p>
<p>are um kind of explicit things that you want it to learn now that that&rsquo;s what one has to do for</p>
<p>many forms of of uh machine learning um it can be non-trivial to get the data often there are</p>
<p>sources of data that where you&rsquo;re sort of piggybacking on something else like you might</p>
<p>get images from the web and they might have alt tags that were text describing the image and that&rsquo;s</p>
<p>how you might be able to associate the you know the description of the image the fact this is a cat</p>
<p>to the actual image or you might have you know if you&rsquo;re doing um uh audio kinds of things you might</p>
<p>have something where you um uh you might say let&rsquo;s get a bunch of videos which have closed captions</p>
<p>and that will give us the the uh sort of supervised information on here&rsquo;s the audio here&rsquo;s the text</p>
<p>that corresponds with that audio that&rsquo;s what we have to learn so that&rsquo;s um that&rsquo;s sort of one</p>
<p>style of of uh teaching neural nets is supervised learning where you&rsquo;ve got data which explicitly</p>
<p>is examples of here&rsquo;s the input you&rsquo;re supposed to that you&rsquo;re going to get here&rsquo;s the output</p>
<p>you&rsquo;re supposed to give and that&rsquo;s great when you can get it um sometimes it&rsquo;s very very difficult</p>
<p>to get the um the necessary data to be able to train the the machine learning system and when</p>
<p>people say oh can you use machine learning for this task well if there&rsquo;s no training data the</p>
<p>answer is probably going to be no um unless that task is something that you can either get a sort</p>
<p>of proxy for that task from somewhere else or you can or you just have to blindly hope that</p>
<p>something that um uh sort of was transferred from some other domain might might work just as</p>
<p>when you&rsquo;re doing mathematical models you might sort of say well linear models or something</p>
<p>worked in these places maybe we can blindly hope they&rsquo;ll work here doesn&rsquo;t doesn&rsquo;t tend</p>
<p>to work that well okay the other the other form of um uh no i should explain another thing about</p>
<p>about neural nets it&rsquo;s kind of important which is that there&rsquo;s something been very critical over</p>
<p>the last decade or so the notion of transfer learning so that once you&rsquo;ve kind of learned</p>
<p>a certain amount with a neural net being able to transfer the learning that&rsquo;s happened in one</p>
<p>neural net to a new neural net to give it a kind of head start is very important now that that</p>
<p>transfer might be for the first neural net learnt the most important features to pick out an image</p>
<p>let&rsquo;s feed the second neural net those most important features and let it go on from there</p>
<p>or it might be something uh where you&rsquo;re using one neural net uh to provide training data for</p>
<p>another neural net so you&rsquo;re making them compete against each other a variety of other things like</p>
<p>that that those are actually those have different different names the transfer learning thing is</p>
<p>mostly the first thing i was talking about okay so there are issues about how do you get enough</p>
<p>training data how many times do you show the same example to a neural net you know it&rsquo;s probably a</p>
<p>little bit like humans for us when we memorize things it&rsquo;s often useful to go back and just</p>
<p>rethink about that exact same example that you were trying to memorize before again so it is</p>
<p>with neural nets and the uh there&rsquo;s also questions like well you know you&rsquo;ve got the image of a cat</p>
<p>that looks like this maybe you can get what is the equivalent of another image of a cat just by</p>
<p>doing some simple image processing on the first cat and it turns out that that seems to work that</p>
<p>notion of data augmentation seems to work surprisingly well even fairly simple transformations</p>
<p>are almost as good as new in terms of providing more data well uh okay the um um sort of a the</p>
<p>other big um form of of um of learning that uh learning methodology that that one tends to use</p>
<p>is unsupervised learning where you don&rsquo;t have to explicitly give sort of uh thing you got as input</p>
<p>example of output so for example in um in the case of uh just trying to keep track of of um</p>
<p>um yeah the um uh in the case of something like chat gpt there&rsquo;s a there&rsquo;s a wonderful trick you</p>
<p>can use let&rsquo;s say chat gpt&rsquo;s mission is to continue a piece of text okay how do you train it well</p>
<p>you&rsquo;ve just got a whole bunch of text and you say okay you know chat gpt network here&rsquo;s the text up</p>
<p>to this point let&rsquo;s mask out the text after that point can you predict what&rsquo;s going to come what</p>
<p>you know can you learn to predict what happens if you take off the mask and that&rsquo;s the task that it</p>
<p>you don&rsquo;t have to explicitly give it you know input output you&rsquo;re you&rsquo;re you&rsquo;re you&rsquo;re implicitly</p>
<p>able to get that just from the original data that you&rsquo;ve been provided so essentially what&rsquo;s</p>
<p>happening when you&rsquo;re training the neural net of chat gpt is you&rsquo;re saying here&rsquo;s all this</p>
<p>english text it&rsquo;s from billions of web pages now look at the text up to this point and say can you</p>
<p>correctly predict what text will come later okay gets it wrong you can say well it&rsquo;s it&rsquo;s it&rsquo;s</p>
<p>giving it getting it it&rsquo;s getting it wrong so let&rsquo;s that&rsquo;s provides uh you know that that means</p>
<p>it&rsquo;s has a uh there&rsquo;s some loss associated with that let&rsquo;s see if we can tweak the weights in</p>
<p>the neural net to get it closer to correctly predicting what&rsquo;s going to come next so in any</p>
<p>case the the end result of all of this is you um make a neural net i i could show you neural net</p>
<p>training in uh i could show you more from language it&rsquo;s very easy to train uh neural nets to um oh</p>
<p>let&rsquo;s see now maybe i shouldn&rsquo;t do the spell let&rsquo;s see um let&rsquo;s just let&rsquo;s just do one</p>
<p>so here&rsquo;s here&rsquo;s a collection of handwritten digits um this is what is this going to be</p>
<p>there&rsquo;s maybe 50,000 handwritten digits uh oh there we go so this is a supervised training</p>
<p>story where where here all the zeros and they say that that&rsquo;s a hundred and zero and it says it&rsquo;s a</p>
<p>zero those are the nines it says it&rsquo;s a nine okay so let&rsquo;s take a random sample of um i don&rsquo;t know</p>
<p>2,000 of those um and now we&rsquo;re going to use that okay there&rsquo;s our random sample of 2,000 um</p>
<p>handwritten digit and what it was supposed to be okay so let&rsquo;s take it let&rsquo;s get a neural net</p>
<p>let&rsquo;s say let&rsquo;s try taking this lunette neural net this is now a um um um</p>
<p>um an un an untrained neural net um and now we can just say if we wanted to we could should be</p>
<p>able to say uh just train that neural net with this data there&rsquo;s that data there uh go on line 32</p>
<p>um let&rsquo;s say uh train this and so what&rsquo;s going to happen is this is showing us the loss and this is</p>
<p>showing us as it&rsquo;s as it&rsquo;s being presented with more and more of those examples and it&rsquo;s being</p>
<p>shown the same example many many times you&rsquo;ll see the loss is going down and it&rsquo;s gradually learning</p>
<p>okay now now we have a trained neural net and now we could go back to our original collection um</p>
<p>of uh of digits let&rsquo;s close that up um let&rsquo;s go back to our original collection of digits let&rsquo;s</p>
<p>pick a random digit here let&rsquo;s see whether from um let&rsquo;s just pick let&rsquo;s just pick another random</p>
<p>sample here um let&rsquo;s pick five examples there from um oh i should have not told it to do that okay</p>
<p>there we go so now we can take this trained neural net here&rsquo;s our trained neural net</p>
<p>and let&rsquo;s take the trained neural net and let&rsquo;s feed it that particular nine there now remember</p>
<p>we only trained it on 2 000 examples so it didn&rsquo;t have very much training but oops i shouldn&rsquo;t have</p>
<p>done that i should have just used that okay um okay it successfully told us it was a nine that&rsquo;s</p>
<p>kind of what it looks like to train this is you know wolfram language version of training a neural</p>
<p>net this was a super simple neural net with only 2 000 examples um but that&rsquo;s kind of what it looks</p>
<p>like to do that um do that training okay so uh let&rsquo;s see the um uh the thing with with chat gpt</p>
<p>is that your um well let&rsquo;s let&rsquo;s yeah we can we can keep going and talk about the training of of</p>
<p>of of that but let&rsquo;s um before we get to training of of chat gpt we need to talk about one more</p>
<p>thing which is we need to talk about uh this question of kind of well let&rsquo;s see do we really</p>
<p>need to talk about this yeah we probably should talk about this the question of how you represent</p>
<p>uh kind of things like words with numbers so let&rsquo;s say we are going to have um we&rsquo;re we&rsquo;ve got</p>
<p>all these words and we could just number every word in english we could say apple is 75 pair is</p>
<p>43 etc etc etc um but there&rsquo;s more useful ways to label words in english by numbers and the more</p>
<p>useful way is to get collections of numbers that have the property that words with nearby meanings</p>
<p>have nearby collections of numbers so it&rsquo;s as if we&rsquo;re we&rsquo;re placing every word somewhere in some</p>
<p>meaning space and we&rsquo;re trying to set it up so that words will have a position in meaning space</p>
<p>with the property that if two words are nearby in meaning space they must mean close to the same</p>
<p>close to the same thing so here for example are a collection of words laid out in one of these</p>
<p>meaning spaces um sort of actual meaning spaces like the one used by chat gpt are like uh what</p>
<p>is that one it&rsquo;s probably 12 000 dimensional maybe um this one here is just two-dimensional</p>
<p>we&rsquo;re just putting things like dog and cat alligator crocodile and then a bunch of fruits</p>
<p>here and what the main thing to notice about this is that things with similar meanings like alligator</p>
<p>and crocodile wind up nearby in this meaning space and you know peach and apricot wind up nearby in</p>
<p>meaning space so in other words we&rsquo;re representing these words by collections of numbers in this case</p>
<p>just pairs of numbers coordinates which have the property that those coordinates are some kind of</p>
<p>representation of the meaning of these words so and we can do the same thing when it comes to images</p>
<p>uh for example we could ask whether um when we looked at and that&rsquo;s exactly what we had when we</p>
<p>were looking at some uh a picture like this we&rsquo;re sort of laying out different handwritten digits</p>
<p>in some kind of uh meaning of the of the handwritten digit space where in that meaning</p>
<p>space the one the ones that mean one were over here the ones that mean three were over here and</p>
<p>so on so a question is how do you find how do you actually uh generate coordinates that represent</p>
<p>the so-called embeddings of of of things so that when they&rsquo;re sort of nearby in meaning they will</p>
<p>have nearby coordinates okay so there&rsquo;s a number of neat tricks that are used to do this so a</p>
<p>typical kind of setup is um imagine we have this is just a representation of the neural net that</p>
<p>we use to recognize digits it has these multiple layers each one there&rsquo;s just a little wolfman</p>
<p>language representation of that um what actually does this network do well in the end what it&rsquo;s</p>
<p>doing is it&rsquo;s taking that collection of pixels at the beginning and in the end what it&rsquo;s doing</p>
<p>is it&rsquo;s computing um what are the probabilities for a particular configuration it&rsquo;s going to</p>
<p>produce a collection of numbers at the end because remember neural nets all they ever deal with are</p>
<p>collections of numbers so what it&rsquo;s going to do is it&rsquo;s going to produce a collection of numbers at</p>
<p>the end where uh each position in this collection of numbers there&rsquo;ll be 10 numbers here each</p>
<p>position is the probability that the thing that the neural net was shown corresponded to a zero</p>
<p>or one or two or three or four so what you see here is the numbers are absurdly small except</p>
<p>in the case of four so we can then deduce from this immediately okay that image was was supposed</p>
<p>to be a four so this is kind of the output of the neural net is this collection of probabilities</p>
<p>where in this particular case it was really certain that the thing is a four so that&rsquo;s what we deduce</p>
<p>now the the thing we can do is we say well let&rsquo;s let&rsquo;s back up one layer in the neural net</p>
<p>before we get to that that um let&rsquo;s just say before we had there&rsquo;s a there&rsquo;s a layer that</p>
<p>kind of tries to tries to make the neural net actually make a decision it&rsquo;s i think it&rsquo;s a</p>
<p>soft max layer um that uh is um is at the end that&rsquo;s trying to sort of force the decision it&rsquo;s</p>
<p>trying to exponentially pull apart these numbers so that the big number gets bigger and the small</p>
<p>numbers get smaller okay but one layer before those numbers are a bit more sober in size before</p>
<p>it&rsquo;s been sort of torn apart to make a decision those numbers are much more sober in size and</p>
<p>these numbers at this layer give some pretty decent indication of of the fourness of what</p>
<p>we&rsquo;re seeing they this has more information about what that thing that was shown actually is and we</p>
<p>can think about these numbers as giving some kind of signature some kind of um some some kind of</p>
<p>trace of what kind of a thing we were seeing this is sort of specifying in some sense features of</p>
<p>what we were seeing that later on we&rsquo;ll just decide that&rsquo;s a four but all these other sort of</p>
<p>subsidiary numbers are already useful we go back so you know this is um we can define these feature</p>
<p>vectors that represent this is kind of the a feature vector representing that image there</p>
<p>that&rsquo;s the feature representing this image here and we see that yeah these the the features for</p>
<p>different fours these vectors will be a little bit different um but they&rsquo;re dramatically different</p>
<p>between a four and an eight but we can use these these vectors to represent kind of uh the the</p>
<p>important aspects of of this four here for for instance and if we go back a couple more layers in</p>
<p>that neural net it turns out we can get an array of like 500 numbers that are a pretty good</p>
<p>representation a pretty good sort of feature signature of of any of these images and we do</p>
<p>the same thing for pictures of cats and dogs we can get this kind of signature of what what this</p>
<p>sort of feature vector associated with what is important about that image and then we can take</p>
<p>those those feature vectors and we can say let&rsquo;s let&rsquo;s um let&rsquo;s lay things out according to</p>
<p>different values in those feature vectors and then we&rsquo;ll get this kind of um uh embedding in</p>
<p>in the case of what we can think of as some kind of meaning space in the case of words if we look</p>
<p>at the raw um uh yeah so so how do we do that for words well the idea is uh just like for the</p>
<p>for for getting sort of a a a feature vector associated with like let&rsquo;s say images we have</p>
<p>a task like we&rsquo;re trying to recognize digits and then we back up from the from the final answer</p>
<p>we&rsquo;re training a neural net to do that task but what we end up doing is we back up from that final</p>
<p>we we we nailed the task and we say what was the thing that was just before you you managed to nail</p>
<p>the task that&rsquo;s our representation of the relevant features of the thing well you can do the same</p>
<p>thing for words so for example if we say the blank cat and we then ask in in our training data what</p>
<p>is that blank likely to be um the you know is it black is it white whatever else um that we could</p>
<p>try and make a network that predicts what is that intermediate word likely to be what are the</p>
<p>probabilities for that intermediate word we can train a network to be good at predicting the</p>
<p>probabilities for blackness versus whiteness versus whatever other tabbyness for cats or</p>
<p>whatever it is um and uh once we&rsquo;ve got that we can then back up from the final answer and say</p>
<p>let&rsquo;s look at the innards of the network and let&rsquo;s see what it had done as it got towards coming up</p>
<p>with that final result that thing we get right before it gets to a little bit before it gets</p>
<p>the final result that will be a good representation of features that were important about those words</p>
<p>and that&rsquo;s how we can then deduce what we can deduce these sort of feature vectors for words</p>
<p>so um in the case of gpt2 for example um we can uh we can compute those feature vectors</p>
<p>they&rsquo;re extremely uninformative when we look at them in the full feature vectors uh if we</p>
<p>what is more informative is we sort of project these feature vectors down to a smaller number</p>
<p>of dimensions we&rsquo;ll discover that the cat one is closer to the dog one probably than it is to the</p>
<p>chair one but that&rsquo;s that&rsquo;s kind of so what what what um chat gpt is doing when it deals with words</p>
<p>is it uh it&rsquo;s it&rsquo;s always representing them using these feature vectors that um using this kind of</p>
<p>um embedding that turns them into these collections of numbers that have the property that nearby</p>
<p>words are have have similar representations actually i&rsquo;m i&rsquo;m i&rsquo;m getting a little bit ahead</p>
<p>of myself there because because the the the way chat gpt works it uses these kinds of embeddings</p>
<p>but it does so for for whole chunks of text rather than for individual words we&rsquo;ll get there okay</p>
<p>so i think we&rsquo;re we&rsquo;re getting on getting on fairly well here um how about the actuality of</p>
<p>of of chat gpt well it&rsquo;s big neural net millions of neurons uh 175 billion connections between them</p>
<p>um and uh what is its basic architecture um the um um it&rsquo;s uh the sort of a big idea that actually</p>
<p>came out of language translation networks where the task was start from english end up with french</p>
<p>or whatever else was this idea of what are called transformers it&rsquo;s an architecture of neural nets</p>
<p>that were more complicated architectures used before there&rsquo;s actually a simpler one um and the</p>
<p>notion is as i mentioned when one&rsquo;s dealing with images it&rsquo;s convenient to have these neurons kind</p>
<p>of attached to pixels at least to sort of laid out in a kind of which pixel is next to which pixel</p>
<p>kind of way there was a so-called convolutional neural nets or conv nets are the the typical</p>
<p>things that are used there in the case of language what transformers do is they are dealing with the</p>
<p>fact that language is in a sequence and with a conv net for an image one saying there&rsquo;s this</p>
<p>pixel here what what&rsquo;s happening in the neighboring nearby pixels in the image in a transformer what</p>
<p>one&rsquo;s doing is one saying this is here&rsquo;s a word let&rsquo;s look at the preceding words let&rsquo;s look at</p>
<p>the words that came before this word and in particular let&rsquo;s pay attention differently</p>
<p>to different ones of those words so i mean this gets this gets quite elaborate and engineering</p>
<p>quite quickly um and uh uh you know it&rsquo;s it&rsquo;s it&rsquo;s very typical of a sophisticated engineering system</p>
<p>that there&rsquo;s lots of detail here and i&rsquo;m not going to go into much of that detail but but um</p>
<p>um this is a piece of the um uh this is sort of the in a sense the front end of of okay so remember</p>
<p>what is chat gpt ultimately doing it&rsquo;s a neural net whose goal is to continue a piece of text</p>
<p>so it&rsquo;s going to it&rsquo;s going to essentially ingest the piece of text so far reading in each token of</p>
<p>the text the tokens are either words or pieces of words like things like the ing at the end of a</p>
<p>word might be a separate token they&rsquo;re they&rsquo;re sort of convenient pieces of words they&rsquo;re about</p>
<p>50 000 different possible tokens it&rsquo;s reading through the text the prompt that you wrote the</p>
<p>text that it&rsquo;s generated so far it&rsquo;s reading through all of those things it is then going to</p>
<p>to generate uh it&rsquo;s it&rsquo;s it&rsquo;s it&rsquo;s it&rsquo;s then going to its goal is to then continue that text</p>
<p>in particular it&rsquo;s going to tell you every time you run through this whole neural net it&rsquo;s going</p>
<p>to give you one new token it&rsquo;s going to tell you what the next token should be or what the</p>
<p>probabilities for different choices of the next token should be so one piece of this is the</p>
<p>embedding uh part where what&rsquo;s happening is it&rsquo;s reading a token and it is doing i mean this is</p>
<p>just uh you know it&rsquo;s it gets into a lot of detail here so for example let&rsquo;s say that the the sequence</p>
<p>we were reading was hello hello hello hello hello bye bye bye bye bye this is showing the resulting</p>
<p>um this is showing the embeddings that you get okay this this is showing you&rsquo;re trying to</p>
<p>represent i said before we were talking about embeddings for words now we&rsquo;re talking about</p>
<p>embeddings for whole chunks of text and we&rsquo;re asking what is the sequence of numbers that</p>
<p>should represent that collection of that piece of text and the way you set that up i mean again this</p>
<p>is this is getting pretty deep into the entrails of the creature um and uh uh well what what what</p>
<p>what you can think of is there are different components to this embedding vector and</p>
<p>let&rsquo;s see what am i doing here this this picture is showing across the page it&rsquo;s showing the</p>
<p>contribution from each word and down the page it&rsquo;s showing the different uh different pieces</p>
<p>of the feature vector that are being built up and the way it works is to it takes each word and it</p>
<p>has um it then the position of the word is encoded by a um uh you could just encode it by saying the</p>
<p>binary but the the position of the word as a binary digit that says is word number seven it&rsquo;s</p>
<p>you know zero zero zero one one one or something but that doesn&rsquo;t work as well as essentially</p>
<p>learning this sort of random looking collection of things which are essentially position tags for</p>
<p>words anyway the end result is you&rsquo;re going to make this thing that represents the um uh</p>
<p>where you have both where each level is a different sort of feature associated with</p>
<p>each of these words and uh that&rsquo;s that&rsquo;s the thing that&rsquo;s going to be fed into the next level of the</p>
<p>of the neural net okay so the next big piece is so-called the tension block in which i i don&rsquo;t</p>
<p>know how much this is worth explaining i i talk about this a bit more in the in the thing that</p>
<p>i wrote but essentially what&rsquo;s happening is the in the end it&rsquo;s just a great big neural net but</p>
<p>that neural net has doesn&rsquo;t have every possible connection in it it has connections for example</p>
<p>only connections that look back in the that look to places that were earlier in that text and the</p>
<p>it is in a sense concentrating differently on different parts of that text and you can</p>
<p>you can make a picture here of the amount of attention that it is paying and by attention i</p>
<p>mean it&rsquo;s literally the number the the the the size of effectively the weights that it&rsquo;s that it&rsquo;s</p>
<p>using to uh with which it is waiting different parts of the sequence that came in and the way</p>
<p>it works i think for um for gpt3 what it does is it um uh so first of all it has this embedding</p>
<p>vector which for gpt3 is about is 12 288 um i don&rsquo;t know why it&rsquo;s that particular oh i do know</p>
<p>why it&rsquo;s that number it&rsquo;s multiples of things um long and uh it&rsquo;s it&rsquo;s taking</p>
<p>it&rsquo;s trying to put together a an embedding vector to represent the text so far in which</p>
<p>it has had contributions from words at different positions and it&rsquo;s it&rsquo;s sort of it&rsquo;s figured out</p>
<p>how much contribution it should get from words at each different position um well okay so it does</p>
<p>that then it feeds the whole thing to a a layer of neural net where sort of it has um uh it&rsquo;s a</p>
<p>um what is it it&rsquo;s a a 12 000 by 12 000 array um which specifies what where there are 12 000 by 12</p>
<p>000 weights which specify for each incoming neuron each each neuron that&rsquo;s incoming has this weight</p>
<p>to this outgoing neuron and the result is you get this whole assembly of weights which looks like</p>
<p>nothing in particular this is but these are weights that have been learnt by by chat gpt</p>
<p>to be useful for its task of continuing text and you know you can play little games you can</p>
<p>you can try and visualize those weights by kind of making moving averages and you can kind of see</p>
<p>that the weights sort of roughly are kind of like randomly chosen but this is kind of showing you a</p>
<p>little bit of the detail inside that randomness and in a sense you can think of this as being</p>
<p>sort of a view into the brain of the of chat gpt of showing you at the level of these individual</p>
<p>weights that are in this neural net um what what its representation of human language is</p>
<p>right down the level you know it&rsquo;s kind of like you take apart a computer and you look at individual</p>
<p>bits inside the cpu this is kind of the same sort of thing for the representation that chat gpt has</p>
<p>of language and turns out there isn&rsquo;t just one of these attention layers okay what happens is</p>
<p>the the different elements of the feature vector for the text get there are different blocks</p>
<p>of that feature vector that get separated out and handled differently nobody really knows what the</p>
<p>interpretation of those blocks is it&rsquo;s just been found to be a good thing to do to not treat the</p>
<p>whole feature vector the same but to break it into blocks and treat blocks of pieces in that feature</p>
<p>vector differently maybe there&rsquo;s an interpretation of one piece of that feature vector that this is</p>
<p>i don&rsquo;t know words that are about motion or something it won&rsquo;t be anything like that it</p>
<p>won&rsquo;t be anything as human understandable as that it&rsquo;s kind of like a human genome or something</p>
<p>it&rsquo;s all all the traits are all mixed up in the specification it&rsquo;s like what uh it&rsquo;s it&rsquo;s not</p>
<p>something where we can easily have a sort of narrative description of what&rsquo;s going on</p>
<p>but what&rsquo;s been found is that you break this kind of feature vector of of features of the text up</p>
<p>and you have these separate attention heads that um have this sort of re-weighting process going</p>
<p>on for each one you do that and this is where you know this is like it&rsquo;s crazy that things like this</p>
<p>work but um you do that let&rsquo;s see 96 times for for chat gpt you&rsquo;re making you&rsquo;re doing the same</p>
<p>process 96 times over and uh this is for gpt2 the simpler version this is kind of a representation</p>
<p>of the of the of the things that come out of these attention layers um attention blocks what the uh</p>
<p>what the sort of weights that were used there were and you know these may look there there</p>
<p>is some regularity i don&rsquo;t know what it means but if you look at the the size of the weights</p>
<p>they&rsquo;re not perfectly for some layers they&rsquo;re gaussian distributed for some layers they&rsquo;re not</p>
<p>i have no idea what the significance of that is it&rsquo;s just a feature of what um uh what chat gpt</p>
<p>learnt as it was trying to understand human language from from the web um so</p>
<p>so okay the um uh so again that there&rsquo;s you know we we&rsquo;ve talked about kind of what&rsquo;s the the</p>
<p>in the end that what&rsquo;s happening is it&rsquo;s just a great big neural net and it&rsquo;s being it&rsquo;s being</p>
<p>trained from it we&rsquo;re trying to deduce the weights for the neural net by showing it a whole bunch of</p>
<p>text and saying uh what weights do you have to have in the neural net so that the um uh so that</p>
<p>the continuation of the text will have the right probabilities for what word comes next that&rsquo;s its</p>
<p>goal so how uh and so i&rsquo;ve sort of described the outline of how that&rsquo;s done um in the end one has</p>
<p>to feed it the reason it&rsquo;s sort of even possible to do this is that there&rsquo;s a lot of training data</p>
<p>to feed it so it&rsquo;s been fed a significant fraction of what&rsquo;s on the web there are maybe</p>
<p>i don&rsquo;t know it depends how you describe this but there are maybe six billion maybe</p>
<p>10 billion uh kind of reasonably human written pages on the web where humans actually type that</p>
<p>stuff it wasn&rsquo;t mostly machine generated etc etc etc that&rsquo;s on the publicly visible web</p>
<p>not having programs go in and not not selecting lots of different things and seeing what you get</p>
<p>that&rsquo;s just kind of raw what&rsquo;s on the web page maybe there&rsquo;s 10 maybe 100 times as much as that</p>
<p>if you were able to make selections to drill down to go into internal web pages things like this but</p>
<p>so you&rsquo;ve got something like um uh some you know some number of billions of human written pages</p>
<p>pages and uh there&rsquo;s a convenient collection called common crawl that&rsquo;s got where where one</p>
<p>goes where uh it&rsquo;s um you know you start from one web page you follow all the links you collect all</p>
<p>those pages you keep going just following links following links until you&rsquo;ve until you&rsquo;ve visited</p>
<p>all the connected parts of the web but um the result of this is there&rsquo;s a trillion words of text</p>
<p>that you can readily get from uh from the web um they&rsquo;re also they&rsquo;re probably 100 million books</p>
<p>that have been published maybe 100 i think the best estimate maybe 130 million books that have</p>
<p>been published of which five or ten million exist in digitized form and you can use those as a</p>
<p>training data as well and that&rsquo;s another 100 billion or so uh words of of of of text so you&rsquo;ve</p>
<p>got trillion-ish words of text and that&rsquo;s what um uh and there&rsquo;s probably much more than that in</p>
<p>if you have the um uh the transcriptions of videos and things like this you know for me personally</p>
<p>i&rsquo;ve kind of been um uh you know as a kind of a personal estimate of these things i i realized that</p>
<p>the things i&rsquo;ve written over my lifetime constitute about three million words the um the emails i&rsquo;ve</p>
<p>sent over the last 30 years are another 15 million words and the total uh number of</p>
<p>words i&rsquo;ve typed is around 50 million um interestingly in the live streams i&rsquo;ve done</p>
<p>just in the last couple of years i have spoken another 10 million words so it gives a sense of</p>
<p>what you know human output is what but the main point is there&rsquo;s a trillion words available on on</p>
<p>uh that you can use to train uh a neural net to be able to do this task of of continuing from from</p>
<p>things um it&rsquo;s uh let&rsquo;s see in um right so so the actual process of um uh one thing to understand</p>
<p>about training a neural net there&rsquo;s sort of a question okay there&rsquo;s a there&rsquo;s a question when</p>
<p>we looked at those functions before and we said how many neurons do we have to have to represent</p>
<p>this function well how many training examples do we have to give to get the the neural net trained</p>
<p>to represent that function in those cases we didn&rsquo;t need very big neural nets we need a lot</p>
<p>of training examples there&rsquo;s all kinds of effort to understand how many training examples do you</p>
<p>actually need how big a neural net do you actually need to to uh do something like do this text</p>
<p>translation uh uh thing well well it&rsquo;s not really known but uh you know with 175 billion weights</p>
<p>the sort of the surprise is that chat gpt does pretty well now you can ask the question um what</p>
<p>um what&rsquo;s the uh uh how much training does it need um and uh how many times does it have to be shown</p>
<p>those trillion words what&rsquo;s the relationship between the trillion words and the number of</p>
<p>weights in the in the um in the network and it seems to be the case that for text um that sort</p>
<p>of the number of weights in the network is sort of comparable to the number of training examples</p>
<p>you sort of show it the training examples about once if you show it too many times it actually</p>
<p>gets worse in its performance it&rsquo;s very different from what happens when you&rsquo;re training for</p>
<p>mathematical functions and things like this um but uh one of the things that&rsquo;s that&rsquo;s an issue</p>
<p>is that if you&rsquo;re every time then i should say every every time i i should explain by the way</p>
<p>that the the the every time the neural net runs what&rsquo;s happening is you&rsquo;re giving it in the case</p>
<p>of chat gpt you&rsquo;re giving it this collection of numbers that represents the text it&rsquo;s gotten so</p>
<p>far and then that collection numbers is the input to the neural net then you sort of ripple through</p>
<p>the neural net layer after layer after layer it&rsquo;s got about 400 layers um sort of core layers um</p>
<p>it ripples through all those layers and then at the end you get some array of numbers that array</p>
<p>of numbers actually are probabilities for each of the 50,000 possible words in english um and uh</p>
<p>that uh based on that it then picks the next word but so the main operation of chat gpt is a very</p>
<p>just straight through you know you&rsquo;ve got this text so far given that percolate through this</p>
<p>network say what the next result should be it&rsquo;s very it just runs through one time it&rsquo;s actually</p>
<p>very different from the way computers tend to work for other purposes most non-trivial computations</p>
<p>you&rsquo;re taking the same piece of of of of sort of computational material the same piece of data</p>
<p>and you compute on it over and over and over again in sort of simple models of computation</p>
<p>like turing machines that&rsquo;s what&rsquo;s happening all the time that&rsquo;s what&rsquo;s happening that&rsquo;s what makes</p>
<p>computers able to do the non-trivial things computers do or that is that they are taking</p>
<p>a sort of maybe a small number of pieces of data and they&rsquo;re just read reprocessing things over and</p>
<p>over again what&rsquo;s happening in something like chat gpt is you&rsquo;ve got this big network you just</p>
<p>percolate through it once for every token the only sense in which there&rsquo;s any feedback is that once</p>
<p>you get an output you add that token to the input you feed it on the next step so it&rsquo;s kind of an</p>
<p>outer loop where you&rsquo;re giving feedback by adding tokens to the text then that percolates through</p>
<p>then you get another token that percolates through so it&rsquo;s a very it&rsquo;s a very big outer loop it&rsquo;s</p>
<p>probably the case certainly in computers in in lots of non-trivial computations that we do there</p>
<p>are lots of inside loops that are happening quite possibly in the brain there are inside loops that</p>
<p>are happening as well but the model that we have in chat gpt is this kind of just percolate through</p>
<p>once kind of model with a very complicated network but it&rsquo;s just percolating through once</p>
<p>so that&rsquo;s how it works but one of the things that&rsquo;s tricky is that every time it percolates</p>
<p>through it has to use every single one of those weights so every token that chat gpt is producing</p>
<p>it&rsquo;s essentially doing 175 billion mathematical operations to see how to use each of those weights</p>
<p>to compute the results most likely that&rsquo;s not necessary actually but we don&rsquo;t know how to</p>
<p>how to get do any better than that right now but that&rsquo;s what it&rsquo;s doing so every time if it has</p>
<p>it&rsquo;s percolating through doing that well the when you train chat gpt and you are sort of</p>
<p>you&rsquo;re working out you know how do you deal with oh making the weights change based on the loss</p>
<p>that&rsquo;s another you&rsquo;re kind of every time you you make a training step you&rsquo;re having to kind of do</p>
<p>a reverse version of that forward so-called inference process it turns out that reverse</p>
<p>process isn&rsquo;t that much more expensive than the forward process but you have to do it a whole lot</p>
<p>of times in the training so typically if you have a model of size n roughly for text it seems like</p>
<p>you need about n squared amount of computational effort to do the training and n is pretty big for</p>
<p>the case when you&rsquo;re dealing with sort of language and things of the size of chat gpt and so the</p>
<p>training process that that just a little bit mathematical square is a is a really big deal</p>
<p>and it means that you are you know to spend hundreds of millions of dollars potentially</p>
<p>on doing the training with current gpus and things like this is is what you have to think about doing</p>
<p>based on the current model of how neural nets work now i mean i have to say that that there&rsquo;s a lot</p>
<p>of aspects of the current model that probably aren&rsquo;t the final model um and you know we can</p>
<p>plainly see that there are big differences between for example things the brain manages to do for</p>
<p>example one big difference is most of the time when you&rsquo;re training a neural net most of the</p>
<p>uh the the memory and the the the you&rsquo;re doing that by having you have a bunch of of of things</p>
<p>in memory and you have some computation that&rsquo;s going on but the things that are in memory are</p>
<p>mostly idle most of the time and there&rsquo;s just a little bit of computation that&rsquo;s going on</p>
<p>in brains every one of our neurons is both a place that stores memory and a place that computes</p>
<p>that&rsquo;s a different kind of setup and we don&rsquo;t know how to do neural nets training the various</p>
<p>things that have been looked at from the distant past actually about how to do this even from the</p>
<p>1940s people were starting to think about some distributed ways to to do learning in neural</p>
<p>nets but that&rsquo;s not something that&rsquo;s that&rsquo;s landed yet as a thing we can do okay case of chat gpt</p>
<p>um an important thing was and this was something you know six months ago a year ago there were</p>
<p>kind of early versions of of the gpt family uh text completion systems and so on and they were</p>
<p>kind of the text they produced was only so so um and then something was done by open ai uh with</p>
<p>chat gpt which was that there was an additional step a reinforcement learning training step</p>
<p>that was was done where essentially what was done was humans told chat gpt go and make an essay go</p>
<p>and be be a chat bot you know have a conversation with me and the humans rated what came out and</p>
<p>said but that&rsquo;s terrible that&rsquo;s better that&rsquo;s terrible etc etc etc and the thing that was done</p>
<p>was then that that little bit of poking turns out to have had it seems to have had a very big effect</p>
<p>that little bit of kind of human guidance of yes you got the thing from the statistics of the web</p>
<p>now when you look at what you got this direction you&rsquo;re going in is a bad direction to go and it&rsquo;s</p>
<p>going to lead to a really boring essay or whatever else um and so that kind of and by the way there&rsquo;s</p>
<p>a place where where a lot of kind of complication about well what do the humans really think the</p>
<p>the network should be that the system should be producing if the humans say we really don&rsquo;t want</p>
<p>you to talk about this we really don&rsquo;t want you to talk about that that&rsquo;s the place that gets</p>
<p>injected is in this is in this reinforcement learning step uh at the end um and but what you</p>
<p>do is for example is uh given that sort of the way the humans poked at those essays you can watch</p>
<p>what they did when they poked at those essays and rated what happened and so on and you can try and</p>
<p>machine learn that set of things that the humans did then you can use that to provide much more</p>
<p>training data to then retrain a piece of of this uh do retraining of the network to based on the</p>
<p>sort of the tweaking that the humans did you can do sort of fine tuning of this network based on</p>
<p>the the particular poking that the humans did turned into another network that can then be</p>
<p>used to do the training to produce the examples to do the training of of the of the main network</p>
<p>so that&rsquo;s a thing that seems to have had a big effect on the actual sort of human perception</p>
<p>of what happens in in um uh in in chat upt and i think um uh the other thing that is a um um</p>
<p>sort of a surprise is that you can give it these long prompts in which you tell it all kinds of</p>
<p>things and it will then sort of make use of that in a rather human kind of way in generating</p>
<p>the text that comes later okay big question is how come this works why is it that a thing with only</p>
<p>you know 100 billion or so weights or something can reproduce this sort of amazing thing that</p>
<p>seems to require all of the sort of depth of human thinking and and brains and things like that human</p>
<p>language how does that manage to work and i think the um uh the key thing to realize is what it&rsquo;s</p>
<p>really telling us is a science fact it&rsquo;s telling us there&rsquo;s more regularity in human language than</p>
<p>their thought than we thought there was it&rsquo;s telling us that this this thing that&rsquo;s that is</p>
<p>human language has a lot of structure in it and what it&rsquo;s done is it&rsquo;s learned a bunch of that</p>
<p>it&rsquo;s learned a bunch of that structure and it&rsquo;s learned structure that we never even really</p>
<p>noticed was there and that&rsquo;s what&rsquo;s allowing it to generate these kind of plausible pieces of text</p>
<p>that are you know that are making use of the structure we know so we know certain kinds of</p>
<p>structure that exists in language we know the um so for example um uh here&rsquo;s an example so</p>
<p>one one piece of structure that we know um share this again um</p>
<p>one piece of structure we know is grammatical syntax um the the uh syntactic grammar we know</p>
<p>that the that sentences aren&rsquo;t random jumbles of words sentences are made up with nouns in</p>
<p>particular places verbs in particular places and we can represent that by a parse tree</p>
<p>in which we say you know here&rsquo;s the whole sentence there&rsquo;s a noun phrase a verb phrase</p>
<p>another noun phrase these are broken down in certain ways this is the parse tree and there</p>
<p>are certain that in order for this to be a grammatically correct sentence this has there</p>
<p>are only certain possible forms of parse tree that correspond to a grammatically correct sentence</p>
<p>so this is a regularity of language that we&rsquo;ve known for a couple of thousand years it&rsquo;s only</p>
<p>really been codified uh it was big effort to codify it in 1956 um but it was sort of known</p>
<p>this general idea was was known for a long time um but uh then this um um that that we can kind</p>
<p>of represent the sort of syntactic grammar of language by these kinds of rules that say you</p>
<p>can put nouns only together with verbs in this way and that way and to any set of rules and this has</p>
<p>been a big source of controversy in linguistics to any set of rules you can define there&rsquo;ll always</p>
<p>be some weird exception where people typically say this rather than that but if you you know it&rsquo;s at</p>
<p>the much like happens in typical machine learning you know if you&rsquo;re interested in the 95 percent</p>
<p>result then there are just rigid rules and there are a few exceptions here and there okay so that&rsquo;s</p>
<p>one form of regularity that we know exists in language is is this um uh syntactic um regularity</p>
<p>now what one thing we can do we can ask for sort of um uh chat gpt has effectively implicitly learned</p>
<p>this um syntactic grammar nobody ever told it verbs and nouns go this way and that way it</p>
<p>implicitly learned it by virtue of seeing a trillion words of text on the web which all</p>
<p>have these properties and when it&rsquo;s saying well what are the typical words that follow well it&rsquo;s</p>
<p>going to be words that followed in the in the examples it had and those will follow mostly</p>
<p>correct grammar now we can we can take a simpler version of this we can just understand what&rsquo;s</p>
<p>going on we can take a very very trivial grammar we can take a grammar that&rsquo;s just a parentheses</p>
<p>just open and close parentheses and something is grammatically correct if we open parentheses and</p>
<p>they always eventually close and this is a parse tree for a um uh for a parenthesis uh you know</p>
<p>open open open close open close etc etc etc this is the parse tree that sort of shows how you can</p>
<p>it&rsquo;s a representation of of of uh of the sort of the the um the parsing of this sequence of</p>
<p>open and closed parentheses okay so we might say well can we train a neural net to what would it</p>
<p>take to train a neural net to know even this particular kind of syntactic grammar so we looked</p>
<p>at a simple how big was it it was um pretty small uh okay we made a transformer net with eight heads</p>
<p>and length 128 so um uh but but our thing was a was a lot simpler than than um uh than chat gpt but</p>
<p>you can you can use one of these transformers and if you look at the the um uh the post i made that</p>
<p>there&rsquo;s the actual transformer is there and you can you can play with it in wolfram language um</p>
<p>but in any case if you if you give that transformer this sequence here you say what comes next it says</p>
<p>it says okay uh well 54 probability that there&rsquo;s a closed parameter based on oh it&rsquo;s training data</p>
<p>was a randomly selected collection of correct open close open close parenthesis um uh sequences</p>
<p>it has a little bit of a goof here because it says with 0.0838 probability this is the end of</p>
<p>the sequence which would of course be grammatically incorrect because there&rsquo;s no close for this there&rsquo;s</p>
<p>for the for the open parentheses here if um uh if we give something which is correctly uh closing</p>
<p>then it says okay great there&rsquo;s a 34 probability this is the end of the sequence there were no</p>
<p>further opens here it has a little bit of a goof here because it says 15 probability</p>
<p>there&rsquo;s a closed parenthesis that should occur here which can&rsquo;t possibly be right because if we</p>
<p>put a closed parenthesis here doesn&rsquo;t have a corresponding open parenthesis it&rsquo;s not grammatically</p>
<p>correct but in any case this gives a sense of what it takes for one of these transformer nets we can</p>
<p>look inside this transformer net we can see sort of what it took to learn this very simple grammar</p>
<p>chat gpt is learning the much more complicated grammar of english it&rsquo;s actually easier</p>
<p>probably to learn the grammar of english because there&rsquo;s so many clues in the actual words that</p>
<p>are used to how they&rsquo;re grammatically put together and there&rsquo;s so many things that we</p>
<p>humans wouldn&rsquo;t even notice as wrong in some sense of wrong because they&rsquo;re they&rsquo;re kind of just what</p>
<p>we do but in this more austere case of just this sort of mathematically defined parenthesis language</p>
<p>we do notice so if we just give it a bunch of open paren open paren etc and we ask it what&rsquo;s</p>
<p>the highest probability continuation you&rsquo;ll see it does pretty good up to this point and then</p>
<p>it starts losing it and it&rsquo;s kind of a little bit like what would happen with humans you know we can</p>
<p>tell at some point here that by just by eye that these are correctly closed it becomes more difficult</p>
<p>to tell that when we get out here and it becomes more difficult for the for the network to tell</p>
<p>that too and this is a typical feature of these neural nets that with these sort of shallow</p>
<p>questions of oh you just have you know you can just see this block of things you see another</p>
<p>block of things it does fine when it has to go to to much greater depth it&rsquo;s it doesn&rsquo;t work so well</p>
<p>for a sort of regular computer that can do loops and things inside it&rsquo;s very easy to to figure out</p>
<p>what&rsquo;s happening here because you effectively just count up the number of open parens count down the</p>
<p>number of closed parens and so on by the way if you try this in actual chat gpt it also it will</p>
<p>confidently assert that it&rsquo;s it&rsquo;s match parentheses but it will often be wrong for larger parenthesis</p>
<p>sequences it has the exact same problem it&rsquo;s it&rsquo;s a it fails at a slightly larger size but it&rsquo;s</p>
<p>still going to fail and that&rsquo;s just a feature of of this kind of thing so uh well okay so one type</p>
<p>of regularity in language that chat gpt has learnt is syntactic grammar um another type of regularity</p>
<p>there&rsquo;s there&rsquo;s one more that that you can readily identify and that&rsquo;s logic and what is logic well</p>
<p>originally when logic was in was invented you know by aristotle so far as we know you know</p>
<p>what aristotle did was effectively a bit like a machine learning system he looked at lots of</p>
<p>examples of rhetoric lots of example speeches people gave he said what are some forms of</p>
<p>argument that appear repeatedly if somebody says you know uh something like people might have said</p>
<p>you know all men are mortal socrates is a man therefore socrates is mortal um all all x&rsquo;s are</p>
<p>all x&rsquo;s are y um uh z is a is a is an x therefore z is a y um the uh uh that that logic is taking</p>
<p>sort of forms of of of language and saying these are patterns that are repeated possible patterns</p>
<p>in these in these pieces of language that are meaningful sequences and originally in</p>
<p>syllogistic logic which is what aristotle originally invented it really was very language</p>
<p>based and people would memorize you know the middle ages people would memorize these forms</p>
<p>of syllogism the barbarous syllogism the seller and syllogism and so on which were just these</p>
<p>these patterns of of word usage where you could substitute in a different word for socrates</p>
<p>but it was still that same pattern that same structure so that was that was that&rsquo;s kind of</p>
<p>another form of regularity and when chat gpt is says it&rsquo;s oh it&rsquo;s it&rsquo;s figuring things out</p>
<p>well part of what&rsquo;s figuring out is it knows syllogistic logic because it&rsquo;s seen a zillion</p>
<p>examples just like aristotle presumably seen a bunch of examples when he invented logic it&rsquo;s</p>
<p>seen a bunch of examples of this sentence follows this sentence in this way and so it can it&rsquo;s going</p>
<p>to do that too when it says what&rsquo;s the statistical thing that&rsquo;s going to happen based on based on the</p>
<p>web um and so so that&rsquo;s um uh so by the way when logic developed by the 1800s when people like</p>
<p>boule were getting into the picture and making formal logic um it was no longer just these</p>
<p>patterns boom it&rsquo;s a pattern it looks like this it was more this thing you could build up many</p>
<p>many layers of structure and you could build you know very complicated logical expressions where</p>
<p>the whole thing was deeply nested and of course our computers today are based on those deeply nested</p>
<p>logical expressions chat gpt doesn&rsquo;t stand a chance of of decoding what&rsquo;s going on with one</p>
<p>of those deeply nested kind of mathematical computational style um boolean expressions</p>
<p>but it does well at this kind of aristotle level kind of um uh you know structure of of</p>
<p>sort of templated structure of logic okay well i wanted to talk just for a little bit and then we</p>
<p>should wrap up here and i can try and answer some questions um the uh about kind of what so what are</p>
<p>the regularities that chat gpt has discovered in this thing that we do which is language and all</p>
<p>the thinking that goes on around around language and i don&rsquo;t know the answer to this i have some</p>
<p>ideas about what&rsquo;s going on i&rsquo;ll just you know give a little bit of a tour we talked about kind</p>
<p>of meaning space the sort of space of of how words arrange in in some how you can arrange words in</p>
<p>some kind of meaning space and we can we can kind of see how words arrange these are different parts</p>
<p>of speech for a given word there may be different places in meaning space where different instances</p>
<p>of that word occur this is the word crane and this is different sentences there are two obvious</p>
<p>meanings of crane you know the bird and the the and the machine and they sort of break up in</p>
<p>meaning space where they are we can look at the sort of structure of meaning space another thing</p>
<p>we can ask is is meaning space like physical space is it the case that there are parallel lines in</p>
<p>meaning space are there things where we can go from place a to place b and we and then in parallel we</p>
<p>transport to new places well so we can ask you know if we have analogies is it the case that</p>
<p>we can go you know from woman to man from queen to king that those are sort of parallel</p>
<p>pods in meaning space the answer is well maybe a bit not very convincingly that&rsquo;s really the</p>
<p>question in in space in physical space this is the question whether this is like flat space</p>
<p>it&rsquo;s like if we have things moving in flat space you know um newton&rsquo;s first law says if the thing</p>
<p>is not acted on by a force it&rsquo;ll just keep going in a straight line well then we have gravity and</p>
<p>we can represent gravity by talking about the curvature of space here this question is when we</p>
<p>go from uh you know ear to hear eye to see those are sort of uh we&rsquo;re moving in a certain direction</p>
<p>in meaning space and in a sense the question of whether these things correspond to whether we can</p>
<p>do this kind of parallel transport idea is something like how flat is meaning space how</p>
<p>much effective gravity is there in meaning space or something like that meaning space is probably</p>
<p>not something that&rsquo;s represented in terms of the kinds of things that physical space is represented</p>
<p>in terms of but that&rsquo;s a question so now when it comes to the operation of chat qpt we can think</p>
<p>we can think about how is it moving around in meaning space it&rsquo;s got its prompt you know the</p>
<p>best thing about ai is is it&rsquo;s is its ability to okay um and uh that&rsquo;s the prompt moving around</p>
<p>in meaning space effectively and then what chat gpt does is it it continues that by continuing</p>
<p>to move in meaning space and so the question is is there something like a semantic law of motion</p>
<p>an analog of of kind of the laws of motion that we have in physical space</p>
<p>in the meaning space of of concepts words something where we can say okay if it&rsquo;s gone if it&rsquo;s moved</p>
<p>around this way it&rsquo;s like it&rsquo;s got momentum in this direction in meaning space it&rsquo;s going to</p>
<p>keep going in that meaning space it&rsquo;s nothing like that simple but the question is what are</p>
<p>how do we think about how do we represent kind of um the the the sort of the the process of going</p>
<p>through meaning space well we can start looking at that we can say uh for example the different</p>
<p>possible continuations that we get the best thing about ai&rsquo;s ability to and then what&rsquo;s the next</p>
<p>word well we can look at this kind of fan of different directions that it could go in meaning</p>
<p>space at that point and we can kind of see there&rsquo;s some there&rsquo;s some direction in meaning space it</p>
<p>tends to go in that direction it&rsquo;s not going all the way over here at least not with high probability</p>
<p>okay well if we keep going we can kind of just see sort of how that fan develops as we go further</p>
<p>out as we continue that sentence and we can kind of this is kind of like our motion in meaning</p>
<p>space kind of question and you know i don&rsquo;t know what this exactly means yet but this is kind of</p>
<p>what it looks like what the trajectory in meaning space as chat gpt tries to continue a sentence</p>
<p>looks like the green is that is the actual thing it chose i think this is a zero temperature case</p>
<p>and the the gray things are the other things that were lower probability cases so that&rsquo;s that&rsquo;s um</p>
<p>that&rsquo;s kind of what um uh that&rsquo;s some a view if we want to look at we don&rsquo;t want to want to do</p>
<p>natural science on chat gpt and say what did it discover what did it discover about how language</p>
<p>is put together one possibility is that there are these sort of semantic laws of motion</p>
<p>that describe sort of how meaning how you move through the space of meanings as you add words</p>
<p>into a into a piece of text i think a slightly different way to think about this is in terms of</p>
<p>what one could call semantic grammar so syntactic grammar is just about you know nouns verbs things</p>
<p>like that parts of speech things of that kind but we can also ask is there a generalization</p>
<p>of that that is sort of more semantic that doesn&rsquo;t just look at that has finer gradations and just</p>
<p>saying it&rsquo;s a noun it&rsquo;s a verb and says oh well that verb means motion and when we put this noun</p>
<p>together with this noun that&rsquo;s a thing you can move together with this motion word it does this</p>
<p>we kind of have buckets of meaning that are finer gradations than just parts of speech but not</p>
<p>but not necessarily individual words is there a kind of a semantic grammar that we can identify</p>
<p>that is kind of this construction kit for how we put together not just sentences that are</p>
<p>grammatically correct that are syntactically grammatically correct but sentences which are</p>
<p>somehow semantically correct now that that um i i strongly think this is possible and it&rsquo;s</p>
<p>sort of what aristotle was going for he even talks about categories of um uh sort of semantic</p>
<p>categories and things like this he talks about a variety of things he does it in a in a way that&rsquo;s</p>
<p>based on the fact that it was 2 000 years ago and we didn&rsquo;t know about computers and we didn&rsquo;t know</p>
<p>about a lot of kinds of formal things that we know about now uh strangely enough the amount of work</p>
<p>that&rsquo;s been done trying to make kind of a semantic grammar in the last 2 000 years has been rather</p>
<p>small it&rsquo;s there was a bit of an effort in the 1600s with people like leibniz with his</p>
<p>characteristica universalis and various other people trying to make what they call philosophical</p>
<p>languages uh sort of language word independent ways of describing meaning and then the more</p>
<p>recent efforts but they&rsquo;ve tended to be fairly specific fairly based on linguistics um and uh</p>
<p>fairly based on the details of structure of human language and so on um and i think this this uh this</p>
<p>idea that you can kind of have a semantic grammar is is a um and that that&rsquo;s what&rsquo;s sort of being</p>
<p>discovered is that there are these rules that go beyond that that are just rules for how you put</p>
<p>together a a meaningful sentence now you know you can get a meaningful sentence could be something</p>
<p>like the elephant flew to the moon does that sentence mean something sure it means something</p>
<p>it has a perfectly we can conjure up an image of what that means has it happened in the world no</p>
<p>it hasn&rsquo;t happened so far as we know um and uh uh so there&rsquo;s a but you know could it be in a story</p>
<p>could it be in a fictional world absolutely so this thing about this sort of semantic grammar</p>
<p>will allow you allows you to put together things which are somehow which are sort of um uh meaningful</p>
<p>things to describe about the world um the question of whether they are realized in the world or have</p>
<p>been realized in the world is a separate question but in any case the um the thing that um uh that</p>
<p>is to me interesting about this is it&rsquo;s it&rsquo;s something i&rsquo;ve long thought about because</p>
<p>i&rsquo;ve spent a large part of my life building a computational language uh wolfman language um</p>
<p>system that is an effort to represent the world computationally so to speak to take the things</p>
<p>that we know about about chemicals or lines or or images or whatever else and have a computational</p>
<p>and have a computational representation for all those things and have a computational language</p>
<p>which knows how all those things work it knows how to compute the distance between two cities</p>
<p>it knows all of those kinds of things and in in um uh and so this is i&rsquo;ve been spending the last</p>
<p>four decades or so trying to find a way to represent things in the world in this computational</p>
<p>fashion so that you can then compute uh uh you can then compute things about those things uh</p>
<p>in an explicit computational way it&rsquo;s something where uh and we&rsquo;ve been very successful at being</p>
<p>able to do that in a sense the story of modern science is a story of being able to formalize</p>
<p>lots of kinds of things in the world and we&rsquo;re kind of leveraging that in our computational language</p>
<p>to be able to formalize things in the world to compute things about how they&rsquo;ll work</p>
<p>now the um one feature of that computing about how things work is that inevitably some of those</p>
<p>computations are deep computations they&rsquo;re computations that something like a chat gpt</p>
<p>can&rsquo;t possibly do and in a sense there&rsquo;s sort of a a difference between the things that are</p>
<p>the kind of the the um the sort of shallow computations that you can learn from examples</p>
<p>in something like a chat gpt that you can say this piece of language that i saw on the web here</p>
<p>is you know statistically uh i can sort of fit that in in this place just fitting together these</p>
<p>sort of puzzle pieces of language is a very different thing from taking the world and</p>
<p>actually representing it in some truly sort of formal way computationally so that you can compute</p>
<p>things about how the world works it&rsquo;s kind of like well back before people had kind of thought of</p>
<p>this idea of of formal formalism maybe 400 years ago or more um you know everything that anybody</p>
<p>figured out was just you think about it in terms of language in terms of words in terms of sort of</p>
<p>immediate human thinking um what what then sort of came in with with mathematical science at first</p>
<p>and then computation was this idea of formalizing things and getting these much deeper uh sort of</p>
<p>ways to deduce what happens and and thing i&rsquo;ve figured out well 30 40 years ago now was</p>
<p>was this phenomenon of computational irreducibility this idea that there really are things in the world</p>
<p>where to compute what&rsquo;s going to happen you have no choice but to follow all those computational</p>
<p>steps you can&rsquo;t just jump to the end and say i know what&rsquo;s going to happen it&rsquo;s a shallow kind</p>
<p>of thing and so you know when we look at something like chat gpt there are certain kinds of things</p>
<p>it can do by sort of matching together matching these pieces of language there are other kinds</p>
<p>of things it&rsquo;s not going to be able to do it&rsquo;s not going to be able to do</p>
<p>not going to be able to do sort of the mathematical computation the the kind of the the thing which</p>
<p>requires an actual computational representation of the world for those things like us humans</p>
<p>it&rsquo;s kind of a used tools type uh type situation and very conveniently our wolfman alpha system</p>
<p>that uh um used in a bunch of intelligent assistants and so on is uh has this feature</p>
<p>that it&rsquo;s using our wolfman language computational language underneath but it actually takes natural</p>
<p>language input so it&rsquo;s actually able to take the natural language that is produced by a chat gpt</p>
<p>for example take that and then turn that into computational language do a computation work out</p>
<p>the result get the right answer feed that back to chat gpt and then it can talk sense so to speak</p>
<p>rather than just following sort of the statistics of words on the web so it&rsquo;s a way of you know by</p>
<p>by allowing but you can get sort of the best of both worlds by having something where you have</p>
<p>this sort of flow of of language um as well as as as something where you have this sort of depth</p>
<p>of computation by having chat gpt use wolfman alpha as a tool and i wrote a bunch of stuff</p>
<p>about that and all kinds of things are happening with that um but uh the thing that um uh you know</p>
<p>talking about what did chat gpt discover i think the thing it discovered is there is a semantic</p>
<p>grammar to a lot of things there is a way to represent uh using sort of computational primitives</p>
<p>lots of things that we talk about in in text and in our computational language we&rsquo;ve got</p>
<p>representations of lots of kinds of things whether it&rsquo;s foods or chemicals or or stars or whatever</p>
<p>else but when it comes to something like i&rsquo;m going to eat a piece of chocolate we have a great</p>
<p>representation of the piece of chocolate we know all its nutrition properties we know everything</p>
<p>about it um but we don&rsquo;t have a good representation yet of i&rsquo;m going to eat the i&rsquo;m going to eat part</p>
<p>what i think chat gpt has shown us is that it&rsquo;s very plausible to get sort of this semantic grammar</p>
<p>of how one has these pieces of of representing these sort of lumps of meaning in language and</p>
<p>i think what&rsquo;s going to happen and i&rsquo;ve been interested in doing this for a long time i think</p>
<p>this is now finally the impetus to really uh really roll up one&rsquo;s sleeves and do it um it&rsquo;s a it&rsquo;s a</p>
<p>somewhat complicated project for a variety of reasons not least that you have to make these</p>
<p>kind of uh uh well you you have it has to be you have to make sort of this process of designing a</p>
<p>language is something i happen to have been doing for 40 years designing our computational language</p>
<p>this is a language design problem and those are to my mind those are actually the the single most</p>
<p>concentrated intellectually difficult thing that i know is this problem of language design so this</p>
<p>is sort of a generalization of that but i think chat gpt has kind of shown us what you know i</p>
<p>didn&rsquo;t know how hard it was going to be i&rsquo;m now convinced it&rsquo;s it&rsquo;s doable so to speak so what</p>
<p>what does this um uh you know you might ask the question you know people might have said okay</p>
<p>look you know we we&rsquo;ve seen neural nets that do speech to text we&rsquo;ve seen neural nets do</p>
<p>image identification now we&rsquo;ve seen neural nets that can write essays surely if we have a big</p>
<p>enough neural net it can do everything well not the neural nets of the kind we have so far that</p>
<p>have the training structure that they have so far not on their own they will not be able to do these</p>
<p>irreducible computations now these irreducible computations are not easy for us humans either</p>
<p>you know when it comes to doing piece of math or worse if somebody says here&rsquo;s a program run this</p>
<p>program in your head good luck you know very few people can do that um it um it&rsquo;s something where</p>
<p>there is a a difference between what is sort of immediate and easy for us humans and what is sort</p>
<p>of computationally possible now another question is maybe we don&rsquo;t care about the things that</p>
<p>aren&rsquo;t easy for humans it&rsquo;s turned out that we built an awful lot of good technology over the</p>
<p>last few centuries based on what amounts to a much deeper level we haven&rsquo;t really in our</p>
<p>technology we&rsquo;re not actually going even that far into irreducible computation but going far enough</p>
<p>that it&rsquo;s beyond what we humans can readily do or what we can do with kind of the neural nets that</p>
<p>exist today um so i think the uh that that&rsquo;s the kind of the thing to understand that there&rsquo;s a</p>
<p>there&rsquo;s a certain set of things what&rsquo;s what&rsquo;s happening in chat gpt is it&rsquo;s kind of taking the</p>
<p>average of the web plus books and so on and it&rsquo;s saying you know i&rsquo;m going to fit things together</p>
<p>based on that and that&rsquo;s how it&rsquo;s writing its essays and it&rsquo;s and when it is deducing things</p>
<p>when it&rsquo;s doing logic things like that what it&rsquo;s doing is it&rsquo;s doing logic like the way aristotle</p>
<p>discovered logic it&rsquo;s figuring out oh there&rsquo;s a pattern of words that looks like this and it tends</p>
<p>to follow it like that because that&rsquo;s what i&rsquo;ve seen in in a hundred thousand examples on the web</p>
<p>um so that that&rsquo;s that&rsquo;s kind of what it&rsquo;s doing and it it kind of that gives us some sense of what</p>
<p>what it&rsquo;s going to be able to do and i think the most important thing it&rsquo;s able to do is it&rsquo;s a</p>
<p>form of user interface you know we can get i might get something where i know oh what really matters</p>
<p>in three bullet points but if i&rsquo;m going to communicate that to somebody else they&rsquo;re</p>
<p>really not going to understand my three bullet points they need wrapping around that they need</p>
<p>something which is a whole essay describing you know that that&rsquo;s the human interface so to speak</p>
<p>it&rsquo;s just like you could have you know the raw bits or something and that wouldn&rsquo;t be useful</p>
<p>to us humans we have to wrap it in a human like in a sort of human compatible way and language</p>
<p>is sort of our richest human compatible medium and what what chat gpt is doing is it&rsquo;s able to</p>
<p>i think what the way to think about it is it&rsquo;s providing this interface that is well it is just</p>
<p>it&rsquo;s generating pieces of language that are consistent but if you feed it specific things</p>
<p>that it will talk about so to speak then it&rsquo;s kind of wrapping the thing the specifics with</p>
<p>this interface that corresponds to kind of flowing human language all right i went on much longer</p>
<p>than i intended um and uh uh i see there are a bunch of questions here and i&rsquo;m going to go from</p>
<p>um and to try and address some of these as a question from antipas are constructed languages</p>
<p>like esperanto more amenable to semantic grammar ai approach very good very interesting question</p>
<p>so i think the one that i was experimenting with was the smallest of the constructed languages</p>
<p>a language called toki pona that has only 130 words in it um it is not a a language that allows</p>
<p>one to express you know everything one might want to express but it&rsquo;s a good kind of uh</p>
<p>small talk type language a small language for doing small talk so to speak but it expresses</p>
<p>a bunch of decent ideas and so i was i was going to look at yes that it&rsquo;s a good clue</p>
<p>again to semantic grammar that there are these small constructed languages it also helps um</p>
<p>um i think well i i also think the probably the largest the constructed language is ithquil</p>
<p>is another uh interesting uh source it&rsquo;s a language which has tried to pull in all of</p>
<p>the kind of language structures from all the all known languages in some first approximation um</p>
<p>the uh that&rsquo;s um um uh yeah that that&rsquo;s that yeah so i think the answer is that yes i think</p>
<p>they&rsquo;re a good uh stimulus for um for thinking about semantic grammar in a sense when people</p>
<p>were trying to do this back in the 1600s they&rsquo;re very confused about many things but you know</p>
<p>one gives them a lot of they&rsquo;ve gone a long way given that it was the 1600s they were confused</p>
<p>about things like whether the actual letters that were written as you wrote the language mattered</p>
<p>and how that was you know uh more so than the than the structure of things but but uh there was the</p>
<p>beginning of that um uh that kind of idea um okay i&rsquo;m going to take these from the end but i want</p>
<p>to go back to some of these others um okay tori is asking how come on study what&rsquo;s the best way</p>
<p>of prompting chat gpt could a semantic law of motion be helpful undoubtedly yes i don&rsquo;t know</p>
<p>the answer that i think it&rsquo;s a good question and i don&rsquo;t really know um the uh um you know i i think</p>
<p>um yeah i don&rsquo;t know uh albert is asking is the 4000 token limits analogous to working memory</p>
<p>would accessing larger memory be increasing the token limits or increasing such capabilities</p>
<p>reinforcement learning well i think that the the token limits that exist right now uh are you know</p>
<p>if you want to have a coherent essay and you want it to know what it was talking about back in that</p>
<p>early part of the essay you better have enough tokens in that are being fed into the neural net</p>
<p>every time it gets a new token if it just doesn&rsquo;t know what it was talking about if it forgot what</p>
<p>it was talking about 5 000 tokens ago it may be saying totally silly things now because it didn&rsquo;t</p>
<p>know what was there before so in some sense it&rsquo;s like it&rsquo;s i don&rsquo;t think it&rsquo;s i don&rsquo;t think it&rsquo;s</p>
<p>like our short working memory but i think um you know it&rsquo;s kind of like you ramble on i ramble on</p>
<p>a lot you know talking about things and like i might have forgotten half an hour later that i</p>
<p>talked about that already i might be telling the same story again i hope i don&rsquo;t do that i don&rsquo;t</p>
<p>think i do that too badly um but but you know that that&rsquo;s a question of what that that&rsquo;s the</p>
<p>kind of thing that happens with this token limit um let&rsquo;s see let me go back to some of the questions</p>
<p>that were asked earlier here um okay erin was asking talking more about the tension between</p>
<p>super intelligence and computational irreducibility how far can llm intelligence go i think i talked a</p>
<p>little bit about that i think this question oh boy this is this is kind of complicated i mean so</p>
<p>this question about okay</p>
<p>the the universe the world is full of computational irreducibility</p>
<p>that&rsquo;s it&rsquo;s full of situations where we know the underlying rules but we run them as a computation</p>
<p>and you can&rsquo;t shortcut the steps what what we&rsquo;ve discovered from our physics project is it looks</p>
<p>like the very lowest level of space time works just that way in fact just earlier today saw a</p>
<p>lovely um uh work um about uh um doing practical simulation of space times and things using</p>
<p>using those ideas and very much supporting again this it&rsquo;s really computationally irreducible at</p>
<p>the lowest level just like in a in something like a gas the molecules are bouncing around in this</p>
<p>computationally irreducible way what we humans do is we sample sort of aspects of the universe that</p>
<p>have enough reducibility that we can predict enough that we can kind of go about our lives</p>
<p>like we don&rsquo;t pay attention to all those individual gas molecules bouncing around</p>
<p>we only pay attention to the aggregate of the pressure of the gas or whatever else we don&rsquo;t</p>
<p>pay attention to all the atoms of space we only pay attention to the fact that there&rsquo;s this thing</p>
<p>that we can think of as more or less continuous space so our story has been a story of finding</p>
<p>slices of reducibility slices places where we can predict things about the universe there&rsquo;s a lot</p>
<p>about the universe we cannot predict we don&rsquo;t know and if our existence depended on those things if</p>
<p>we had not found kind of these these slices of reducibility uh we wouldn&rsquo;t we wouldn&rsquo;t be able</p>
<p>to have a coherent existence of the kind that we do so if you ask sort of where do you go with that</p>
<p>well there are there are an infinite collection there&rsquo;s an infinite kind of web of pieces of</p>
<p>computational reducibility there&rsquo;s sort of an infinite set of things to discover about that</p>
<p>we have discovered some of them as we advance in our science and with our technology for for things</p>
<p>we get to explore more of that kind of web of reducibility but that&rsquo;s that&rsquo;s really the issue</p>
<p>now that the problem is that the way that we humans kind of react to that is we have ways to</p>
<p>describe what what we can describe we have a we have words that describe things that are common</p>
<p>in our world we have a word for a camera we have a word for a chair those kinds of things we don&rsquo;t</p>
<p>have words for things which have not yet been common in our world and you know when we look</p>
<p>at the innards of chat gpt it&rsquo;s got all kinds of stuff going on in it maybe some of those things</p>
<p>happen quite quite often but we don&rsquo;t have words for those we don&rsquo;t have a way we haven&rsquo;t yet found</p>
<p>a way to describe them when we look at the natural world we&rsquo;ve there are things that we&rsquo;ve seen</p>
<p>repeatedly in the natural world we have words to describe them we&rsquo;ve built up this kind of</p>
<p>descriptive layer for for talking about things but one of the things that happens is that if we</p>
<p>kind of jump out to somewhere else in the sort of universe of possible computations</p>
<p>there may be pieces of reducibility there but we don&rsquo;t have words to describe those things we only</p>
<p>have we know about the things that are near us so to speak and so and gradually as science advances</p>
<p>is we get to expand the domain that we can talk about so to speak or everything advances we get</p>
<p>to have more words we get to be able to talk about more things but in a sense to have something which</p>
<p>operates it&rsquo;s this gradual process of us sort of societally in a sense learning more concepts</p>
<p>we kind of can exchange concepts we can build on those concepts and so on but if you throw us out</p>
<p>into some other place in what I call the roulade the space of all possible computational processes</p>
<p>if you throw us out into an arbitrary place there we will be completely confused because there will</p>
<p>be things we can tell there are actual computations going on here there are things happening there&rsquo;s</p>
<p>even pieces of reducibility but we don&rsquo;t we don&rsquo;t relate to those things so it&rsquo;s kind of like</p>
<p>imagine that you were you know you&rsquo;re here now and you&rsquo;re you know chronically frozen for 500</p>
<p>years and you wake up again and there&rsquo;s all these other things in the world and it&rsquo;s hard to</p>
<p>reorient for all those other things without having seen the intermediate steps and I think that that</p>
<p>when you talk about kind of what where can you go from what we have now how can you sort of add</p>
<p>more you&rsquo;re basically intelligence is all about these kind of pieces of reducibility these ways</p>
<p>to jump ahead and not just say it&rsquo;s what we what we think of as sort of human-like intelligence</p>
<p>is about those kinds of things and I think the you know so what&rsquo;s the vision of what will happen</p>
<p>you know when when the world is full of AIs sort of interesting because actually we&rsquo;ve seen it before</p>
<p>I mean when the world is full of AIs and they&rsquo;re doing all these things and there&rsquo;s all this</p>
<p>computational irreducibility there are all these pockets of reducibility that we don&rsquo;t have access</p>
<p>to because we haven&rsquo;t sort of you know incrementally got to that point what what&rsquo;s going to</p>
<p>be happening is there&rsquo;s all this stuff happening among the AIs and it&rsquo;s happening in this layer</p>
<p>that we don&rsquo;t understand it&rsquo;s already happening in plenty of places on the web and you know</p>
<p>bidding for ads or showing you content on the web whatever there&rsquo;s a layer of AI that&rsquo;s happening</p>
<p>that we don&rsquo;t understand particularly well we have a very clear model for that which is nature</p>
<p>nature is full of things going on that are often computationally irreducible that we don&rsquo;t</p>
<p>understand what we&rsquo;ve been able to do is to carve out an existence so to speak that is coherent for</p>
<p>us even though there&rsquo;s all this computational irreducibility going on we&rsquo;ve got these little</p>
<p>niches with respect to nature which which are convenient for us as as humans so to speak and</p>
<p>I think it&rsquo;s sort of the same thing with the the AI world as it becomes like the natural world</p>
<p>and it becomes sort of not immediately comprehensible to us that&rsquo;s we are we are kind of</p>
<p>we&rsquo;re you know our view of it has to be oh that&rsquo;s just you know the operation of nature that&rsquo;s just</p>
<p>something I&rsquo;m not going to understand oh that&rsquo;s just the operation of the AI is not going to</p>
<p>understand that there&rsquo;s this piece that we&rsquo;ve actually managed to humanize that we can understand</p>
<p>so that&rsquo;s that&rsquo;s a little bit of the the thought about about how that develops in other words you</p>
<p>know you can say I&rsquo;m going to throw you out to some random place in the roolyad there&rsquo;s incredible</p>
<p>computations happening it&rsquo;s like great that&rsquo;s nice I&rsquo;ve spent a bunch of my life studying those kinds</p>
<p>of things but pulling them back reeling them back into something which has sort of direct human</p>
<p>understandability is is a difficult thing uh Aaron is asking more of a business question about</p>
<p>about Google and the transformer architecture and why you know it&rsquo;s been a very interesting thing</p>
<p>that the sort of neural nets were this small field very fragmented for many many years and</p>
<p>then suddenly things started to work in 2012 and a lot of what worked and what was really worked on</p>
<p>was done in a small number of large tech companies and some not so large tech companies</p>
<p>and uh it&rsquo;s sort of a different picture of where innovation is happening than has existed in other</p>
<p>fields and it&rsquo;s it&rsquo;s kind of interesting it&rsquo;s kind of potentially a model for what will happen in</p>
<p>other places but but you know it&rsquo;s always complicated what um what causes one group to do</p>
<p>this another group to do that and there are the entrepreneurial folk who are smaller and more</p>
<p>who are smaller and more agile and and they&rsquo;re the folks who have more the more resources and</p>
<p>so on it&rsquo;s always complicated um okay Nicola is asking do you think the pre-training a large</p>
<p>biologically inspired language model might be feasible in the future I don&rsquo;t know um I think</p>
<p>that the the figuring out how to train something that is you know we don&rsquo;t know what parts of the</p>
<p>biology are important one of the one incredibly important things we just learned is that probably</p>
<p>there&rsquo;s not much more to brains that really matters for their information processing</p>
<p>than the the neurons and their connections and so on it could have been the case that every molecule</p>
<p>has you know some quantum process that&rsquo;s going on and that&rsquo;s where thinking really happens but</p>
<p>it doesn&rsquo;t seem to be the case because this this pinnacle of kind of our sort of thinking powers</p>
<p>of being able to write long essays and so on it&rsquo;s it seems like that can be done with just a bunch</p>
<p>of neurons with weights now which other parts of biology are important like uh uh you know uh</p>
<p>actually Terry Sinofsky just wrote this paper talking about how there are more backwards going</p>
<p>uh uh neural connections in brains than forwards going ones so in that sense it looks like maybe</p>
<p>maybe we missed the point with these feed forward networks that that&rsquo;s something like chat gpt</p>
<p>basically is and that the feedback is is uh you know is really important but we don&rsquo;t yet we</p>
<p>haven&rsquo;t yet really got the right idealized model of that I do think that the uh you know the sort</p>
<p>of the the what&rsquo;s the next McCulloch-Pitts type thing what&rsquo;s the next sort of simple meta model</p>
<p>of of this is important I also think that there&rsquo;s probably a bunch of essential mathematical</p>
<p>structure to learn about general mathematical structure to learn neural nets you know I was</p>
<p>interested in neural nets back around 1980 and I kind of was trying to simplify simplify simplify</p>
<p>models of things and neural nets I went I went past them because they weren&rsquo;t simple enough for</p>
<p>me they had all these different weights and all these different network architectures and so on</p>
<p>and I ended up studying cellular automata um and and generalizations of that where where you know</p>
<p>you have something where everything is much simpler there are no real numbers there are no</p>
<p>arbitrary connections there are no this that and the other things</p>
<p>but what what matters and what doesn&rsquo;t um we just don&rsquo;t know that yet</p>
<p>uh Paul is asking what about a five senses multimodal model to actually ground the system</p>
<p>in the real world with real human-like experience I think that will be important and that will no</p>
<p>doubt happen and you know you&rsquo;ll be more human-like look this chat gpt is pretty human-like when it</p>
<p>comes to text because by golly it just read a large fraction of the text that we humans at</p>
<p>least publicly wrote um and but it didn&rsquo;t know it hasn&rsquo;t had the experience of walking upstairs and</p>
<p>doing you know doing this or that thing and so it&rsquo;s not going to be very human-like when it</p>
<p>comes to those sorts of things if it has those experiences then then I think we get to um uh</p>
<p>you know then that that will be interesting um okay someone&rsquo;s commenting on the fact that I</p>
<p>should do the same kind of description for image generation uh generative AI for images um the uh</p>
<p>the thing that I like to think about there is I think that&rsquo;s that&rsquo;s one of our first moments</p>
<p>of communication with an alien intelligence in other words we in some sense we&rsquo;re talking</p>
<p>to the generative AI in English words or whatever and it&rsquo;s going into its alien mind so to speak</p>
<p>and plucking out the stuff that is these images and so on it&rsquo;s it&rsquo;s less so you know with chat</p>
<p>GPT what the output is something that is already intended to be very human it&rsquo;s human language</p>
<p>with with um uh an image generation system it&rsquo;s more uh uh it&rsquo;s really it&rsquo;s producing something</p>
<p>which has to be somewhat recognizable to us it&rsquo;s not a random bunch of pixels it&rsquo;s something that</p>
<p>resonates with things we know but in a sense it can be it can be more completely creative</p>
<p>in what it&rsquo;s showing us and in a sense as one tries to sort of uh you know navigate around its</p>
<p>space of what it&rsquo;s going to show us it feels a lot like kind of you&rsquo;re communicating with an alien</p>
<p>intelligence and it&rsquo;s kind of uh it&rsquo;s kind of showing you things about how it thinks about</p>
<p>things by saying oh you said those words I&rsquo;m going to do this and so on I mean I have to say that</p>
<p>I&rsquo;m I&rsquo;m if if we can&rsquo;t you know the other examples of alien intelligences that we have all around</p>
<p>the planet are lots of lots of critters from the cetaceans on so to speak um that uh and I have to</p>
<p>believe that if we could correlate kind of the experiences of those critters cats dogs you know</p>
<p>cockatoos whatever else um and the vocalizations that they have and so on and we could you know</p>
<p>that that it&rsquo;s it&rsquo;s talk to the animals time so to speak I mean I think that&rsquo;s a that feels like</p>
<p>that that&rsquo;s you know the kinds of things we&rsquo;ve learned from chat gpt about the structure of human</p>
<p>language I am quite certain that if there&rsquo;s any linguistic structure for other through other</p>
<p>animals it&rsquo;ll be similar because it&rsquo;s one of the lessons of biology is you know there are fewer</p>
<p>ideas than you think the you know these things that we have have precursors in biology long long</p>
<p>ago we may have made innovations in language it&rsquo;s kind of the key innovation of our species</p>
<p>but whatever is there had precursors in in other organisms and and that&rsquo;s what um and and the fact</p>
<p>that we now have this much better way of kind of teasing out a model for for language in humans</p>
<p>means we should be able to do that elsewhere as well uh okay David is saying chat gpt&rsquo;s developers</p>
<p>seem committed to injecting uh sort of political curtailments in the code um because uh to avoid</p>
<p>it talking about controversial topics how is that done it&rsquo;s done through this reinforcement learning</p>
<p>stage I think maybe there&rsquo;s also some actual you know if it&rsquo;s starting to use these words just</p>
<p>just stop it type things I think maybe that&rsquo;s being done a little bit more with maybe with</p>
<p>being than it is with with chat gpt at this point um I think that the um uh I have to say the one</p>
<p>thing that I consider a you know so far as I know chat gpt is a g-rated you know thing and that&rsquo;s</p>
<p>an achievement in its own right that it doesn&rsquo;t um maybe I shouldn&rsquo;t say that because probably</p>
<p>maybe they&rsquo;re a horrible counterexamples to that but I think that was a um you know in terms of</p>
<p>one of the things that happens is well you have a bunch of humans and they are giving it this</p>
<p>training and those humans have opinions and they will have you know there&rsquo;ll be this kind of</p>
<p>politics or that kind of politics or they&rsquo;ll believe in this or that or the other and they are</p>
<p>uh you know whether purposefully or not they&rsquo;re you know they&rsquo;re going to impose those opinions</p>
<p>because there is no you know the opinion is what you&rsquo;re doing when you tell chat gpt that essay is</p>
<p>good that essay isn&rsquo;t good you know at some level that&rsquo;s an opinion now that opinion may or may not</p>
<p>be colored into something that is uh about you know politics or something like that but it&rsquo;s it&rsquo;s</p>
<p>sort of inevitable that you have that I mean I have to say you know something I&rsquo;ve thought about</p>
<p>a little bit in connection with with general sort of uh ai injection into sort of the things we see</p>
<p>in the world like social media content and so on I tend to think that the right way to solve this</p>
<p>is to say okay let&rsquo;s have multiple you know chatbots or whatever and they are in effect trained</p>
<p>with different criteria by different groups under different banners so to speak and you know you get</p>
<p>to pick the banner of chatbots that you want to be using and then then you&rsquo;re happy because you&rsquo;re</p>
<p>not seeing things that horrify you and and so on and and you can discuss you know whether you want</p>
<p>to pick the chatbot that that accepts the most diverse views or whether you want to you know</p>
<p>that that&rsquo;s a that&rsquo;s that sort of throws one back into um into kind of standard issues of political</p>
<p>philosophy and things like this I mean I think the thing to realize is that there is a there&rsquo;s</p>
<p>sort of an ethics you know one wants to put ethics somehow into what&rsquo;s going on but when one says</p>
<p>let&rsquo;s have the ai&rsquo;s you know do the ethics it&rsquo;s like that&rsquo;s hopeless ethics is a there is no sort</p>
<p>of mathematically definable perfect ethics ethics is a the way humans want things to be and then you</p>
<p>have to choose you know well is it the average ethics is it the you know the ethics which makes</p>
<p>only five percent of the people unhappy is it this that and the other these are old questions of</p>
<p>political philosophy that don&rsquo;t really have so far as we know good answers and but once thrown into</p>
<p>those questions there&rsquo;s no you know oh we&rsquo;ll get a machine to do it and it&rsquo;ll be perfect it won&rsquo;t</p>
<p>happen because these are questions that that aren&rsquo;t solvable for a machine because they&rsquo;re</p>
<p>questions that in a sense come right from us these are I mean the thing to realize about chat gpt in</p>
<p>general chat gpt is a mirror on us it&rsquo;s taken what we wrote on the web so to speak in a in in aggregate</p>
<p>and it&rsquo;s reflecting that back to us so insofar as it does goofy things and says goofy things</p>
<p>you know some that&rsquo;s really on us I mean that&rsquo;s you know it&rsquo;s our sort of it&rsquo;s it&rsquo;s the average</p>
<p>um kind of uh the the the sort of the average web that we&rsquo;re seeing here um</p>
<p>tenacious is asking about a particular paper which I sounds interesting but I don&rsquo;t know about it</p>
<p>uh let&rsquo;s see up up soon here um</p>
<p>okay tragath is wondering how neural net ai compares to other living multicellular intelligence</p>
<p>uh plant roots</p>
<p>um nerve nets and things like jellyfish and so on biofilms yeah well okay so</p>
<p>one of the big things that&rsquo;s come out of a bunch of science that I&rsquo;ve done is this thing</p>
<p>I call the principle of computational equivalence which essentially says that as soon as you have</p>
<p>a system that is not computationally trivial it will ultimately be equivalent in its computational</p>
<p>capabilities and that&rsquo;s an important thing when you talk about computational irreducibility</p>
<p>because computational irreducibility arises because that you&rsquo;ve got a system doing its computation</p>
<p>there&rsquo;s no system you can&rsquo;t expect there to be all other systems will just be equivalent</p>
<p>in their computational sophistication you can&rsquo;t expect a super system that&rsquo;s going to jump ahead</p>
<p>and just say oh you went through all these computational steps but I can jump ahead</p>
<p>and just get to the answer now a question that is a really good question is when we look at okay</p>
<p>one of the things that is characteristic of our consciousness for example relative to all</p>
<p>the computational irreducibility in the universe is the fact that we have coherent consciousness</p>
<p>is a consequence of the fact that we are two things it seems to me we are computationally</p>
<p>bounded we&rsquo;re not capable of looking at all those molecules bouncing around we only see</p>
<p>various aggregate effects point one and point two that we are we believe that we are persistent in</p>
<p>time we believe we have a persistent thread of of existence through time turns out big fact of our</p>
<p>last few years for me is that the big facts of physics general relativity theory of gravity</p>
<p>quantum mechanics and statistical mechanics the second law of thermodynamics law of entropy</p>
<p>increase all three of those big theories of physics that arose in the 20th century all three</p>
<p>of those can be derived from knowing that we human observers are noticing those laws and we human</p>
<p>observers have those two characteristics i just mentioned i consider this a a very important</p>
<p>beautiful sort of profound result about kind of the fact that we observe the physics we observe</p>
<p>because we are observers of the kind that we are now interesting question i suppose is when we</p>
<p>so we are limited we are computationally limited things and the very fact that we observe physics</p>
<p>the way we observe physics is a consequence of those computational limitations so a question is</p>
<p>how similar are the computational limitations in these other kinds of systems in a sense</p>
<p>the fungus as observer so to speak how similar is that kind of observer to a human observer</p>
<p>and in terms of sort of what computational capabilities it has and so on my guess is it&rsquo;s</p>
<p>pretty similar and in fact one of my next projects is a thing i&rsquo;m calling observer theory which is</p>
<p>kind of a general theory of uh of kinds of observers that you can have of things and so</p>
<p>maybe we&rsquo;ll learn something from that but it&rsquo;s a it&rsquo;s a very interesting question uh dugan is</p>
<p>commenting um uh chat gpt can be improved using an automated fact checking system like an adversarial</p>
<p>network for instance um could one basically could one train chat gpt with wolfram alpha</p>
<p>and have it get better the answer is surely up to a point but then it will it will lose it just</p>
<p>like it does with parentheses i mean there&rsquo;s a certain with a network of that architecture</p>
<p>there&rsquo;s a certain set of things one can learn but one cannot learn what is computationally</p>
<p>irreducible i mean it&rsquo;s in other words you can learn the common cases but there&rsquo;ll always be</p>
<p>surprises there&rsquo;ll always be unexpected things that you can only get to by just explicitly doing</p>
<p>those computations bob is asking can chat gpt play a text-based adventure game i bet it can</p>
<p>i don&rsquo;t know i haven&rsquo;t seen anybody try that but i bet it can um</p>
<p>okay there&rsquo;s a question here from software uh aside from being trained on a huge corpus what</p>
<p>is it about gpt3 that makes it so good at language i think i tried to talk about that a bit</p>
<p>about the fact that we it&rsquo;s it&rsquo;s um uh that there&rsquo;s you know there&rsquo;s regularity in language i</p>
<p>think the the particulars of the transformer architecture of this kind of looking back on</p>
<p>looking back on sequences and things that&rsquo;s been helpful in refining the way that you can train it</p>
<p>and that that seems to be important uh let&rsquo;s see um</p>
<p>atoria is asking could feature impact scores help us understand gpt better</p>
<p>uh well so what that what that&rsquo;s about is when you run a neural net you can kind of uh you can say</p>
<p>uh sort of how much what was the how much did some particular feature affect the output that</p>
<p>the neural net gave chapter gpt is just a really pretty complicated thing i mean i started digging</p>
<p>around trying to understand sort of what as a natural scientist you know i&rsquo;m like i couldn&rsquo;t do</p>
<p>sort of neuroscience with actual brains because i&rsquo;m a hundred times thousand times too squeamish</p>
<p>for that but you know i can dig around inside an artificial brain and i started trying to do that</p>
<p>and it&rsquo;s it&rsquo;s it&rsquo;s difficult i mean i i i didn&rsquo;t look at feature impact scores i i think one could</p>
<p>um the uh okay so um but by the way i mean i&rsquo;m i&rsquo;m amused by these questions because</p>
<p>because i i can kind of you know i can still tell you guys are not bots i think and um uh</p>
<p>uh let&rsquo;s see</p>
<p>um ron is asking about implications like i have to work late tonight what does that mean um yeah</p>
<p>absolutely chat gpt is learning stuff like that because because it&rsquo;s seen you know a bunch of</p>
<p>texts that says i have to work work late tonight um so i can&rsquo;t do this it&rsquo;s seen examples of that</p>
<p>it&rsquo;s kind of doing the aristotle again it&rsquo;s just seeing this uh you know these patterns of language</p>
<p>and that&rsquo;s what it&rsquo;s learning from so to speak um so yes these things we might say how do we think</p>
<p>about that formally oh it seems kind of complicated to us but that pattern of</p>
<p>language has has occurred before all right last last thing perhaps um</p>
<p>uh okay albert is asking do you think humans learn efficiently because they&rsquo;re born with</p>
<p>the right networks to learn language more easily or is there some difference i think</p>
<p>it is important the architecture of the brain undoubtedly is important i mean uh you know</p>
<p>my impression is that there are uh you know it&rsquo;s it&rsquo;s a matter for the neuroscientists to go and</p>
<p>find out now that we know that certain things can be made to work with artificial neural nets</p>
<p>did the actual brain discover those things too and the answer will be often yes i mean just like</p>
<p>there are things we probably have learned from you know the flight of drones or the flight of planes</p>
<p>that we can go back and say oh did we did biology actually already have that idea um i think that</p>
<p>the um uh there are undoubtedly features of human language which depend on aspects of the brain i</p>
<p>mean like for example one you know talking to terry it&rsquo;s an oscar you know we&rsquo;re talking about</p>
<p>the um the loop between the basal ganglia and the cortex and the possibility that you know the outer</p>
<p>loop of chat gpt is a little bit like that loop and it&rsquo;s kind of like i&rsquo;m turning things</p>
<p>over in my mind one might say maybe that&rsquo;s actually a loop of data going around this</p>
<p>literal loop from one part of the brain to another maybe maybe not but sometimes those</p>
<p>those sayings have a habit of of being more true than you think and maybe the reason that when we</p>
<p>think about things we have these certain time frames when you think about things there&rsquo;s certain</p>
<p>times between when words come out and so on maybe those times are literally associated with the</p>
<p>amount of time it takes for signals to propagate through some number of layers in our in our uh</p>
<p>in in our brains and i think that in that sense if that&rsquo;s the case there will be features of</p>
<p>language which are yes we&rsquo;ve got this brain architecture we&rsquo;re going to have this these</p>
<p>features of language and insofar as language evolves as so far as it&rsquo;s it&rsquo;s adaptively</p>
<p>worthwhile to have a different form of language that is optimized by having some different form</p>
<p>of brain structure that&rsquo;s what will have been driven by by natural selection and so on i mean</p>
<p>i think you know there are aspects of language like we know if you you know we tend to remember</p>
<p>five chunks you know chunks of five so to speak things at a time and we know that if we try and</p>
<p>give a sentence which has more and more and more deeper deeper deeper sub clauses we lose it off</p>
<p>to some point and that&rsquo;s presumably a hardware limitation of our brains uh okay dave is asking</p>
<p>this is a good last question how difficult will it be for individuals to train something like a</p>
<p>personal chat gpt that learns to behave more and more like a clone of the user i think it i don&rsquo;t</p>
<p>know um i&rsquo;m going to try it i have a lot of training data as i mentioned you know 50 million</p>
<p>typed words type yeah typed words for example uh for me um and uh um my guess is i mean i know</p>
<p>somebody tried to train a um an earlier gpt3 on on stuff of mine wasn&rsquo;t i didn&rsquo;t think it</p>
<p>was terribly good when i read ones trained for other people i thought they were pretty decent</p>
<p>when i when i looked at one trained for myself because i kind of know myself better than i know</p>
<p>anybody else i think um the uh uh you know it didn&rsquo;t ring true so to speak um and uh but i i</p>
<p>do think that that will be a uh you know being able to write emails like i write emails it&rsquo;ll</p>
<p>do a decent job of that i suspect uh you know i would like to believe that uh you know one still</p>
<p>as an as a human one still has an edge because in a sense one knows what the goals are the the</p>
<p>you know know this system its goal is to complete english text and you know the bigger picture of</p>
<p>what&rsquo;s going on is not going to be part of what it has except insofar as it learns the aggregate</p>
<p>bigger picture from just reading lots of text so uh you know but but i i do think it&rsquo;ll be an</p>
<p>interesting i i i expect that you know i as a person who gets a lot of email some of which is</p>
<p>fairly easy to answer in principle that you know maybe my bot will be able to answer the easiest</p>
<p>stuff for me all right that&rsquo;s probably a good place to to wrap this up um thanks for joining me</p>
<p>and uh uh i would like to say that for those interested in more technical details uh some</p>
<p>of the folks in our machine learning group are going to be doing some more detailed technical</p>
<p>webinars about uh about this material and uh really going into how you would um you know how</p>
<p>you build these things from scratch and so on um and uh what some of the more more more detail about</p>
<p>what&rsquo;s happening uh actually is but i should wrap up here for now and um thanks for joining me</p>
<p>and uh bye for now</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/english/">English</a>
        
            <a href="/tags/video-transcripts/">Video Transcripts</a>
        
            <a href="/tags/wolfram/">Wolfram</a>
        
    </section>


    </footer>


    
</article>

    

    

<div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9206135835124064"
         crossorigin="anonymous"></script>
    <ins class="adsbygoogle"
         style="display:block; text-align:center;"
         data-ad-layout="in-article"
         data-ad-format="fluid"
         data-ad-client="ca-pub-9206135835124064"
         data-ad-slot="1055602464"></ins>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
</div>
<aside class="related-content--wrapper">
    <h2 class="section-title">Related content</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/en/q4df3j4sace/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/q4DF3j4saCE" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">The Most Extreme Explosion in the Universe ÔΩú Kurzgesagt</h2>
            
            <footer class="article-time">
                <time datetime=''>Oct 22, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/cfslusyfzpc/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/cFslUSyfZPc" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">What Happens If You Destroy A Black HoleÔºü ÔΩú Kurzgesagt</h2>
            
            <footer class="article-time">
                <time datetime=''>Oct 22, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/kl39khs07xc/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/kl39KHS07Xc" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">Universal Basic Income Explained ‚Äì Free Money for EverybodyÔºü UBI ÔΩú Kurzgesagt</h2>
            
            <footer class="article-time">
                <time datetime=''>Oct 22, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/wski8hfcxek/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/WSKi8HfcxEk" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">The Rise of the Machines ‚Äì Why Automation is Different this Time ÔΩú Kurzgesagt</h2>
            
            <footer class="article-time">
                <time datetime=''>Oct 22, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/en/b3qtaghlweg/">
        
	
        
        <div class="article-image">
            
            <img src="/img/related-content.png" loading="lazy" 
            data-key="en/B3QTAgHlwEg" data-hash="" 
            style="opacity: 0.3;"/>
        </div>
        
        <div class="article-details">
            <h2 class="article-title">The Warrior Kingdoms of the Weaver Ant ÔΩú Kurzgesagt</h2>
            
            <footer class="article-time">
                <time datetime=''>Oct 22, 2023</time>
            </footer>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2021 - 
        
        2023 SWIEST - Transcripts ¬∑ Screenplays ¬∑ Lyrics
    </section>
    
    <section class="powerby">
        

        As an Amazon Associate I earn from qualifying purchases üõí<br/>

        Built with <a href="https://swiest.com/" target="_blank" rel="noopener">(Ôæâ‚óï„ÉÆ‚óï)Ôæâü™Ñüíûüíñü•∞ across the glüåçüåèüåébe</a> <br />
        
        
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>


<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Serif&family=Noto+Serif+Armenian&family=Noto+Serif+Bengali&family=Noto+Serif+Devanagari&family=Noto+Serif+Georgian&family=Noto+Serif+Gujarati&family=Noto+Serif+HK&family=Noto+Serif+Hebrew&family=Noto+Serif+JP&family=Noto+Serif+KR&family=Noto+Serif+Kannada&family=Noto+Serif+Khmer&family=Noto+Serif+Lao&family=Noto+Serif+Makasar&family=Noto+Serif+Malayalam&family=Noto+Serif+Myanmar&family=Noto+Serif+Oriya&family=Noto+Serif+SC&family=Noto+Serif+Sinhala&family=Noto+Serif+TC&family=Noto+Serif+Tamil&family=Noto+Serif+Telugu&family=Noto+Serif+Thai&family=Noto+Serif+Tibetan&display=swap" rel="stylesheet">

    </body>
</html>
